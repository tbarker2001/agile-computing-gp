__label__performance __label__optimization __label__java __label__branch-prediction __label__c++ Here is a piece of C++ code that shows some very peculiar behavior. For some strange reason, sorting the data miraculously makes the code almost six times faster: Initially, I thought this might be just a language or compiler anomaly, so I tried Java: With a similar but less extreme result. My first thought was that sorting brings the data into the cache, but then I thought how silly that was because the array was just generated. The code is summing up some independent terms, so the order should not matter. You are a victim of branch prediction fail. Consider a railroad junction:  Image by Mecanismo, via Wikimedia Commons. Used under the CC-By-SA 3.0 license. Now for the sake of argument, suppose this is back in the 1800s - before long distance or radio communication. You are the operator of a junction and you hear a train coming. You have no idea which way it is supposed to go. You stop the train to ask the driver which direction they want. And then you set the switch appropriately. Trains are heavy and have a lot of inertia. So they take forever to start up and slow down. Is there a better way? You guess which direction the train will go! If you guess right every time, the train will never have to stop. If you guess wrong too often, the train will spend a lot of time stopping, backing up, and restarting. Consider an if-statement: At the processor level, it is a branch instruction:  You are a processor and you see a branch. You have no idea which way it will go. What do you do? You halt execution and wait until the previous instructions are complete. Then you continue down the correct path. Modern processors are complicated and have long pipelines. So they take forever to "warm up" and "slow down". Is there a better way? You guess which direction the branch will go! If you guess right every time, the execution will never have to stop. If you guess wrong too often, you spend a lot of time stalling, rolling back, and restarting. This is branch prediction. I admit it's not the best analogy since the train could just signal the direction with a flag. But in computers, the processor doesn't know which direction a branch will go until the last moment. So how would you strategically guess to minimize the number of times that the train must back up and go down the other path? You look at the past history! If the train goes left 99% of the time, then you guess left. If it alternates, then you alternate your guesses. If it goes one way every three times, you guess the same... In other words, you try to identify a pattern and follow it. This is more or less how branch predictors work. Most applications have well-behaved branches. So modern branch predictors will typically achieve >90% hit rates. But when faced with unpredictable branches with no recognizable patterns, branch predictors are virtually useless. Further reading: "Branch predictor" article on Wikipedia. Notice that the data is evenly distributed between 0 and 255. When the data is sorted, roughly the first half of the iterations will not enter the if-statement. After that, they will all enter the if-statement. This is very friendly to the branch predictor since the branch consecutively goes the same direction many times. Even a simple saturating counter will correctly predict the branch except for the few iterations after it switches direction. Quick visualization: However, when the data is completely random, the branch predictor is rendered useless, because it can't predict random data. Thus there will probably be around 50% misprediction (no better than random guessing). So what can be done? If the compiler isn't able to optimize the branch into a conditional move, you can try some hacks if you are willing to sacrifice readability for performance. Replace: with: This eliminates the branch and replaces it with some bitwise operations. (Note that this hack is not strictly equivalent to the original if-statement. But in this case, it's valid for all the input values of data[].) Benchmarks: Core i7 920 @ 3.5 GHz C++ - Visual Studio 2010 - x64 Release Java - NetBeans 7.1.1 JDK 7 - x64 Observations: A general rule of thumb is to avoid data-dependent branching in critical loops (such as in this example). Update: GCC 4.6.1 with -O3 or -ftree-vectorize on x64 is able to generate a conditional move. So there is no difference between the sorted and unsorted data - both are fast. (Or somewhat fast: for the already-sorted case, cmov can be slower especially if GCC puts it on the critical path instead of just add, especially on Intel before Broadwell where cmov has 2 cycle latency: gcc optimization flag -O3 makes code slower than -O2) VC++ 2010 is unable to generate conditional moves for this branch even under /Ox. Intel C++ Compiler (ICC) 11 does something miraculous. It interchanges the two loops, thereby hoisting the unpredictable branch to the outer loop. So not only is it immune to the mispredictions, it is also twice as fast as whatever VC++ and GCC can generate! In other words, ICC took advantage of the test-loop to defeat the benchmark... If you give the Intel compiler the branchless code, it just out-right vectorizes it... and is just as fast as with the branch (with the loop interchange). This goes to show that even mature modern compilers can vary wildly in their ability to optimize code... Branch prediction. With a sorted array, the condition data[c] >= 128 is first false for a streak of values, then becomes true for all later values. That's easy to predict. With an unsorted array, you pay for the branching cost. The reason why performance improves drastically when the data is sorted is that the branch prediction penalty is removed, as explained beautifully in Mysticial's answer. Now, if we look at the code we can find that the meaning of this particular if... else... branch is to add something when a condition is satisfied. This type of branch can be easily transformed into a conditional move statement, which would be compiled into a conditional move instruction: cmovl, in an x86 system. The branch and thus the potential branch prediction penalty is removed. In C, thus C++, the statement, which would compile directly (without any optimization) into the conditional move instruction in x86, is the ternary operator ... ? ... : .... So we rewrite the above statement into an equivalent one: While maintaining readability, we can check the speedup factor. On an Intel Core i7-2600K @ 3.4 GHz and Visual Studio 2010 Release Mode, the benchmark is: x86 x64 The result is robust in multiple tests. We get a great speedup when the branch result is unpredictable, but we suffer a little bit when it is predictable. In fact, when using a conditional move, the performance is the same regardless of the data pattern. Now let's look more closely by investigating the x86 assembly they generate. For simplicity, we use two functions max1 and max2. max1 uses the conditional branch if... else ...: max2 uses the ternary operator ... ? ... : ...: On a x86-64 machine, GCC -S generates the assembly below. max2 uses much less code due to the usage of instruction cmovge. But the real gain is that max2 does not involve branch jumps, jmp, which would have a significant performance penalty if the predicted result is not right. So why does a conditional move perform better? In a typical x86 processor, the execution of an instruction is divided into several stages. Roughly, we have different hardware to deal with different stages. So we do not have to wait for one instruction to finish to start a new one. This is called pipelining. In a branch case, the following instruction is determined by the preceding one, so we cannot do pipelining. We have to either wait or predict. In a conditional move case, the execution conditional move instruction is divided into several stages, but the earlier stages like Fetch and Decode do not depend on the result of the previous instruction; only latter stages need the result. Thus, we wait a fraction of one instruction's execution time. This is why the conditional move version is slower than the branch when the prediction is easy. The book Computer Systems: A Programmer's Perspective, second edition explains this in detail. You can check Section 3.6.6 for Conditional Move Instructions, entire Chapter 4 for Processor Architecture, and Section 5.11.2 for special treatment for Branch Prediction and Misprediction Penalties. Sometimes, some modern compilers can optimize our code to assembly with better performance, sometimes some compilers can't (the code in question is using Visual Studio's native compiler). Knowing the performance difference between a branch and a conditional move when unpredictable can help us write code with better performance when the scenario gets so complex that the compiler can not optimize them automatically. If you are curious about even more optimizations that can be done to this code, consider this: Starting with the original loop: With loop interchange, we can safely change this loop to: Then, you can see that the if conditional is constant throughout the execution of the i loop, so you can hoist the if out: Then, you see that the inner loop can be collapsed into one single expression, assuming the floating point model allows it (/fp:fast is thrown, for example) That one is 100,000 times faster than before. No doubt some of us would be interested in ways of identifying code that is problematic for the CPU's branch-predictor. The Valgrind tool cachegrind has a branch-predictor simulator, enabled by using the --branch-sim=yes flag. Running it over the examples in this question, with the number of outer loops reduced to 10000 and compiled with g++, gives these results: Sorted: Unsorted: Drilling down into the line-by-line output produced by cg_annotate we see for the loop in question: Sorted: Unsorted: This lets you easily identify the problematic line - in the unsorted version the if (data[c] >= 128) line is causing 164,050,007 mispredicted conditional branches (Bcm) under cachegrind's branch-predictor model, whereas it's only causing 10,006 in the sorted version. Alternatively, on Linux you can use the performance counters subsystem to accomplish the same task, but with native performance using CPU counters. Sorted: Unsorted: It can also do source code annotation with dissassembly. See the performance tutorial for more details. I just read up on this question and its answers, and I feel an answer is missing. A common way to eliminate branch prediction that I've found to work particularly good in managed languages is a table lookup instead of using a branch (although I haven't tested it in this case). This approach works in general if: Background and why From a processor perspective, your memory is slow. To compensate for the difference in speed, a couple of caches are built into your processor (L1/L2 cache). So imagine that you're doing your nice calculations and figure out that you need a piece of memory. The processor will get its 'load' operation and loads the piece of memory into cache -- and then uses the cache to do the rest of the calculations. Because memory is relatively slow, this 'load' will slow down your program. Like branch prediction, this was optimized in the Pentium processors: the processor predicts that it needs to load a piece of data and attempts to load that into the cache before the operation actually hits the cache. As we've already seen, branch prediction sometimes goes horribly wrong -- in the worst case scenario you need to go back and actually wait for a memory load, which will take forever (in other words: failing branch prediction is bad, a memory load after a branch prediction fail is just horrible!). Fortunately for us, if the memory access pattern is predictable, the processor will load it in its fast cache and all is well. The first thing we need to know is what is small? While smaller is generally better, a rule of thumb is to stick to lookup tables that are <= 4096 bytes in size. As an upper limit: if your lookup table is larger than 64K it's probably worth reconsidering. Constructing a table So we've figured out that we can create a small table. Next thing to do is get a lookup function in place. Lookup functions are usually small functions that use a couple of basic integer operations (and, or, xor, shift, add, remove and perhaps multiply). You want to have your input translated by the lookup function to some kind of 'unique key' in your table, which then simply gives you the answer of all the work you wanted it to do. In this case: >= 128 means we can keep the value, < 128 means we get rid of it. The easiest way to do that is by using an 'AND': if we keep it, we AND it with 7FFFFFFF; if we want to get rid of it, we AND it with 0. Notice also that 128 is a power of 2 -- so we can go ahead and make a table of 32768/128 integers and fill it with one zero and a lot of 7FFFFFFFF's. Managed languages You might wonder why this works well in managed languages. After all, managed languages check the boundaries of the arrays with a branch to ensure you don't mess up... Well, not exactly... :-) There has been quite some work on eliminating this branch for managed languages. For example: In this case, it's obvious to the compiler that the boundary condition will never be hit. At least the Microsoft JIT compiler (but I expect Java does similar things) will notice this and remove the check altogether. WOW, that means no branch. Similarly, it will deal with other obvious cases. If you run into trouble with lookups in managed languages -- the key is to add a & 0x[something]FFF to your lookup function to make the boundary check predictable -- and watch it going faster. The result of this case As data is distributed between 0 and 255 when the array is sorted, around the first half of the iterations will not enter the if-statement (the if statement is shared below). The question is: What makes the above statement not execute in certain cases as in case of sorted data? Here comes the "branch predictor". A branch predictor is a digital circuit that tries to guess which way a branch (e.g. an if-then-else structure) will go before this is known for sure. The purpose of the branch predictor is to improve the flow in the instruction pipeline. Branch predictors play a critical role in achieving high effective performance! Let's do some bench marking to understand it better The performance of an if-statement depends on whether its condition has a predictable pattern. If the condition is always true or always false, the branch prediction logic in the processor will pick up the pattern. On the other hand, if the pattern is unpredictable, the if-statement will be much more expensive. Let’s measure the performance of this loop with different conditions: Here are the timings of the loop with different true-false patterns: A “bad” true-false pattern can make an if-statement up to six times slower than a “good” pattern! Of course, which pattern is good and which is bad depends on the exact instructions generated by the compiler and on the specific processor. So there is no doubt about the impact of branch prediction on performance! One way to avoid branch prediction errors is to build a lookup table, and index it using the data.  Stefan de Bruijn discussed that in his answer. But in this case, we know values are in the range [0, 255] and we only care about values >= 128.  That means we can easily extract a single bit that will tell us whether we want a value or not: by shifting the data to the right 7 bits, we are left with a 0 bit or a 1 bit, and we only want to add the value when we have a 1 bit.  Let's call this bit the "decision bit". By using the 0/1 value of the decision bit as an index into an array, we can make code that will be equally fast whether the data is sorted or not sorted.  Our code will always add a value, but when the decision bit is 0, we will add the value somewhere we don't care about.  Here's the code: This code wastes half of the adds but never has a branch prediction failure.  It's tremendously faster on random data than the version with an actual if statement. But in my testing, an explicit lookup table was slightly faster than this, probably because indexing into a lookup table was slightly faster than bit shifting.  This shows how my code sets up and uses the lookup table (unimaginatively called lut for "LookUp Table" in the code).  Here's the C++ code: In this case, the lookup table was only 256 bytes, so it fits nicely in a cache and all was fast.  This technique wouldn't work well if the data was 24-bit values and we only wanted half of them... the lookup table would be far too big to be practical.  On the other hand, we can combine the two techniques shown above: first shift the bits over, then index a lookup table.  For a 24-bit value that we only want the top half value, we could potentially shift the data right by 12 bits, and be left with a 12-bit value for a table index.  A 12-bit table index implies a table of 4096 values, which might be practical. The technique of indexing into an array, instead of using an if statement, can be used for deciding which pointer to use.  I saw a library that implemented binary trees, and instead of having two named pointers (pLeft and pRight or whatever) had a length-2 array of pointers and used the "decision bit" technique to decide which one to follow.  For example, instead of: this library would do something like: Here's a link to this code: Red Black Trees, Eternally Confuzzled In the sorted case, you can do better than relying on successful branch prediction or any branchless comparison trick: completely remove the branch. Indeed, the array is partitioned in a contiguous zone with data < 128 and another with data >= 128. So you should find the partition point with a dichotomic search (using Lg(arraySize) = 15 comparisons), then do a straight accumulation from that point. Something like (unchecked) or, slightly more obfuscated A yet faster approach, that gives an approximate solution for both sorted or unsorted is: sum= 3137536; (assuming a truly uniform distribution, 16384 samples with expected value 191.5) :-) The above behavior is happening because of Branch prediction. To understand branch prediction one must first understand Instruction Pipeline: Any instruction is broken into a sequence of steps so that different steps can be executed concurrently in parallel. This technique is known as instruction pipeline and this is used to increase throughput in modern processors. To understand this better please see this example on Wikipedia. Generally, modern processors have quite long pipelines, but for ease let's consider these 4 steps only. 4-stage pipeline in general for 2 instructions.  Moving back to the above question let's consider the following instructions: Without branch prediction, the following would occur: To execute instruction B or instruction C the processor will have to wait till the instruction A doesn't reach till EX stage in the pipeline, as the decision to go to instruction B or instruction C depends on the result of instruction A. So the pipeline will look like this. when if condition returns true:  When if condition returns false:  As a result of waiting for the result of instruction A, the total CPU cycles spent in the above case (without branch prediction; for both true and false) is 7. So what is branch prediction? Branch predictor will try to guess which way a branch (an if-then-else structure) will go before this is known for sure. It will not wait for the instruction A to reach the EX stage of the pipeline, but it will guess the decision and go to that instruction (B or C in case of our example). In case of a correct guess, the pipeline looks something like this:  If it is later detected that the guess was wrong then the partially executed instructions are discarded and the pipeline starts over with the correct branch, incurring a delay.  The time that is wasted in case of a branch misprediction is equal to the number of stages in the pipeline from the fetch stage to the execute stage. Modern microprocessors tend to have quite long pipelines so that the misprediction delay is between 10 and 20 clock cycles. The longer the pipeline the greater the need for a good branch predictor. In the OP's code, the first time when the conditional, the branch predictor does not have any information to base up prediction, so the first time it will randomly choose the next instruction. Later in the for loop, it can base the prediction on the history.  For an array sorted in ascending order, there are three possibilities: Let us assume that the predictor will always assume the true branch on the first run. So in the first case, it will always take the true branch since historically all its predictions are correct. In the 2nd case, initially it will predict wrong, but after a few iterations, it will predict correctly. In the 3rd case, it will initially predict correctly till the elements are less than 128. After which it will fail for some time and the correct itself when it sees branch prediction failure in history.  In all these cases the failure will be too less in number and as a result, only a few times it will need to discard the partially executed instructions and start over with the correct branch, resulting in fewer CPU cycles.  But in case of a random unsorted array, the prediction will need to discard the partially executed instructions and start over with the correct branch most of the time and result in more CPU cycles compared to the sorted array. An official answer would be from You can also see from this lovely diagram why the branch predictor gets confused.  Each element in the original code is a random value so the predictor will change sides as the std::rand() blow. On the other hand, once it's sorted, the predictor will first move into a state of strongly not taken and when the values change to the high value the predictor will in three runs through change all the way from strongly not taken to strongly taken. In the same line (I think this was not highlighted by any answer) it's good to mention that sometimes (specially in software where the performance matters—like in the Linux kernel) you can find some if statements like the following: or similarly: Both likely() and unlikely() are in fact macros that are defined by using something like the GCC's __builtin_expect to help the compiler insert prediction code to favour the condition taking into account the information provided by the user. GCC supports other builtins that could change the behavior of the running program or emit low level instructions like clearing the cache, etc. See this documentation that goes through the available GCC's builtins. Normally this kind of optimizations are mainly found in hard-real time applications or embedded systems where execution time matters and it's critical. For example, if you are checking for some error condition that only happens 1/10000000 times, then why not inform the compiler about this? This way, by default, the branch prediction would assume that the condition is false. Frequently used Boolean operations in C++ produce many branches in the compiled program. If these branches are inside loops and are hard to predict they can slow down execution significantly. Boolean variables are stored as 8-bit integers with the value 0 for false and 1 for true. Boolean variables are overdetermined in the sense that all operators that have Boolean variables as input check if the inputs have any other value than 0 or 1, but operators that have Booleans as output can produce no other value than 0 or 1. This makes operations with Boolean variables as input less efficient than necessary. Consider example: This is typically implemented by the compiler in the following way: This code is far from optimal. The branches may take a long time in case of mispredictions. The Boolean operations can be made much more efficient if it is known with certainty that the operands have no other values than 0 and 1. The reason why the compiler does not make such an assumption is that the variables might have other values if they are uninitialized or come from unknown sources. The above code can be optimized if a and b has been initialized to valid values or if they come from operators that produce Boolean output. The optimized code looks like this: char is used instead of bool in order to make it possible to use the bitwise operators (& and |) instead of the Boolean operators (&& and ||). The bitwise operators are single instructions that take only one clock cycle. The OR operator (|) works even if a and b have other values than 0 or 1. The AND operator (&) and the EXCLUSIVE OR operator (^) may give inconsistent results if the operands have other values than 0 and 1. ~ can not be used for NOT. Instead, you can make a Boolean NOT on a variable which is known to be 0 or 1 by XOR'ing it with 1: can be optimized to: a && b cannot be replaced with a & b if b is an expression that should not be evaluated if a is false ( && will not evaluate b, & will). Likewise, a || b can not be replaced with a | b if b is an expression that should not be evaluated if a is true. Using bitwise operators is more advantageous if the operands are variables than if the operands are comparisons: is optimal in most cases (unless you expect the && expression to generate many branch mispredictions). That's for sure!... Branch prediction makes the logic run slower, because of the switching which happens in your code! It's like you are going a straight street or a street with a lot of turnings, for sure the straight one is going to be done quicker!... If the array is sorted, your condition is false at the first step: data[c] >= 128, then becomes a true value for the whole way to the end of the street. That's how you get to the end of the logic faster. On the other hand, using an unsorted array, you need a lot of turning and processing which make your code run slower for sure... Look at the image I created for you below. Which street is going to be finished faster?  So programmatically, branch prediction causes the process to be slower... Also at the end, it's good to know we have two kinds of branch predictions that each is going to affect your code differently: 1. Static 2. Dynamic  Static branch prediction is used by the microprocessor the first time a conditional branch is encountered, and dynamic branch prediction is used for succeeding executions of the conditional branch code. In order to effectively write your code to take advantage of these rules, when writing if-else or switch statements, check the most common cases first and work progressively down to the least common. Loops do not necessarily require any special ordering of code for static branch prediction, as only the condition of the loop iterator is normally used. This question has already been answered excellently many times over. Still I'd like to draw the group's attention to yet another interesting analysis. Recently this example (modified very slightly) was also used as a way to demonstrate how a piece of code can be profiled within the program itself on Windows. Along the way, the author also shows how to use the results to determine where the code is spending most of its time in both the sorted & unsorted case. Finally the piece also shows how to use a little known feature of the HAL (Hardware Abstraction Layer) to determine just how much branch misprediction is happening in the unsorted case. The link is here: A Demonstration of Self-Profiling As what has already been mentioned by others, what behind the mystery is Branch Predictor.  I'm not trying to add something but explaining the concept in another way.  There is a concise introduction on the wiki which contains text and diagram. I do like the explanation below which uses a diagram to elaborate the Branch Predictor intuitively. In computer architecture, a branch predictor is a   digital circuit that tries to guess which way a branch (e.g. an   if-then-else structure) will go before this is known for sure. The   purpose of the branch predictor is to improve the flow in the   instruction pipeline. Branch predictors play a critical role in   achieving high effective performance in many modern pipelined   microprocessor architectures such as x86. Two-way branching is usually implemented with a conditional jump   instruction. A conditional jump can either be "not taken" and continue   execution with the first branch of code which follows immediately   after the conditional jump, or it can be "taken" and jump to a   different place in program memory where the second branch of code is   stored. It is not known for certain whether a conditional jump will be   taken or not taken until the condition has been calculated and the   conditional jump has passed the execution stage in the instruction   pipeline (see fig. 1).  Based on the described scenario, I have written an animation demo to show how instructions are executed in a pipeline in different situations. Without branch prediction, the processor would have to wait until the   conditional jump instruction has passed the execute stage before the   next instruction can enter the fetch stage in the pipeline. The example contains three instructions and the first one is a conditional jump instruction. The latter two instructions can go into the pipeline until the conditional jump instruction is executed.   It will take 9 clock cycles for 3 instructions to be completed.  It will take 7 clock cycles for 3 instructions to be completed.  It will take 9 clock cycles for 3 instructions to be completed. The time that is wasted in case of a branch misprediction is equal to   the number of stages in the pipeline from the fetch stage to the   execute stage. Modern microprocessors tend to have quite long   pipelines so that the misprediction delay is between 10 and 20 clock   cycles. As a result, making a pipeline longer increases the need for a   more advanced branch predictor. As you can see, it seems we don't have a reason not to use Branch Predictor. It's quite a simple demo that clarifies the very basic part of Branch Predictor. If those gifs are annoying, please feel free to remove them from the answer and visitors can also get the live demo source code from BranchPredictorDemo Branch-prediction gain! It is important to understand that branch misprediction doesn't slow down programs. The cost of a missed prediction is just as if branch prediction didn't exist and you waited for the evaluation of the expression to decide what code to run (further explanation in the next paragraph). Whenever there's an if-else \ switch statement, the expression has to be evaluated to determine which block should be executed. In the assembly code generated by the compiler, conditional branch instructions are inserted. A branch instruction can cause a computer to begin executing a different instruction sequence and thus deviate from its default behavior of executing instructions in order (i.e. if the expression is false, the program skips the code of the if block) depending on some condition, which is the expression evaluation in our case. That being said, the compiler tries to predict the outcome prior to it being actually evaluated. It will fetch instructions from the if block, and if the expression turns out to be true, then wonderful! We gained the time it took to evaluate it and made progress in the code; if not then we are running the wrong code, the pipeline is flushed, and the correct block is run. Let's say you need to pick route 1 or route 2. Waiting for your partner to check the map, you have stopped at ## and waited, or you could just pick route1 and if you were lucky (route 1 is the correct route), then great you didn't have to wait for your partner to check the map (you saved the time it would have taken him to check the map), otherwise you will just turn back. While flushing pipelines is super fast, nowadays taking this gamble is worth it. Predicting sorted data or a data that changes slowly is always easier and better than predicting fast changes. On ARM, there is no branch needed, because every instruction has a 4-bit condition field, which tests (at zero cost) any of 16 different different conditions that may arise in the Processor Status Register, and if the condition on an instruction is false, the instruction is skipped. This eliminates the need for short branches, and there would be no branch prediction hit for this algorithm. Therefore, the sorted version of this algorithm would run slower than the unsorted version on ARM, because of the extra overhead of sorting. The inner loop for this algorithm would look something like the following in ARM assembly language: But this is actually part of a bigger picture: CMP opcodes always update the status bits in the Processor Status Register (PSR), because that is their purpose, but most other instructions do not touch the PSR unless you add an optional S suffix to the instruction, specifying that the PSR should be updated based on the result of the instruction. Just like the 4-bit condition suffix, being able to execute instructions without affecting the PSR is a mechanism that reduces the need for branches on ARM, and also facilitates out of order dispatch at the hardware level, because after performing some operation X that updates the status bits, subsequently (or in parallel) you can do a bunch of other work that explicitly should not affect (or be affected by) the status bits, then you can test the state of the status bits set earlier by X. The condition testing field and the optional "set status bit" field can be combined, for example: Most processor architectures do not have this ability to specify whether or not the status bits should be updated for a given operation, which can necessitate writing additional code to save and later restore status bits, or may require additional branches, or may limit the processor's out of order execution efficiency: one of the side effects of most CPU instruction set architectures forcibly updating status bits after most instructions is that it is much harder to tease apart which instructions can be run in parallel without interfering with each other. Updating status bits has side effects, therefore has a linearizing effect on code. ARM's ability to mix and match branch-free condition testing on any instruction with the option to either update or not update the status bits after any instruction is extremely powerful, for both assembly language programmers and compilers, and produces very efficient code. When you don't have to branch, you can avoid the time cost of flushing the pipeline for what would otherwise be short branches, and you can avoid the design complexity of many forms of speculative evalution. The performance impact of the initial naive imlementations of the mitigations for many recently discovered processor vulnerabilities (Spectre etc.) shows you just how much the performance of modern processors depends upon complex speculative evaluation logic. With a short pipeline and the dramatically reduced need for branching, ARM just doesn't need to rely on speculative evaluation as much as CISC processors. (Of course high-end ARM implementations do include speculative evaluation, but it's a smaller part of the performance story.) If you have ever wondered why ARM has been so phenomenally successful, the brilliant effectiveness and interplay of these two mechanisms (combined with another mechanism that lets you "barrel shift" left or right one of the two arguments of any arithmetic operator or offset memory access operator at zero additional cost) are a big part of the story, because they are some of the greatest sources of the ARM architecture's efficiency. The brilliance of the original designers of the ARM ISA back in 1983, Steve Furber and Roger (now Sophie) Wilson, cannot be overstated. Besides the fact that the branch prediction may slow you down, a sorted array has another advantage:    You can have a stop condition instead of just checking the value, this way you only loop over the relevant data, and ignore the rest. The branch prediction will miss only once. It's about branch prediction. What is it? A branch predictor is one of the ancient performance improving techniques which still finds relevance into modern architectures. While the simple prediction techniques provide fast lookup and power efficiency they suffer from a high misprediction rate. On the other hand, complex branch predictions –either neural based or variants of two-level branch prediction –provide better prediction accuracy, but they consume more power and complexity increases exponentially. In addition to this, in complex prediction techniques the time taken to predict the branches is itself very high –ranging from 2 to 5 cycles –which is comparable to the execution time of actual branches. Branch prediction is essentially an optimization (minimization) problem where the emphasis is on to achieve lowest possible miss rate, low power consumption, and low complexity with minimum resources. There really are three different kinds of branches: Forward conditional branches - based on a run-time condition, the PC (program counter) is changed to point to an address forward in the instruction stream. Backward conditional branches - the PC is changed to point backward in the instruction stream. The branch is based on some condition, such as branching backwards to the beginning of a program loop when a test at the end of the loop states the loop should be executed again. Unconditional branches - this includes jumps, procedure calls and returns that have no specific condition. For example, an unconditional jump instruction might be coded in assembly language as simply "jmp", and the instruction stream must immediately be directed to the target location pointed to by the jump instruction, whereas a conditional jump that might be coded as "jmpne" would redirect the instruction stream only if the result of a comparison of two values in a previous "compare" instructions shows the values to not be equal. (The segmented addressing scheme used by the x86 architecture adds extra complexity, since jumps can be either "near" (within a segment) or "far" (outside the segment). Each type has different effects on branch prediction algorithms.) Static/dynamic Branch Prediction: Static branch prediction is used by the microprocessor the first time a conditional branch is encountered, and dynamic branch prediction is used for succeeding executions of the conditional branch code. References: Branch predictor A Demonstration of Self-Profiling Branch Prediction Review Branch Prediction (Using wayback machine) Sorted arrays are processed faster than an unsorted array, due to a phenomena called branch prediction. The branch predictor is a digital circuit (in computer architecture) trying to predict which way a branch will go, improving the flow in the instruction pipeline. The circuit/computer predicts the next step and executes it. Making a wrong prediction leads to going back to the previous step, and executing with another prediction. Assuming the prediction is correct, the code will continue to the next step. A wrong prediction results in repeating the same step, until a correct prediction occurs. The answer to your question is very simple. In an unsorted array, the computer makes multiple predictions, leading to an increased chance of errors. Whereas, in a sorted array, the computer makes fewer predictions, reducing the chance of errors. Making more predictions requires more time. Sorted Array: Straight Road Unsorted Array: Curved Road Branch prediction: Guessing/predicting which road is straight and following it without checking Although both the roads reach the same destination, the straight road is shorter, and the other is longer. If then you choose the other by mistake, there is no turning back, and so you will waste some extra time if you choose the longer road. This is similar to what happens in the computer, and I hope this helped you understand better. Also I want to cite @Simon_Weaver from the comments: It doesn’t make fewer predictions - it makes fewer incorrect predictions. It still has to predict for each time through the loop... I tried the same code with MATLAB 2011b with my MacBook Pro (Intel i7, 64 bit, 2.4 GHz) for the following MATLAB code: The results for the above MATLAB code are as follows: The results of the C code as in @GManNickG I get: Based on this, it looks MATLAB is almost 175 times slower than the C implementation without sorting and 350 times slower with sorting. In other words, the effect (of branch prediction) is 1.46x for MATLAB implementation and 2.7x for the C implementation. The assumption by other answers that one needs to sort the data is not correct. The following code does not sort the entire array, but only 200-element segments of it, and thereby runs the fastest. Sorting only k-element sections completes the pre-processing in linear time, O(n), rather than the O(n.log(n)) time needed to sort the entire array. This also "proves" that it has nothing to do with any algorithmic issue such as sort order, and it is indeed branch prediction. Bjarne Stroustrup's Answer to this question: That sounds like an interview question. Is it true? How would you know? It is a bad idea to answer questions about efficiency without first doing some measurements, so it is important to know how to measure. So, I tried with a vector of a million integers and got: I ran that a few times to be sure. Yes, the phenomenon is real. My key code was: At least the phenomenon is real with this compiler, standard library, and optimizer settings. Different implementations can and do give different answers. In fact, someone did do a more systematic study (a quick web search will find it) and most implementations show that effect. One reason is branch prediction: the key operation in the sort algorithm is “if(v[i] < pivot]) …” or equivalent. For a sorted sequence that test is always true whereas, for a random sequence, the branch chosen varies randomly. Another reason is that when the vector is already sorted, we never need to move elements to their correct position. The effect of these little details is the factor of five or six that we saw. Quicksort (and sorting in general) is a complex study that has attracted some of the greatest minds of computer science. A good sort function is a result of both choosing a good algorithm and paying attention to hardware performance in its implementation. If you want to write efficient code, you need to know a bit about machine architecture. This question is rooted in branch prediction models on CPUs. I'd recommend reading this paper: Increasing the Instruction Fetch Rate via Multiple Branch Prediction and a Branch Address Cache When you have sorted elements, the IR can not be bothered to fetch all CPU instructions, again and again. It fetches them from the cache. One way to avoid branch prediction errors is to build a lookup table, and index it using the data. Stefan de Bruijn discussed that in his answer. But in this case, we know values are in the range [0, 255] and we only care about values >= 128. That means we can easily extract a single bit that will tell us whether we want a value or not: by shifting the data to the right 7 bits, we are left with a 0 bit or a 1 bit, and we only want to add the value when we have a 1 bit. Let's call this bit the "decision bit". By using the 0/1 value of the decision bit as an index into an array, we can make code that will be equally fast whether the data is sorted or not sorted. Our code will always add a value, but when the decision bit is 0, we will add the value somewhere we don't care about. Here's the code: // Test This code wastes half of the adds but never has a branch prediction failure. It's tremendously faster on random data than the version with an actual if statement. But in my testing, an explicit lookup table was slightly faster than this, probably because indexing into a lookup table was slightly faster than bit shifting. This shows how my code sets up and uses the lookup table (unimaginatively called lut for "LookUp Table" in the code). Here's the C++ code: // Declare and then fill in the lookup table In this case, the lookup table was only 256 bytes, so it fits nicely in a cache and all was fast. This technique wouldn't work well if the data was 24-bit values and we only wanted half of them... the lookup table would be far too big to be practical. On the other hand, we can combine the two techniques shown above: first shift the bits over, then index a lookup table. For a 24-bit value that we only want the top half value, we could potentially shift the data right by 12 bits, and be left with a 12-bit value for a table index. A 12-bit table index implies a table of 4096 values, which might be practical. The technique of indexing into an array, instead of using an if statement, can be used for deciding which pointer to use. I saw a library that implemented binary trees, and instead of having two named pointers (pLeft and pRight or whatever) had a length-2 array of pointers and used the "decision bit" technique to decide which one to follow. For example, instead of: It's a nice solution and maybe it will work.
__label__git __label__version-control __label__undo __label__git-commit I accidentally committed the wrong files to Git, but didn't push the commit to the server yet. How can I undo those commits from the local repository? This command is responsible for the undo. It will undo your last commit while leaving your working tree (the state of your files on disk) untouched. You'll need to add them again before you can commit them again). Make corrections to working tree files. git add anything that you want to include in your new commit. Commit the changes, reusing the old commit message. reset copied the old head to .git/ORIG_HEAD; commit with -c ORIG_HEAD will open an editor, which initially contains the log message from the old commit and allows you to edit it. If you do not need to edit the message, you could use the -C option. Alternatively, to edit the previous commit (or just its commit message), commit --amend will add changes within the current index to the previous commit. To remove (not revert) a commit that has been pushed to the server, rewriting history with git push origin master --force is necessary. How can I move HEAD back to a previous location? (Detached head) & Undo commits The above answer will show you git reflog, which you can use to determine the SHA-1 for the commit to which you wish to revert. Once you have this value, use the sequence of commands as explained above. HEAD~ is the same as HEAD~1. The article What is the HEAD in git? is helpful if you want to uncommit multiple commits. Undoing a commit is a little scary if you don't know how it works.  But it's actually amazingly easy if you do understand. I'll show you the 4 different ways you can undo a commit. Say you have this, where C is your HEAD and (F) is the state of your files. You want to nuke commit C and never see it again and lose all the changes in locally modified files.  You do this: The result is: Now B is the HEAD.  Because you used --hard, your files are reset to their state at commit B. Ah, but suppose commit C wasn't a disaster, but just a bit off.  You want to undo the commit but keep your changes for a bit of editing before you do a better commit.  Starting again from here, with C as your HEAD: You can do this, leaving off the --hard: In this case the result is: In both cases, HEAD is just a pointer to the latest commit.  When you do a git reset HEAD~1, you tell Git to move the HEAD pointer back one commit.  But (unless you use --hard) you leave your files as they were.  So now git status shows the changes you had checked into C.  You haven't lost a thing! For the lightest touch, you can even undo your commit but leave your files and your index: This not only leaves your files alone, it even leaves your index alone.  When you do git status, you'll see that the same files are in the index as before.  In fact, right after this command, you could do git commit and you'd be redoing the same commit you just had. One more thing: Suppose you destroy a commit as in the first example, but then discover you needed it after all?  Tough luck, right? Nope, there's still a way to get it back.  Type git reflog and you'll see a list of (partial) commit shas (that is, hashes) that you've moved around in.  Find the commit you destroyed, and do this: You've now resurrected that commit.  Commits don't actually get destroyed in Git for some 90 days, so you can usually go back and rescue one you didn't mean to get rid of. There are two ways to "undo" your last commit, depending on whether or not you have already made your commit public (pushed to your remote repository): Let's say I committed locally, but now I want to remove that commit. To restore everything back to the way it was prior to the last commit, we need to reset to the commit before HEAD: Now git log will show that our last commit has been removed. If you have already made your commits public, you will want to create a new commit which will "revert" the changes you made in your previous commit (current HEAD). Your changes will now be reverted and ready for you to commit: For more information, check out Git Basics - Undoing Things. Add/remove files to get things the way you want: Then amend the commit: The previous, erroneous commit will be edited to reflect the new index state - in other words, it'll be like you never made the mistake in the first place. Note that you should only do this if you haven't pushed yet. If you have pushed, then you'll just have to commit a fix normally. or Warning: The above command will permanently remove the modifications to the .java files (and any other files) that you wanted to commit. The hard reset to HEAD-1 will set your working copy to the state of the commit before your wrong commit. Replace the files in the index: Then, if it's a private branch, amend the commit: Or, if it's a shared branch, make a new commit:  (To change a previous commit, use the awesome interactive rebase.) ProTip™: Add *.class to a gitignore to stop this happening again. Amending a commit is the ideal solution if you need to change the last commit, but a more general solution is reset. You can reset Git to any commit with: Where N is the number of commits before HEAD, and @~ resets to the previous commit. So, instead of amending the commit, you could use: Check out git help reset, specifically the sections on --soft --mixed and --hard, for a better understanding of what this does. If you mess up, you can always use the reflog to find dropped commits:  Use git revert <commit-id>. To get the commit ID, just use git log. If you are planning to undo a local commit entirely, whatever you change you did on the commit, and if you don't worry anything about that, just do the following command. (This command will ignore your entire commit and your changes will be lost completely from your local working tree). If you want to undo your commit, but you want your changes in the staging area (before commit just like after git add) then do the following command. Now your committed files come into the staging area. Suppose if you want to upstage the files, because you need to edit some wrong content, then do the following command Now committed files to come from the staged area into the unstaged area. Now files are ready to edit, so whatever you change, you want to go edit and added it and make a fresh/new commit. More (link broken) (Archived version) If you have Git Extras installed, you can run git undo to undo the latest commit. git undo 3 will undo the last three commits. I wanted to undo the latest five commits in our shared repository. I looked up the revision id that I wanted to rollback to. Then I typed in the following. I prefer to use git rebase -i for this job, because a nice list pops up where I can choose the commits to get rid of. It might not be as direct as some other answers here, but it just feels right. Choose how many commits you want to list, then invoke like this (to enlist last three) Sample list Then Git will remove commits for any line that you remove. Use git-gui (or similar) to perform a git commit --amend. From the GUI you can add or remove individual files from the commit. You can also modify the commit message.  Just reset your branch to the previous location (for example, using gitk or git rebase). Then reapply your changes from a saved copy. After garbage collection in your local repository, it will be like the unwanted commit never happened. To do all of that in a single command, use git reset HEAD~1. Word of warning: Careless use of git reset is a good way to get your working copy into a confusing state. I recommend that Git novices avoid this if they can. Perform a reverse cherry pick (git-revert) to undo the changes. If you haven't yet pulled other changes onto your branch, you can simply do... Then push your updated branch to the shared repository. The commit history will show both commits, separately. Also note: You don't want to do this if someone else may be working on the branch. Clean up your branch locally then repush... In the normal case, you probably needn't worry about your private-branch commit history being pristine.  Just push a followup commit (see 'How to undo a public commit' above), and later, do a squash-merge to hide the history. If you want to permanently undo it and you have cloned some repository  The commit id can be seen by  Then you can do - If you have committed junk but not pushed, HEAD~1 is a shorthand for the commit before head. Alternatively you can refer to the SHA-1 of the hash if you want to reset to. --soft option will delete the commit but it will leave all your changed files "Changes to be committed", as git status would put it. If you want to get rid of any changes to tracked files in the working tree since the commit before head use "--hard" instead. OR If you already pushed and someone pulled which is usually my case, you can't use git reset. You can however do a git revert, This will create a new commit that reverses everything introduced by the accidental commit. On SourceTree (GUI for GitHub), you may right-click the commit and do a 'Reverse Commit'. This should undo your changes. On the terminal: You may alternatively use: Or: A single command: It works great to undo the last local commit! Just reset it doing the command below using git: Explain: what git reset does, it's basically reset to any commit you'd like to go back to, then if you combine it with --soft key, it will go back, but keep the  changes in your file(s), so you get back to the stage which the file was just added, HEAD is the head of the branch and if you combine with ~1 (in this case you also use HEAD^), it will go back only one commit which what you want... I create the steps in the image below in more details for you, including all steps that may happens in real situations and committing the code:  How to undo the last Git commit? To restore everything back to the way it was prior to the last commit, we need to reset to the commit before HEAD. If you don't want to keep your changes that you made: If you want to keep your changes: Now check your git log. It will show that our last commit has been removed. "Reset the working tree to the last commit" "Clean unknown files from the working tree" see - Git Quick Reference NOTE: This command will delete your previous commit, so use with caution! git reset --hard is safer. Use reflog to find a correct state  REFLOG BEFORE RESET Select the correct reflog (f3cb6e2 in my case) and type  After that the repo HEAD will be reset to that HEADid  LOG AFTER RESET Finally the reflog looks like the picture below  REFLOG FINAL First run:  It will show you all the possible actions you have performed on your repository, for example, commit, merge, pull, etc. Then do: git reset --soft HEAD^ or git reset --soft HEAD~ This will undo the last commit. Here --soft means reset into staging. HEAD~ or HEAD^ means to move to commit before HEAD. It will replace the last commit with the new commit. Another way: Checkout the branch you want to revert, then reset your local working copy back to the commit that you want to be the latest one on the remote server (everything after it will go bye-bye). To do this, in SourceTree I right-clicked on the and selected "Reset BRANCHNAME to this commit". Then navigate to your repository's local directory and run this command: This will erase all commits after the current one in your local repository but only for that one branch. Type git log and find the last commit hash code and then enter: In my case I accidentally committed some files I did not want to. So I did the following and it worked: Verify the results with gitk or git log --stat Simple, run this in your command line: There are two main scenarios You haven't pushed the commit yet If the problem was extra files you commited (and you don't want those on repository), you can remove them using git rm and then commiting with --amend You can also remove entire directories with -r, or even combine with other Bash commands After removing the files, you can commit, with --amend option This will rewrite your recent local commit removing the extra files, so, these files will never be sent on push and also will be removed from your local .git repository by GC. You already pushed the commit You can apply the same solution of the other scenario and then doing git push with the -f option, but it is not recommended since it overwrites the remote history with a divergent change (it can mess your repository). Instead, you have to do the commit without --amend (remember this about -amend`: That option rewrites the history on the last commit). or if you do not remember exactly in which commit it is, you might use The proper way of removing files from the repository history is using git filter-branch. That is, But I recomnend you use this command with care. Read more at git-filter-branch(1) Manual Page. To reset to the previous revision, permanently deleting all uncommitted changes:  There are many ways to do it: Git command to undo the last commit/ previous commits: Warning: Do Not use --hard if you do not know what you are doing. --hard is too dangerous, and it might delete your files. Basic command to revert the commit in Git is: or COMMIT-ID: ID for the commit n:  is number of last commits you want to revert You can get the commit id as shown below: where d81d3f1 and be20eb8 are commit id. Now let's see some cases: Suppose you want to revert the last commit 'd81d3f1'.  Here are two options: or Suppose you want to revert the commit 'be20eb8': For more detailed information you can refer and try out some other commands too for resetting head to a specified state:
__label__git __label__version-control __label__git-branch __label__git-remote __label__git-push I want to delete a branch both locally and remotely. What should I do differently to successfully delete the remotes/origin/bugfix branch both locally and remotely? Note that in most cases the remote name is origin. In such a case you'll have to use the command like so. To delete the local branch use one of the following: Note: The -d option is an alias for --delete, which only deletes the branch if it has already been fully merged in its upstream branch. You could also use -D, which is an alias for --delete --force, which deletes the branch "irrespective of its merged status." [Source: man git-branch] Also note that git branch -d branch_name will fail if you are currently in the branch you want to remove. The message starts with error: Cannot delete the branch 'branch_name'. If so, first switch to some other branch, for example: git checkout master. As of Git v1.7.0, you can delete a remote branch using which might be easier to remember than which was added in Git v1.5.0 "to delete a remote branch or a tag." Starting on Git v2.8.0 you can also use git push with the -d option as an alias for --delete. Therefore, the version of Git you have installed will dictate whether you need to use the easier or harder syntax. From Chapter 3 of Pro Git by Scott Chacon: Suppose you’re done with a remote branch — say, you and your collaborators are finished with a feature and have merged it into your remote’s master branch (or whatever branch your stable code-line is in). You can delete a remote branch using the rather obtuse syntax git push [remotename] :[branch]. If you want to delete your server-fix branch from the server, you run the following: Boom. No more branches on your server. You may want to dog-ear this page, because you’ll need that command, and you’ll likely forget the syntax. A way to remember this command is by recalling the git push [remotename] [localbranch]:[remotebranch] syntax that we went over a bit earlier. If you leave off the [localbranch] portion, then you’re basically saying, “Take nothing on my side and make it be [remotebranch].” I issued git push origin: bugfix and it worked beautifully. Scott Chacon was right—I will want to dog ear that page (or virtually dog ear by answering this on Stack Overflow). Then you should execute this on other machines to propagate changes. Matthew's answer is great for removing remote branches and I also appreciate the explanation, but to make a simple distinction between the two commands: To remove a local branch from your machine: git branch -d {the_local_branch} (use -D instead to force deleting the branch without checking merged status) To remove a remote branch from the server: git push origin --delete {the_remote_branch} Reference: Git: Delete a branch (local or remote) If you want more detailed explanations of the following commands, then see the long answers in the next section. When you're dealing with deleting branches both locally and remotely, keep in mind that there are three different branches involved:  The original poster used: Which only deleted his local remote-tracking branch origin/bugfix, and not the actual remote branch bugfix on origin.  To delete that actual remote branch, you need  The following sections describe additional details to consider when deleting your remote and remote-tracking branches. Note that deleting the remote branch X from the command line using a git push will also remove the local remote-tracking branch origin/X, so it is not necessary to prune the obsolete remote-tracking branch with git fetch --prune or git fetch -p. However, it wouldn't hurt if you did it anyway. You can verify that the remote-tracking branch origin/X was also deleted by running the following: If you didn't delete your remote branch X from the command line (like above), then your local repository will still contain (a now obsolete) remote-tracking branch origin/X. This can happen if you deleted a remote branch directly through GitHub's web interface, for example. A typical way to remove these obsolete remote-tracking branches (since Git version 1.6.6) is to simply run git fetch with the --prune or shorter -p. Note that this removes all obsolete local remote-tracking branches for any remote branches that no longer exist on the remote: Here is the relevant quote from the 1.6.6 release notes (emphasis mine): "git fetch" learned --all and --multipleoptions, to run fetch from   many repositories, and --prune option to remove remote tracking   branches that went stale.  These make "git remote update" and "git   remote prune" less necessary (there is no plan to remove "remote   update" nor "remote prune", though). Alternatively, instead of pruning your obsolete local remote-tracking branches through git fetch -p, you can avoid making the extra network operation by just manually removing the branch(es) with the --remote or -r flags: For deleting the remote branch: For deleting the local branch, you have three ways: Explain: OK, just explain what's going on here! Simply do git push origin --delete to delete your remote branch only, add the name of the branch at the end and this will delete and push it to remote at the same time... Also, git branch -D, which simply delete the local branch only!... -D stands for --delete --force which will delete the branch even it's not merged (force delete), but you can also use -d which stands for --delete which throw an error respective of the branch merge status... I also create the image below to show the steps:  You can also use the following to delete the remote branch Which does the same thing as but it may be easier to remember. Tip: When you delete branches using or only the references are deleted. Even though the branch is actually removed on the remote, the references to it still exists in the local repositories of your team members. This means that for other team members the deleted branches are still visible when they do a git branch -a. To solve this, your team members can prune the deleted branches with This is typically git remote prune origin. If you want to delete a branch, first checkout to the branch other than the branch to be deleted. Deleting the local branch: Deleting the remote branch:  It's very simple: To delete the remote branch Or To forcefully delete local branch Happy Coding :) This is simple: Just run the following command: To delete a Git branch both locally and remotely, first delete the local branch using this command: (Here example is the branch name.) And after that, delete the remote branch using this command: Another approach is: WARNING: This will delete all remote branches that do not exist locally. Or more comprehensively, will effectively make the remote repository look like the local copy of the repository (local heads, remotes and tags are mirrored on remote). I use the following in my Bash settings: Then you can call: Delete locally: To delete a local branch, you can use: To delete a branch forcibly, use -D instead of -d. Delete remotely: There are two options: I would suggest you use the second way as it is more intuitive. Since January 2013, GitHub included a Delete branch button next to each branch in your "Branches" page. Relevant blog post: Create and delete branches If you want to complete both these steps with a single command, you can make an alias for it by adding the below to your ~/.gitconfig: Alternatively, you can add this to your global configuration from the command line using NOTE: If using -d (lowercase d), the branch will only be deleted if it has been merged. To force the delete to happen, you will need to use -D (uppercase D). To delete your branch locally and remotely  Checkout to master branch -  git checkout master Delete your remote branch - git push origin --delete <branch-name> Delete your local branch - git branch --delete <branch-name> You can also do this using git remote prune origin It prunes and deletes remote-tracking branches from a git branch -r listing. In addition to the other answers, I often use the git_remote_branch tool. It's an extra install, but it gets you a convenient way to interact with remote branches. In this case, to delete: I find that I also use the publish and track commands quite often. A one-liner command to delete both local, and remote: Or add the alias below to your ~/.gitconfig. Usage: git kill branch-name Deleting Branches Let's assume our work on branch "contact-form" is done and we've already integrated it into "master". Since we don't need it anymore, we can delete it (locally): And for deleting the remote branch: Delete remote branch git push origin :<branchname> Delete local branch git branch -D <branchname> Delete local branch steps: Simply say: is easier to remember than Now you can do it with the GitHub Desktop application. After launching the application Switch to the branch you would like to delete  From the "Branch" menu, select, "Unpublish...", to have the branch deleted from the GitHub servers.  From the "Branch" menu, select, 'Delete "branch_name"...', to have the branch deleted off of your local machine (AKA the machine you are currently working on)  To delete locally - (normal) If your branch is in a rebasing/merging progress and that was not done properly, it means you will get an error, Rebase/Merge in progress, so in that case, you won't be able to delete your branch. So either you need to solve the rebasing/merging. Otherwise, you can do force delete by using, To delete in remote: You can do the same using: Graphical representation:  This won't work if you have a tag with the same name as the branch on the remote: In that case you need to specify that you want to delete the branch, not the tag: Similarly, to delete the tag instead of the branch you would use: Many of the other answers will lead to errors/warnings. This approach is relatively fool proof although you may still need git branch -D branch_to_delete if it's not fully merged into some_other_branch, for example. Remote pruning isn't needed if you deleted the remote branch. It's only used to get the most up-to-date remotes available on a repository you're tracking. I've observed git fetch will add remotes, not remove them. Here's an example of when git remote prune origin will actually do something: User A does the steps above. User B would run the following commands to see the most up-to-date remote branches: I got sick of googling for this answer, so I took a similar approach to the answer that crizCraig posted earlier. I added the following to my Bash profile: Then every time I'm done with a branch (merged into master, for example) I run the following in my terminal: ...which then deletes my-branch-name from origin as as well as locally. Before executing make sure you determine first what the exact name of the remote branch is by executing: This will tell you what to enter exactly for <branch> value. (branch is case sensitive!) Use: If you are sure you want to delete it, run Now to clean up deleted remote branches run
__label__git __label__version-control __label__git-pull __label__git-fetch What are the differences between git pull and git fetch? In the simplest terms, git pull does a git fetch followed by a git merge. You can do a git fetch at any time to update your remote-tracking branches under refs/remotes/<remote>/. This operation never changes any of your own local branches under refs/heads, and is safe to do without changing your working copy. I have even heard of people running git fetch periodically in a cron job in the background (although I wouldn't recommend doing this). A git pull is what you would do to bring a local branch up-to-date with its remote version, while also updating your other remote-tracking branches. From the Git documentation for git pull: In its default mode, git pull is shorthand for git fetch followed by git merge FETCH_HEAD. When you use pull, Git tries to automatically do your work for you. It is context sensitive, so Git will merge any pulled commits into the branch you are currently working in.  pull automatically merges the commits without letting you review them first. If you don’t closely manage your branches, you may run into frequent conflicts. When you fetch, Git gathers any commits from the target branch that do not exist in your current branch and stores them in your local repository. However, it does not merge them with your current branch. This is particularly useful if you need to keep your repository up to date, but are working on something that might break if you update your files.  To integrate the commits into your master branch, you use merge. It is important to contrast the design philosophy of git with the philosophy of a more traditional source control tool like SVN. Subversion was designed and built with a client/server model. There is a single repository that is the server, and several clients can fetch code from the server, work on it, then commit it back to the server. The assumption is that the client can always contact the server when it needs to perform an operation. Git was designed to support a more distributed model with no need for a central repository (though you can certainly use one if you like). Also git was designed so that the client and the "server" don't need to be online at the same time. Git was designed so that people on an unreliable link could exchange code via email, even. It is possible to work completely disconnected and burn a CD to exchange code via git. In order to support this model git maintains a local repository with your code and also an additional local repository that mirrors the state of the remote repository. By keeping a copy of the remote repository locally, git can figure out the changes needed even when the remote repository is not reachable.  Later when you need to send the changes to someone else, git can transfer them as a set of changes from a point in time known to the remote repository. git fetch is the command that says "bring my local copy of the remote repository up to date."  git pull says "bring the changes in the remote repository to where I keep my own code." Normally git pull does this by doing a git fetch to bring the local copy of the remote repository up to date, and then merging the changes into your own code repository and possibly your working copy. The take away is to keep in mind that there are often at least three copies of a project on your workstation. One copy is your own repository with your own commit history. The second copy is your working copy where you are editing and building. The third copy is your local "cached" copy of a remote repository. Here is Oliver Steele's image of how all it all fits together:  If there is sufficient interest, I suppose I could update the image to add git clone and git merge...  One use case of git fetch is that the following will tell you any changes in the remote branch since your last pull... so you can check before doing an actual pull, which could change files in your current branch and working copy. See: https://git-scm.com/docs/git-diff regarding double- and triple-dot syntax in the diff command It cost me a little bit to understand what was the difference, but this is a simple explanation. master in your localhost is a branch. When you clone a repository you fetch the entire repository to you local host. This means that at that time you have an origin/master pointer to HEAD and master pointing to the same HEAD. when you start working and do commits you advance the master pointer to HEAD + your commits. But the origin/master pointer is still pointing to what it was when you cloned. So the difference will be: Sometimes a visual representation helps.  Briefly git fetch is similar to pull but doesn't merge. i.e. it fetches remote updates (refs and objects) but your local stays the same (i.e. origin/master gets updated but master stays the same) . git pull pulls down from a remote and instantly merges. More git clone clones a repo. git rebase saves stuff from your current branch that isn't in the upstream branch to a temporary area. Your branch is now the same as before you started your changes. So, git pull -rebase will pull down the remote changes, rewind your local branch, replay your changes over the top of your current branch one by one until you're up-to-date. Also, git branch -a will show you exactly what’s going on with all your branches - local and remote. This blog post was useful: The difference between git pull, git fetch and git clone (and git rebase) - Mike Pearce and covers git pull, git fetch, git clone and git rebase. I thought I'd update this to show how you'd actually use this in practice. Update your local repo from the remote (but don't merge): After downloading the updates, let's see the differences: If you're happy with those updates, then merge: Notes: On step 2: For more on diffs between local and remotes, see: How to compare a local git branch with its remote branch? On step 3: It's probably more accurate (e.g. on a fast changing repo) to do a git rebase origin here. See @Justin Ohms comment in another answer. See also: http://longair.net/blog/2009/04/16/git-fetch-and-merge/ You would pull if you want the histories merged, you'd fetch if you just 'want the codez' as some person has been tagging some articles around here. You can fetch from a remote repository, see the differences and then pull or merge. This is an example for a remote repository called origin and a branch called master tracking the remote branch origin/master: The short and easy answer is that git pull is simply git fetch followed by git merge. It is very important to note that git pull will automatically merge whether you like it or not. This could, of course, result in merge conflicts. Let's say your remote is origin and your branch is master. If you git diff origin/master before pulling, you should have some idea of potential merge conflicts and could prepare your local branch accordingly.  In addition to pulling and pushing, some workflows involve git rebase, such as this one, which I paraphrase from the linked article: If you find yourself in such a situation, you may be tempted to git pull --rebase. Unless you really, really know what you are doing, I would advise against that. This warning is from the man page for git-pull, version 2.3.5: This is a potentially dangerous mode of operation. It rewrites   history, which does not bode well when you published that history   already. Do not use this option unless you have read git-rebase(1)   carefully. OK, here is some information about git pull and git fetch, so you can understand the actual differences... in few simple words, fetch gets the latest data, but not the code changes and not going to mess with your current  local branch code, but pull get the code changes and merge it your local branch, read on to get more details about each: It will download all refs and objects and any new branches to your local Repository... Fetch branches and/or tags (collectively, "refs") from one or more other repositories, along with the objects necessary to complete their histories. Remote-tracking branches are updated (see the description of  below for ways to control this behavior). By default, any tag that points into the histories being fetched is also fetched; the effect is to fetch tags that point at branches that you are interested in. This default behavior can be changed by using the --tags or --no-tags options or by configuring remote..tagOpt. By using a refspec that fetches tags explicitly, you can fetch tags that do not point into branches you are interested in as well. git fetch can fetch from either a single named repository or URL or from several repositories at once if  is given and there is a remotes. entry in the configuration file. (See git-config1). When no remote is specified, by default the origin remote will be used, unless there’s an upstream branch configured for the current branch. The names of refs that are fetched, together with the object names they point at, are written to .git/FETCH_HEAD. This information may be used by scripts or other git commands, such as git-pull. It will apply the changes from remote to the current branch in local... Incorporates changes from a remote repository into the current branch. In its default mode, git pull is shorthand for git fetch followed by git merge FETCH_HEAD. More precisely, git pull runs git fetch with the given parameters and calls git merge to merge the retrieved branch heads into the current branch. With --rebase, it runs git rebase instead of git merge.  should be the name of a remote repository as passed to git-fetch1.  can name an arbitrary remote ref (for example, the name of a tag) or even a collection of refs with corresponding remote-tracking branches (e.g., refs/heads/:refs/remotes/origin/), but usually it is the name of a branch in the remote repository. Default values for  and  are read from the "remote" and "merge" configuration for the current branch as set by git-branch --track. I also create the visual below to show you how git fetch and git pull working together...   This interactive graphical representation is very helpful in understanging git: http://ndpsoftware.com/git-cheatsheet.html  git fetch just "downloads" the changes from the remote to your local repository. git pull downloads the changes and merges them into your current branch. "In its default mode, git pull is shorthand for git fetch followed by git merge FETCH_HEAD." In speaking of pull & fetch in the above answers, I would like to share an interesting trick, This above command is the most useful command in my git life which saved a lots of time. Before pushing your new commits to server, try this command and it will automatically sync latest server changes (with a fetch + merge) and will place your commit at the top in git log. No need to worry about manual pull/merge. Find details at: http://gitolite.com/git-pull--rebase I like to have some visual representation of the situation to grasp these things. Maybe other developers would like to see it too, so here's my addition. I'm not totally sure that it all is correct, so please comment if you find any mistakes. Some major advantages for having a fetched mirror of the remote are: I have struggled with this as well.  In fact I got here with a google search of exactly the same question.  Reading all these answers finally painted a picture in my head and I decided to try to get this down looking at the state of the 2 repositories and 1 sandbox and actions performed over time while watching the version of them.  So here is what I came up with.  Please correct me if I messed up anywhere. The three repos with a fetch: The three repos with a pull This helped me understand why a fetch is pretty important. The Difference between GIT Fetch and GIT Pull can be explained with the following scenario: (Keeping in mind that pictures speak louder than words!, I have provided pictorial representation) Let's take an example that you are working on a project with your team members. So there will be one main Branch of the project and all the contributors must fork it to their own local repository and then work on this local branch to modify/Add modules then push back to the main branch. So, Initial State of the two Branches when you forked the main project on your local repository will be like this- (A, B and C are Modules already completed of the project)  Now, you have started working on the new module (suppose D)  and when you have completed the D module you want to push it to the main branch, But meanwhile what happens is that one of your teammates has developed new Module E, F and modified C. So now what has happened is that your local repository is lacking behind the original progress of the project and thus pushing of your changes to the main branch can lead to conflict and may cause your Module D to malfunction.  To avoid such issues and to work parallel with the original progress of the project there are Two ways: 1. Git Fetch- This will Download all the changes that have been made to the origin/main branch project which are not present in your local branch. And will wait for the Git Merge command to apply the changes that have been fetched to your Repository or branch.  So now You can carefully monitor the files before merging it to your repository. And you can also modify D if required because of Modified C.  2. Git Pull- This will update your local branch with the origin/main branch i.e. actually what it does is a combination of Git Fetch and Git merge one after another. But this may Cause Conflicts to occur, so it’s recommended to use Git Pull with a clean copy.  We simply say: If you run git pull, you do not need to merge the data to local. If you run git fetch, it means you must run git merge for getting the latest code to your local machine. Otherwise, the local machine code would not be changed without merge.  So in the Git Gui, when you do fetch, you have to merge the data. Fetch itself won't make the code changes at your local. You can check that when you update the code by fetching once fetch and see; the code it won't change. Then you merge... You will see the changed code. git fetch pulls down the code from the remote server to your tracking branches in your local repository.  If your remote is named origin (the default) then these branches will be within origin/, for example origin/master, origin/mybranch-123, etc.  These are not your current branches, they are local copies of those branches from the server. git pull does a git fetch but then also merges the code from the tracking branch into your current local version of that branch.  If you're not ready for that changes yet, just git fetch first. git fetch will retrieve remote branches so that you can git diff or git merge them with the current branch. git pull will run fetch on the remote brach tracked by the current branch and then merge the result. You can use git fetch to see if there are any updates to the remote branch without necessary merging them with your local branch. Git Fetch You download changes to your local branch from origin through fetch. Fetch asks the remote repo for all commits that others have made but you don't have on your local repo. Fetch downloads these commits and adds them to the local repository. Git Merge You can apply changes downloaded through fetch using the merge command. Merge will take the commits retrieved from fetch and try to add them to your local branch. The merge will keep the commit history of your local changes so that when you share your branch with push, Git will know how others can merge your changes. Git Pull Fetch and merge run together often enough that a command that combines the two, pull, was created. Pull does a fetch and then a merge to add the downloaded commits into your local branch. In simple terms, if you were about to hop onto a plane without any Internet connection...before departing you could just do git fetch origin <branch>. It would fetch all the changes into your computer, but keep it separate from your local development/workspace. On the plane, you could make changes to your local workspace and then merge it with what you've fetched and resolve potential merge conflicts all without a connection to the Internet. And unless someone had made new changes to the remote repository then once you arrive at the destination you would do git push origin <branch> and go get your coffee. From this awesome Atlassian tutorial: The git fetch command downloads commits, files, and refs from a remote repository into your local repository. Fetching is what you do when you want to see what everybody else has been working on. It’s similar to SVN update in that it lets you see how the central history has progressed, but it doesn’t force you to actually merge the changes into your repository. Git isolates fetched content as a from existing local content, it has absolutely no effect on your local development work. Fetched content has to be explicitly checked out using the git checkout command. This makes fetching a safe way to review commits before integrating them with your local repository. When downloading content from a remote repository, git pull and git fetch commands are available to accomplish the task. You can consider git fetch the 'safe' version of the two commands. It will download the remote content, but not update your local repository's working state, leaving your current work intact. git pull is the more aggressive alternative, it will download the remote content for the active local branch and immediately execute git merge to create a merge commit for the new remote content. If you have pending changes in progress this will cause conflicts and kickoff the merge conflict resolution flow. With git pull: Hmmm...so if I'm not updating the working copy with git fetch, then where am I making changes? Where does Git fetch store the new commits? Great question. It puts it somewhere isolated from your working copy. But again where? Let's find out. In your project directory (i.e., where you do your git commands) do: ls. This will show the files & directories. Nothing cool, I know. Now do ls -a. This will show dot files, i.e., files beginning with . You will then be able to see a directory named: .git. Do cd .git. This will obviously change your directory. Now comes the fun part; do ls. You will see a list of directories. We're looking for refs. Do cd refs. It's interesting to see what's inside all directories, but let's focus on two of them. heads and remotes. Use cd to check inside them too. Any git fetch that you do will update items in the /.git/refs/remotes directory. It won't update anything in the /.git/refs/heads directory. Any git pull will first do the git fetch, update items in the /.git/refs/remotes directory, then merge with your local and then change the head inside the /.git/refs/heads directory. A very good related answer can also be found in Where does 'git fetch' place itself?. Also, look for "Slash notation" from the Git branch naming conventions post. It helps you better understand how Git places things in different directories. Just do: If the remote master was updated you'll get a message like this: If you didn't fetch and just did git checkout master then your local git wouldn't know that there are 2 commits added. And it would just say: But that's outdated and incorrect. It's because git will give you feedback solely based on what it knows. It's oblivious to new commits that it hasn't pulled down yet... Some IDEs (e.g. Xcode) are super smart and use the result of a git fetch and can annotate the lines of code that have been changed in remote branch of your current working branch. If that line has been changed by both local changes and remote branch, then that line gets annotated with red. This isn't a merge conflict. It's a potential merge conflict. It's a headsup that you can use to resolve the future merge conflict before doing git pull from the remote branch.  If you fetched a remote branch e.g. did: Then this would go into your remotes directory. It's still not available to your local directory. However, it simplifies your checkout to that remote branch by DWIM (Do what I mean): you no longer need to do: For more on that read here The only difference between git pull and git fetch is that : git pull pulls from a remote branch and merges it. git fetch only fetches from the remote branch but it does not merge i.e. git pull = git fetch + git merge ... Git allows chronologically older commits to be applied after newer commits. Because of this, the act of transferring commits between repositories is split into two steps: Copying new commits from remote branch to copy of this remote branch inside local repo.   (repo to repo operation) master@remote >> remote/origin/master@local Integrating new commits to local branch (inside-repo operation) remote/origin/master@local >> master@local There are two ways of doing step 2. You can: In git terminology, step 1 is git fetch, step 2 is git merge or git rebase git pull is git fetch and git merge Git obtains the branch of the latest version from the remote to the local using two commands:  git fetch: Git is going to get the latest version from remote to local,  but it do not automatically merge.      git fetch origin master git log -p master..origin/master git merge origin/master      The commands above mean that download latest version of the main branch from origin from the remote to origin master branch. And then compares the local master branch and origin master branch. Finally, merge.  git pull: Git is going to get the latest version from the remote and merge into the local.     git pull origin master      The command above is the equivalent to git fetch and git merge. In practice, git fetch maybe more secure because before the merge we can see the changes and decide whether to merge. What is the difference between git pull and git fetch? To understand this, you first need to understand that your local git maintains not only your local repository, but it also maintains a local copy of the remote repository. git fetch brings your local copy of the remote repository up to date. For example, if your remote repository is GitHub - you may want to fetch any changes made in the remote repository to your local copy of it the remote repository. This will allow you to perform operations such as compare or merge. git pull on the other hand will bring down the changes in the remote repository to where you keep your own code. Typically, git pull will do a git fetch first to bring the local copy of the remote repository up to date, and then it will merge the changes into your own code repository and possibly your working copy.  git pull == ( git fetch + git merge)  git fetch does not changes to local branches. If you already have a local repository with a remote set up for the desired project, you can grab all branches and tags for the existing remote using git fetch . ... Fetch does not make any changes to local branches, so you will need to merge a remote branch with a paired local branch to incorporate newly fetch changes. from github A simple Graphical Representation for Beginners,  here, will fetch code from repository and rebase with your local... in git pull there is possibility of new commits getting created. but in ,  git fetch  will fetch code from repository and we need to rebase it manually by using git rebase eg: i am going to fetch from server master and rebase it in my local master. 1) git pull ( rebase will done automatically): here origin is your remote repo master is your branch 2) git fetch (need to rebase manually): it will fetch server changes from origin. and it will be in your local until you rebase it on your own. we need to fix conflicts manually by checking codes. this will rebase code into local. before that ensure you're in right branch. Trying to be clear and simple. The git pull command is actually a shortcut for git fetch followed by the git merge or the git rebase command depending on your configuration. You can configure your Git repository so that git pull is a fetch followed by a rebase. Actually Git maintains a copy of your own code and  the remote repository. The command git fetch makes your local copy up to date by getting data from remote repository. The reason we need this is because somebody else might have made some changes to the code and you want to keep yourself updated.   The command git pull brings the changes in the remote repository to where you keep your own code. Normally, git pull does this by doing a ‘git fetch’ first to bring the local copy of the remote repository up to date, and then it merges the changes into your own code repository and possibly your working copy.
__label__iterator __label__yield __label__generator __label__python __label__coroutine What is the use of the yield keyword in Python, and what does it do? For example, I'm trying to understand this code1: And this is the caller: What happens when the method _get_child_candidates is called? Is a list returned? A single element? Is it called again? When will subsequent calls stop?  1. This piece of code was written by Jochen Schulz (jrschulz), who made a great Python library for metric spaces. This is the link to the complete source: Module mspace. To understand what yield does, you must understand what generators are. And before you can understand generators, you must understand iterables. When you create a list, you can read its items one by one. Reading its items one by one is called iteration: mylist is an iterable. When you use a list comprehension, you create a list, and so an iterable: Everything you can use "for... in..." on is an iterable; lists, strings, files... These iterables are handy because you can read them as much as you wish, but you store all the values in memory and this is not always what you want when you have a lot of values. Generators are iterators, a kind of iterable you can only iterate over once. Generators do not store all the values in memory, they generate the values on the fly: It is just the same except you used () instead of []. BUT, you cannot perform for i in mygenerator a second time since generators can only be used once: they calculate 0, then forget about it and calculate 1, and end calculating 4, one by one. yield is a keyword that is used like return, except the function will return a generator. Here it's a useless example, but it's handy when you know your function will return a huge set of values that you will only need to read once. To master yield, you must understand that when you call the function, the code you have written in the function body does not run. The function only returns the generator object, this is a bit tricky :-) Then, your code will continue from where it left off each time for uses the generator. Now the hard part: The first time the for calls the generator object created from your function, it will run the code in your function from the beginning until it hits yield, then it'll return the first value of the loop. Then, each subsequent call will run another iteration of the loop you have written in the function and return the next value. This will continue until the generator is considered empty, which happens when the function runs without hitting yield. That can be because the loop has come to an end, or because you no longer satisfy an "if/else". Generator: Caller: This code contains several smart parts: The loop iterates on a list, but the list expands while the loop is being iterated :-) It's a concise way to go through all these nested data even if it's a bit dangerous since you can end up with an infinite loop. In this case, candidates.extend(node._get_child_candidates(distance, min_dist, max_dist)) exhaust all the values of the generator, but while keeps creating new generator objects which will produce different values from the previous ones since it's not applied on the same node. The extend() method is a list object method that expects an iterable and adds its values to the list. Usually we pass a list to it: But in your code, it gets a generator, which is good because: And it works because Python does not care if the argument of a method is a list or not. Python expects iterables so it will work with strings, lists, tuples, and generators! This is called duck typing and is one of the reasons why Python is so cool. But this is another story, for another question... You can stop here, or read a little bit to see an advanced use of a generator: Note: For Python 3, useprint(corner_street_atm.__next__()) or print(next(corner_street_atm)) It can be useful for various things like controlling access to a resource. The itertools module contains special functions to manipulate iterables. Ever wish to duplicate a generator? Chain two generators? Group values in a nested list with a one-liner? Map / Zip without creating another list? Then just import itertools. An example? Let's see the possible orders of arrival for a four-horse race: Iteration is a process implying iterables (implementing the __iter__() method) and iterators (implementing the __next__() method). Iterables are any objects you can get an iterator from. Iterators are objects that let you iterate on iterables. There is more about it in this article about how for loops work. When you see a function with yield statements, apply this easy trick to understand what will happen: This trick may give you an idea of the logic behind the function, but what actually happens with yield is significantly different than what happens in the list based approach. In many cases, the yield approach will be a lot more memory efficient and faster too. In other cases, this trick will get you stuck in an infinite loop, even though the original function works just fine. Read on to learn more... First, the iterator protocol - when you write Python performs the following two steps: Gets an iterator for mylist: Call iter(mylist) -> this returns an object with a next() method (or __next__() in Python 3). [This is the step most people forget to tell you about] Uses the iterator to loop over items: Keep calling the next() method on the iterator returned from step 1. The return value from next() is assigned to x and the loop body is executed. If an exception StopIteration is raised from within next(), it means there are no more values in the iterator and the loop is exited. The truth is Python performs the above two steps anytime it wants to loop over the contents of an object - so it could be a for loop, but it could also be code like otherlist.extend(mylist) (where otherlist is a Python list). Here mylist is an iterable because it implements the iterator protocol. In a user-defined class, you can implement the __iter__() method to make instances of your class iterable. This method should return an iterator. An iterator is an object with a next() method. It is possible to implement both __iter__() and next() on the same class, and have __iter__() return self. This will work for simple cases, but not when you want two iterators looping over the same object at the same time. So that's the iterator protocol, many objects implement this protocol: Note that a for loop doesn't know what kind of object it's dealing with - it just follows the iterator protocol, and is happy to get item after item as it calls next(). Built-in lists return their items one by one, dictionaries return the keys one by one, files return the lines one by one, etc. And generators return... well that's where yield comes in: Instead of yield statements, if you had three return statements in f123() only the first would get executed, and the function would exit. But f123() is no ordinary function. When f123() is called, it does not return any of the values in the yield statements! It returns a generator object. Also, the function does not really exit - it goes into a suspended state. When the for loop tries to loop over the generator object, the function resumes from its suspended state at the very next line after the yield it previously returned from, executes the next line of code, in this case, a yield statement, and returns that as the next item. This happens until the function exits, at which point the generator raises StopIteration, and the loop exits.  So the generator object is sort of like an adapter - at one end it exhibits the iterator protocol, by exposing __iter__() and next() methods to keep the for loop happy. At the other end, however, it runs the function just enough to get the next value out of it, and puts it back in suspended mode. Usually, you can write code that doesn't use generators but implements the same logic. One option is to use the temporary list 'trick' I mentioned before. That will not work in all cases, for e.g. if you have infinite loops, or it may make inefficient use of memory when you have a really long list. The other approach is to implement a new iterable class SomethingIter that keeps the state in instance members and performs the next logical step in it's next() (or __next__() in Python 3) method. Depending on the logic, the code inside the next() method may end up looking very complex and be prone to bugs. Here generators provide a clean and easy solution. Think of it this way: An iterator is just a fancy sounding term for an object that has a next() method.  So a yield-ed function ends up being something like this: Original version: This is basically what the Python interpreter does with the above code: For more insight as to what's happening behind the scenes, the for loop can be rewritten to this: Does that make more sense or just confuse you more?  :) I should note that this is an oversimplification for illustrative purposes. :) The yield keyword is reduced to two simple facts: In a nutshell: a generator is a lazy, incrementally-pending list, and yield statements allow you to use function notation to program the list values the generator should incrementally spit out. Let's define a function makeRange that's just like Python's range. Calling makeRange(n) RETURNS A GENERATOR: To force the generator to immediately return its pending values, you can pass it into list() (just like you could any iterable): The above example can be thought of as merely creating a list which you append to and return: There is one major difference, though; see the last section. An iterable is the last part of a list comprehension, and all generators are iterable, so they're often used like so: To get a better feel for generators, you can play around with the itertools module (be sure to use chain.from_iterable rather than chain when warranted). For example, you might even use generators to implement infinitely-long lazy lists like itertools.count(). You could implement your own def enumerate(iterable): zip(count(), iterable), or alternatively do so with the yield keyword in a while-loop. Please note: generators can actually be used for many more things, such as implementing coroutines or non-deterministic programming or other elegant things. However, the "lazy lists" viewpoint I present here is the most common use you will find. This is how the "Python iteration protocol" works. That is, what is going on when you do list(makeRange(5)). This is what I describe earlier as a "lazy, incremental list". The built-in function next() just calls the objects .next() function, which is a part of the "iteration protocol" and is found on all iterators. You can manually use the next() function (and other parts of the iteration protocol) to implement fancy things, usually at the expense of readability, so try to avoid doing that... Normally, most people would not care about the following distinctions and probably want to stop reading here. In Python-speak, an iterable is any object which "understands the concept of a for-loop" like a list [1,2,3], and an iterator is a specific instance of the requested for-loop like [1,2,3].__iter__(). A generator is exactly the same as any iterator, except for the way it was written (with function syntax). When you request an iterator from a list, it creates a new iterator. However, when you request an iterator from an iterator (which you would rarely do), it just gives you a copy of itself. Thus, in the unlikely event that you are failing to do something like this... ... then remember that a generator is an iterator; that is, it is one-time-use. If you want to reuse it, you should call myRange(...) again. If you need to use the result twice, convert the result to a list and store it in a variable x = list(myRange(5)). Those who absolutely need to clone a generator (for example, who are doing terrifyingly hackish metaprogramming) can use itertools.tee if absolutely necessary, since the copyable iterator Python PEP standards proposal has been deferred. What does the yield keyword do in Python? yield is only legal inside of a function definition, and the inclusion of yield in a function definition makes it return a generator. The idea for generators comes from other languages (see footnote 1) with varying implementations. In Python's Generators, the execution of the code is frozen at the point of the yield. When the generator is called (methods are discussed below) execution resumes and then freezes at the next yield. yield provides an easy way of implementing the iterator protocol, defined by the following two methods: __iter__ and next (Python 2) or __next__ (Python 3).  Both of those methods make an object an iterator that you could type-check with the Iterator Abstract Base Class from the collections module. The generator type is a sub-type of iterator: And if necessary, we can type-check like this: A feature of an Iterator is that once exhausted, you can't reuse or reset it: You'll have to make another if you want to use its functionality again (see footnote 2): One can yield data programmatically, for example: The above simple generator is also equivalent to the below - as of Python 3.3 (and not available in Python 2), you can use yield from: However, yield from also allows for delegation to subgenerators, which will be explained in the following section on cooperative delegation with sub-coroutines. yield forms an expression that allows data to be sent into the generator (see footnote 3) Here is an example, take note of the received variable, which will point to the data that is sent to the generator: First, we must queue up the generator with the builtin function, next. It will call the appropriate next or __next__ method, depending on the version of Python you are using: And now we can send data into the generator. (Sending None is the same as calling next.) : Now, recall that yield from is available in Python 3. This allows us to delegate coroutines to a subcoroutine: And now we can delegate functionality to a sub-generator and it can be used by a generator just as above: Now simulate adding another 1,000 to the account plus the return on the account (60.0): You can read more about the precise semantics of yield from in PEP 380. The close method raises GeneratorExit at the point the function execution was frozen. This will also be called by __del__ so you can put any cleanup code where you handle the GeneratorExit: You can also throw an exception which can be handled in the generator or propagated back to the user: Raises: I believe I have covered all aspects of the following question: What does the yield keyword do in Python? It turns out that yield does a lot. I'm sure I could add even more thorough examples to this. If you want more or have some constructive criticism, let me know by commenting below. The grammar currently allows any expression in a list comprehension. Since yield is an expression, it has been touted by some as interesting to use it in comprehensions or generator expression - in spite of citing no particularly good use-case. The CPython core developers are discussing deprecating its allowance. Here's a relevant post from the mailing list: On 30 January 2017 at 19:05, Brett Cannon  wrote: On Sun, 29 Jan 2017 at 16:39 Craig Rodrigues  wrote: I'm OK with either approach.  Leaving things the way they are in Python 3 is no good, IMHO. My vote is it be a SyntaxError since you're not getting what you expect from the syntax. I'd agree that's a sensible place for us to end up, as any code relying on the current behaviour is really too clever to be maintainable. In terms of getting there, we'll likely want: Cheers, Nick. --  Nick Coghlan   |   ncoghlan at gmail.com   |   Brisbane, Australia Further, there is an outstanding issue (10544) which seems to be pointing in the direction of this never being a good idea (PyPy, a Python implementation written in Python, is already raising syntax warnings.) Bottom line, until the developers of CPython tell us otherwise: Don't put yield in a generator expression or comprehension. In Python 2: In a generator function, the return statement is not allowed to include an expression_list. In that context, a bare return indicates that the generator is done and will cause StopIteration to be raised. An expression_list is basically any number of expressions separated by commas - essentially, in Python 2, you can stop the generator with return, but you can't return a value. In Python 3: In a generator function, the return statement indicates that the generator is done and will cause StopIteration to be raised. The returned value (if any) is used as an argument to construct StopIteration and becomes the StopIteration.value attribute. The languages CLU, Sather, and Icon were referenced in the proposal to introduce the concept of generators to Python. The general idea is that a function can maintain internal state and yield intermediate data points on demand by the user. This promised to be superior in performance to other approaches, including Python threading, which isn't even available on some systems.  This means, for example, that range objects aren't Iterators, even though they are iterable, because they can be reused. Like lists, their __iter__ methods return iterator objects. yield was originally introduced as a statement, meaning that it could only appear at the beginning of a line in a code block. Now yield creates a yield expression. https://docs.python.org/2/reference/simple_stmts.html#grammar-token-yield_stmt This change was proposed to allow a user to send data into the generator just as one might receive it. To send data, one must be able to assign it to something, and for that, a statement just won't work. yield is just like return - it returns whatever you tell it to (as a generator). The difference is that the next time you call the generator, execution starts from the last call to the yield statement. Unlike return, the stack frame is not cleaned up when a yield occurs, however control is transferred back to the caller, so its state will resume the next time the function is called. In the case of your code, the function get_child_candidates is acting like an iterator so that when you extend your list, it adds one element at a time to the new list. list.extend calls an iterator until it's exhausted. In the case of the code sample you posted, it would be much clearer to just return a tuple and append that to the list. There's one extra thing to mention: a function that yields doesn't actually have to terminate. I've written code like this: Then I can use it in other code like this: It really helps simplify some problems, and makes some things easier to work with.  For those who prefer a minimal working example, meditate on this interactive Python session: TL;DR Whenever you find yourself building a list from scratch, yield each piece instead.  This was my first "aha" moment with yield. yield is a sugary way to say  build a series of stuff Same behavior: Different behavior: Yield is single-pass: you can only iterate through once. When a function has a yield in it we call it a generator function. And an iterator is what it returns. Those terms are revealing. We lose the convenience of a container, but gain the power of a series that's computed as needed, and arbitrarily long. Yield is lazy, it puts off computation. A function with a yield in it doesn't actually execute at all when you call it. It returns an iterator object that remembers where it left off. Each time you call next() on the iterator (this happens in a for-loop) execution inches forward to the next yield. return raises StopIteration and ends the series (this is the natural end of a for-loop). Yield is versatile. Data doesn't have to be stored all together, it can be made available one at a time. It can be infinite. If you need multiple passes and the series isn't too long, just call list() on it: Brilliant choice of the word yield because both meanings apply: yield — produce or provide (as in agriculture) ...provide the next data in the series. yield — give way or relinquish (as in political power) ...relinquish CPU execution until the iterator advances. Yield gives you a generator.  As you can see, in the first case foo holds the entire list in memory at once. It's not a big deal for a list with 5 elements, but what if you want a list of 5 million? Not only is this a huge memory eater, it also costs a lot of time to build at the time that the function is called. In the second case, bar just gives you a generator. A generator is an iterable--which means you can use it in a for loop, etc, but each value can only be accessed once. All the values are also not stored in memory at the same time; the generator object "remembers" where it was in the looping the last time you called it--this way, if you're using an iterable to (say) count to 50 billion, you don't have to count to 50 billion all at once and store the 50 billion numbers to count through. Again, this is a pretty contrived example, you probably would use itertools if you really wanted to count to 50 billion. :) This is the most simple use case of generators. As you said, it can be used to write efficient permutations, using yield to push things up through the call stack instead of using some sort of stack variable. Generators can also be used for specialized tree traversal, and all manner of other things. It's returning a generator. I'm not particularly familiar with Python, but I believe it's the same kind of thing as C#'s iterator blocks if you're familiar with those. The key idea is that the compiler/interpreter/whatever does some trickery so that as far as the caller is concerned, they can keep calling next() and it will keep returning values - as if the generator method was paused. Now obviously you can't really "pause" a method, so the compiler builds a state machine for you to remember where you currently are and what the local variables etc look like. This is much easier than writing an iterator yourself. There is one type of answer that I don't feel has been given yet, among the many great answers that describe how to use generators. Here is the programming language theory answer: The yield statement in Python returns a generator. A generator in Python is a function that returns continuations (and specifically a type of coroutine, but continuations represent the more general mechanism to understand what is going on). Continuations in programming languages theory are a much more fundamental kind of computation, but they are not often used, because they are extremely hard to reason about and also very difficult to implement. But the idea of what a continuation is, is straightforward: it is the state of a computation that has not yet finished. In this state, the current values of variables, the operations that have yet to be performed, and so on, are saved. Then at some point later in the program the continuation can be invoked, such that the program's variables are reset to that state and the operations that were saved are carried out. Continuations, in this more general form, can be implemented in two ways. In the call/cc way, the program's stack is literally saved and then when the continuation is invoked, the stack is restored. In continuation passing style (CPS), continuations are just normal functions (only in languages where functions are first class) which the programmer explicitly manages and passes around to subroutines. In this style, program state is represented by closures (and the variables that happen to be encoded in them) rather than variables that reside somewhere on the stack. Functions that manage control flow accept continuation as arguments (in some variations of CPS, functions may accept multiple continuations) and manipulate control flow by invoking them by simply calling them and returning afterwards. A very simple example of continuation passing style is as follows: In this (very simplistic) example, the programmer saves the operation of actually writing the file into a continuation (which can potentially be a very complex operation with many details to write out), and then passes that continuation (i.e, as a first-class closure) to another operator which does some more processing, and then calls it if necessary. (I use this design pattern a lot in actual GUI programming, either because it saves me lines of code or, more importantly, to manage control flow after GUI events trigger.) The rest of this post will, without loss of generality, conceptualize continuations as CPS, because it is a hell of a lot easier to understand and read.  Now let's talk about generators in Python. Generators are a specific subtype of continuation. Whereas continuations are able in general to save the state of a computation (i.e., the program's call stack), generators are only able to save the state of iteration over an iterator. Although, this definition is slightly misleading for certain use cases of generators. For instance: This is clearly a reasonable iterable whose behavior is well defined -- each time the generator iterates over it, it returns 4 (and does so forever). But it isn't probably the prototypical type of iterable that comes to mind when thinking of iterators (i.e., for x in collection: do_something(x)). This example illustrates the power of generators: if anything is an iterator, a generator can save the state of its iteration. To reiterate: Continuations can save the state of a program's stack and generators can save the state of iteration. This means that continuations are more a lot powerful than generators, but also that generators are a lot, lot easier. They are easier for the language designer to implement, and they are easier for the programmer to use (if you have some time to burn, try to read and understand this page about continuations and call/cc). But you could easily implement (and conceptualize) generators as a simple, specific case of continuation passing style: Whenever yield is called, it tells the function to return a continuation.  When the function is called again, it starts from wherever it left off. So, in pseudo-pseudocode (i.e., not pseudocode, but not code) the generator's next method is basically as follows: where the yield keyword is actually syntactic sugar for the real generator function, basically something like: Remember that this is just pseudocode and the actual implementation of generators in Python is more complex. But as an exercise to understand what is going on, try to use continuation passing style to implement generator objects without use of the yield keyword. Here is an example in plain language. I will provide a correspondence between high-level human concepts to low-level Python concepts. I want to operate on a sequence of numbers, but I don't want to bother my self with the creation of that sequence, I want only to focus on the operation I want to do. So, I do the following: This is what a generator does (a function that contains a yield); it starts executing, pauses whenever it does a yield, and when asked for a .next() value it continues from the point it was last. It fits perfectly by design with the iterator protocol of Python, which describes how to sequentially request values. The most famous user of the iterator protocol is the for command in Python. So, whenever you do a: it doesn't matter if sequence is a list, a string, a dictionary or a generator object like described above; the result is the same: you read items off a sequence one by one. Note that defining a function which contains a yield keyword is not the only way to create a generator; it's just the easiest way to create one. For more accurate information, read about iterator types, the yield statement and generators in the Python documentation. While a lot of answers show why you'd use a yield to create a generator, there are more uses for yield.  It's quite easy to make a coroutine, which enables the passing of information between two blocks of code.  I won't repeat any of the fine examples that have already been given about using yield to create a generator. To help understand what a yield does in the following code, you can use your finger to trace the cycle through any code that has a yield.  Every time your finger hits the yield, you have to wait for a next or a send to be entered.  When a next is called, you trace through the code until you hit the yield… the code on the right of the yield is evaluated and returned to the caller… then you wait.  When next is called again, you perform another loop through the code.  However, you'll note that in a coroutine, yield can also be used with a send… which will send a value from the caller into the yielding function. If a send is given, then yield receives the value sent, and spits it out the left hand side… then the trace through the code progresses until you hit the yield again (returning the value at the end, as if next was called). For example: There is another yield use and meaning (since Python 3.3): From PEP 380 -- Syntax for Delegating to a Subgenerator: A syntax is proposed for a generator to delegate part of its operations to another generator. This allows a section of code containing 'yield' to be factored out and placed in another generator. Additionally, the subgenerator is allowed to return with a value, and the value is made available to the delegating generator. The new syntax also opens up some opportunities for optimisation when one generator re-yields values produced by another. Moreover this will introduce (since Python 3.5): to avoid coroutines being confused with a regular generator (today yield is used in both). All great answers, however a bit difficult for newbies. I assume you have learned the return statement. As an analogy, return and yield are twins. return means 'return and stop' whereas 'yield` means 'return, but continue' Run it: See, you get only a single number rather than a list of them. return never allows you prevail happily, just implements once and quit. Replace return with yield: Now, you win to get all the numbers. Comparing to return which runs once and stops, yield runs times you planed. You can interpret return as return one of them, and yield as return all of them. This is called iterable. It's the core about yield. The difference between a list return outputs and the object yield output is: You will always get [0, 1, 2] from a list object but only could retrieve them from 'the object yield output' once. So, it has a new name generator object as displayed in Out[11]: <generator object num_list at 0x10327c990>. In conclusion, as a metaphor to grok it: From a programming viewpoint, the iterators are implemented as thunks. To implement iterators, generators, and thread pools for concurrent execution, etc. as thunks, one uses messages sent to a closure object, which has a dispatcher, and the dispatcher answers to "messages". "next" is a message sent to a closure, created by the "iter" call. There are lots of ways to implement this computation. I used mutation, but it is possible to do this kind of computation without mutation, by returning the current value and the next yielder (making it referential transparent).  Racket uses a sequence of transformations of the initial program in some intermediary languages, one of such rewriting making the yield operator to be transformed in some language with simpler operators. Here is a demonstration of how yield could be rewritten, which uses the structure of R6RS, but the semantics is identical to Python's. It's the same model of computation, and only a change in syntax is required to rewrite it using yield of Python. Here are some Python examples of how to actually implement generators as if Python did not provide syntactic sugar for them: As a Python generator: Using lexical closures instead of generators Using object closures instead of generators (because ClosuresAndObjectsAreEquivalent) I was going to post "read page 19 of Beazley's 'Python: Essential Reference' for a quick description of generators", but so many others have posted good descriptions already. Also, note that yield can be used in coroutines as the dual of their use in generator functions.  Although it isn't the same use as your code snippet, (yield) can be used as an expression in a function.  When a caller sends a value to the method using the send() method, then the coroutine will execute until the next (yield) statement is encountered. Generators and coroutines are a cool way to set up data-flow type applications.  I thought it would be worthwhile knowing about the other use of the yield statement in functions. Here is a simple example: Output: I am not a Python developer, but it looks to me yield holds the position of program flow and the next loop start from "yield" position. It seems like it is waiting at that position, and just before that, returning a value outside, and next time continues to work. It seems to be an interesting and nice ability :D Here is a mental image of what yield does. I like to think of a thread as having a stack (even when it's not implemented that way). When a normal function is called, it puts its local variables on the stack, does some computation, then clears the stack and returns. The values of its local variables are never seen again. With a yield function, when its code begins to run (i.e. after the function is called, returning a generator object, whose next() method is then invoked), it similarly puts its local variables onto the stack and computes for a while. But then, when it hits the yield statement, before clearing its part of the stack and returning, it takes a snapshot of its local variables and stores them in the generator object. It also writes down the place where it's currently up to in its code (i.e. the particular yield statement). So it's a kind of a frozen function that the generator is hanging onto. When next() is called subsequently, it retrieves the function's belongings onto the stack and re-animates it. The function continues to compute from where it left off, oblivious to the fact that it had just spent an eternity in cold storage. Compare the following examples: When we call the second function, it behaves very differently to the first. The yield statement might be unreachable, but if it's present anywhere, it changes the nature of what we're dealing with. Calling yielderFunction() doesn't run its code, but makes a generator out of the code. (Maybe it's a good idea to name such things with the yielder prefix for readability.) The gi_code and gi_frame fields are where the frozen state is stored. Exploring them with dir(..), we can confirm that our mental model above is credible. An easy example to understand what it is: yield The output is:  Like every answer suggests, yield is used for creating a sequence generator. It's used for generating some sequence dynamically. For example, while reading a file line by line on a network, you can use the yield function as follows: You can use it in your code as follows: Execution Control Transfer gotcha The execution control will be transferred from getNextLines() to the for loop when yield is executed. Thus, every time getNextLines() is invoked, execution begins from the point where it was paused last time. Thus in short, a function with the following code will print (My below answer only speaks from the perspective of using Python generator, not the underlying implementation of generator mechanism, which involves some tricks of stack and heap manipulation.) When yield is used instead of a return in a python function, that function is turned into something special called generator function. That function will return an object of generator type. The yield keyword is a flag to notify the python compiler to treat such function specially. Normal functions will terminate once some value is returned from it. But with the help of the compiler, the generator function can be thought of as resumable. That is, the execution context will be restored and the execution will continue from last run. Until you explicitly call return, which will raise a StopIteration exception (which is also part of the iterator protocol), or reach the end of the function. I found a lot of references about generator but this one from the functional programming perspective is the most digestable. (Now I want to talk about the rationale behind generator, and the iterator based on my own understanding. I hope this can help you grasp the essential motivation of iterator and generator. Such concept shows up in other languages as well such as C#.) As I understand, when we want to process a bunch of data, we usually first store the data somewhere and then process it one by one. But this naive approach is problematic. If the data volume is huge, it's expensive to store them as a whole beforehand. So instead of storing the data itself directly, why not store some kind of metadata indirectly, i.e. the logic how the data is computed.  There are 2 approaches to wrap such metadata. Either way, an iterator is created, i.e. some object that can give you the data you want. The OO approach may be a bit complex. Anyway, which one to use is up to you. In summary, the yield statement transforms your function into a factory that produces a special object called a generator which wraps around the body of your original function. When the generator is iterated, it executes your function  until it reaches the next yield then suspends execution and evaluates to the value passed to yield. It repeats this process on each iteration until the path of execution exits the function. For instance, simply outputs The power comes from using the generator with a loop that calculates a sequence, the generator executes the loop stopping each time to 'yield' the next result of the calculation, in this way it calculates a list on the fly, the benefit being the memory saved for especially large calculations Say you wanted to create a your own range function that produces an iterable range of numbers, you could do it like so, and use it like this; But this is inefficient because Luckily Guido and his team were generous enough to develop generators so we could just do this; Now upon each iteration a function on the generator called next() executes the function until it either reaches a 'yield' statement in which it stops and  'yields' the value or reaches the end of the function. In this case on the first call, next() executes up to the yield statement and yield 'n', on the next call it will execute the  increment statement, jump back to the 'while', evaluate it, and if true, it will stop and yield 'n' again, it will continue that way until the while condition returns false and the generator jumps to the end of the function. Yield is an object A return in a function will return a single value. If you want a function to return a huge set of values, use yield. More importantly, yield is a barrier. like barrier in the CUDA language, it will not transfer control until it gets   completed. That is, it will run the code in your function from the beginning until it hits yield. Then, it’ll return the first value of the loop. Then, every other call will run the loop you have written in the function one more time, returning the next value until there isn't any value to return. Many people use return rather than yield, but in some cases yield can be more efficient and easier to work with. Here is an example which yield is definitely best for: return (in function) yield (in function) Calling functions Both functions do the same thing, but yield uses three lines instead of five and has one less variable to worry about. This is the result from the code:  As you can see both functions do the same thing. The only difference is return_dates() gives a list and yield_dates() gives a generator. A real life example would be something like reading a file line by line or if you just want to make a generator. yield is like a return element for a function. The difference is, that the yield element turns a function into a generator. A generator behaves just like a function until something is 'yielded'. The generator stops until it is next called, and continues from exactly the same point as it started. You can get a sequence of all the 'yielded' values in one, by calling list(generator()). The yield keyword simply collects returning results. Think of yield like return += Here's a simple yield based approach, to compute the fibonacci series, explained: When you enter this into your REPL and then try and call it, you'll get a mystifying result: This is because the presence of yield signaled to Python that you want to create a generator, that is, an object that generates values on demand. So, how do you generate these values? This can either be done directly by using the built-in function next, or, indirectly by feeding it to a construct that consumes values.  Using the built-in next() function, you directly invoke .next/__next__, forcing the generator to produce a value: Indirectly, if you provide fib to a for loop, a list initializer, a tuple initializer, or anything else that expects an object that generates/produces values, you'll "consume" the generator until no more values can be produced by it (and it returns): Similarly, with a tuple initializer:  A generator differs from a function in the sense that it is lazy. It accomplishes this by maintaining it's local state and allowing you to resume whenever you need to.  When you first invoke fib by calling it: Python compiles the function, encounters the yield keyword and simply returns a generator object back at you. Not very helpful it seems.  When you then request it generates the first value, directly or indirectly, it executes all statements that it finds, until it encounters a yield, it then yields back the value you supplied to yield and pauses. For an example that better demonstrates this, let's use some print calls (replace with print "text" if on Python 2): Now, enter in the REPL: you have a generator object now waiting for a command for it to generate a value. Use next and see what get's printed: The unquoted results are what's printed. The quoted result is what is returned from yield. Call next again now: The generator remembers it was paused at yield value and resumes from there. The next message is printed and the search for the yield statement to pause at it performed again (due to the while loop).
__label__json __label__content-type __label__http-headers I've been messing around with JSON for some time, just pushing it out as text and it hasn't hurt anybody (that I know of), but I'd like to start doing things properly. I have seen so many purported "standards" for the JSON content type: But which one is correct, or best? I gather that there are security and browser support issues varying between them. I know there's a similar question, What MIME type if JSON is being returned by a REST API?, but I'd like a slightly more targeted answer. For JSON text: application/json The MIME media type for JSON text is application/json. The default encoding is UTF-8. (Source: RFC 4627). For JSONP (runnable JavaScript) with callback: application/javascript Here are some blog posts that were mentioned in the relevant comments: IANA has registered the official MIME Type for JSON as application/json. When asked about why not text/json, Crockford seems to have said JSON is not really JavaScript nor text and also IANA was more likely to hand out application/* than text/*. More resources: For JSON: For JSON-P: Of course, the correct MIME media type for JSON is application/json, but it's necessary to realize what type of data is expected in your application. For example, I use Ext GWT and the server response must go as text/html but contains JSON data. Client side, Ext GWT form listener In case of using application/json response type, the browser suggests me to save the file. Server side source code snippet using Spring MVC Response is dynamically generated data, according to the query parameters passed in the URL. Example: Content-Type: application/json JSON with padding. Response is JSON data, with a function call wrapped around it. Example: Content-Type: application/javascript If you are using Ubuntu or Debian and you serve .json files through Apache, you might want to serve the files with the correct content type. I am doing this primarily because I want to use the Firefox extension JSONView The Apache module mod_mime will help to do this easily. However, with Ubuntu you need to edit the file /etc/mime.types and add the line Then restart Apache: If you're calling ASP.NET Web Services from the client-side you have to use application/json for it to work. I believe this is the same for the jQuery and Ext frameworks.  The right content type for JSON is application/json UNLESS you're using JSONP, also known as JSON with Padding, which is actually JavaScript and so the right content type would be application/javascript. There is no doubt that application/json is the best MIME type for a JSON response. But I had some experience where I had to use application/x-javascript because of some compression issues. My hosting environment is shared hosting with GoDaddy. They do not allow me to change server configurations. I had added the following code to my web.config file for compressing responses. By using this, the .aspx pages was compressed with g-zip but JSON responses were not. I added in the static and dynamic types sections. But this does not compress JSON responses at all. After that I removed this newly added type and added in both the static and dynamic types sections, and changed the response type in .ashx (asynchronous handler) to And now I found that my JSON responses were compressed with g-zip. So I personally recommend to use only if you want to compress your JSON responses on a shared hosting environment. Because in shared hosting, they do not allow you to change IIS configurations. Only when using application/json as the MIME type I have the following (as of November 2011 with the most recent versions of Chrome, Firefox with Firebug): Not everything works for content type application/json. If you are using Ext JS form submit to upload file, be aware that the server response is parsed by the browser to create the document for the <iframe>. If the server is using JSON to send the return object, then the Content-Type header must be set to text/html in order to tell the browser to insert the text unchanged into the document body. See the Ext JS 3.4.0 API documentation. JSON is a domain-specific language (DSL) and a data format independent of JavaScript, and as such has its own MIME type, application/json. Respect for MIME types is of course client driven, so text/plain may do for transfer of bytes, but then you would be pushing up interpretation to the vendor application domain unnecessarily - application/json. Would you transfer XML via text/plain? But honestly, your choice of MIME type is advice to the client as to how to interpret the data- text/plain or text/HTML (when it's not HTML) is like type erasure- it's as uninformative as making all your objects of type Object in a typed language. No browser runtime I know of will take a JSON document and automatically make it available to the runtime as a JavaScript accessible object without intervention, but if you are working with a crippled client, that's an entirely different matter. But that's not the whole story- RESTful JSON services often don't have JavaScript runtimes, but it doesn't stop them using JSON as a viable data interchange format. If clients are that crippled... then I would consider perhaps HTML injection via an Ajax templating service instead. Application/JSON! If you're in a client-side environment, investigating about the cross-browser support is mandatory for a well supported web application. The right HTTP Content-Type would be application/json, as others already highlighted too, but some clients do not handle it very well, that's why jQuery recommends the default text/html. The correct answer is: As many others have mentioned, application/json is the correct answer. But what haven't been explained yet is what the other options you proposed mean. application/x-javascript: Experimental MIME type for JavaScript before application/javascript was made standard. text/javascript: Now obsolete. You should use application/javascript when using javascript. text/x-javascript: Experimental MIME type for the above situation. text/x-json: Experimental MIME type for JSON before application/json got officially registered. All in all, whenever you have any doubts about content types, you should check this link  In JSP, you can use this in page directive: The correct MIME media type for JSON is application/json.  JSP will use it for sending a response to the client. “application/json” is the correct JSON content type. The IANA registration for application/json says Applications that use this media type:  JSON has been used to      exchange data between applications written in all of these      programming languages: ActionScript, C, C#, Clojure, ColdFusion,      Common Lisp, E, Erlang, Go, Java, JavaScript, Lua, Objective CAML,      Perl, PHP, Python, Rebol, Ruby, Scala, and Scheme. You'll notice that IANA.org doesn't list any of these other media types, in fact even application/javascript is now obsolete. So application/json is really the only possible correct answer.  Browser support is another thing.  The most widely supported non-standard media types are text/json or text/javascript. But some big names even use text/plain.  Even more strange is the Content-Type header sent by Flickr, who returns JSON as text/xml. Google uses text/javascript for some of it's ajax apis. Examples: Output: Content-Type: text/javascript Output: Content-Type: text/xml The right MIME type is application/json BUT I experienced many situations where the browser type or the framework user needed: I use the below The Content-Type header should be set to 'application/json' when posting. Server listening for the request should include "Accept=application/json". In Spring MVC you can do it like this: Add headers to the response: The application/json works great in PHP to store an array or object   data. I use this code to put data in JSON on Google Cloud Storage (GCS) which is set publically viewable: To get back the data is straight forward: In Spring you have a defined type: MediaType.APPLICATION_JSON_VALUE which is equivalent to application/json. For JSON, I am using: This is described in the IETF's JSON Data Interchange Format 7158 proposal, Section 1.2: Specifications of JSON. If the JSON is with padding then it will be application/jsonp. If the JSON is without padding then it will be application/json. To deal with both, it is a good practice to use: 'application/javascript' without bothering whether it is with padding or without padding. PHP developers use this: Extending the accepted responses, when you are using JSON in a REST context... There is a strong argument about using application/x-resource+json and application/x-collection+json when you are representing REST resources and collections. And if you decide to follow the jsonapi specification, you should use of application/vnd.api+json, as it is documented. Altough there is not an universal standard, it is clear that the added semantic to the resources being transfered justify a more explicit Content-Type than just application/json. Following this reasoning, other contexts could justify a more specific Content-Type. If you get data from REST API in JSON so you have to use content-type  Content-Type: application/json - json Content-Type: application/javascript - json-P Content-Type: application/x-javascript - javascript Content-Type: text/javascript - javascript BUT obsolete, older IE versions used to use as html attribute. Content-Type: text/x-javascript - JavaScript Media Types BUT obsolete Content-Type: text/x-json - json before application/json got officially registered. JSON (JavaScript Object Notation) and JSONP ("JSON with padding") formats seems to be very similar and therefore it might be very confusing which MIME type they should be using. Even though the formats are similar, there are some subtle differences between them. So whenever in any doubts, I have a very simple approach (which works perfectly fine in most cases), namely, go and check corresponding RFC document. JSON RFC 4627 (The application/json Media Type for JavaScript Object Notation (JSON)) is a specifications of JSON format. It says in section 6, that the MIME media type for JSON text is  JSONP JSONP ("JSON with padding") is handled different way than JSON, in a browser. JSONP is treated as a regular JavaScript script and therefore it should use application/javascript, the current official MIME type for JavaScript. In many cases, however, text/javascript MIME type will work fine too. Note that text/javascript has been marked as obsolete by RFC 4329 (Scripting Media Types) document and it is recommended to use application/javascript type instead. However, due to legacy reasons, text/javascript is still widely used and it has cross-browser support (which is not always a case with application/javascript MIME type, especially with older browsers).
__label__git I mistakenly added files to Git using the command: I have not yet run git commit. Is there a way to undo this, so these files won't be included in the commit? You can undo git add before commit with which will remove it from the current index (the "about to be committed" list) without changing anything else. You can use without any file name to unstage all due changes. This can come in handy when there are too many files to be listed one by one in a reasonable amount of time. In old versions of Git, the above commands are equivalent to git reset HEAD <file> and git reset HEAD respectively, and will fail if HEAD is undefined (because you haven't yet made any commits in your repository) or ambiguous (because you created a branch called HEAD, which is a stupid thing that you shouldn't do). This was changed in Git 1.8.2, though, so in modern versions of Git you can use the commands above even prior to making your first commit: "git reset" (without options or parameters) used to error out when you do not have any commits in your history, but it now gives you an empty index (to match non-existent commit you are not even on). Documentation: git reset You want: Reasoning: When I was new to this, I first tried (to undo my entire initial add), only to get this (not so) helpful message: It turns out that this is because the HEAD ref (branch?) doesn't exist until after the first commit. That is, you'll run into the same beginner's problem as me if your workflow, like mine, was something like: git status ... lots of crap scrolls by ... => Damn, I didn't want to add all of that. google "undo git add" => find Stack Overflow - yay git reset . =>    fatal: Failed to resolve 'HEAD' as a valid ref. It further turns out that there's a bug logged against the unhelpfulness of this in the mailing list. And that the correct solution was right there in the Git status output (which, yes, I glossed over as 'crap) And the solution indeed is to use git rm --cached FILE. Note the warnings elsewhere here - git rm deletes your local working copy of the file, but not if you use --cached.  Here's the result of git help rm: --cached       Use this option to unstage and remove paths only from the index.       Working tree files, whether modified or not, will be left. I proceed to use to remove everything and start again. Didn't work though, because while add . is recursive, turns out rm needs -r to recurse. Sigh. Okay, now I'm back to where I started. Next time I'm going to use -n to do a dry run and see what will be added: I zipped up everything to a safe place before trusting git help rm about the --cached not destroying anything (and what if I misspelled it). If you type: Git will tell you what is staged, etc., including instructions on how to unstage: I find Git does a pretty good job of nudging me to do the right thing in situations like this. Note: Recent Git versions (1.8.4.x) have changed this message: To clarify: git add moves changes from the current working directory to the staging area (index). This process is called staging. So the most natural command to stage the changes (changed files) is the obvious one: git add is just an easier-to-type alias for git stage Pity there is no git unstage nor git unadd commands. The relevant one is harder to guess or remember, but it is pretty obvious: We can easily create an alias for this: And finally, we have new commands: Personally I use even shorter aliases: An addition to the accepted answer, if your mistakenly-added file was huge, you'll probably notice that, even after removing it from the index with 'git reset', it still seems to occupy space in the .git directory. This is nothing to be worried about; the file is indeed still in the repository, but only as a "loose object". It will not be copied to other repositories (via clone, push), and the space will be eventually reclaimed - though perhaps not very soon. If you are anxious, you can run: Update (what follows is my attempt to clear some confusion that can arise from the most upvoted answers): So, which is the real undo of git add? git reset HEAD <file> ? or git rm --cached <file>? Strictly speaking, and if I'm not mistaken: none. git add cannot be undone - safely, in general. Let's recall first what git add <file> actually does: If <file> was not previously tracked, git add adds it to the cache, with its current content. If <file> was already tracked, git add saves the current content (snapshot, version) to the cache. In Git, this action is still called add, (not mere update it), because two different versions (snapshots) of a file are regarded as two different items: hence, we are indeed adding a new item to the cache, to be eventually committed later. In light of this, the question is slightly ambiguous: I mistakenly added files using the command... The OP's scenario seems to be the first one (untracked file),  we want the "undo" to remove the file (not just the current contents) from the tracked items. If this is the case, then it's ok to run  git rm --cached <file>. And we could also run git reset HEAD <file>. This is in general preferable, because it works in both scenarios: it also does the undo when we wrongly added a version of an already tracked item. But there are two caveats. First: There is (as pointed out in the answer) only one scenario in which git reset HEAD doesn't work, but git rm --cached does: a new repository (no commits). But, really, this a practically irrelevant case. Second: Be aware that git reset HEAD  can't magically recover the previously cached file contents, it just resynchronises it from the HEAD. If our misguided git add overwrote a previous staged uncommitted version, we can't recover it. That's why, strictly speaking, we cannot undo [*]. Example: Of course, this is not very critical if we just follow the usual lazy workflow of doing 'git add' only for adding new files (case 1), and we update new contents via the commit, git commit -a command.  * (Edit: the above is practically correct, but still there can be some slightly hackish/convoluted ways for recovering changes that were staged, but not committed and then overwritten - see the comments by Johannes Matokic and iolsmit)  Undo a file which has already been added is quite easy using Git. For resetting myfile.txt, which have already been added, use: Explanation: After you staged unwanted file(s), to undo, you can do git reset. Head is head of your file in the local and the last parameter is the name of your file. I have created the steps in the image below in more details for you, including all steps which may happen in these cases:  will "un-add" everything you've added from your current directory recursively  Run and remove all the files manually or by selecting all of them and clicking on the unstage from commit button. Git has commands for every action imaginable, but it needs extensive knowledge to get things right and because of that it is counter-intuitive at best... What you did before: What you want: Remove the file from the index, but keep it versioned and left with uncommitted changes in working copy: Reset the file to the last state from HEAD, undoing changes and removing them from the index: This is needed since git reset --hard HEAD won't work with single files. Remove <file> from index and versioning, keeping the un-versioned file with changes in working copy: Remove <file> from working copy and versioning completely: The question is not clearly posed. The reason is that git add has two meanings: If in doubt, use Because it does the expected thing in both cases. Warning: if you do git rm --cached file on a file that was modified (a file that existed before in the repository), then the file will be removed on git commit! It will still exist in your file system, but if anybody else pulls your commit, the file will be deleted from their work tree. git status will tell you if the file was a new file or modified: If you're on your initial commit and you can't use git reset, just declare "Git bankruptcy" and delete the .git folder and start over As per many of the other answers, you can use git reset BUT: I found this great little post that actually adds the Git command (well, an alias) for git unadd: see git unadd for details or.. Simply, Now you can Use git add -i to remove just-added files from your upcoming commit.  Example: Adding the file you didn't want: Going into interactive add to undo your add (the commands typed at git here are "r" (revert), "1" (first entry in the list revert shows), 'return' to drop out of revert mode, and "q" (quit): That's it!  Here's your proof, showing that "foo" is back on the untracked list: git remove or git rm can be used for this, with the --cached flag. Try: Here's a way to avoid this vexing problem when you start a new project: Git makes it really hard to do git reset if you don't have any commits.  If you create a tiny initial commit just for the sake of having one, after that you can git add -A and git reset as many times as you want in order to get everything right. Another advantage of this method is that if you run into line-ending troubles later and need to refresh all your files, it's easy: Note that if you fail to specify a revision then you have to include a separator. Example from my console: (Git version 1.7.5.4) Maybe Git has evolved since you posted your question. Now, you can try: This should be what you are looking for. To remove new files from the staging area (and only in case of a new file), as suggested above: Use rm --cached only for new files accidentally added. To reset every file in a particular folder (and its subfolders), you can use the following command: Use the * command to handle multiple files at a time: etc. Just type git reset it will revert back and it is like you never typed git add . since your last commit. Make sure you have committed before.  Suppose I create a new file, newFile.txt:  Suppose I add the file accidentally, git add newFile.txt:  Now I want to undo this add, before commit, git reset newFile.txt:  For a specific file: For all added files: Note: checkout changes the code in the files and moves to the last updated (committed) state. reset doesn't change the codes; it just resets the header. To undo git add, use: There is also interactive mode: Choose option 3 to un add files. In my case I often want to add more than one file, and with interactive mode you can use numbers like this to add files. This will take all but 4: 1, 2, 3, and 5 To choose a sequence, just type 1-5 to take all from 1 to 5. Git staging files This command will unstash your changes: You can also use  to add parts of files. git add myfile.txt # This will add your file into the to-be-committed list Quite opposite to this command is, so, you will be in the previous state. Specified will be again in untracked list (previous state). It will reset your head with that specified file. so, if your head doesn't have it means, it will simply reset it. Will remove a file named filename.txt from the current index, the "about to be committed" area, without changing anything else. Will remove a file named filename.txt from the current index, the "about to be committed" area, without changing anything else. In Sourcetree you can do this easily via the GUI. You can check which command Sourcetree uses to unstage a file. I created a new file and added it to Git. Then I unstaged it using the Sourcetree GUI. This is the result: Unstaging files [08/12/15 10:43]   git -c diff.mnemonicprefix=false -c core.quotepath=false -c credential.helper=sourcetree reset -q -- path/to/file/filename.java Sourcetree uses reset to unstage new files.
__label__code-formatting __label__c __label__operators __label__standards-compliance __label__c++ After reading Hidden Features and Dark Corners of C++/STL on comp.lang.c++.moderated, I was completely surprised that the following snippet compiled and worked in both Visual Studio 2008 and G++ 4.4. Here's the code: Output: I'd assume this is C, since it works in GCC as well. Where is this defined in the standard, and where has it come from? --> is not an operator. It is in fact two separate operators, -- and >. The conditional's code decrements x, while returning x's original (not decremented) value, and then compares the original value with 0 using the > operator. To better understand, the statement could be written as follows: Or for something completely different... x slides to 0. Not so mathematical, but... every picture paints a thousand words... That's a very complicated operator, so even ISO/IEC JTC1 (Joint Technical Committee 1) placed its description in two different parts of the C++ Standard. Joking aside, they are two different operators: -- and > described respectively in §5.2.6/2 and §5.9 of the C++03 Standard. It's equivalent to x-- (post decrement) is equivalent to x = x-1 so, the code transforms to: x can go to zero even faster in the opposite direction: 8 6 4 2 You can control speed with an arrow! 90 80 70 60 50 40 30 20 10 ;) It's Just the space makes the things look funny, -- decrements and > compares. The usage of --> has historical relevance. Decrementing was (and still is in some cases), faster than incrementing on the x86 architecture. Using --> suggests that x is going to 0, and appeals to those with mathematical backgrounds. is how that's parsed. Utterly geek, but I will be using this: One book I read (I don't remember correctly which book) stated: Compilers try to parse expressions to the biggest token by using the left right rule. In this case, the expression: Parses to biggest tokens:  The same rule applies to this expression: After parse: I hope this helps to understand the complicated expression ^^ This is exactly the same as for non-negative numbers Anyway, we have a "goes to" operator now. "-->" is easy to be remembered as a direction, and "while x goes to zero" is meaning-straight. Furthermore, it is a little more efficient than "for (x = 10; x > 0; x --)" on some platforms. This code first compares x and 0 and then decrements x. (Also said in the first answer: You're post-decrementing x and then comparing x and 0 with the > operator.) See the output of this code: We now first compare and then decrement by seeing 0 in the output. If we want to first decrement and then compare, use this code: That output is: My compiler will print out 9876543210 when I run this code. As expected. The while( x-- > 0 ) actually means while( x > 0). The x-- post decrements x. is a different way of writing the same thing. It is nice that the original looks like "while x goes to 0" though. There is a space missing between -- and >. x is post decremented, that is, decremented after checking the condition x>0 ?. -- is the decrement operator and > is the greater-than operator. The two operators are applied as a single one like -->. It's a combination of two operators. First -- is for decrementing the value, and > is for checking whether the value is greater than the right-hand operand. The output will be: Actually, x is post-decrementing and with that condition is being checked. It's not -->, it's (x--) > 0 Note: value of x is changed after the condition is checked, because it post-decrementing. Some similar cases can also occur, for example: C and C++ obey the "maximum munch" rule. The same way a---b is translated to (a--) - b, in your case  x-->0 translates to (x--)>0. What the rule says essentially is that going left to right, expressions are formed by taking the maximum of characters which will form an valid expression.  Why all the complication? The simple answer to the original question is just: It does the same thing. I am not saying you should do it like this, but it does the same thing and would have answered the question in one post. The x-- is just shorthand for the above, and > is just a normal greater-than operator. No big mystery! There are too many people making simple things complicated nowadays  ;) Conventional way we define condition in while loop parenthesis"()" and terminating condition inside the braces"{}", but this -- & > is a way one defines all at once. For example: It says, decrement a and run the loop till the time a is greater than 0 Other way it should have been like: Both ways, we do the same thing and achieve the same goals. (x --> 0) means (x-- > 0). Output:  9 8 7 6 5 4 3 2 1 Output: 9 8 7 6 5 4 3 2 1 0 Output: 9 8 7 6 5 4 3 2 1 0 Output: 9 8 7 6 5 4 3 2 1 0 Likewise, you can try lot of methods to execute this command successfully. Here -- is the unary post decrement operator. This --> is not an operator at all. We have an operator like ->, but not like -->. It is just a wrong interpretation of while(x-- >0) which simply means x has the post decrement operator and this loop will run till it is greater than zero. Another simple way of writing this code would be while(x--). The  while loop will stop whenever it gets a false condition and here there is only one case, i.e., 0. So it will stop when the x value is decremented to zero. For larger numbers, C++20 introduces some more advanced looping features. First to catch i we can build an inverse loop-de-loop and deflect it onto the std::ostream. However, the speed of i is implementation-defined, so we can use the new C++20 speed operator <<i<< to speed it up. We must also catch it by building wall, if we don't, i leaves the scope and de referencing it causes undefined behavior. To specify the separator, we can use: and there we have a for loop from 67 to 1.
__label__git __label__version-control __label__git-branch I don't want to rename a remote branch, as described in Rename master branch for both local and remote Git repositories. How can I rename a local branch which hasn't been pushed to a remote branch? In case you need to rename remote branch as well: How do I rename both a Git local and remote branch name If you want to rename a branch while pointed to any branch, do: If you want to rename the current branch, you can do: A way to remember this is -m is for "move" (or mv), which is how you rename files. Adding an alias could also help. To do so, run the following: If you are on Windows or another case-insensitive filesystem, and there are only capitalization changes in the name, you need to use -M, otherwise, git will throw branch already exists error: The above command will change your branch name, but you have to be very careful using the renamed branch, because it will still refer to the old upstream branch associated with it, if any. If you want to push some changes into master after your local branch is renamed into new_branch_name (example name): git push origin new_branch_name:master (now changes will go to master branch but your local branch name is new_branch_name) For more details, see "How to rename your local branch name in Git." To rename your current branch: Here are the steps to rename the branch: EDIT (12/01/2017): Make sure you run command git status and check that the newly created branch is pointing to its own ref and not the older one. If you find the reference to the older branch, you need to unset the upstream using: Rename the branch will be useful once your branch is finished. Then new stuff is coming, and you want to develop in the same branch instead of deleting it and create the new one. From my experience, to rename a local and remote branch in Git you should do the following steps. Quoting from Multiple States - Rename a local and remote branch in   git If you are on the branch you want to rename: If you are on a different branch: The answers so far have been correct, but here is some additional information: One can safely rename a branch with '-m' (move), but one has to be careful with '-M', because it forces the rename, even if there is an existing branch with the same name already. Here is the excerpt from the 'git-branch' man page: With a -m or -M option, <oldbranch> will be renamed to <newbranch>. If <oldbranch> had a corresponding reflog, it is renamed to match <newbranch>, and a reflog entry is created to remember the branch renaming. If <newbranch> exists, -M must be used to force the rename to happen. If it is your current branch, just do If it is another branch you want to rename - If your branch was pushed, then after renaming you need to delete it from the remote Git repository and ask your new local to track a new remote branch: I foolishly named a branch starting with a hyphen, and then checked out master.  I didn't want to delete my branch, I had work in it. Neither of these worked: git checkout -dumb-name git checkout -- -dumb-name "s, 's and \s didn't help either.  git branch -m doesn't work. Here's how I finally fixed it. Go into your working copy's .git/refs/heads, find the filename "-dumb-name", get the hash of the branch.  Then this will check it out, make a new branch with a sane name, and delete the old one. To rename a branch locally: Now you'll have to propagate these changes on your remote server as well. To push changes of the deleted old branch: To push changes of creation of new branch: Just three steps to replicate change in name on remote as well as on GitHub: Step 1 git branch -m old_branchname new_branchname Step 2 git push origin :old_branchname new_branchname Step 3 git push --set-upstream origin new_branchname Rename the branch using this command: -m: It renames/moves the branch. If there is already a branch, you will get an error. If there is already a branch and you want to rename with that branch, use: For more information about help, use this command in the terminal: or Advanced Git users can rename manually using: If you are on the branch you want to rename: If you are on a different branch: git push origin :old-name new-name git push origin -u new-name Or for a fast way to do that, you can use these 3 steps: # Rename branch locally  # Delete the old remote branch  # Push the new branch, set local branch to track the new remote Referance: https://www.w3docs.com/snippets/git/how-to-rename-git-local-and-remote-branches.html Here are three steps: A command that you can call inside your terminal and change branch name. If you need more: step-by-step, How To Change Git Branch Name is a good article about that. Probably as mentioned by others, this will be a case mismatch in branch naming. If you have such a situation, I can guess that you're on Windows which will also lead you to: Then you have to do an intermediate step: Nothing more. Trying to answer specifically to the question (at least the title). You can also rename local branch, but keeps tracking the old name on the remote. Now, when you run git push, the remote old_branch ref is updated with your local new_branch. You have to know and remember this configuration. But it can be useful if you don't have the choice for the remote branch name, but you don't like it (oh, I mean, you've got a very good reason not to like it !) and prefer a clearer name for your local branch. Playing with the fetch configuration, you can even rename the local remote-reference. i.e, having a refs/remote/origin/new_branch ref pointer to the branch, that is in fact the old_branch on origin. However, I highly discourage this, for the safety of your mind. Changing the branch locally is quite easy... If you are on the branch you want to change the name for, simply do this: Otherwise, if you are on master or any other branch other than the one you'd like to change the name, simply do: Also, I create the image below to show this in action on a command line. In this case, you are on master branch, for example:  To rename the current branch (except for detached HEAD state) you can also use this alias: If you are willing to use SourceTree (which I strongly recommend), you can right click your branch and chose 'Rename'.  Another option is not to use the command line at all. Git GUI clients such as SourceTree take away much of the syntactical learning curve / pain that causes questions such as this one to be amongst the most viewed on Stack Overflow. In SourceTree, right click on any local branch in the "Branches" pane on the left and select "Rename ...". A simple way to do it: For more, see this. Since you do not want to push the branch to a remote server, this example will be useful: Let's say you have an existing branch called "my-hot-feature," and you want to rename it to "feature-15." First, you want to change your local branch. This couldn't be easier: For more information, you can visit Locally and Remotely Renaming a Branch in Git. Git version 2.9.2 If you want to change the name of the local branch you are on: If you want to change the name of a different branch: If you want to change the name of a different branch to a name that already exists: Note: The last command is destructive and will rename your branch, but you will lose the old branch with that name and those commits because branch names must be unique. If you want to change the name of the current branch, run: If you want to delete the old remote branch, run: If you want to delete the old remote branch and create a new remote branch, run: Actually you have three steps because the local branch has a duplicate on the server so we have one step for local on two steps on the server: Git branch rename can be done by using: git branch -m oldBranch newBranch git branch -M oldBranch ExistingBranch The difference between -m and -M: -m: if you're trying to rename your branch with an existing branch name using -m. It will raise an error saying that the branch already exists. You need to give unique name. But, -M: this will help you to force rename with a given name, even it is exists. So an existing branch will overwrite entirely with it... Here is a Git terminal example, For Git GUI users it couldn't be much simpler. In Git GUI, choose the branch name from the drop down list in the "Rename Branch" dialog box created from the menu item Branch:Rename, type a New Name, and click "Rename". I have highlighted where to find the drop down list.  All of the previous answers are talking about git branch -m. Of course, it's easy to operate, but for me, it may be a little hard to remember another Git command. So I tried to get the work done by the command I was familiar with. Yeah, you may guessed it. I use git branch -b <new_branch_name>. And if you don't want to save the old branch now you can execute git branch -D <old_branch_name> to remove it. I know it may be a little tedious, but it's easier to understand and remember. I hope it‘s helpful for you. If you want to: Before we begin, make sure you’ve selected the branch you want to rename: If you want to see all of your local branches, use the following command: When you’re all clear, follow these steps: Using the Git rename branch command will require you to add an -m option to your command: You can also rename a local branch from another branch by using the following two commands: Lastly, this command will list all — both local and remote — branches to verify that it has been renamed: Although it isn’t possible to rename a remote branch directly, the process of renaming one involves these three easy steps: To start, you will need to rename a local branch by following the previous steps. 2.Then delete the old branch and push the new one. You can do this easily with the following commands: Reset the upstream branch for your new local branch and you will be all set:
__label__javascript __label__arrays I have an array of numbers and I'm using the .push() method to add elements to it. Is there a simple way to remove a specific element from an array? I'm looking for the equivalent of something like: I have to use core JavaScript. Frameworks are not allowed. Find the index of the array element you want to remove using indexOf, and then remove that index with splice. The splice() method changes the contents of an array by removing   existing elements and/or adding new elements.   const array = [2, 5, 9];  console.log(array);  const index = array.indexOf(5); if (index > -1) {   array.splice(index, 1); }  // array = [2, 9] console.log(array);     The second parameter of splice is the number of elements to remove. Note that splice modifies the array in place and returns a new array containing the elements that have been removed. For the reason of completeness, here are functions. The first function removes only a single occurrence (i.e. removing the first match of 5 from [2,5,9,1,5,8,5]), while the second function removes all occurrences:   function removeItemOnce(arr, value) {   var index = arr.indexOf(value);   if (index > -1) {     arr.splice(index, 1);   }   return arr; }  function removeItemAll(arr, value) {   var i = 0;   while (i < arr.length) {     if (arr[i] === value) {       arr.splice(i, 1);     } else {       ++i;     }   }   return arr; } //Usage console.log(removeItemOnce([2,5,9,1,5,8,5], 5)) console.log(removeItemAll([2,5,9,1,5,8,5], 5))    Edited on 2016 October In this code example I use "array.filter(...)" function to remove unwanted items from an array. This function doesn't change the original array and creates a new one. If your browser doesn't support this function (e.g. Internet Explorer before version 9, or Firefox before version 1.5), consider using the filter polyfill from Mozilla. IMPORTANT ECMAScript 6 "() => {}" arrow function syntax is not supported in Internet Explorer at all, Chrome before 45 version, Firefox before 22 version, and Safari before 10 version. To use ECMAScript 6 syntax in old browsers you can use BabelJS. An additional advantage of this method is that you can remove multiple items IMPORTANT "array.includes(...)" function is not supported in Internet Explorer at all, Chrome before 47 version, Firefox before 43 version, Safari before 9 version, and Edge before 14 version so here is polyfill from Mozilla. If the "This-Binding Syntax" proposal is ever accepted, you'll be able to do this: Try it yourself in BabelJS :) Reference I don't know how you are expecting array.remove(int) to behave. There are three possibilities I can think of that you might want. To remove an element of an array at an index i: If you want to remove every element with value number from the array: If you just want to make the element at index i no longer exist, but you don't want the indexes of the other elements to change: It depends on whether you want to keep an empty spot or not. If you do want an empty slot: If you don't want an empty slot: And if you need the value of that item, you can just store the returned array's element: If you want to remove at either end of the array, you can use array.pop() for the last one or array.shift() for the first one (both return the value of the item as well). If you don't know the index of the item, you can use array.indexOf(item) to get it (in a if() to get one item or in a while() to get all of them). array.indexOf(item) returns either the index or -1 if not found.  A friend was having issues in Internet Explorer 8 and showed me what he did. I told him it was wrong, and he told me he got the answer here. The current top answer will not work in all browsers (Internet Explorer 8 for example), and it will only remove the first occurrence of the item. It loops through the array backwards (since indices and length will change as items are removed) and removes the item if it's found. It works in all browsers. There are two major approaches: splice(): anArray.splice(index, 1); delete: delete anArray[index]; Be careful when you use to delete for an array. It is good for deleting attributes of objects, but not so good for arrays. It is better to use splice for arrays. Keep in mind that when you use delete for an array you could get wrong results for anArray.length. In other words, delete would remove the element, but it wouldn't update the value of the length property. You can also expect to have holes in index numbers after using delete, e.g. you could end up with having indexes 1, 3, 4, 8, 9, and 11 and length as it was before using delete. In that case, all indexed for loops would crash, since indexes are no longer sequential. If you are forced to use delete for some reason, then you should use for each loops when you need to loop through arrays. As the matter of fact, always avoid using indexed for loops, if possible. That way the code would be more robust and less prone to problems with indexes.   Array.prototype.remove_by_value = function(val) {   for (var i = 0; i < this.length; i++) {     if (this[i] === val) {       this.splice(i, 1);       i--;     }   }   return this; }  var rooms = ['hello', 'something']  rooms = rooms.remove_by_value('hello')  console.log(rooms)    There is no need to use indexOf or splice. However, it performs better if you only want to remove one occurrence of an element.  Find and move (move): Use indexOf and splice (indexof): Use only splice (splice): Run-times on nodejs for array with 1000 elements (average over 10000 runs): indexof is approximately 10x slower than move. Even if improved by removing the call to indexOf in splice it performs much worse than move.  This provides a predicate instead of a value. NOTE: it will update the given array, and return the affected rows. You can do it easily with the filter method:   function remove(arrOriginal, elementToRemove){     return arrOriginal.filter(function(el){return el !== elementToRemove}); } console.log(remove([1, 2, 1, 0, 3, 1, 4], 1));    This removes all elements from the array and also works faster than a combination of slice and indexOf. John Resig posted a good implementation: If you don’t want to extend a global object, you can do something like the following, instead: But the main reason I am posting this is to warn users against the alternative implementation suggested in the comments on that page (Dec 14, 2007): It seems to work well at first, but through a painful process I discovered it fails when trying to remove the second to last element in an array. For example, if you have a 10-element array and you try to remove the 9th element with this: You end up with an 8-element array. Don't know why but I confirmed John's original implementation doesn't have this problem. Underscore.js can be used to solve issues with multiple browsers. It uses in-build browser methods if present. If they are absent like in the case of older Internet Explorer versions it uses its own custom methods. A simple example to remove elements from array (from the website): You can use ES6. For example to delete the value '3' in this case: Output :  If you want a new array with the deleted positions removed, you can always delete the specific element and filter out the array. It might need an extension of the array object for browsers that don't implement the filter method, but in the long term it's easier since all you do is this: It should display [1, 2, 3, 4, 6]. Here are a few ways to remove an item from an array using JavaScript. All the method described do not mutate the original array, and instead create a new one. Suppose you have an array, and you want to remove an item in position i. One method is to use slice():   const items = ['a', 'b', 'c', 'd', 'e', 'f'] const i = 3 const filteredItems = items.slice(0, i).concat(items.slice(i+1, items.length))  console.log(filteredItems)    slice() creates a new array with the indexes it receives. We simply create a new array, from start to the index we want to remove, and concatenate another array from the first position following the one we removed to the end of the array. In this case, one good option is to use filter(), which offers a more declarative approach:   const items = ['a', 'b', 'c', 'd', 'e', 'f'] const valueToRemove = 'c' const filteredItems = items.filter(item => item !== valueToRemove)  console.log(filteredItems)    This uses the ES6 arrow functions. You can use the traditional functions to support older browsers:   const items = ['a', 'b', 'c', 'd', 'e', 'f'] const valueToRemove = 'c' const filteredItems = items.filter(function(item) {   return item !== valueToRemove })  console.log(filteredItems)    or you can use Babel and transpile the ES6 code back to ES5 to make it more digestible to old browsers, yet write modern JavaScript in your code. What if instead of a single item, you want to remove many items? Let's find the simplest solution. You can just create a function and remove items in series:   const items = ['a', 'b', 'c', 'd', 'e', 'f']  const removeItem = (items, i) =>   items.slice(0, i-1).concat(items.slice(i, items.length))  let filteredItems = removeItem(items, 3) filteredItems = removeItem(filteredItems, 5) //["a", "b", "c", "d"]  console.log(filteredItems)    You can search for inclusion inside the callback function:   const items = ['a', 'b', 'c', 'd', 'e', 'f'] const valuesToRemove = ['c', 'd'] const filteredItems = items.filter(item => !valuesToRemove.includes(item)) // ["a", "b", "e", "f"]  console.log(filteredItems)    splice() (not to be confused with slice()) mutates the original array, and should be avoided. (originally posted on my site https://flaviocopes.com/how-to-remove-item-from-array/) Check out this code. It works in every major browser.   remove_item = function(arr, value) {  var b = '';  for (b in arr) {   if (arr[b] === value) {    arr.splice(b, 1);    break;   }  }  return arr; };  var array = [1,3,5,6,5,9,5,3,55] var res = remove_item(array,5); console.log(res)    ES6 & without mutation:  (October 2016)   const removeByIndex = (list, index) =>       [         ...list.slice(0, index),         ...list.slice(index + 1)       ];           output = removeByIndex([33,22,11,44],1) //=> [33,11,44]        console.log(output)    You can use lodash _.pull (mutate array), _.pullAt (mutate array) or _.without (does't mutate array),  Removing a particular element/string from an array can be done in a one-liner: where: theArray: the array you want to remove something particular from stringToRemoveFromArray: the string you want to be removed and 1 is the number of elements you want to remove. NOTE: If "stringToRemoveFromArray" is not located in the array, this will remove the last element of the array. It's always good practice to check if the element exists in your array first, before removing it. Depending if you have newer or older version of Ecmascript running on your client's computers: OR Where '3' is the value you want to be removed from the array. The array would then become : ['1','2','4','5','6'] Today (2019-12-09) I conduct performance tests on macOS v10.13.6 (High Sierra) for chosen solutions. I show delete (A), but I do not use it in comparison with other methods, because it left empty space in the array. The conclusions In tests, I remove the middle element from the array in different ways. The A, C solutions are in-place. The B, D, E, F, G, H solutions are immutable. Results for an array with 10 elements  In Chrome the array.splice (C) is the fastest in-place solution. The array.filter (D) is the fastest immutable solution. The slowest is array.slice (F). You can perform the test on your machine here. Results for an array with 1.000.000 elements  In Chrome the array.splice (C) is the fastest in-place solution (the delete (C) is similar fast - but it left an empty slot in the array (so it does not perform a 'full remove')). The array.slice-splice (H) is the fastest immutable solution. The slowest is array.filter (D and E). You can perform the test on your machine here.   var a = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]; var log = (letter,array) => console.log(letter, array.join `,`);  function A(array) {   var index = array.indexOf(5);   delete array[index];   log('A', array); }  function B(array) {   var index = array.indexOf(5);   var arr = Array.from(array);   arr.splice(index, 1)   log('B', arr); }  function C(array) {   var index = array.indexOf(5);   array.splice(index, 1);   log('C', array); }  function D(array) {   var arr = array.filter(item => item !== 5)   log('D', arr); }  function E(array) {   var index = array.indexOf(5);   var arr = array.filter((item, i) => i !== index)   log('E', arr); }  function F(array) {   var index = array.indexOf(5);   var arr = array.slice(0, index).concat(array.slice(index + 1))   log('F', arr); }  function G(array) {   var index = array.indexOf(5);   var arr = [...array.slice(0, index), ...array.slice(index + 1)]   log('G', arr); }  function H(array) {   var index = array.indexOf(5);   var arr = array.slice(0);   arr.splice(index, 1);   log('H', arr); }  A([...a]); B([...a]); C([...a]); D([...a]); E([...a]); F([...a]); G([...a]); H([...a]); This snippet only presents code used in performance tests - it does not perform tests itself.    Comparison for browsers: Chrome v78.0.0, Safari v13.0.4, and Firefox v71.0.0  OK, for example you have the array below: And we want to delete number 4. You can simply use the below code: If you are reusing this function, you write a reusable function which will be attached to the native array function like below: But how about if you have the below array instead with a few [5]s in the array? We need a loop to check them all, but an easier and more efficient way is using built-in JavaScript functions, so we write a function which use a filter like below instead: Also there are third-party libraries which do help you to do this, like Lodash or Underscore. For more information, look at lodash _.pull, _.pullAt or _.without. I'm pretty new to JavaScript and needed this functionality. I merely wrote this: Then when I want to use it: Output - As expected. ["item1", "item1"] You may have different needs than I, so you can easily modify it to suit them. I hope this helps someone. If you have complex objects in the array you can use filters?  In situations where $.inArray or array.splice is not as easy to use. Especially if the objects are perhaps shallow in the array. E.g. if you have an object with an Id field and you want the object removed from an array: I want to answer based on ECMAScript 6. Assume, you have an array like below: If you want to delete at a special index like 2, write the below code: But if you want to delete a special item like 3 and you don't know its index, do like below: Hint: please use an arrow function for filter callback unless you will get an empty array. This post summarizes common approaches to element removal from an array as of ECMAScript 2019 (ES10). | In-place: Yes |  | Removes duplicates: Yes(loop), No(indexOf) |  | By value / index: By index | If you know the value you want to remove from an array you can use the splice method. First, you must identify the index of the target item. You then use the index as the start element and remove just one element. | In-place: No |  | Removes duplicates: Yes |  | By value / index: By value |  The specific element can be filtered out from the array, by providing a filtering function. Such function is then called for every element in the array. | In-place: Yes/No (Depends on implementation) |  | Removes duplicates: Yes/No (Depends on implementation) |  | By value / index: By index / By value (Depends on implementation) | The prototype of Array can be extended with additional methods. Such methods will be then available to use on created arrays. Note: Extending prototypes of objects from the standard library of JavaScript (like Array) is considered by some as an antipattern. | In-place: Yes |  | Removes duplicates: No |  | By value / index: By index | Using the delete operator does not affect the length property. Nor does it affect the indexes of subsequent elements. The array becomes sparse, which is a fancy way of saying the deleted item is not removed but becomes undefined. The delete operator is designed to remove properties from JavaScript objects, which arrays are objects. | In-place: No |  | Removes duplicates: Yes |  | By value / index: By value | ES10 introduced Object.fromEntries, which can be used to create the desired Array from any Array-like object and filter unwanted elements during the process. | In-place: Yes |  | Removes duplicates: No |  | By value / index: N/A | JavaScript Array elements can be removed from the end of an array by setting the length property to a value less than the current value. Any element whose index is greater than or equal to the new length will be removed. | In-place: Yes |  | Removes duplicates: No |  | By value / index: N/A | The pop method removes the last element of the array, returns that element, and updates the length property. The pop method modifies the array on which it is invoked, This means unlike using delete the last element is removed completely and the array length reduced. | In-place: Yes |  | Removes duplicates: No |  | By value / index: N/A | The .shift() method works much like the pop method except it removes the first element of a JavaScript array instead of the last. When the element is removed the remaining elements are shifted down. | In-place: Yes |  | Removes duplicates: N/A |  | By value / index: N/A | The fastest technique is to set an array variable to an empty array. Alternatively technique from 2.1.1 can be used by setting length to 0. Update: This method is recommended only if you cannot use ECMAScript 2015 (formerly known as ES6). If you can use it, other answers here provide much neater implementations. This gist here will solve your problem, and also deletes all occurrences of the argument instead of just 1 (or a specified value). Usage: You should never mutate your array. As this is against the functional programming pattern. You can create a new array without referencing the array you want to change data of using the ECMAScript 6 method filter; Suppose you want to remove 5 from the array, you can simply do it like this: This will give you a new array without the value you wanted to remove. So the result will be: For further understanding you can read the MDN documentation on Array.filter. A more modern, ECMAScript 2015 (formerly known as Harmony or ES 6) approach. Given: Then: Yielding:  You can use Babel and a polyfill service to ensure this is well supported across browsers. You can do a backward loop to make sure not to screw up the indexes, if there are multiple instances of the element.   var myElement = "chocolate"; var myArray = ['chocolate', 'poptart', 'poptart', 'poptart', 'chocolate', 'poptart', 'poptart', 'chocolate'];  /* Important code */ for (var i = myArray.length - 1; i >= 0; i--) {   if (myArray[i] == myElement) myArray.splice(i, 1); } console.log(myArray);    Live Demo You have 1 to 9 in the array, and you want remove 5. Use the below code:   var numberArray = [1, 2, 3, 4, 5, 6, 7, 8, 9];  var newNumberArray = numberArray.filter(m => {   return m !== 5; });  console.log("new Array, 5 removed", newNumberArray);    If you want to multiple values. Example:- 1,7,8   var numberArray = [1, 2, 3, 4, 5, 6, 7, 8, 9];  var newNumberArray = numberArray.filter(m => {   return (m !== 1) && (m !== 7) && (m !== 8); });  console.log("new Array, 1,7 and 8 removed", newNumberArray);    If you want to remove an array value in an array. Example: [3,4,5]   var numberArray = [1, 2, 3, 4, 5, 6, 7, 8, 9]; var removebleArray = [3,4,5];  var newNumberArray = numberArray.filter(m => {     return !removebleArray.includes(m); });  console.log("new Array, [3,4,5] removed", newNumberArray);    Includes supported browser is link.
__label__dynamic-memory-allocation __label__stack __label__heap __label__memory-management __label__language-agnostic Programming language books explain that value types are created on the stack, and reference types are created on the heap, without explaining what these two things are. I haven't read a clear explanation of this.  I understand what a stack is. But,  The stack is the memory set aside as scratch space for a thread of execution.  When a function is called, a block is reserved on the top of the stack for local variables and some bookkeeping data.  When that function returns, the block becomes unused and can be used the next time a function is called.  The stack is always reserved in a LIFO (last in first out) order; the most recently reserved block is always the next block to be freed.  This makes it really simple to keep track of the stack; freeing a block from the stack is nothing more than adjusting one pointer. The heap is memory set aside for dynamic allocation.  Unlike the stack, there's no enforced pattern to the allocation and deallocation of blocks from the heap; you can allocate a block at any time and free it at any time.  This makes it much more complex to keep track of which parts of the heap are allocated or free at any given time; there are many custom heap allocators available to tune heap performance for different usage patterns. Each thread gets a stack, while there's typically only one heap for the application (although it isn't uncommon to have multiple heaps for different types of allocation). To answer your questions directly:   To what extent are they controlled by the OS or language runtime? The OS allocates the stack for each system-level thread when the thread is created.  Typically the OS is called by the language runtime to allocate the heap for the application. What is their scope? The stack is attached to a thread, so when the thread exits the stack is reclaimed.  The heap is typically allocated at application startup by the runtime, and is reclaimed when the application (technically process) exits. What determines the size of each of them?  The size of the stack is set when a thread is created.  The size of the heap is set on application startup, but can grow as space is needed (the allocator requests more memory from the operating system). What makes one faster? The stack is faster because the access pattern makes it trivial to allocate and deallocate memory from it (a pointer/integer is simply incremented or decremented), while the heap has much more complex bookkeeping involved in an allocation or deallocation.  Also, each byte in the stack tends to be reused very frequently which means it tends to be mapped to the processor's cache, making it very fast. Another performance hit for the heap is that the heap, being mostly a global resource, typically has to be multi-threading safe, i.e. each allocation and deallocation needs to be - typically - synchronized with "all" other heap accesses in the program. A clear demonstration:  Image source: vikashazrati.wordpress.com Stack: Heap: Example: The most important point is that heap and stack are generic terms for ways in which memory can be allocated.  They can be implemented in many different ways, and the terms apply to the basic concepts. In a stack of items, items sit one on top of the other in the order they were placed there, and you can only remove the top one (without toppling the whole thing over).  The simplicity of a stack is that you do not need to maintain a table containing a record of each section of allocated memory; the only state information you need is a single pointer to the end of the stack.  To allocate and de-allocate, you just increment and decrement that single pointer.  Note: a stack can sometimes be implemented to start at the top of a section of memory and extend downwards rather than growing upwards. In a heap, there is no particular order to the way items are placed.  You can reach in and remove items in any order because there is no clear 'top' item.  Heap allocation requires maintaining a full record of what memory is allocated and what isn't, as well as some overhead maintenance to reduce fragmentation, find contiguous memory segments big enough to fit the requested size, and so on.  Memory can be deallocated at any time leaving free space.  Sometimes a memory allocator will perform maintenance tasks such as defragmenting memory by moving allocated memory around, or garbage collecting - identifying at runtime when memory is no longer in scope and deallocating it.  These images should do a fairly good job of describing the two ways of allocating and freeing memory in a stack and a heap.  Yum! To what extent are they controlled by the OS or language runtime? As mentioned, heap and stack are general terms, and can be implemented in many ways.  Computer programs typically have a stack called a call stack which stores information relevant to the current function such as a pointer to whichever function it was called from, and any local variables.  Because functions call other functions and then return, the stack grows and shrinks to hold information from the functions further down the call stack.  A program doesn't really have runtime control over it; it's determined by the programming language, OS and even the system architecture. A heap is a general term used for any memory that is allocated dynamically and randomly; i.e. out of order.  The memory is typically allocated by the OS, with the application calling API functions to do this allocation.  There is a fair bit of overhead required in managing dynamically allocated memory, which is usually handled by the runtime code of the programming language or environment used. What is their scope? The call stack is such a low level concept that it doesn't relate to 'scope' in the sense of programming.  If you disassemble some code you'll see relative pointer style references to portions of the stack, but as far as a higher level language is concerned, the language imposes its own rules of scope.  One important aspect of a stack, however, is that once a function returns, anything local to that function is immediately freed from the stack.  That works the way you'd expect it to work given how your programming languages work.  In a heap, it's also difficult to define.  The scope is whatever is exposed by the OS, but your programming language probably adds its rules about what a "scope" is in your application.  The processor architecture and the OS use virtual addressing, which the processor translates to physical addresses and there are page faults, etc.  They keep track of what pages belong to which applications.  You never really need to worry about this, though, because you just use whatever method your programming language uses to allocate and free memory, and check for errors (if the allocation/freeing fails for any reason). What determines the size of each of them? Again, it depends on the language, compiler, operating system and architecture.  A stack is usually pre-allocated, because by definition it must be contiguous memory.  The language compiler or the OS determine its size.  You don't store huge chunks of data on the stack, so it'll be big enough that it should never be fully used, except in cases of unwanted endless recursion (hence, "stack overflow") or other unusual programming decisions. A heap is a general term for anything that can be dynamically allocated.  Depending on which way you look at it, it is constantly changing size.  In modern processors and operating systems the exact way it works is very abstracted anyway, so you don't normally need to worry much about how it works deep down, except that (in languages where it lets you) you mustn't use memory that you haven't allocated yet or memory that you have freed. What makes one faster? The stack is faster because all free memory is always contiguous.  No list needs to be maintained of all the segments of free memory, just a single pointer to the current top of the stack.  Compilers usually store this pointer in a special, fast register for this purpose.  What's more, subsequent operations on a stack are usually concentrated within very nearby areas of memory, which at a very low level is good for optimization by the processor on-die caches. (I have moved this answer from another question that was more or less a dupe of this one.) The answer to your question is implementation specific and may vary across compilers and processor architectures. However, here is a simplified explanation.   Can a function be allocated on the heap instead of a stack? No, activation records for functions (i.e. local or automatic variables) are allocated on the stack that is used not only to store these variables, but also to keep track of nested function calls. How the heap is managed is really up to the runtime environment. C uses malloc and C++ uses new, but many other languages have garbage collection. However, the stack is a more low-level feature closely tied to the processor architecture. Growing the heap when there is not enough space isn't too hard since it can be implemented in the library call that handles the heap. However, growing the stack is often impossible as the stack overflow only is discovered when it is too late; and shutting down the thread of execution is the only viable option. In the following C# code Here's how the memory is managed  Local Variables that only need to last as long as the function invocation go in the stack. The heap is used for variables whose lifetime we don't really know up front but we expect them to last a while. In most languages it's critical that we know at compile time how large a variable is if we want to store it on the stack.  Objects (which vary in size as we update them) go on the heap because we don't know at creation time how long they are going to last. In many languages the heap is garbage collected to find objects (such as the cls1 object) that no longer have any references.  In Java, most objects go directly into the heap. In languages like C / C++, structs and classes can often remain on the stack when you're not dealing with pointers. More information can be found here: The difference between stack and heap memory allocation «  timmurphy.org and here:  Creating Objects on the Stack and Heap This article is the source of picture above: Six important .NET concepts: Stack, heap, value types, reference types, boxing, and unboxing - CodeProject but be aware it may contain some inaccuracies.  The Stack When you call a function the arguments to that function plus some other overhead is put on the stack. Some info (such as where to go on return) is also stored there. When you declare a variable inside your function, that variable is also allocated on the stack.  Deallocating the stack is pretty simple because you always deallocate in the reverse order in which you allocate. Stack stuff is added as you enter functions, the corresponding data is removed as you exit them. This means that you tend to stay within a small region of the stack unless you call lots of functions that call lots of other functions (or create a recursive solution). The Heap The heap is a generic name for where you put the data that you create on the fly. If you don't know how many spaceships your program is going to create, you are likely to use the new (or malloc or equivalent) operator to create each spaceship. This allocation is going to stick around for a while, so it is likely we will free things in a different order than we created them.  Thus, the heap is far more complex, because there end up being regions of memory that are unused interleaved with chunks that are - memory gets fragmented. Finding free memory of the size you need is a difficult problem. This is why the heap should be avoided (though it is still often used). Implementation Implementation of both the stack and heap is usually down to the runtime / OS. Often games and other applications that are performance critical create their own memory solutions that grab a large chunk of memory from the heap and then dish it out internally to avoid relying on the OS for memory.  This is only practical if your memory usage is quite different from the norm - i.e for games where you load a level in one huge operation and can chuck the whole lot away in another huge operation. Physical location in memory This is less relevant than you think because of a technology called Virtual Memory which makes your program think that you have access to a certain address where the physical data is somewhere else (even on the hard disc!). The addresses you get for the stack are in increasing order as your call tree gets deeper. The addresses for the heap are un-predictable (i.e implimentation specific) and frankly not important. To clarify, this answer has incorrect information (thomas fixed his answer after comments, cool :) ). Other answers just avoid explaining what static allocation means. So I will explain the three main forms of allocation and how they usually relate to the heap, stack, and data segment below. I also will show some examples in both C/C++ and Python to help people understand. "Static" (AKA statically allocated) variables are not allocated on the stack. Do not assume so - many people do only because "static" sounds a lot like "stack". They actually exist in neither the stack nor the heap. The are part of what's called the data segment. However, it is generally better to consider "scope" and "lifetime" rather than "stack" and "heap". Scope refers to what parts of the code can access a variable. Generally we think of local scope (can only be accessed by the current function) versus global scope (can be accessed anywhere) although scope can get much more complex. Lifetime refers to when a variable is allocated and deallocated during program execution. Usually we think of static allocation (variable will persist through the entire duration of the program, making it useful for storing the same information across several function calls) versus automatic allocation (variable only persists during a single call to a function, making it useful for storing information that is only used during your function and can be discarded once you are done) versus dynamic allocation (variables whose duration is defined at runtime, instead of compile time like static or automatic). Although most compilers and interpreters implement this behavior similarly in terms of using stacks, heaps, etc, a compiler may sometimes break these conventions if it wants as long as behavior is correct. For instance, due to optimization a local variable may only exist in a register or be removed entirely, even though most local variables exist in the stack. As has been pointed out in a few comments, you are free to implement a compiler that doesn't even use a stack or a heap, but instead some other storage mechanisms (rarely done, since stacks and heaps are great for this). I will provide some simple annotated C code to illustrate all of this. The best way to learn is to run a program under a debugger and watch the behavior. If you prefer to read python, skip to the end of the answer :) A particularly poignant example of why it's important to distinguish between lifetime and scope is that a variable can have local scope but static lifetime - for instance, "someLocalStaticVariable" in the code sample above. Such variables can make our common but informal naming habits very confusing. For instance when we say "local" we usually mean "locally scoped automatically allocated variable" and when we say global we usually mean "globally scoped statically allocated variable". Unfortunately when it comes to things like "file scoped statically allocated variables" many people just say... "huh???". Some of the syntax choices in C/C++ exacerbate this problem - for instance many people think global variables are not "static" because of the syntax shown below. Note that putting the keyword "static" in the declaration above prevents var2 from having global scope. Nevertheless, the global var1 has static allocation. This is not intuitive! For this reason, I try to never use the word "static" when describing scope, and instead say something like "file" or "file limited" scope. However many people use the phrase "static" or "static scope" to describe a variable that can only be accessed from one code file. In the context of lifetime, "static" always means the variable is allocated at program start and deallocated when program exits. Some people think of these concepts as C/C++ specific. They are not. For instance, the Python sample below illustrates all three types of allocation (there are some subtle differences possible in interpreted languages that I won't get into here). Others have answered the broad strokes pretty well, so I'll throw in a few details. Stack and heap need not be singular. A common situation in which you have more than one stack is if you have more than one thread in a process.  In this case each thread has its own stack. You can also have more than one heap, for example some DLL configurations can result in different DLLs allocating from different heaps, which is why it's generally a bad idea to release memory allocated by a different library. In C you can get the benefit of variable length allocation through the use of alloca, which allocates on the stack, as opposed to alloc, which allocates on the heap. This memory won't survive your return statement, but it's useful for a scratch buffer. Making a huge temporary buffer on Windows that you don't use much of is not free. This is because the compiler will generate a stack probe loop that is called every time your function is entered to make sure the stack exists (because Windows uses a single guard page at the end of your stack to detect when it needs to grow the stack. If you access memory more than one page off the end of the stack you will crash). Example: Others have directly answered your question, but when trying to understand the stack and the heap, I think it is helpful to consider the memory layout of a traditional UNIX process (without threads and mmap()-based allocators). The Memory Management Glossary web page has a diagram of this memory layout. The stack and heap are traditionally located at opposite ends of the process's virtual address space. The stack grows automatically when accessed, up to a size set by the kernel (which can be adjusted with setrlimit(RLIMIT_STACK, ...)). The heap grows when the memory allocator invokes the brk() or sbrk() system call, mapping more pages of physical memory into the process's virtual address space.  In systems without virtual memory, such as some embedded systems, the same basic layout often applies, except the stack and heap are fixed in size. However, in other embedded systems (such as those based on Microchip PIC microcontrollers), the program stack is a separate block of memory that is not addressable by data movement instructions, and can only be modified or read indirectly through program flow instructions (call, return, etc.). Other architectures, such as Intel Itanium processors, have multiple stacks. In this sense, the stack is an element of the CPU architecture. The stack is a portion of memory that can be manipulated via several key assembly language instructions, such as 'pop' (remove and return a value from the stack) and 'push' (push a value to the stack), but also call (call a subroutine - this pushes the address to return to the stack) and return (return from a subroutine - this pops the address off of the stack and jumps to it).  It's the region of memory below the stack pointer register, which can be set as needed.  The stack is also used for passing arguments to subroutines, and also for preserving the values in registers before calling subroutines. The heap is a portion of memory that is given to an application by the operating system, typically through a syscall like malloc.  On modern OSes this memory is a set of pages that only the calling process has access to. The size of the stack is determined at runtime, and generally does not grow after the program launches.  In a C program, the stack needs to be large enough to hold every variable declared within each function.  The heap will grow dynamically as needed, but the OS is ultimately making the call (it will often grow the heap by more than the value requested by malloc, so that at least some future mallocs won't need to go back to the kernel to get more memory.  This behavior is often customizable) Because you've allocated the stack before launching the program, you never need to malloc before you can use the stack, so that's a slight advantage there.  In practice, it's very hard to predict what will be fast and what will be slow in modern operating systems that have virtual memory subsystems, because how the pages are implemented and where they are stored is an implementation detail.   What is a stack? A stack is a pile of objects, typically one that is neatly arranged.  Stacks in computing architectures are regions of memory where data is added or removed in a last-in-first-out manner.  In a multi-threaded application, each thread will have its own stack. What is a heap? A heap is an untidy collection of things piled up haphazardly.  In computing architectures the heap is an area of dynamically-allocated memory that is managed automatically by the operating system or the memory manager library.  Memory on the heap is allocated, deallocated, and resized regularly during program execution, and this can lead to a problem called fragmentation.  Fragmentation occurs when memory objects are allocated with small spaces in between that are too small to hold additional memory objects.  The net result is a percentage of the heap space that is not usable for further memory allocations. Both together In a multi-threaded application, each thread will have its own stack. But, all the different threads will share the heap.  Because the different threads share the heap in a multi-threaded application, this also means that there has to be some coordination between the threads so that they don’t try to access and manipulate the same piece(s) of memory in the heap at the same time. Which is faster – the stack or the heap? And why? The stack is much faster than the heap.  This is because of the way that memory is allocated on the stack.  Allocating memory on the stack is as simple as moving the stack pointer up. For people new to programming, it’s probably a good idea to use the stack since it’s easier.  Because the stack is small, you would want to use it when you know exactly how much memory you will need for your data, or if you know the size of your data is very small.  It’s better to use the heap when you know that you will need a lot of memory for your data, or you just are not sure how much memory you will need (like with a dynamic array).  The stack is the area of memory where local variables (including method parameters) are stored. When it comes to object variables, these are merely references (pointers) to the actual objects on the heap. Every time an object is instantiated, a chunk of heap memory is set aside to hold the data (state) of that object. Since objects can contain other objects, some of this data can in fact hold references to those nested objects. I think many other people have given you mostly correct answers on this matter. One detail that has been missed, however, is that the "heap" should in fact probably be called the "free store".  The reason for this distinction is that the original free store was implemented with a data structure known as a "binomial heap."  For that reason, allocating from early implementations of malloc()/free() was allocation from a heap.  However, in this modern day, most free stores are implemented with very elaborate data structures that are not binomial heaps. You can do some interesting things with the stack.  For instance, you have functions like alloca (assuming you can get past the copious warnings concerning its use), which is a form of malloc that specifically uses the stack, not the heap, for memory. That said, stack-based memory errors are some of the worst I've experienced.  If you use heap memory, and you overstep the bounds of your allocated block, you have a decent chance of triggering a segment fault.  (Not 100%: your block may be incidentally contiguous with another that you have previously allocated.)  But since variables created on the stack are always contiguous with each other, writing out of bounds can change the value of another variable.  I have learned that whenever I feel that my program has stopped obeying the laws of logic, it is probably buffer overflow. Simply, the stack is where local variables get created. Also, every time you call a subroutine the program counter (pointer to the next machine instruction) and any important registers, and sometimes the parameters get pushed on the stack. Then any local variables inside the subroutine are pushed onto the stack (and used from there). When the subroutine finishes, that stuff all gets popped back off the stack. The PC and register data gets and put back where it was as it is popped, so your program can go on its merry way. The heap is the area of memory dynamic memory allocations are made out of (explicit "new" or "allocate" calls). It is a special data structure that can keep track of blocks of memory of varying sizes and their allocation status. In "classic" systems RAM was laid out such that the stack pointer started out at the bottom of memory, the heap pointer started out at the top, and they grew towards each other. If they overlap, you are out of RAM. That doesn't work with modern multi-threaded OSes though. Every thread has to have its own stack, and those can get created dynamicly. From WikiAnwser. When a function or a method calls another function which in turns calls another function, etc., the execution of all those functions remains suspended until the very last function returns its value. This chain of suspended function calls is the stack, because elements in the stack (function calls) depend on each other. The stack is important to consider in exception handling and thread executions. The heap is simply the memory used by programs to store variables. Element of the heap (variables) have no dependencies with each other and can always be accessed randomly at any time. Stack Heap A stack is used for static memory allocation and a heap for dynamic memory allocation, both stored in the computer's RAM. The Stack The stack is a "LIFO" (last in, first out) data structure, that is managed and optimized by the CPU quite closely. Every time a function declares a new variable, it is "pushed" onto the stack. Then every time a function exits, all of the variables pushed onto the stack by that function, are freed (that is to say, they are deleted). Once a stack variable is freed, that region of memory becomes available for other stack variables. The advantage of using the stack to store variables, is that memory is managed for you. You don't have to allocate memory by hand, or free it once you don't need it any more. What's more, because the CPU organizes stack memory so efficiently, reading from and writing to stack variables is very fast. More can be found here. The Heap The heap is a region of your computer's memory that is not managed automatically for you, and is not as tightly managed by the CPU. It is a more free-floating region of memory (and is larger). To allocate memory on the heap, you must use malloc() or calloc(), which are built-in C functions. Once you have allocated memory on the heap, you are responsible for using free() to deallocate that memory once you don't need it any more. If you fail to do this, your program will have what is known as a memory leak. That is, memory on the heap will still be set aside (and won't be available to other processes). As we will see in the debugging section, there is a tool called Valgrind that can help you detect memory leaks. Unlike the stack, the heap does not have size restrictions on variable size (apart from the obvious physical limitations of your computer). Heap memory is slightly slower to be read from and written to, because one has to use pointers to access memory on the heap. We will talk about pointers shortly. Unlike the stack, variables created on the heap are accessible by any function, anywhere in your program. Heap variables are essentially global in scope. More can be found here. Variables allocated on the stack are stored directly to the memory and access to this memory is very fast, and its allocation is dealt with when the program is compiled. When a function or a method calls another function which in turns calls another function, etc., the execution of all those functions remains suspended until the very last function returns its value. The stack is always reserved in a LIFO order, the most recently reserved block is always the next block to be freed. This makes it really simple to keep track of the stack, freeing a block from the stack is nothing more than adjusting one pointer. Variables allocated on the heap have their memory allocated at run time and accessing this memory is a bit slower, but the heap size is only limited by the size of virtual memory. Elements of the heap have no dependencies with each other and can always be accessed randomly at any time. You can allocate a block at any time and free it at any time. This makes it much more complex to keep track of which parts of the heap are allocated or free at any given time.  You can use the stack if you know exactly how much data you need to allocate before compile time, and it is not too big. You can use the heap if you don't know exactly how much data you will need at runtime or if you need to allocate a lot of data. In a multi-threaded situation each thread will have its own completely independent stack, but they will share the heap. The stack is thread specific and the heap is application specific. The stack is important to consider in exception handling and thread executions. Each thread gets a stack, while there's typically only one heap for the application (although it isn't uncommon to have multiple heaps for different types of allocation).  At run-time, if the application needs more heap, it can allocate memory from free memory and if the stack needs memory, it can allocate memory from free memory allocated memory for the application. Even, more detail is given here and here. Now come to your question's answers. To what extent are they controlled by the OS or language runtime? The OS allocates the stack for each system-level thread when the thread is created. Typically the OS is called by the language runtime to allocate the heap for the application. More can be found here. What is their scope? Already given in top. "You can use the stack if you know exactly how much data you need to allocate before compile time, and it is not too big. You can use the heap if you don't know exactly how much data you will need at runtime or if you need to allocate a lot of data." More can be found in here. What determines the size of each of them? The size of the stack is set by OS when a thread is created. The size of the heap is set on application startup, but it can grow as space is needed (the allocator requests more memory from the operating system). What makes one faster? Stack allocation is much faster since all it really does is move the stack pointer. Using memory pools, you can get comparable performance out of heap allocation, but that comes with a slight added complexity and its own headaches. Also, stack vs. heap is not only a performance consideration; it also tells you a lot about the expected lifetime of objects. Details can be found from here. OK, simply and in short words, they mean ordered and not ordered...! Stack: In stack items, things get on the top of each-other, means gonna be faster and more efficient to be processed!...  So there is always an index to point the specific item, also processing gonna be faster, there is relationship between the items as well!... Heap: No order, processing gonna be slower and values are messed up together with no specific order or index... there are random and there is no relationship between them... so execution and usage time could be vary... I also create the image below to show how they may look like:  stack, heap and data of each process in virtual memory:  In the 1980s, UNIX propagated like bunnies with big companies rolling their own. Exxon had one as did dozens of brand names lost to history. How memory was laid out was at the discretion of the many implementors. A typical C program was laid out flat in memory with an opportunity to increase by changing the brk() value. Typically, the HEAP was just below this brk value and increasing brk increased the amount of available heap. The single STACK was typically an area below HEAP which was a tract of memory containing nothing of value until the top of the next fixed block of memory. This next block was often CODE which could be overwritten by stack data in one of the famous hacks of its era. One typical memory block was BSS (a block of zero values) which was accidentally not zeroed in one manufacturer's offering. Another was DATA containing initialized values, including strings and numbers. A third was CODE containing CRT (C runtime), main, functions, and libraries. The advent of virtual memory in UNIX changes many of the constraints. There is no objective reason why these blocks need be contiguous, or fixed in size, or ordered a particular way now. Of course, before UNIX was Multics which didn't suffer from these constraints. Here is a schematic showing one of the memory layouts of that era.  A couple of cents: I think, it will be good to draw memory graphical and more simple:   Arrows - show where grow stack and heap, process stack size have limit, defined in OS, thread stack size limits by parameters in thread create API usually. Heap usually limiting by process maximum virtual memory size, for 32 bit 2-4 GB for example. So simple way: process heap is general for process and all threads inside, using for memory allocation in common case with something like malloc(). Stack is quick memory for store in common case function return pointers and variables, processed as parameters in function call, local function variables. Since some answers went nitpicking, I'm going to contribute my mite. Surprisingly, no one has mentioned that multiple (i.e. not related to the number of running OS-level threads) call stacks are to be found not only in exotic languages (PostScript) or platforms (Intel Itanium), but also in fibers, green threads and some implementations of coroutines. Fibers, green threads and coroutines are in many ways similar, which leads to much confusion.  The difference between fibers and green threads is that the former use cooperative multitasking, while the latter may feature either cooperative or preemptive one (or even both). For the distinction between fibers and coroutines, see here. In any case, the purpose of both fibers, green threads and coroutines is having multiple functions executing concurrently, but not in parallel (see this SO question for the distinction) within a single OS-level thread, transferring control back and forth from one another in an organized fashion. When using fibers, green threads or coroutines, you usually have a separate stack per function. (Technically, not just a stack but a whole context of execution is per function. Most importantly, CPU registers.) For every thread there're as many stacks as there're concurrently running functions, and the thread is switching between executing each function according to the logic of your program. When a function runs to its end, its stack is destroyed. So, the number and lifetimes of stacks are dynamic and are not determined by the number of OS-level threads! Note that I said "usually have a separate stack per function". There're both stackful and stackless implementations of couroutines. Most notable stackful C++ implementations are Boost.Coroutine and Microsoft PPL's async/await. (However, C++'s resumable functions (a.k.a. "async and await"), which were proposed to C++17, are likely to use stackless coroutines.) Fibers proposal to the C++ standard library is forthcoming. Also, there're some third-party libraries. Green threads are extremely popular in languages like Python and Ruby. I have something to share, although the major points are already covered. Stack  Heap Interesting note: Wow! So many answers and I don't think one of them got it right... 1) Where and what are they (physically in a real computer's memory)? The stack is memory that begins as the highest memory address allocated to your program image, and it then decrease in value from there. It is reserved for called function parameters and for all temporary variables used in functions. There are two heaps: public and private. The private heap begins on a 16-byte boundary (for 64-bit programs) or a 8-byte boundary (for 32-bit programs) after the last byte of code in your program, and then increases in value from there. It is also called the default heap. If the private heap gets too large it will overlap the stack area, as will the stack overlap the heap if it gets too big. Because the stack starts at a higher address and works its way down to lower address, with proper hacking you can get make the stack so large that it will overrun the private heap area and overlap the code area. The trick then is to overlap enough of the code area that you can hook into the code. It's a little tricky to do and you risk a program crash, but it's easy and very effective. The public heap resides in it's own memory space outside of your program image space. It is this memory that will be siphoned off onto the hard disk if memory resources get scarce. 2) To what extent are they controlled by the OS or language runtime? The stack is controlled by the programmer, the private heap is managed by the OS, and the public heap is not controlled by anyone because it is an OS service -- you make requests and either they are granted or denied. 2b) What is their scope? They are all global to the program, but their contents can be private, public, or global. 2c) What determines the size of each of them? The size of the stack and the private heap are determined by your compiler runtime options. The public heap is initialized at runtime using a size parameter. 2d) What makes one faster? They are not designed to be fast, they are designed to be useful. How the programmer utilizes them determines whether they are "fast" or "slow" REF: https://norasandler.com/2019/02/18/Write-a-Compiler-10.html https://docs.microsoft.com/en-us/windows/desktop/api/heapapi/nf-heapapi-getprocessheap https://docs.microsoft.com/en-us/windows/desktop/api/heapapi/nf-heapapi-heapcreate A lot of answers are correct as concepts, but we must note that a stack is needed by the hardware (i.e. microprocessor) to allow calling subroutines (CALL in assembly language..). (OOP guys will call it methods) On the stack you save return addresses and call → push / ret → pop is managed directly in hardware. You can use the stack to pass parameters.. even if it is slower than using registers (would a microprocessor guru say or a good 1980s BIOS book...) Stack usage is faster as: Heap is an area of dynamically-allocated memory that is managed automatically by the operating system or the memory manager library. You can allocate a block at any time and free it at any time. Heap allocation requires maintaining a full record of what memory is allocated and what isn’t, as well as some overhead maintenance to reduce fragmentation, find contiguous memory segments big enough to fit the requested size, and so on. Memory can be deallocated at any time leaving free space. As the heap grows new blocks are often allocated from lower addresses towards higher addresses. Thus you can think of the heap as a heap of memory blocks that grows in size as memory is allocated. If the heap is too small for an allocation the size can often be increased by acquiring more memory from the underlying operating system. Memory allocated from the heap will remain allocated until one of the following occurs: Stack: Heap: The stack is essentially an easy-to-access memory that simply manages its items    as a - well - stack. Only items for which the size is known in advance can go onto the stack. This is the case for numbers, strings, booleans. The heap is a memory for items of which you can’t predetermine the   exact size and structure. Since objects and arrays can be mutated and   change at runtime, they have to go into the heap. Source: Academind CPU stack and heap are physically related to how CPU and registers works with memory, how machine-assembly language works, not high-level languages themselves, even if these languages can decide little things. All modern CPUs work with the "same" microprocessor theory: they are all based on what's called "registers" and some are for "stack" to gain performance. All CPUs have stack registers since the beginning and they had been always here, way of talking, as I know. Assembly languages are the same since the beginning, despite variations... up to Microsoft and its Intermediate Language (IL) that changed the paradigm to have a OO virtual machine assembly language. So we'll be able to have some CLI/CIL CPU in the future (one project of MS). CPUs have stack registers to speed up memories access, but they are limited compared to the use of others registers to get full access to all the available memory for the processus. It why we talked about stack and heap allocations. In summary, and in general, the heap is hudge and slow and is for "global" instances and objects content, as the stack is little and fast and for "local" variables and references (hidden pointers to forget to manage them). So when we use the new keyword in a method, the reference (an int) is created in the stack, but the object and all its content (value-types as well as objects) is created in the heap, if I remember. But local elementary value-types and arrays are created in the stack. The difference in memory access is at the cells referencing level: addressing the heap, the overall memory of the process, requires more complexity in terms of handling CPU registers, than the stack which is "more" locally in terms of addressing because the CPU stack register is used as base address, if I remember. It is why when we have very long or infinite recurse calls or loops, we got stack overflow quickly, without freezing the system on modern computers... C# Heap(ing) Vs Stack(ing) In .NET Stack vs Heap: Know the Difference  Static class memory allocation where it is stored C# What and where are the stack and heap? https://en.wikipedia.org/wiki/Memory_management https://en.wikipedia.org/wiki/Stack_register Assembly language resources: Assembly Programming Tutorial Intel® 64 and IA-32 Architectures Software Developer Manuals Thank you for a really good discussion but as a real noob I wonder where instructions are kept? In the BEGINNING scientists were deciding between two architectures (von NEUMANN where everything is considered DATA and HARVARD where an area of memory was reserved for instructions and another for data). Ultimately, we went with the von Neumann design and now everything is considered 'the same'. This made it hard for me when I was learning assembly  https://www.cs.virginia.edu/~evans/cs216/guides/x86.html because they talk about registers and stack pointers.   Everything above talks about DATA. My guess is that since an instruction is a defined thing with a specific memory footprint, it would go on the stack and so all 'those' registers discussed in assembly are on the stack. Of course then came object oriented programming with instructions and data comingled into a structure that was dynamic so now instructions would be kept on the heap as well?
__label__json __label__comments Can I use comments inside a JSON file? If so, how? No. The JSON is data only, and if you include a comment, then it will be data too. You could have a designated data element called "_comment" (or something) that should be ignored by apps that use the JSON data. You would probably be better having the comment in the processes that generates/receives the JSON, as they are supposed to know what the JSON data will be in advance, or at least the structure of it. But if you decided to: No, comments of the form //… or /*…*/ are not allowed in JSON. This answer is based on: Include comments if you choose; strip them out with a minifier before parsing or transmitting. I just released JSON.minify() which strips out comments and whitespace from a block of JSON and makes it valid JSON that can be parsed. So, you might use it like:   When I released it, I got a huge backlash of people disagreeing with even the idea of it, so I decided that I'd write a comprehensive blog post on why comments make sense in JSON. It includes this notable comment from the creator of JSON: Suppose you are using JSON to keep configuration files, which you would like to annotate. Go ahead and insert all the comments you like. Then pipe it through JSMin before handing it to your JSON parser. - Douglas Crockford, 2012 Hopefully that's helpful to those who disagree with why JSON.minify() could be useful. Comments were removed from JSON by design. I removed comments from JSON because I saw people were using them to hold parsing directives, a practice which would have destroyed interoperability. I know that the lack of comments makes some people sad, but it shouldn't. Suppose you are using JSON to keep configuration files, which you would like to annotate. Go ahead and insert all the comments you like. Then pipe it through JSMin before handing it to your JSON parser. Source: Public statement by Douglas Crockford on G+ JSON does not support comments. It was also never intended to be used for configuration files where comments would be needed. Hjson is a configuration file format for humans. Relaxed syntax, fewer mistakes, more comments.  See hjson.github.io for JavaScript, Java, Python, PHP, Rust, Go, Ruby, C++ and C# libraries. DISCLAIMER: YOUR WARRANTY IS VOID As has been pointed out, this hack takes advantage of the implementation of the spec. Not all JSON parsers will understand this sort of JSON. Streaming parsers in particular will choke. It's an interesting curiosity, but you should really not be using it for anything at all. Below is the original answer. I've found a little hack that allows you to place comments in a JSON file that will not affect the parsing, or alter the data being represented in any way. It appears that when declaring an object literal you can specify two values with the same key, and the last one takes precedence. Believe it or not, it turns out that JSON parsers work the same way. So we can use this to create comments in the source JSON that will not be present in a parsed object representation.  If we apply this technique, your commented JSON file might look like this: The above code is valid JSON. If you parse it, you'll get an object like this: Which means there is no trace of the comments, and they won't have weird side-effects. Happy hacking! Consider using YAML. It's nearly a superset of JSON (virtually all valid JSON is valid YAML) and it allows comments. You can't. At least that's my experience from a quick glance at json.org. JSON has its syntax visualized on that page. There isn't any note about comments. Comments are not an official standard, although some parsers support C++-style comments. One that I use is JsonCpp. In the examples there is this one: jsonlint does not validate this. So comments are a parser specific extension and not standard. Another parser is JSON5. An alternative to JSON TOML. A further alternative is jsonc. The latest version of nlohmann/json has optional support for ignoring comments on parsing. You should write a JSON schema instead. JSON schema is currently a proposed Internet draft specification. Besides documentation, the schema can also be used for validating your JSON data. Example: You can provide documentation by using the description schema attribute. If you are using Jackson as your JSON parser then this is how you enable it to allow comments: Then you can have comments like this: And you can also have comments starting with # by setting: But in general (as answered before) the specification does not allow comments. Here is what I found in the Google Firebase documentation that allows you to put comments in JSON: NO. JSON used to support comments, but they were abused and removed from the standard. From the creator of JSON: I removed comments from JSON because I saw people were using them to hold parsing directives, a practice which would have destroyed interoperability.  I know that the lack of comments makes some people sad, but it shouldn't.  - Douglas Crockford, 2012 The official JSON site is at JSON.org. JSON is defined as a standard by ECMA International. There is always a petition process to have standards revised. It is unlikely that annotations will be added to the JSON standard for several reasons. JSON by design is an easily reverse-engineered (human parsed) alternative to XML. It is simplified even to the point that annotations are unnecessary. It is not even a markup language. The goal is stability and  interoperablilty. Anyone who understands the "has-a" relationship of object orientation can understand any JSON structure - that is the whole point. It is just a  directed acyclic graph (DAG) with node tags (key/value pairs), which is a near universal data structure. This only annotation required might be "//These are DAG tags". The key names can be as informative as required, allowing arbitrary semantic arity. Any platform can parse JSON with just a few lines of code. XML requires complex OO libraries that are not viable on many platforms. Annotations would just make JSON make less interoperable. There is simply nothing else to add, unless what you really need is a markup language (XML), and don't care if your persisted data is easily parsed. BUT as the creator of JSON also observed, there has always been JS pipeline support for comments: Go ahead and insert all the comments you like.   Then pipe it through JSMin before handing it to your JSON parser. - Douglas Crockford, 2012 If your text file, which is a JSON string, is going to be read by some program, how difficult would it be to strip out either C or C++ style comments before using it? Answer: It would be a one liner. If you do that then JSON files could be used as configuration files. If you are using the Newtonsoft.Json library with ASP.NET to read/deserialize you can use comments in the JSON content: //"name": "string" //"id": int or /* This is a comment example */ PS: Single-line comments are only supported with 6+ versions of Newtonsoft Json. Additional note for people who can't think out of the box: I use the JSON format for basic settings in an ASP.NET web application I made. I read the file, convert it into the settings object with the Newtonsoft library and use it when necessary. I prefer writing comments about each individual setting in the JSON file itself, and I really don't care about the integrity of the JSON format as long as the library I use is OK with it. I think this is an 'easier to use/understand' way than creating a separate 'settings.README' file and explaining the settings in it. If you have a problem with this kind of usage; sorry, the genie is out of the lamp. People would find other usages for JSON format, and there is nothing you can do about it. The idea behind JSON is to provide simple data exchange between applications. These are typically web based and the language is JavaScript. It doesn't really allow for comments as such, however, passing a comment as one of the name/value pairs in the data would certainly work, although that data would obviously need to be ignored or handled specifically by the parsing code. All that said, it's not the intention that the JSON file should contain comments in the traditional sense. It should just be the data. Have a look at the JSON website for more detail. JSON does not support comments natively, but you can make your own decoder or at least preprocessor to strip out comments, that's perfectly fine (as long as you just ignore comments and don't use them to guide how your application should process the JSON data). JSON does not have comments. A JSON encoder MUST NOT output comments.   A JSON decoder MAY accept and ignore comments. Comments should never be used to transmit anything meaningful. That is   what JSON is for. Cf: Douglas Crockford, author of JSON spec. I just encountering this for configuration files. I don't want to use XML (verbose, graphically, ugly, hard to read), or "ini" format (no hierarchy, no real standard, etc.) or Java "Properties" format (like .ini). JSON can do all they can do, but it is way less verbose and more human readable - and parsers are easy and ubiquitous in many languages. It's just a tree of data. But out-of-band comments are a necessity often to document "default" configurations and the like. Configurations are never to be "full documents", but trees of saved data that can be human readable when needed. I guess one could use "#": "comment", for "valid" JSON. It depends on your JSON library. Json.NET supports JavaScript-style comments, /* commment */. See another Stack Overflow question. JSON makes a lot of sense for config files and other local usage because it's ubiquitous and because it's much simpler than XML.  If people have strong reasons against having comments in JSON when communicating data (whether valid or not), then possibly JSON could be split into two: JSON-DOC will allow comments, and other minor differences might exist such as handling whitespace. Parsers can easily convert from one spec to the other.  With regards to the remark made by Douglas Crockford on this issues (referenced by @Artur Czajka) Suppose you are using JSON to keep configuration files, which you would like to annotate. Go ahead and insert all the comments you like. Then pipe it through JSMin before handing it to your JSON parser. We're talking about a generic config file issue (cross language/platform), and he's answering with a JS specific utility! Sure a JSON specific minify can be implemented in any language, but standardize this so it becomes ubiquitous across parsers in all languages and platforms so people stop wasting their time lacking the feature because they have good use-cases for it, looking the issue up in online forums, and getting people telling them it's a bad idea or suggesting it's easy to implement stripping comments out of text files. The other issue is interoperability. Suppose you have a library or API or any kind of subsystem which has some config or data files associated with it. And this subsystem is to be accessed from different languages.  Then do you go about telling people: by the way don't forget to strip out the comments from the JSON files before passing them to the parser! If you use JSON5 you can include comments. JSON5 is a proposed extension to JSON that aims to make it easier for humans to write and maintain by hand. It does this by adding some minimal syntax features directly from ECMAScript 5. The Dojo Toolkit JavaScript toolkit (at least as of version 1.4), allows you to include comments in your JSON. The comments can be of /* */ format. Dojo Toolkit consumes the JSON via the dojo.xhrGet() call. Other JavaScript toolkits may work similarly. This can be helpful when experimenting with alternate data structures (or even data lists) before choosing a final option. JSON is not a framed protocol. It is a language free format. So a comment's format is not defined for JSON. As many people have suggested, there are some tricks, for example, duplicate keys or a specific key _comment that you can use. It's up to you. You can have comments in JSONP, but not in pure JSON. I've just spent an hour trying to make my program work with this example from Highcharts: http://www.highcharts.com/samples/data/jsonp.php?filename=aapl-c.json&callback=? If you follow the link, you will see Since I had a similar file in my local folder, there were no issues with the Same-origin policy, so I decided to use pure JSON... and, of course, $.getJSON was failing silently because of the comments. Eventually I just sent a manual HTTP request to the address above and realized that the content-type was text/javascript since, well, JSONP returns pure JavaScript. In this case comments are allowed. But my application returned content-type application/json, so I had to remove the comments. This is a "can you" question. And here is a "yes" answer. No, you shouldn't use duplicative object members to stuff side channel data into a JSON encoding. (See "The names within an object SHOULD be unique" in the RFC). And yes, you could insert comments around the JSON, which you could parse out. But if you want a way of inserting and extracting arbitrary side-channel data to a valid JSON, here is an answer. We take advantage of the non-unique representation of data in a JSON encoding. This is allowed* in section two of the RFC under "whitespace is allowed before or after any of the six structural characters". *The RFC only states "whitespace is allowed before or after any of the six structural characters", not explicitly mentioning strings, numbers, "false", "true", and "null". This omission is ignored in ALL implementations. First, canonicalize your JSON by minifying it: Then encode your comment in binary: Then steg your binary: Here is your output: Disclaimer: This is silly There is actually a way to add comments, and stay within the specification (no additional parser needed). It will not result into human-readable comments without any sort of parsing though. You could abuse the following: Insignificant whitespace is allowed before or after any token. Whitespace is any sequence of one or more of the following code points: character tabulation (U+0009), line feed (U+000A), carriage return (U+000D), and space (U+0020). In a hacky way, you can abuse this to add a comment. For instance: start and end your comment with a tab. Encode the comment in base3 and use the other whitespace characters to represent them. For instance. (hello base three in ASCII) But instead of 0 use space, for 1 use line feed and for 2 use carriage return. This will just leave you with a lot of unreadable whitespace (unless you make an IDE plugin to encode/decode it on the fly). I never even tried this, for obvious reasons and neither should you. JSON doesn't allow comments, per se. The reasoning is utterly foolish, because you can use JSON itself to create comments, which obviates the reasoning entirely, and loads the parser data space for no good reason at all for exactly the same result and potential issues, such as they are: a JSON file with comments. If you try to put comments in (using // or /* */ or # for instance), then some parsers will fail because this is strictly not within the JSON specification. So you should never do that. Here, for instance, where my image manipulation system has saved image notations and some basic formatted (comment) information relating to them (at the bottom): We are using strip-json-comments for our project. It supports something like: Simply npm install --save strip-json-comments to install and use it like: In my case, I need to use comments for debug purposes right before the output of the JSON structure. So I decided to use debug information in the HTTP header, to avoid breaking the client:  To cut a JSON item into parts I add "dummy comment" lines:
__label__javascript __label__visibility __label__jquery __label__dom Is it possible to toggle the visibility of an element, using the functions .hide(), .show() or .toggle()? How would you test if an element is visible or hidden? Since the question refers to a single element, this code might be more suitable: It is the same as twernt's suggestion, but applied to a single element; and it matches the algorithm recommended in the jQuery FAQ. We use jQuery's is() to check the selected element with another element, selector or any jQuery object. This method traverses along the DOM elements to find a match, which satisfies the passed parameter. It will return true if there is a match, otherwise return false. You can use the hidden selector: And the visible selector: The above method does not consider the visibility of the parent. To consider the parent as well, you should use .is(":hidden") or .is(":visible"). For example, The above method will consider div2 visible while :visible not. But the above might be useful in many cases, especially when you need to find if there is any error divs visible in the hidden parent because in such conditions :visible will not work. None of these answers address what I understand to be the question, which is what I was searching for, "How do I handle items that have visibility: hidden?". Neither :visible nor :hidden will handle this, as they are both looking for display per the documentation.  As far as I could determine, there is no selector to handle CSS visibility.  Here is how I resolved it (standard jQuery selectors, there may be a more condensed syntax): From How do I determine the state of a toggled element? You can determine whether an element is collapsed or not by using the :visible and :hidden selectors. If you're simply acting on an element based on its visibility, you can just include :visible or :hidden in the selector expression. For example: Often when checking if something is visible or not, you are going to go right ahead immediately and do something else with it. jQuery chaining makes this easy. So if you have a selector and you want to perform some action on it only if is visible or hidden, you can use filter(":visible") or filter(":hidden") followed by chaining it with the action you want to take. So instead of an if statement, like this: Or more efficient, but even uglier: You can do it all in one line: The :visible selector according to the jQuery documentation: Elements with visibility: hidden or opacity: 0 are considered to be visible, since they still consume space in the layout. This is useful in some cases and useless in others, because if you want to check if the element is visible (display != none), ignoring the parents visibility, you will find that doing .css("display") == 'none' is not only faster, but will also return the visibility check correctly. If you want to check visibility instead of display, you should use: .css("visibility") == "hidden". Also take into consideration the additional jQuery notes: Because :visible is a jQuery extension and not part of the CSS specification, queries using :visible cannot take advantage of the performance boost provided by the native DOM querySelectorAll() method. To achieve the best performance when using :visible to select elements, first select the elements using a pure CSS selector, then use .filter(":visible"). Also, if you are concerned about performance, you should check Now you see me… show/hide performance (2010-05-04). And use other methods to show and hide elements. This works for me, and I am using show() and hide() to make my div hidden/visible: How element visibility and jQuery works; An element could be hidden with display:none, visibility:hidden or opacity:0. The difference between those methods: opacity:0 hides the element as "visibility:hidden", and it still takes up space in the layout; the only difference is that opacity lets one to make an element partly transparent;    Useful jQuery toggle methods:  You can also do this using plain JavaScript: Notes: Works everywhere Works for nested elements Works for CSS and inline styles Doesn't require a framework I would use CSS class .hide { display: none!important; }.  For hiding/showing, I call .addClass("hide")/.removeClass("hide"). For checking visibility, I use .hasClass("hide"). It's a simple and clear way to check/hide/show elements, if you don't plan to use .toggle() or .animate() methods. Demo Link    $('#clickme').click(function() {   $('#book').toggle('slow', function() {     // Animation complete.     alert($('#book').is(":visible")); //<--- TRUE if Visible False if Hidden   }); }); <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <div id="clickme">   Click here </div> <img id="book" src="https://upload.wikimedia.org/wikipedia/commons/8/87/Google_Chrome_icon_%282011%29.png" alt="" width="300"/>    Source:  Blogger Plug n Play - jQuery Tools and Widgets: How to See if Element is hidden or Visible Using jQuery One can simply use the hidden or visible attribute, like: Or you can simplify the same with is as follows. ebdiv should be set to style="display:none;". It works for both show and hide: Another answer you should put into consideration is if you are hiding an element, you should use jQuery, but instead of actually hiding it, you remove the whole element, but you copy its HTML content and the tag itself into a jQuery variable, and then all you need to do is test if there is such a tag on the screen, using the normal if (!$('#thetagname').length). When testing an element against :hidden selector in jQuery it should be considered that an absolute positioned element may be recognized as hidden although their child elements are visible. This seems somewhat counter-intuitive in the first place – though having a closer look at the jQuery documentation gives the relevant information: Elements can be considered hidden for several reasons: [...] Their width and height are explicitly set to 0. [...] So this actually makes sense in regards to the box-model and the computed style for the element. Even if width and height are not set explicitly to 0 they may be set implicitly. Have a look at the following example:   console.log($('.foo').is(':hidden')); // true console.log($('.bar').is(':hidden')); // false .foo {   position: absolute;   left: 10px;   top: 10px;   background: #ff0000; }  .bar {   position: absolute;   left: 10px;   top: 10px;   width: 20px;   height: 20px;   background: #0000ff; } <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <div class="foo">   <div class="bar"></div> </div>    Update for jQuery 3.x: With jQuery 3 the described behavior will change! Elements will be considered visible if they have any layout boxes, including those of zero width and/or height. JSFiddle with jQuery 3.0.0-alpha1: http://jsfiddle.net/pM2q3/7/ The same JavaScript code will then have this output: This may work: Example:    $(document).ready(function() {   if ($("#checkme:hidden").length) {     console.log('Hidden');   } }); <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <div id="checkme" class="product" style="display:none">   <span class="itemlist"><!-- Shows Results for Fish --></span> Category:Fish   <br>Product: Salmon Atlantic   <br>Specie: Salmo salar   <br>Form: Steaks </div>    To check if it is not visible I use !: Or the following is also the sam, saving the jQuery selector in a variable to have better performance when you need it multiple times: Using classes designated for "hiding" elements is easy and also one of the most efficient methods. Toggling a class 'hidden' with a Display style of 'none' will perform faster than editing that style directly. I explained some of this pretty thoroughly in Stack Overflow question Turning two elements visible/hidden in the same div. Here is a truly enlightening video of a Google Tech Talk by Google front-end engineer Nicholas Zakas: Example of using the visible check for adblocker is activated:   $(document).ready(function(){   if(!$("#ablockercheck").is(":visible"))     $("#ablockermsg").text("Please disable adblocker.").show(); }); <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <div class="ad-placement" id="ablockercheck"></div> <div id="ablockermsg" style="display: none"></div>    "ablockercheck" is a ID which adblocker blocks. So checking it if it is visible you are able to detect if adblocker is turned On. After all, none of examples suits me, so I wrote my own. Tests (no support of Internet Explorer filter:alpha): a) Check if the document is not hidden b) Check if an element has zero width / height / opacity or display:none / visibility:hidden in inline styles c) Check if the center (also because it is faster than testing every pixel / corner) of element is not hidden by other element (and all ancestors, example: overflow:hidden / scroll / one element over another) or screen edges d) Check if an element has zero width / height / opacity or display:none / visibility:hidden in computed styles (among all ancestors) Tested on Android 4.4 (Native browser/Chrome/Firefox), Firefox (Windows/Mac), Chrome (Windows/Mac), Opera (Windows Presto/Mac WebKit), Internet Explorer (Internet Explorer 5-11 document modes + Internet Explorer 8 on a virtual machine), and Safari (Windows/Mac/iOS). How to use: You need to check both... Display as well as visibility: If we check for $(this).is(":visible"), jQuery checks for both the things automatically. Maybe you can do something like this   $(document).ready(function() {    var visible = $('#tElement').is(':visible');     if(visible) {       alert("visible");                     // Code    }    else    {       alert("hidden");    } }); <script src="https://code.jquery.com/jquery-1.10.2.js"></script>  <input type="text" id="tElement" style="display:block;">Firstname</input>    Simply check visibility by checking for a boolean value, like: I used this code for each function. Otherwise you can use is(':visible') for checking the visibility of an element. Because Elements with visibility: hidden or opacity: 0 are considered visible, since they still consume space in the layout (as described for jQuery :visible Selector) - we can check if element is really visible in this way: But what if the element's CSS is like the following? So this answer to Stack Overflow question How to check if an element is off-screen should also be considered. A function can be created in order to check for visibility/display attributes in order to gauge whether the element is shown in the UI or not.  Working Fiddle Also here's a ternary conditional expression to check the state of the element and then to toggle it: 
__label__html __label__background-color __label__browser How come certain random strings produce colors when entered as background colors in HTML? For example:   <body bgcolor="chucknorris"> test </body>    ...produces a document with a red background across all browsers and platforms. Interestingly, while chucknorri produces a red background as well, chucknorr produces a yellow background. What’s going on here? It’s a holdover from the Netscape days: Missing digits are treated as 0[...]. An incorrect digit is simply interpreted as 0. For example the values #F0F0F0, F0F0F0, F0F0F, #FxFxFx and FxFxFx are all the same. It is from the blog post A little rant about Microsoft Internet Explorer's color parsing which covers it in great detail, including varying lengths of color values, etc. If we apply the rules in turn from the blog post, we get the following: Replace all nonvalid hexadecimal characters with 0’s: Pad out to the next total number of characters divisible by 3 (11 → 12): Split into three equal groups, with each component representing the corresponding colour component of an RGB colour: Truncate each of the arguments from the right down to two characters. Which, finally, gives the following result: Here’s an example demonstrating the bgcolor attribute in action, to produce this “amazing” colour swatch:   <table>   <tr>     <td bgcolor="chucknorris" cellpadding="8" width="100" align="center">chuck norris</td>     <td bgcolor="mrt"         cellpadding="8" width="100" align="center" style="color:#ffffff">Mr T</td>     <td bgcolor="ninjaturtle" cellpadding="8" width="100" align="center" style="color:#ffffff">ninjaturtle</td>   </tr>   <tr>     <td bgcolor="sick"  cellpadding="8" width="100" align="center">sick</td>     <td bgcolor="crap"  cellpadding="8" width="100" align="center">crap</td>     <td bgcolor="grass" cellpadding="8" width="100" align="center">grass</td>   </tr> </table>    This also answers the other part of the question: Why does bgcolor="chucknorr" produce a yellow colour? Well, if we apply the rules, the string is: Which gives a light yellow gold colour. As the string starts off as 9 characters, we keep the second ‘C’ this time around, hence it ends up in the final colour value. I originally encountered this when someone pointed out that you could do color="crap" and, well, it comes out brown. I'm sorry to disagree, but according to the rules for parsing a legacy color value posted by @Yuhong Bao, chucknorris DOES NOT equate to #CC0000, but rather to #C00000, a very similar but slightly different hue of red. I used the Firefox ColorZilla add-on to verify this. The rules state:   I was able to use these rules to correctly interpret the following strings: UPDATE: The original answerers who said the color was #CC0000 have since edited their answers to include the correction. Most browsers will simply ignore any NON-hex values in your color string, substituting non-hex digits with zeros. ChuCknorris translates to c00c0000000.  At this point, the browser will divide the string into three equal sections, indicating Red, Green and Blue values: c00c 0000 0000.  Extra bits in each section will be ignored, which makes the final result #c00000 which is a reddish color. Note, this does not apply to CSS color parsing, which follow the CSS standard.  <p><font color='chucknorris'>Redish</font></p> <p><font color='#c00000'>Same as above</font></p> <p><span style="color: chucknorris">Black</span></p>    The reason is the browser can not understand it and try to somehow translate it to what it can understand and in this case into a hexadecimal value!... chucknorris starts with c which is recognised character in hexadecimal, also it's converting all unrecognised characters into 0! So chucknorris in hexadecimal format becomes: c00c00000000, all other characters become 0 and c remains where they are... Now they get divided by 3 for RGB(red, green, blue)... R: c00c, G: 0000, B:0000... But we know valid hexadecimal for RGB is just 2 characters, means R: c0, G: 00, B:00 So the real result is: I also added the steps in the image as a quick reference for you:  The browser is trying to convert chucknorris into hex colour code, because it’s not a valid value. This seems to be an issue primarily with Internet Explorer and Opera (12) as both Chrome (31) and Firefox (26) just ignore this. P.S. The numbers in brackets are the browser versions I tested on. On a lighter note Chuck Norris doesn’t conform to web standards. Web standards conform to him. #BADA55 The WHATWG HTML spec has the exact algorithm for parsing a legacy color value: https://html.spec.whatwg.org/multipage/infrastructure.html#rules-for-parsing-a-legacy-colour-value. The code Netscape Classic used for parsing color strings is open source: https://dxr.mozilla.org/classic/source/lib/layout/layimage.c#155. For example, notice that each character is parsed as a hex digit and then is shifted into a 32-bit integer without checking for overflow. Only eight hex digits fit into a 32-bit integer, which is why only the last 8 characters are considered. After parsing the hex digits into 32-bit integers, they are then truncated into 8-bit integers by dividing them by 16 until they fit into 8-bit, which is why leading zeros are ignored. Update: This code does not exactly match what is defined in the spec, but the only difference there is a few lines of code. I think it is these lines that was added (in Netscape 4): Answer:  chucknorris starts with c, and the browser reads it into a hexadecimal value. Because A, B, C, D, E, and F are characters in hexadecimal. The browser converts chucknorris to a hexadecimal value, C00C00000000. Then the C00C00000000 hexadecimal value is converted to RGB format (divided by 3): C00C00000000 ⇒ R:C00C, G:0000, B:0000 The browser needs only two digits to indicate the colour: R:C00C, G:0000, B:0000 ⇒ R:C0, G:00, B:00 ⇒ C00000 Finally, show bgcolor = C00000 in the web browser. Here's an example demonstrating it:   <table>   <tr>     <td bgcolor="chucknorris" cellpadding="10" width="150" align="center">chucknorris</td>     <td bgcolor="c00c00000000" cellpadding="10" width="150" align="center">c00c00000000</td>     <td bgcolor="c00000" cellpadding="10" width="150" align="center">c00000</td>   </tr> </table>    The rules for parsing colors on legacy attributes involves additional steps than those mentioned in existing answers. The truncate component to 2 digits part is described as: Some examples: Below is a partial implementation of the algorithm. It does not handle errors or cases where the user enters a valid color.   function parseColor(input) {   // todo: return error if input is ""   input = input.trim();   // todo: return error if input is "transparent"   // todo: return corresponding #rrggbb if input is a named color   // todo: return #rrggbb if input matches #rgb   // todo: replace unicode code points greater than U+FFFF with 00   if (input.length > 128) {     input = input.slice(0, 128);   }   if (input.charAt(0) === "#") {     input = input.slice(1);   }   input = input.replace(/[^0-9A-Fa-f]/g, "0");   while (input.length === 0 || input.length % 3 > 0) {     input += "0";   }   var r = input.slice(0, input.length / 3);   var g = input.slice(input.length / 3, input.length * 2 / 3);   var b = input.slice(input.length * 2 / 3);   if (r.length > 8) {     r = r.slice(-8);     g = g.slice(-8);     b = b.slice(-8);   }   while (r.length > 2 && r.charAt(0) === "0" && g.charAt(0) === "0" && b.charAt(0) === "0") {     r = r.slice(1);     g = g.slice(1);     b = b.slice(1);   }   if (r.length > 2) {     r = r.slice(0, 2);     g = g.slice(0, 2);     b = b.slice(0, 2);   }   return "#" + r.padStart(2, "0") + g.padStart(2, "0") + b.padStart(2, "0"); }  $(function() {   $("#input").on("change", function() {     var input = $(this).val();     var color = parseColor(input);     var $cells = $("#result tbody td");     $cells.eq(0).attr("bgcolor", input);     $cells.eq(1).attr("bgcolor", color);      var color1 = $cells.eq(0).css("background-color");     var color2 = $cells.eq(1).css("background-color");     $cells.eq(2).empty().append("bgcolor: " + input, "<br>", "getComputedStyle: " + color1);     $cells.eq(3).empty().append("bgcolor: " + color, "<br>", "getComputedStyle: " + color2);   }); }); body { font: medium monospace; } input { width: 20em; } table { table-layout: fixed; width: 100%; } <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js"></script>  <p><input id="input" placeholder="Enter color e.g. chucknorris"></p> <table id="result">   <thead>     <tr>       <th>Left Color</th>       <th>Right Color</th>     </tr>   </thead>   <tbody>     <tr>       <td>&nbsp;</td>       <td>&nbsp;</td>     </tr>     <tr>       <td>&nbsp;</td>       <td>&nbsp;</td>     </tr>   </tbody> </table>   
__label__javascript __label__syntax __label__use-strict __label__jslint Recently, I ran some of my JavaScript code through Crockford's JSLint, and it gave the following error: Problem at line 1 character 1: Missing "use strict" statement. Doing some searching, I realized that some people add "use strict"; into their JavaScript code. Once I added the statement, the error stopped appearing. Unfortunately, Google did not reveal much of the history behind this string statement. Certainly it must have something to do with how the JavaScript is interpreted by the browser, but I have no idea what the effect would be. So what is "use strict"; all about, what does it imply, and is it still relevant? Do any of the current browsers respond to the "use strict"; string or is it for future use? This article about Javascript Strict Mode might interest you: John Resig - ECMAScript 5 Strict Mode, JSON, and More To quote some interesting parts: Strict Mode is a new feature in ECMAScript 5 that allows you to place a program, or a function, in a "strict" operating context. This strict context prevents certain actions from being taken and throws more exceptions. And: Strict mode helps out in a couple ways: Also note you can apply "strict mode" to the whole file... Or you can use it only for a specific function (still quoting from John Resig's article): Which might be helpful if you have to mix old and new code ;-) So, I suppose it's a bit like the "use strict" you can use in Perl (hence the name?): it helps you make fewer errors, by detecting more things that could lead to breakages. Strict mode is now supported by all major browsers. Inside native ECMAScript modules (with import and export statements) and ES6 classes, strict mode is always enabled and cannot be disabled. It's a new feature of ECMAScript 5. John Resig wrote up a nice summary of it. It's just a string you put in your JavaScript files (either at the top of your file or inside of a function) that looks like this: Putting it in your code now shouldn't cause any problems with current browsers as it's just a string. It may cause problems with your code in the future if your code violates the pragma.  For instance, if you currently have foo = "bar" without defining foo first, your code will start failing...which is a good thing in my opinion. The statement "use strict"; instructs the browser to use the Strict mode, which is a reduced and safer feature set of JavaScript.  Disallows global variables. (Catches missing var declarations and typos in variable names)  Silent failing assignments will throw error in strict mode (assigning NaN = 5;)  Attempts to delete undeletable properties will throw (delete Object.prototype)  Requires all property names in an object literal to be unique (var x = {x1: "1", x1: "2"})  Function parameter names must be unique (function sum (x, x) {...})  Forbids octal syntax (var x = 023; some devs assume wrongly that a preceding zero does nothing to change the number.)  Forbids the with keyword  eval in strict mode does not introduce new variables   Forbids deleting plain names (delete x;) Forbids binding or assignment of the names eval and arguments in any form  Strict mode does not alias properties of the arguments object with the formal parameters. (i.e. in function sum (a,b) { return arguments[0] + b;} This works because arguments[0] is bound to a and so on. ) arguments.callee is not supported [Ref: Strict mode, Mozilla Developer Network] If people are worried about using use strict it might be worth checking out this article:  ECMAScript 5 'Strict mode' support in browsers. What does this mean? NovoGeek.com - Krishna's weblog It talks about browser support, but more importantly how to deal with it safely: A word of caution, all you hard-charging programmers:  applying "use strict" to existing code can be hazardous!  This thing is not some feel-good, happy-face sticker that you can slap on the code to make it 'better'.  With the "use strict" pragma, the browser will suddenly THROW exceptions in random places that it never threw before just because at that spot you are doing something that default/loose JavaScript happily allows but strict JavaScript abhors!  You may have strictness violations hiding in seldom used calls in your code that will only throw an exception when they do eventually get run - say, in the production environment that your paying customers use! If you are going to take the plunge, it is a good idea to apply "use strict" alongside comprehensive unit tests and a strictly configured JSHint build task that will give you some confidence that there is no dark corner of your module that will blow up horribly just because you've turned on Strict Mode.  Or, hey, here's another option:  just don't add "use strict" to any of your legacy code, it's probably safer that way, honestly.  DEFINITELY DO NOT add "use strict" to any modules you do not own or maintain, like third party modules. I think even though it is a deadly caged animal, "use strict" can be good stuff, but you have to do it right.  The best time to go strict is when your project is greenfield and you are starting from scratch. Configure JSHint/JSLint with all the warnings and options cranked up as tight as your team can stomach, get a good build/test/assert system du jour rigged like Grunt+Karma+Chai, and only THEN start marking all your new modules as "use strict".  Be prepared to cure lots of niggly errors and warnings.  Make sure everyone understands the gravity by configuring the build to FAIL if JSHint/JSLint produces any violations. My project was not a greenfield project when I adopted "use strict".  As a result, my IDE is full of red marks because I don't have "use strict" on half my modules, and JSHint complains about that.  It's a reminder to me about what refactoring I should do in the future.  My goal is to be red mark free due to all of my missing "use strict" statements, but that is years away now. The JavaScript strict mode is a feature in ECMAScript 5. You can enable the strict mode by declaring this in the top of your script/function. When a JavaScript engine sees this directive, it will start to interpret the code in a special mode. In this mode, errors are thrown up when certain coding practices that could end up being potential bugs are detected (which is the reasoning behind the strict mode). Consider this example: In their obsession to line up the numeric literals, the developer has inadvertently initialized variable b with an octal literal. Non-strict mode will interpret this as a numeric literal with value 24 (in base 10). However, strict mode will throw an error. For a non-exhaustive list of specialties in strict mode, see this answer. In my new JavaScript application: Absolutely! Strict mode can be used as a whistleblower when you are doing something stupid with your code. In my existing JavaScript code: Probably not! If your existing JavaScript code has statements that are prohibited in strict-mode, the application will simply break. If you want strict mode, you should be prepared to debug and correct your existing code. This is why using 'use strict'; does not suddenly make your code better. Insert a 'use strict'; statement on top of your script: Note that everything in the file myscript.js will be interpreted in strict mode. Or, insert a 'use strict'; statement on top of your function body: Everything in the lexical scope of function doSomething will be interpreted in strict mode. The word lexical scope is important here. For example, if your strict code calls a function of a library that is not strict, only your code is executed in strict mode, and not the called function. See this answer for a better explanation. I found a nice article describing several things that are prohibited in strict mode (note that this is not an exclusive list): Historically, JavaScript has been confused about how functions   are scoped. Sometimes they seem to be statically scoped, but some   features make them behave like they are dynamically scoped. This is   confusing, making programs difficult to read and understand.   Misunderstanding causes bugs. It also is a problem for performance.   Static scoping would permit variable binding to happen at compile   time, but the requirement for dynamic scope means the binding must be   deferred to runtime, which comes with a significant performance   penalty. Strict mode requires that all variable binding be done statically.   That means that the features that previously required dynamic binding   must be eliminated or modified. Specifically, the with statement is   eliminated, and the eval function’s ability to tamper with the   environment of its caller is severely restricted. One of the benefits of strict code is that tools like YUI Compressor   can do a better job when processing it. JavaScript has implied global variables. If   you do not explicitly declare a variable, a global variable is   implicitly declared for you. This makes programming easier for   beginners because they can neglect some of their basic housekeeping   chores. But it makes the management of larger programs much more   difficult and it significantly degrades reliability. So in strict   mode, implied global variables are no longer created. You should   explicitly declare all of your variables. There are a number of situations that could cause this   to be bound to the global object. For example, if you forget to   provide the new prefix when calling a constructor function, the   constructor's this will be bound unexpectedly to the global object, so   instead of initializing a new object, it will instead be silently   tampering with global variables. In these situations, strict mode will   instead bind this to undefined, which will cause the constructor to   throw an exception instead, allowing the error to be detected much   sooner. JavaScript has always had read-only properties, but you   could not create them yourself until ES5’s Object.createProperty   function exposed that capability. If you attempted to assign a value   to a read-only property, it would fail silently. The assignment would   not change the property’s value, but your program would proceed as   though it had. This is an integrity hazard that can cause programs to   go into an inconsistent state. In strict mode, attempting to change a   read-only property will throw an exception. The octal (or base 8) representation of numbers was extremely   useful when doing machine-level programming on machines whose word   sizes were a multiple of 3. You needed octal when working with the CDC   6600 mainframe, which had a word size of 60 bits. If you could read   octal, you could look at a word as 20 digits. Two digits represented   the op code, and one digit identified one of 8 registers. During the   slow transition from machine codes to high level languages, it was   thought to be useful to provide octal forms in programming languages. In C, an extremely unfortunate representation of octalness was   selected: Leading zero. So in C, 0100 means 64, not 100, and 08 is an   error, not 8. Even more unfortunately, this anachronism has been   copied into nearly all modern languages, including JavaScript, where   it is only used to create errors. It has no other purpose. So in   strict mode, octal forms are no longer allowed. The arguments pseudo array becomes a little bit more   array-like in ES5. In strict mode, it loses its callee and caller   properties. This makes it possible to pass your arguments to untrusted   code without giving up a lot of confidential context. Also, the   arguments property of functions is eliminated. In strict mode, duplicate keys in a function literal will produce a   syntax error. A function can’t have two parameters with the same name.   A function can’t have a variable with the same name as one of its   parameters. A function can’t delete its own variables. An attempt to   delete a non-configurable property now throws an exception. Primitive   values are not implicitly wrapped. ECMAScript 5 adds a list of reserved words. If you use them as variables or arguments, strict mode will throw an error. The reserved words are: implements, interface, let, package, private, protected, public, static, and yield I strongly recommend every developer to start using strict mode now. There are enough browsers supporting it that strict mode will legitimately help save us from errors we didn’t even know were in your code. Apparently, at the initial stage there will be errors we have never encountered before. To get the full benefit, we need to do proper testing after switching to strict mode to make sure we have caught everything. Definitely we don’t just throw use strict in our code and assume there are no errors. So the churn is that it’s time to start using this incredibly useful language feature to write better code. For example, JSLint is a debugger written by Douglas Crockford. Simply paste in your script, and it’ll quickly scan for any noticeable issues and errors in your code. I would like to offer a somewhat more founded answer complementing the other answers. I was hoping to edit the most popular answer, but failed. I tried to make it as comprehensive and complete as I could. You can refer to the MDN documentation for more information. "use strict" a directive introduced in ECMAScript 5. Directives are similar to statements, yet different. The use strict directive indicates that the following code (in a script or a function) is strict code. The code in the highest level of a script (code that is not in a function) is considered strict code when the script contains a use strict directive. The content of a function is considered strict code when the function itself is defined in a strict code or when the function contains a use strict directive. Code that is passed to an eval() method is considered strict code when eval() was called from a strict code or contains the use strict directive itself. The strict mode of ECMAScript 5 is a restricted subset of the JavaScript language, which eliminates relevant deficits of the language and features more stringent error checking and higher security. The following lists the differences between strict mode and normal mode (of which the first three are particularly important): Also when a function is invoked with call() or apply in strict mode, then this is exactly the value of the first argument of the call()or apply() invocation. (In normal mode null and undefined are replaced by the global Object and values, which are not objects, are cast into objects.) In strict mode you will get a TypeError, when you try to assign to readonly properties or to define new properties for a non extensible object. (In normal mode both simply fail without error message.) My two cents: One of the goals of strict mode is to allow for faster debugging of issues. It helps the developers by throwing exception when certain wrong things occur that can cause silent & strange behaviour of your webpage. The moment we use use strict, the code will throw out errors which helps developer to fix it in advance. Few important things which I have learned after using use strict : Prevents Global Variable Declaration: Now,this code creates nameoftree in global scope which could be accessed using window.nameoftree. When we implement use strict the code would throw error. Uncaught ReferenceError: nameoftree is not defined Sample Eliminates with statement : with statements can't be minified using tools like uglify-js. They're also deprecated and removed from future JavaScript versions. Sample Prevents Duplicates : When we have duplicate property, it throws an exception  Uncaught SyntaxError: Duplicate data property in object literal not   allowed in strict mode There are few more but I need to gain more knowledge on that. If you use a browser released in the last year or so then it most likely supports JavaScript Strict mode. Only older browsers around before ECMAScript 5 became the current standard don't support it.  The quotes around the command make sure that the code will still work in older browsers as well (although the things that generate a syntax error in strict mode will generally just cause the script to malfunction in some hard to detect way in those older browsers). When adding "use strict";, the following cases will throw a SyntaxError before the script is executing: Paving the way for future ECMAScript versions, using one of the newly reserved keywords (in prevision for ECMAScript 6): implements, interface, let, package, private, protected, public, static, and yield. Declaring function in blocks  Octal syntax  this point to the global object. Declaring twice the same name for a property name in an object literal  This is no longer the case in ECMAScript 6 (bug 1041128). Declaring two function arguments with the same name function  Setting a value to an undeclared variable Using delete on a variable name delete myVariable; Using eval or arguments as variable or function argument name Sources: Transitioning to strict mode on MDN Strict mode on MDN JavaScript’s Strict Mode and Why You Should Use It on Colin J. Ihrig's blog (archived version) Strict mode makes several changes to normal JavaScript semantics: eliminates some JavaScript silent errors by changing them to throw errors. fixes mistakes that make it difficult for JavaScript engines to perform optimizations. prohibits some syntax likely to be defined in future versions of ECMAScript. for more information vistit Strict Mode- Javascript "Use Strict"; is an insurance that programmer will not use the loose or the bad properties of JavaScript. It is a guide, just like a ruler will help you make straight lines. "Use Strict" will help you do "Straight coding". Those that prefer not to use rulers to do their lines straight usually end up in those pages asking for others to debug their code. Believe me. The overhead is negligible compared to poorly designed code. Doug Crockford, who has been a senior JavaScript developer for several years, has a very interesting post here. Personally, I like to return to his site all the time to make sure I don't forget my good practice. Modern JavaScript practice should always evoke the "Use Strict"; pragma. The only reason that the ECMA Group has made the "Strict" mode optional is to permit less experienced coders access to JavaScript and give then time to adapt to the new and safer coding practices. Including use strict in the beginning of your all sensitive JavaScript files from this point is a small way to be a better JavaScript programmer and avoid random variables becoming global and things change silently. Quoting from w3schools: The "use strict" directive is new in JavaScript 1.8.5 (ECMAScript   version 5). It is not a statement, but a literal expression, ignored by earlier   versions of JavaScript. The purpose of "use strict" is to indicate that the code should be   executed in "strict mode". With strict mode, you can not, for example, use undeclared variables. Strict mode makes it easier to write "secure" JavaScript. Strict mode changes previously accepted "bad syntax" into real errors. As an example, in normal JavaScript, mistyping a variable name creates   a new global variable. In strict mode, this will throw an error,   making it impossible to accidentally create a global variable. In normal JavaScript, a developer will not receive any error feedback   assigning values to non-writable properties. In strict mode, any assignment to a non-writable property, a   getter-only property, a non-existing property, a non-existing   variable, or a non-existing object, will throw an error. Please refer to http://www.w3schools.com/js/js_strict.asp to know more "use strict" makes JavaScript code to run in strict mode, which basically means everything needs to be defined before use. The main reason for using strict mode is to avoid accidental global uses of undefined methods. Also in strict mode, things run faster, some warnings or silent warnings throw fatal errors, it's better to always use it to make a neater code. "use strict" is widely needed to be used in ECMA5, in ECMA6 it's part of JavaScript by default, so it doesn't need to be added if you're using ES6. Look at these statements and examples from MDN: The "use strict" Directive The "use strict" directive is new in   JavaScript 1.8.5 (ECMAScript version 5). It is not a statement, but a   literal expression, ignored by earlier versions of JavaScript. The   purpose of "use strict" is to indicate that the code should be   executed in "strict mode". With strict mode, you can not, for example,   use undeclared variables. Examples of using "use strict":   Strict mode for functions: Likewise, to invoke strict mode for a   function, put the exact statement "use strict"; (or 'use strict';) in   the function's body before any other statements. 1) strict mode in functions  2) whole-script strict mode  3) Assignment to a non-writable global You can read more on MDN. There's a good talk by some people who were on the ECMAScript committee: Changes to JavaScript, Part 1: ECMAScript 5" about how incremental use of the "use strict" switch allows JavaScript implementers to clean up a lot of the dangerous features of JavaScript without suddenly breaking every website in the world. Of course it also talks about just what a lot of those misfeatures are (were) and how ECMAScript 5 fixes them. Small examples to compare: Non-strict mode:   for (i of [1,2,3]) console.log(i)      // output: // 1 // 2 // 3    Strict mode:   'use strict'; for (i of [1,2,3]) console.log(i)  // output: // Uncaught ReferenceError: i is not defined    Non-strict mode:   String.prototype.test = function () {   console.log(typeof this === 'string'); };  'a'.test();  // output // false      String.prototype.test = function () {   'use strict';      console.log(typeof this === 'string'); };  'a'.test();  // output // true    Note that use strict was introduced in EcmaScript 5 and was kept since then. Below are the conditions to trigger strict mode in ES6 and ES7: The main reasons why developers should use "use strict" are: Prevents accidental declaration of global variables.Using "use strict()" will make sure that variables are declared with var before use.  Eg: The string "arguments" cannot be used as a variable: Will restrict uses of keywords as variables. Trying to use them will throw errors. In short will make your code less error prone and in turn will make you write good code. To read more about it you can refer here. JavaScript “strict” mode was introduced in ECMAScript 5. Writing "use strict"; at the very top of your JS file turns on strict syntax checking. It does the following tasks for us: shows an error if you try to assign to an undeclared variable stops you from overwriting key JS system libraries forbids some unsafe or error-prone language features use strict also works inside of individual functions. It is always a better practice to include use strict in your code. Browser compatibility issue:  The "use" directives are meant to be backwards-compatible. Browsers that do not support them will just see a string literal that isn't referenced further. So, they will pass over it and move on. use strict is a way to make your code safer, because you can't use dangerous features that can work not as you expect. And, as was written before, it makes code more strict. "use strict"; is the ECMA effort to make JavaScript a little bit more robust. It brings in JS an attempt to make it at least a little "strict" (other languages implement strict rules since the 90s). It actually "forces" JavaScript developers to follow some sort of coding best practices. Still, JavaScript is very fragile. There is no such thing as typed variables, typed methods, etc. I strongly recommend JavaScript developers to learn a more robust language such as Java or ActionScript3, and implement the same best practices in your JavaScript code, it will work better and be easier to debug. Normally, JavaScript does not follow strict rules, hence increasing chances of errors. After using "use strict", the JavaScript code should follow strict set of rules as in other programming languages such as use of terminators, declaration before initialization, etc.  If "use strict" is used, the code should be written by following a strict set of rules, hence decreasing the chances of errors and ambiguities. Use Strict is used to show common and repeated errors so that it is handled differently , and changes the way java script runs , such changes are : Prevents accidental globals No duplicates Eliminates with Eliminates this coercion Safer eval() Errors for immutables you can also read this article for the details  "use strict"; Defines that JavaScript code should be executed in    "strict mode". All modern browsers support "use strict" except Internet Explorer 9 and lower. Disadvantage If a developer used a library that was in strict mode, but the developer was used to working in normal mode, they might call some actions on the library that wouldn’t work as expected. Worse, since the developer is in normal mode, they don’t have the advantages of extra errors being thrown, so the error might fail silently. Also, as listed above, strict mode stops you from doing certain things. People generally think that you shouldn’t use those things in the first place, but some developers don’t like the constraint and want to use all the features of the language. For basic example and for reference go through : https://www.tutorialsteacher.com/javascript/javascript-strict Strict mode can prevent memory leaks. Please check the function below written in non-strict mode: In this function, we are using a variable called name inside the function. Internally, the compiler will first check if there is any variable declared with that particular name in that particular function scope. Since the compiler understood that there is no such variable, it will check in the outer scope. In our case, it is the global scope. Again, the compiler understood that there is also no variable declared in the global space with that name, so it creates such a variable for us in the global space. Conceptually, this variable will be created in the global scope and will be available in the entire application. Another scenario is that, say, the variable is declared in a child function. In that case, the compiler checks the validity of that variable in the outer scope, i.e., the parent function. Only then it will check in the global space and create a variable for us there. That means additional checks need to be done. This will affect the performance of the application. Now let's write the same function in strict mode. We will get the following error. Here, the compiler throws the reference error. In strict mode, the compiler does not allow us to use the variable without declaring it. So memory leaks can be prevented. In addition, we can write more optimized code. Strict mode eliminates errors that would be ignored in non-strict mode, thus making javascript “more secured”. Is it considered among best practices? Yes, It's considered part of the best practices while working with javascript to include Strict mode. This is done by adding the below line of code in your JS file. 'use strict';  in your code. What does it mean to user agents? Indicating that code should be interpreted in strict mode specifies to user agents like browsers that they should treat code literally as written, and throw an error if the code doesn't make sense. For example: Consider in your .js file you have the following code: Scenario 1: [NO STRICT MODE] Scenario 2: [NO STRICT MODE] So why does the variable name is being printed in both cases? Without strict mode turned on, user agents often go through a series of modifications to problematic code in an attempt to get it to make sense. On the surface, this can seem like a fine thing, and indeed, working outside of strict mode makes it possible for people to get their feet wet with JavaScript code without having all the details quite nailed down. However, as a developer, I don't want to leave a bug in my code, because I know it could come back and bite me later on, and I also just want to write good code. And that's where strict mode helps out. Scenario 3: [STRICT MODE] Additional tip: To maintain code quality using strict mode, you don't need to write this over and again especially if you have multiple .js file. You can enforce this rule globally in eslint rules as follows: Filename: .eslintrc.js Okay, so what is prevented in strict mode? Using a variable without declaring it will throw an error in strict mode. This is to prevent unintentionally creating global variables throughout your application. The example with printing Chicago covers this in particular. Deleting a variable or a function or an argument is a no-no in strict mode. Duplicating a parameter name is not allowed in strict mode. Reserved words in the Javascript language are not allowed in strict mode. The words are implements interface, let, packages, private, protected, public. static, and yield For a more comprehensive list check out the MDN documentation here: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Strict_mode
__label__git __label__version-control __label__git-pull __label__overwrite __label__git-fetch How do I force an overwrite of local files on a git pull? The scenario is the following: This is the error I'm getting: error: Untracked working tree file 'public/images/icon.gif' would be overwritten by merge How do I force Git to overwrite them? The person is a designer - usually, I resolve all the conflicts by hand, so the server has the most recent version that they just need to update on their computer. If you have any files that are not tracked by Git (e.g. uploaded user content), these files will not be affected. First, run a fetch to update all origin/<branch> refs to latest: Backup your current branch: Then, you have two options: OR If you are on some other branch: git fetch downloads the latest from remote without trying to merge or rebase anything. Then the git reset resets the master branch to what you just fetched. The --hard option changes all the files in your working tree to match the files in origin/master [*]: It's worth noting that it is possible to maintain current local commits by creating a branch from master before resetting: After this, all of the old commits will be kept in new-branch-to-save-current-commits. Uncommitted changes, however (even staged), will be lost. Make sure to stash and commit anything you need. For that you can run the following: And then to reapply these uncommitted changes: Try this: It should do what you want. WARNING: git clean deletes all your untracked files/directories and can't be undone. Sometimes just clean -f does not help. In case you have untracked DIRECTORIES, -d option also needed: WARNING: git clean deletes all your untracked files/directories and can't be undone. Consider using -n (--dry-run) flag first. This will show you what will be deleted without actually deleting anything: Example output: Like Hedgehog I think the answers are terrible. But though Hedgehog's answer might be better, I don't think it is as elegant as it could be.  The way I found to do this is by using "fetch" and "merge" with a defined strategy. Which should make it so that your local changes are preserved as long as they are not one of the files that you are trying to force an overwrite with. ###First do a commit of your changes ###Then fetch the changes and overwrite if there is a conflict "-X" is an option name, and "theirs" is the value for that option. You're choosing to use "their" changes, the other option is "ours" changes if there is a conflict. Instead of doing: I'd advise doing the following: No need to fetch all remotes and branches if you're going to reset to the origin/master branch right? It looks like the best way is to first do: To delete all untracked files and then continue with the usual git pull... Warning, doing this will permanently delete your files if you have any directory/* entries in your gitignore file. Some answers seem to be terrible. Terrible in the sense of what happened to @Lauri by following David Avsajanishvili suggestion. Rather (git > v1.7.6): Later you can clean the stash history. Manually, one-by-one: Brutally, all-at-once: Of course if you want to go back to what you stashed: You might find this command helpful to throw away local changes: And then do a cleanup (removes untracked files from the working tree): If you want to remove untracked directories in addition to untracked files: Instead of merging with git pull, try this:   git fetch --all followed by: git reset --hard origin/master. The only thing that worked for me was: This will take you back five commits and then with I found that by looking up how to undo a Git merge. The problem with all these solutions is that they are all either too complex, or, an even bigger problem, is that they remove all untracked files from the web server, which we don't want since there are always needed configuration files which are on the server and not in the Git repository. Here is the cleanest solution which we are using: The first command fetches newest data. The second command checks if there are any files which are being added to the repository and deletes those untracked files from the local repository which would cause conflicts. The third command checks-out all the files which were locally modified. Finally we do a pull to update to the newest version, but this time without any conflicts, since untracked files which are in the repo don't exist anymore and all the locally modified files are already the same as in the repository. First of all, try the standard way: Warning: Above commands can results in data/files loss only if you don't have them committed! If you're not sure, make the backup first of your whole repository folder. Then pull it again. If above won't help and you don't care about your untracked files/directories (make the backup first just in case), try the following simple steps: This will REMOVE all git files (excempt .git/ dir, where you have all commits) and pull it again. Why git reset HEAD --hard could fail in some cases? Custom rules in .gitattributes file Having eol=lf rule in .gitattributes could cause git to modify some file changes by converting CRLF line-endings into LF in some text files. If that's the case, you've to commit these CRLF/LF changes (by reviewing them in git status), or try: git config core.autcrlf false to temporary ignore them. File system incompability When you're using file-system which doesn't support permission attributes. In example you have two repositories, one on Linux/Mac (ext3/hfs+) and another one on FAT32/NTFS based file-system. As you notice, there are two different kind of file systems, so the one which doesn't support Unix permissions basically can't reset file permissions on system which doesn't support that kind of permissions, so no matter how --hard you try, git always detect some "changes". I had the same problem. No one gave me this solution, but it worked for me. I solved it by: Now it works. In speaking of pull/fetch/merge in the previous answers, I would like to share an interesting and productive trick, This above command is the most useful command in my Git life which saved a lot of time. Before pushing your newly commit to server, try this command and it will automatically synchronise the latest server changes (with a fetch + merge) and will place your commit at the top in the Git log. There isn't any need to worry about manual pull/merge. Find details in What does "git pull --rebase" do?. I had a similar problem.  I had to do this: I summarized other answers. You can execute git pull without errors: Warning: This script is very powerful, so you could lose your changes. Based on my own similar experiences, the solution offered by Strahinja Kustudic above is by far the best.  As others have pointed out, simply doing hard reset will remove all the untracked files which could include lots of things that you don't want removed, such as config files.  What is safer, is to remove only the files that are about to be added, and for that matter, you'd likely also want to checkout any locally-modified files that are about to be updated. That in mind, I updated Kustudic's script to do just that.  I also fixed a typo (a missing ' in the original). I believe there are two possible causes of conflict, which must be solved separately, and as far as I can tell none of the above answers deals with both: Local files that are untracked need to be deleted, either manually (safer) or as suggested in other answers, by git clean -f -d Local commits that are not on the remote branch need to be deleted as well. IMO the easiest way to achieve this is with: git reset --hard origin/master (replace 'master' by whatever branch you are working on, and run a git fetch origin first) An easier way would be to: This will override your local file with the file on git git fetch --all && git reset --hard origin/master && git pull It seems like most answers here are focused on the master branch; however, there are times when I'm working on the same feature branch in two different places and I want a rebase in one to be reflected in the other without a lot of jumping through hoops. Based on a combination of RNA's answer and torek's answer to a similar question, I've come up with this which works splendidly: Run this from a branch and it'll only reset your local branch to the upstream version. This can be nicely put into a git alias (git forcepull) as well: git config alias.forcepull "!git fetch ; git reset --hard @{u}" Or, in your .gitconfig file: Enjoy! I had the same problem and for some reason, even a git clean -f -d would not do it. Here is why: For some reason, if your file is ignored by Git (via a .gitignore entry, I assume), it still bothers about overwriting this with a later pull, but a clean will not remove it, unless you add -x. I know of a much easier and less painful method: That's it! I just solved this myself by: where the last command gives a list of what your local changes were. Keep modifying the "tmp" branch until it is acceptable and then merge back onto master with:  For next time, you can probably handle this in a cleaner way by looking up "git stash branch" though stash is likely to cause you trouble on the first few tries, so do first experiment on a non-critical project... I have a strange situation that neither git clean or git reset works. I have to remove the conflicting file from git index by using the following script on every untracked file: Then I am able to pull just fine. then if you are on the master branch else Despite the original question, the top answers can cause problems for people who have a similar problem, but don't want to lose their local files. For example, see Al-Punk and crizCraig's comments.  The following version commits your local changes to a temporary branch (tmp), checks out the original branch (which I'm assuming is master) and merges the updates. You could do this with stash, but I've found it's usually easier to simply use the branch / merge approach. where we assume the other repository is origin master. Just do So you avoid all unwanted side effects, like deleting files or directories you wanted to keep, etc. Reset the index and the head to origin/master, but do not reset the working tree: These four commands work for me. To check/pull after executing these commands I tried a lot but finally got success with these commands.
__label__javascript __label__jquery __label__redirect How can I redirect the user from one page to another using jQuery or pure JavaScript? jQuery is not necessary, and window.location.replace(...) will best simulate an HTTP redirect.   window.location.replace(...) is better than using window.location.href, because replace() does not keep the originating page in the session history, meaning the user won't get stuck in a never-ending back-button fiasco. If you want to simulate someone clicking on a link, use  location.href If you want to simulate an HTTP redirect, use location.replace For example: WARNING: This answer has merely been provided as a possible solution; it is obviously not the best solution, as it requires jQuery. Instead, prefer the pure JavaScript solution. If you are here because you are losing HTTP_REFERER when redirecting, keep reading: (Otherwise ignore this last part) The following section is for those using HTTP_REFERER as one of many security measures (although it isn't a great protective measure). If you're using Internet Explorer 8 or lower, these variables get lost when using any form of JavaScript page redirection (location.href,  etc.). Below we are going to implement an alternative for IE8 & lower so that we don't lose HTTP_REFERER. Otherwise, you can almost always simply use window.location.href. Testing against HTTP_REFERER (URL pasting, session, etc.) can help tell whether a request is legitimate. (Note: there are also ways to work-around / spoof these referrers, as noted by droop's link in the comments) Simple cross-browser testing solution (fallback to window.location.href for Internet Explorer 9+ and all other browsers) Usage: redirect('anotherpage.aspx'); There are lots of ways of doing this. This works for every browser: It would help if you were a little more descriptive in what you are trying to do.  If you are trying to generate paged data, there are some options in how you do this.  You can generate separate links for each page that you want to be able to get directly to. Note that the current page in the example is handled differently in the code and with CSS. If you want the paged data to be changed via AJAX, this is where jQuery would come in.  What you would do is add a click handler to each of the anchor tags corresponding to a different page.  This click handler would invoke some jQuery code that goes and fetches the next page via AJAX and updates the table with the new data.  The example below assumes that you have a web service that returns the new page data. I also think that location.replace(URL) is the best way, but if you want to notify the search engines about your redirection (they don't analyze JavaScript code to see the redirection) you should add the rel="canonical" meta tag to your website. Adding a noscript section with a HTML refresh meta tag in it, is also a good solution. I suggest you to use this JavaScript redirection tool to create redirections. It also has Internet Explorer support to pass the HTTP referrer. Sample code without delay looks like this: But if someone wants to redirect back to home page then he may use the following snippet. It would be helpful if you have three different environments as development, staging, and production. You can explore this window or window.location object by just putting these words in Chrome Console or Firebug's Console. JavaScript provides you many methods to retrieve and change the current URL which is displayed in browser's address bar. All these methods uses the Location object, which is  a property of the Window object. You can create a new Location object that has the current URL as follows.. Basic Structure of a URL  Protocol -- Specifies the protocol name be used to access the resource on the Internet. (HTTP (without SSL) or HTTPS (with SSL)) hostname -- Host name specifies the host that owns the resource. For example, www.stackoverflow.com. A server provides services using the name of the host. port -- A port number used to recognize a specific process to which an Internet or other network message is to be forwarded when it arrives at a server. pathname -- The path gives info about the specific resource within the host that the Web client wants to access. For example, stackoverflow.com/index.html. query --  A query string follows the path component, and provides a string of information that the resource can utilize for some purpose (for example, as parameters for a search or as data to be processed).  hash -- The anchor portion of a URL, includes the hash sign (#). With these Location object properties you can access all of these URL components Now If you want to change a page or redirect the user to some other page you can use the href property of the Location object like this You can use the href property of the Location object. Location Object also have these three methods You can use assign() and replace methods also to redirect to other pages like these How assign() and replace() differs -- The difference between replace() method and assign() method(), is that replace() removes the URL of the current document from the document history, means it is not possible to use the "back" button to navigate back to the original document. So Use the assign() method if you want to load a new document, andwant to give the option to navigate back to the original document. You can change the location object href property using jQuery also  like this And hence you can redirect the user to some other url. Basically jQuery is just a JavaScript framework and for doing some of the things like redirection in this case, you can just use pure JavaScript, so in that case you have 3 options using vanilla JavaScript: 1) Using location replace, this will replace the current history of the page, means that it is not possible to use the back button to go back to the original page. 2) Using location assign, this will keep the history for you and with using back button, you can go back to the original page: 3) I recommend using one of those previous ways, but this could be the third option using pure JavaScript:  You can also write a function in jQuery to handle it, but not recommended as it's only one line pure JavaScript function, also you can use all of above functions without window if you are already in the window scope, for example window.location.replace("http://stackoverflow.com"); could be location.replace("http://stackoverflow.com"); Also I show them all on the image below:  Should just be able to set using window.location. Example: Here is a past post on the subject: How do I redirect to another webpage? Before I start, jQuery is a JavaScript library used for DOM manipulation. So you should not be using jQuery for a page redirect. A quote from Jquery.com: While jQuery might run without major issues in older browser versions,   we do not actively test jQuery in them and generally do not fix bugs   that may appear in them. It was found here: https://jquery.com/browser-support/ So jQuery is not an end-all and be-all solution for backwards compatibility. The following solution using raw JavaScript works in all browsers and have been standard for a long time so you don't need any libraries for cross browser support. This page will redirect to Google after 3000 milliseconds Different options are as follows: When using replace, the back button will not go back to the redirect page, as if it was never in the history. If you want the user to be able to go back to the redirect page then use window.location.href or window.location.assign. If you do use an option that lets the user go back to the redirect page, remember that when you enter the redirect page it will redirect you back. So put that into consideration when picking an option for your redirect. Under conditions where the page is only redirecting when an action is done by the user then having the page in the back button history will be okay. But if the page auto redirects then you should use replace so that the user can use the back button without getting forced back to the page the redirect sends. You can also use meta data to run a page redirect as followed. META Refresh META Location BASE Hijacking Many more methods to redirect your unsuspecting client to a page they may not wish to go can be found on this page (not one of them is reliant on jQuery): https://code.google.com/p/html5security/wiki/RedirectionMethods I would also like to point out, people don't like to be randomly redirected. Only redirect people when absolutely needed. If you start redirecting people randomly they will never go to your site again. The next paragraph is hypothetical: You also may get reported as a malicious site. If that happens then when people click on a link to your site the users browser may warn them that your site is malicious. What may also happen is search engines may start dropping your rating if people are reporting a bad experience on your site. Please review Google Webmaster Guidelines about redirects: https://support.google.com/webmasters/answer/2721217?hl=en&ref_topic=6001971 Here is a fun little page that kicks you out of the page. If you combine the two page examples together you would have an infant loop of rerouting that will guarantee that your user will never want to use your site ever again.  You can do that without jQuery as: And if you want only jQuery then you can do it like: This works with jQuery: # HTML Page Redirect Using jQuery/JavaScript Method Try this example code: If you want to give a complete URL as window.location = "www.google.co.in";. Original question: "How to redirect using jQuery?", hence the answer implements jQuery >> Complimentary usage case. To just redirect to a page with JavaScript: Or if you need a delay: jQuery allows you to select elements from a web page with ease. You can find anything you want on a page and then use jQuery to add special effects, react to user actions, or show and hide content inside or outside the element you have selected. All these tasks start with knowing how to select an element or an event. Imagine someone wrote a script/plugin with 10000 lines of code. With jQuery you can connect to this code with just a line or two. You need to put this line in your code: If you don't have jQuery, go with JavaScript: So, the question is how to make a redirect page, and not how to redirect to a website? You only need to use JavaScript for this. Here is some tiny code that will create a dynamic redirect page. So say you just put this snippet into a redirect/index.html file on your website you can use it like so. http://www.mywebsite.com/redirect?url=http://stackoverflow.com And if you go to that link it will automatically redirect you to stackoverflow.com. Link to Documentation And that's how you make a Simple redirect page with JavaScript Edit: There is also one thing to note. I have added window.location.replace in my code because I think it suits a redirect page, but, you must know that when using window.location.replace and you get redirected, when you press the back button in your browser it will not got back to the redirect page, and it will go back to the page before it, take a look at this little demo thing. Example: The process: store home => redirect page to google => google When at google: google => back button in browser => store home So, if this suits your needs then everything should be fine. If you want to include the redirect page in the browser history replace this with On your click function, just add: Try this: Code snippet of example. jQuery is not needed. You can do this: It is that easy! The best way to initiate an HTTP request is with document.loacation.href.replace('URL'). First write properly. You want to navigate within an application for another link from your application for another link. Here is the code: And if you want to navigate pages within your application then I also have code, if you want. You can redirect in jQuery like this:  In JavaScript and jQuery we can use the following code to redirect the one page to another page: Please don't kill me, this is a joke. It's a joke. This is a joke. This did "provide an answer to the question", in the sense that it asked for a solution "using jQuery" which in this case entails forcing it into the equation somehow. Ferrybig apparently needs the joke explained (still joking, I'm sure there are limited options on the review form), so without further ado: Other answers are using jQuery's attr() on the location or window objects unnecessarily. This answer also abuses it, but in a more ridiculous way. Instead of using it to set the location, this uses attr() to retrieve a function that sets the location. The function is named jQueryCode even though there's nothing jQuery about it, and calling a function somethingCode is just horrible, especially when the something is not even a language. The "85 bytes" is a reference to Code Golf. Golfing is obviously not something you should do outside of code golf, and furthermore this answer is clearly not actually golfed. Basically, cringe. Javascript: Jquery: Here is a time-delay redirection. You can set the delay time to whatever you want: There are three main ways to do this, and... The last one is best, for a traditional redirect, because it will not save the page you went to before being redirected in your search history. However, if you just want to open a tab with JavaScript, you can use any of the above.1 EDIT: The window prefix is optional.
__label__git __label__git-amend __label__git-commit __label__git-rewrite-history I wrote the wrong thing in a commit message. How can I change the message? The commit has not been pushed yet. will open your editor, allowing you to change the commit message of the most recent commit. Additionally, you can set the commit message directly in the command line with: …however, this can make multi-line commit messages or small corrections more cumbersome to enter. Make sure you don't have any working copy changes staged before doing this or they will get committed too. (Unstaged changes will not get committed.) If you've already pushed your commit up to your remote branch, then - after amending your commit locally (as described above) - you'll also need to force push the commit with: Warning: force-pushing will overwrite the remote branch with the state of your local one. If there are commits on the remote branch that you don't have in your local branch, you will lose those commits. Warning: be cautious about amending commits that you have already shared with other people. Amending commits essentially rewrites them to have different SHA IDs, which poses a problem if other people have copies of the old commit that you've rewritten. Anyone who has a copy of the old commit will need to synchronize their work with your newly re-written commit, which can sometimes be difficult, so make sure you coordinate with others when attempting to rewrite shared commit history, or just avoid rewriting shared commits altogether. Another option is to use interactive rebase. This allows you to edit any message you want to update even if it's not the latest message. In order to do a Git squash, follow these steps: Once you squash your commits - choose the e/r for editing the message:  When you use git rebase -i HEAD~n there can be more than n commits. Git will "collect" all the commits in the last n commits, and if there was a merge somewhere in between that range you will see all the commits as well, so the outcome will be n + . If you have to do it for more than a single branch and you might face conflicts when amending the content, set up git rerere and let Git resolve those conflicts automatically for you. git-commit(1) Manual Page git-rebase(1) Manual Page git-push(1) Manual Page  If the commit you want to fix isn’t the most recent one: git rebase --interactive $parent_of_flawed_commit If you want to fix several flawed commits, pass the parent of the oldest one of them. An editor will come up, with a list of all commits since the one you gave. For each commit you want to reword, Git will drop you back into your editor. For each commit you want to edit, Git drops you into the shell. If you’re in the shell: Most of this sequence will be explained to you by the output of the various commands as you go. It’s very easy; you don’t need to memorise it – just remember that git rebase --interactive lets you correct commits no matter how long ago they were. Note that you will not want to change commits that you have already pushed. Or maybe you do, but in that case you will have to take great care to communicate with everyone who may have pulled your commits and done work on top of them. How do I recover/resynchronise after someone pushes a rebase or a reset to a published branch? To amend the previous commit, make the changes you want and stage those changes, and then run This will open a file in your text editor representing your new commit message. It starts out populated with the text from your old commit message. Change the commit message as you want, then save the file and quit your editor to finish. To amend the previous commit and keep the same log message, run To fix the previous commit by removing it entirely, run If you want to edit more than one commit message, run (Replace commit_count with number of commits that you want to edit.) This command launches your editor. Mark the first commit (the one that you want to change) as “edit” instead of “pick”, then save and exit your editor. Make the change you want to commit and then run Note: You can also "Make the change you want" from the editor opened by git commit --amend As already mentioned, git commit --amend is the way to overwrite the last commit. One note: if you would like to also overwrite the files, the command would be  You also can use git filter-branch for that. It's not as easy as a trivial git commit --amend, but it's especially useful, if you already have some merges after your erroneous commit message. Note that this will try to rewrite every commit between HEAD and the flawed commit, so you should choose your msg-filter command very wisely ;-) I prefer this way: Otherwise, there will be a new commit with a new commit ID. If you are using the Git GUI tool, there is a button named Amend last commit. Click on that button and then it will display your last commit files and message. Just edit that message, and you can commit it with a new commit message. Or use this command from a console/terminal: You can use Git rebasing. For example, if you want to modify back to commit bbc643cd, run In the default editor, modify 'pick' to 'edit' in the line whose commit you want to modify. Make your changes and then stage them with Now you can use to modify the commit, and after that to return back to the previous head commit. If you only want to modify your last commit message, then do: That will drop you into your text editor and let you change the last commit message. If you want to change the last three commit messages, or any of the commit messages up to that point, supply HEAD~3 to the git rebase -i command: If you have to change an old commit message over multiple branches (i.e., the commit with the erroneous message is present in multiple branches) you might want to use: Git will create a temporary directory for rewriting and additionally backup old references in refs/original/. -f will enforce the execution of the operation. This is necessary if the temporary directory is already present or if there are already references stored under refs/original. If that is not the case, you can drop this flag. -- separates filter-branch options from revision options. --all will make sure that all branches and tags are rewritten. Due to the backup of your old references, you can easily go back to the state before executing the command. Say, you want to recover your master and access it in branch old_master: Use To understand it in detail, an excellent post is 4. Rewriting Git History. It also talks about when not to use git commit --amend. If it's your last commit, just amend the commit: (Using the -o (--only) flag to make sure you change only the commit message)  If it's a buried commit, use the awesome interactive rebase: Find the commit you want, change pick to r (reword), and save and close the file. Done!  Miniature Vim tutorial (or, how to rebase with only 8 keystrokes 3jcwrEscZZ): If you edit text a lot, then switch to the Dvorak keyboard layout, learn to touch-type, and learn Vim. Is it worth the effort? Yes.  ProTip™: Don't be afraid to experiment with "dangerous" commands that rewrite history* — Git doesn't delete your commits for 90 days by default; you can find them in the reflog: * Watch out for options like --hard and --force though — they can discard data. *  Also, don't rewrite history on any branches you're collaborating on. You have a couple of options here. You can do as long as it's your last commit. Otherwise, if it's not your last commit, you can do an interactive rebase, Then inside the interactive rebase you simply add edit to that commit. When it comes up, do a git commit --amend and modify the commit message. If you want to roll back before that commit point, you could also use git reflog and just delete that commit. Then you just do a git commit again. If you are using the Git GUI, you can amend the last commit which hasn't been pushed with:   I use the Git GUI as much as I can, and that gives you the option to amend the last commit:  Also, git rebase -i origin/masteris a nice mantra that will always present you with the commits you have done on top of master, and give you the option to amend, delete, reorder or squash. No need to get hold of that hash first. Wow, so there are a lot of ways to do this. Yet another way to do this is to delete the last commit, but keep its changes so that you won't lose your work. You can then do another commit with the corrected message. This would look something like this: I always do this if I forget to add a file or do a change. Remember to specify --soft instead of --hard, otherwise you lose that commit entirely. For anyone looking for a Windows/Mac GUI to help with editing older messages (i.e. not just the latest message), I'd recommend Sourcetree. The steps to follow are below the image.  For commits that haven't been pushed to a remote yet: ...Or... for commits that have already been pushed: Follow the steps in this answer, which are similar to above, but require a further command to be run from the command line (git push origin <branch> -f) to force-push the branch. I'd recommend reading it all and applying the necessary caution! If you just want to edit the latest commit, use: or But if you want to edit several commits in a row, you should use rebasing instead:  In a file, like the one above, write edit/e or one of the other options, and hit save and exit. Now you'll be at the first wrong commit. Make changes in the files, and they'll be automatically staged for you. Type Save and exit that and type to move to next selection until finished with all your selections. Note that these things change all your SHA hashes after that particular commit. If you only want to change your last message you should use the --only flag or its shortcut -o with commit --amend: This ensures that you don't accidentally enhance your commit with staged stuff. Of course it's best to have a proper $EDITOR configuration. Then you can leave the -m option out, and Git will pre-fill the commit message with the old one. In this way it can be easily edited. Update your last wrong commit message with the new commit message in one line: Or, try Git reset like below: git reset can help you to break one commit into multiple commits too: Here you have successfully broken your last commit into two commits. On this question there are a lot of answers, but none of them explains in super detail how to change older commit messages using Vim. I was stuck trying to do this myself, so here I'll write down in detail how I did this especially for people who have no experience in Vim! I wanted to change my five latest commits that I already pushed to the server. This is quite 'dangerous' because if someone else already pulled from this, you can mess things up by changing the commit messages. However, when you’re working on your own little branch and are sure no one pulled it you can change it like this: Let's say you want to change your five latest commits, and then you type this in the terminal: *Where 5 is the number of commit messages you want to change (so if you want to change the 10th to last commit, you type in 10). This command will get you into Vim there you can ‘edit’ your commit history. You’ll see your last five commits at the top like this: Instead of pick you need to write reword. You can do this in Vim by typing in i. That makes you go in to insert mode. (You see that you’re in insert mode by the word INSERT at the bottom.) For the commits you want to change, type in reword instead of pick. Then you need to save and quit this screen. You do that by first going in to ‘command-mode’ by pressing the Escbutton (you can check that you’re in command-mode if the word INSERT at the bottom has disappeared). Then you can type in a command by typing :. The command to save and quit is wq. So if you type in :wq you’re on the right track. Then Vim will go over every commit message you want to reword, and here you can actually change the commit messages. You’ll do this by going into insert mode, changing the commit message, going into the command-mode, and save and quit. Do this five times and you’re out of Vim! Then, if you already pushed your wrong commits, you need to git push --force to overwrite them. Remember that git push --force is quite a dangerous thing to do, so make sure that no one pulled from the server since you pushed your wrong commits! Now you have changed your commit messages! (As you see, I'm not that experienced in Vim, so if I used the wrong 'lingo' to explain what's happening, feel free to correct me!) You can use git-rebase-reword It is designed to edit any commit (not just last) same way as commit --amend It is named after the action on rebase interactive to amend a commit: "reword". See this post and man -section interactive mode- Examples: I have added the aliases reci and recm for recommit (amend) it. Now I can do it with git recm or git recm -m: I realised that I had pushed a commit with a typo in it. In order to undo, I did the following: Warning: force pushing your changes will overwrite the remote branch with your local one. Make sure that you aren't going to be overwriting anything that you want to keep. Also be cautious about force pushing an amended (rewritten) commit if anyone else shares the branch with you, because they'll need to rewrite their own history if they have the old copy of the commit that you've just rewritten. I like to use the following: If you have not pushed the code to your remote branch (GitHub/Bitbucket) you can change the commit message on the command line as below. If you're working on a specific branch do this: If you've already pushed the code with the wrong message, and you need to be careful when changing the message. That is, after you change the commit message and try pushing it again, you end up with having issues. To make it smooth, follow these steps. Please read my entire answer before doing it. Important note: When you use the force push directly you might end up with code issues that other developers are working on the same branch. So to avoid those conflicts, you need to pull the code from your branch before making the force push: This is the best practice when changing the commit message, if it was already pushed.
__label__function __label__scope __label__javascript __label__variables __label__closures How would you explain JavaScript closures to someone with a knowledge of the concepts they consist of (for example functions, variables and the like), but does not understand closures themselves? I have seen the Scheme example given on Wikipedia, but unfortunately it did not help. A closure is a pairing of: A lexical environment is part of every execution context (stack frame) and is a map between identifiers (ie. local variable names) and values. Every function in JavaScript maintains a reference to its outer lexical environment. This reference is used to configure the execution context created when a function is invoked. This reference enables code inside the function to "see" variables declared outside the function, regardless of when and where the function is called. If a function was called by a function, which in turn was called by another function, then a chain of references to outer lexical environments is created. This chain is called the scope chain. In the following code, inner forms a closure with the lexical environment of the execution context created when foo is invoked, closing over variable secret:   function foo() {   const secret = Math.trunc(Math.random()*100)   return function inner() {     console.log(`The secret number is ${secret}.`)   } } const f = foo() // `secret` is not directly accessible from outside `foo` f() // The only way to retrieve `secret`, is to invoke `f`    In other words: in JavaScript, functions carry a reference to a private "box of state", to which only they (and any other functions declared within the same lexical environment) have access. This box of the state is invisible to the caller of the function, delivering an excellent mechanism for data-hiding and encapsulation. And remember: functions in JavaScript can be passed around like variables (first-class functions), meaning these pairings of functionality and state can be passed around your program: similar to how you might pass an instance of a class around in C++. If JavaScript did not have closures, then more states would have to be passed between functions explicitly, making parameter lists longer and code noisier. So, if you want a function to always have access to a private piece of state, you can use a closure. ...and frequently we do want to associate the state with a function. For example, in Java or C++, when you add a private instance variable and a method to a class, you are associating state with functionality. In C and most other common languages, after a function returns, all the local variables are no longer accessible because the stack-frame is destroyed. In JavaScript, if you declare a function within another function, then the local variables of the outer function can remain accessible after returning from it. In this way, in the code above, secret remains available to the function object inner, after it has been returned from foo. Closures are useful whenever you need a private state associated with a function. This is a very common scenario - and remember: JavaScript did not have a class syntax until 2015, and it still does not have a private field syntax. Closures meet this need. In the following code, the function toString closes over the details of the car.   function Car(manufacturer, model, year, color) {   return {     toString() {       return `${manufacturer} ${model} (${year}, ${color})`     }   } } const car = new Car('Aston Martin','V8 Vantage','2012','Quantum Silver') console.log(car.toString())    In the following code, the function inner closes over both fn and args.   function curry(fn) {   const args = []   return function inner(arg) {     if(args.length === fn.length) return fn(...args)     args.push(arg)     return inner   } }  function add(a, b) {   return a + b }  const curriedAdd = curry(add) console.log(curriedAdd(2)(3)()) // 5    In the following code, function onClick closes over variable BACKGROUND_COLOR.   const $ = document.querySelector.bind(document) const BACKGROUND_COLOR = 'rgba(200,200,242,1)'  function onClick() {   $('body').style.background = BACKGROUND_COLOR }  $('button').addEventListener('click', onClick) <button>Set background color</button>    In the following example, all the implementation details are hidden inside an immediately executed function expression. The functions tick and toString close over the private state and functions they need to complete their work. Closures have enabled us to modularise and encapsulate our code.   let namespace = {};  (function foo(n) {   let numbers = []   function format(n) {     return Math.trunc(n)   }   function tick() {     numbers.push(Math.random() * 100)   }   function toString() {     return numbers.map(format)   }   n.counter = {     tick,     toString   } }(namespace))  const counter = namespace.counter counter.tick() counter.tick() console.log(counter.toString())    This example shows that the local variables are not copied in the closure: the closure maintains a reference to the original variables themselves. It is as though the stack-frame stays alive in memory even after the outer function exits.   function foo() {   let x = 42   let inner  = function() { console.log(x) }   x = x+1   return inner } var f = foo() f() // logs 43    In the following code, three methods log, increment, and update all close over the same lexical environment. And every time createObject is called, a new execution context (stack frame) is created and a completely new variable x, and a new set of functions (log etc.) are created, that close over this new variable.   function createObject() {   let x = 42;   return {     log() { console.log(x) },     increment() { x++ },     update(value) { x = value }   } }  const o = createObject() o.increment() o.log() // 43 o.update(5) o.log() // 5 const p = createObject() p.log() // 42    If you are using variables declared using var, be careful you understand which variable you are closing over. Variables declared using var are hoisted. This is much less of a problem in modern JavaScript due to the introduction of let and const. In the following code, each time around the loop, a new function inner is created, which closes over i. But because var i is hoisted outside the loop, all of these inner functions close over the same variable, meaning that the final value of i (3) is printed, three times.   function foo() {   var result = []   for (var i = 0; i < 3; i++) {     result.push(function inner() { console.log(i) } )   }   return result }  const result = foo() // The following will print `3`, three times... for (var i = 0; i < 3; i++) {   result[i]()  }    Every function in JavaScript maintains a link to its outer lexical environment. A lexical environment is a map of all the names (eg. variables, parameters) within a scope, with their values. So, whenever you see the function keyword, code inside that function has access to variables declared outside the function.   function foo(x) {   var tmp = 3;    function bar(y) {     console.log(x + y + (++tmp)); // will log 16   }    bar(10); }  foo(2);    This will log 16 because function bar closes over the parameter x and the variable tmp, both of which exist in the lexical environment of outer function foo. Function bar, together with its link with the lexical environment of function foo is a closure.  A function doesn't have to return in order to create a closure. Simply by virtue of its declaration, every function closes over its enclosing lexical environment, forming a closure.   function foo(x) {   var tmp = 3;    return function (y) {     console.log(x + y + (++tmp)); // will also log 16   } }  var bar = foo(2); bar(10); // 16 bar(10); // 17    The above function will also log 16, because the code inside bar can still refer to argument x and variable tmp, even though they are no longer directly in scope. However, since tmp is still hanging around inside bar's closure, it is available to be incremented. It will be incremented each time you call bar. The simplest example of a closure is this:   var a = 10;  function test() {   console.log(a); // will output 10   console.log(b); // will output 6 } var b = 6; test();    When a JavaScript function is invoked, a new execution context ec is created. Together with the function arguments and the target object, this execution context also receives a link to the lexical environment of the calling execution context, meaning the variables declared in the outer lexical environment (in the above example, both a and b) are available from ec. Every function creates a closure because every function has a link to its outer lexical environment.  Note that variables themselves are visible from within a closure, not copies. FOREWORD: this answer was written when the question was: Like the old Albert said : "If you can't explain it to a six-year old, you really don't understand it yourself.”. Well I tried to explain JS closures to a 27 years old friend and completely failed. Can anybody consider that I am 6 and strangely interested in that subject ? I'm pretty sure I was one of the only people that attempted to take the initial question literally. Since then, the question has mutated several times, so my answer may now seem incredibly silly & out of place. Hopefully the general idea of the story remains fun for some. I'm a big fan of analogy and metaphor when explaining difficult concepts, so let me try my hand with a story. Once upon a time: There was a princess... She lived in a wonderful world full of adventures. She met her Prince Charming, rode around her world on a unicorn, battled dragons, encountered talking animals, and many other fantastical things. But she would always have to return back to her dull world of chores and grown-ups. And she would often tell them of her latest amazing adventure as a princess. But all they would see is a little girl... ...telling stories about magic and fantasy. And even though the grown-ups knew of real princesses, they would never believe in the unicorns or dragons because they could never see them. The grown-ups said that they only existed inside the little girl's imagination. But we know the real truth; that the little girl with the princess inside... ...is really a princess with a little girl inside. Taking the question seriously, we should find out what a typical 6-year-old is capable of cognitively, though admittedly, one who is interested in JavaScript is not so typical.   On  Childhood Development: 5 to 7 Years  it says: Your child will be able to follow two-step directions. For example, if you say to your child, "Go to the kitchen and get me a trash bag" they will be able to remember that direction. We can use this example to explain closures, as follows: The kitchen is a closure that has a local variable, called trashBags.  There is a function inside the kitchen called getTrashBag that gets one trash bag and returns it. We can code this in JavaScript like this:   function makeKitchen() {   var trashBags = ['A', 'B', 'C']; // only 3 at first    return {     getTrashBag: function() {       return trashBags.pop();     }   }; }  var kitchen = makeKitchen();  console.log(kitchen.getTrashBag()); // returns trash bag C console.log(kitchen.getTrashBag()); // returns trash bag B console.log(kitchen.getTrashBag()); // returns trash bag A    Further points that explain why closures are interesting: I need to know how many times a button has been clicked and do something on every third click...   // Declare counter outside event handler's scope var counter = 0; var element = document.getElementById('button');  element.addEventListener("click", function() {   // Increment outside counter   counter++;    if (counter === 3) {     // Do something every third time     console.log("Third time's the charm!");      // Reset counter     counter = 0;   } }); <button id="button">Click Me!</button>    Now this will work, but it does encroach into the outer scope by adding a variable, whose sole purpose is to keep track of the count. In some situations, this would be preferable as your outer application might need access to this information. But in this case, we are only changing every third click's behavior, so it is preferable to enclose this functionality inside the event handler.   var element = document.getElementById('button');  element.addEventListener("click", (function() {   // init the count to 0   var count = 0;    return function(e) { // <- This function becomes the click handler     count++; //    and will retain access to the above `count`      if (count === 3) {       // Do something every third time       console.log("Third time's the charm!");        //Reset counter       count = 0;     }   }; })()); <button id="button">Click Me!</button>    Notice a few things here. In the above example, I am using the closure behavior of JavaScript. This behavior allows any function to have access to the scope in which it was created, indefinitely. To practically apply this, I immediately invoke a function that returns another function, and because the function I'm returning has access to the internal count variable (because of the closure behavior explained above) this results in a private scope for usage by the resulting function... Not so simple? Let's dilute it down... A simple one-line closure All variables outside the returned function are available to the returned function, but they are not directly available to the returned function object... Get it? So in our primary example, the count variable is contained within the closure and always available to the event handler, so it retains its state from click to click. Also, this private variable state is fully accessible, for both readings and assigning to its private scoped variables. There you go; you're now fully encapsulating this behavior. Full Blog Post (including jQuery considerations) Closures are hard to explain because they are used to make some behaviour work that everybody intuitively expects to work anyway. I find the best way to explain them (and the way that I learned what they do) is to imagine the situation without them:   const makePlus = function(x) {     return function(y) { return x + y; }; }  const plus5 = makePlus(5); console.log(plus5(3));    What would happen here if JavaScript didn't know closures? Just replace the call in the last line by its method body (which is basically what function calls do) and you get: Now, where's the definition of x? We didn't define it in the current scope. The only solution is to let plus5 carry its scope (or rather, its parent's scope) around. This way, x is well-defined and it is bound to the value 5. TLDR A closure is a link between a function and its outer lexical (ie. as-written) environment, such that the identifiers (variables, parameters, function declarations etc) defined within that environment are visible from within the function, regardless of when or from where the function is invoked. Details In the terminology of the ECMAScript specification, a closure can be said to be implemented by the [[Environment]] reference of every function-object, which points to the lexical environment within which the function is defined. When a function is invoked via the internal [[Call]] method, the [[Environment]] reference on the function-object is copied into the outer environment reference of the environment record of the newly-created execution context (stack frame). In the following example, function f closes over the lexical environment of the global execution context: In the following example, function h closes over the lexical environment of function g, which, in turn, closes over the lexical environment of the global execution context. If an inner function is returned by an outer, then the outer lexical environment will persist after the outer function has returned. This is because the outer lexical environment needs to be available if the inner function is eventually invoked. In the following example, function j closes over the lexical environment of function i, meaning that variable x is visible from inside function j, long after function i has completed execution:   function i() {     var x = 'mochacchino'     return function j() {         console.log('Printing the value of x, from within function j: ', x)     } }   const k = i() setTimeout(k, 500) // invoke k (which is j) after 500ms    In a closure, the variables in the outer lexical environment themselves are available, not copies.   function l() {   var y = 'vanilla';    return {     setY: function(value) {       y = value;     },     logY: function(value) {       console.log('The value of y is: ', y);     }   } }  const o = l() o.logY() // The value of y is: vanilla o.setY('chocolate') o.logY() // The value of y is: chocolate    The chain of lexical environments, linked between execution contexts via outer environment references, forms a scope chain and defines the identifiers visible from any given function. Please note that in an attempt to improve clarity and accuracy, this answer has been substantially changed from the original. OK, 6-year-old closures fan. Do you want to hear the simplest example of closure? Let's imagine the next situation: a driver is sitting in a car. That car is inside a plane. Plane is in the airport. The ability of driver to access things outside his car, but inside the plane, even if that plane leaves an airport, is a closure. That's it. When you turn 27, look at the more detailed explanation or at the example below. Here is how I can convert my plane story into the code.   var plane = function(defaultAirport) {    var lastAirportLeft = defaultAirport;    var car = {     driver: {       startAccessPlaneInfo: function() {         setInterval(function() {           console.log("Last airport was " + lastAirportLeft);         }, 2000);       }     }   };   car.driver.startAccessPlaneInfo();    return {     leaveTheAirport: function(airPortName) {       lastAirportLeft = airPortName;     }   } }("Boryspil International Airport");  plane.leaveTheAirport("John F. Kennedy");    This is an attempt to clear up several (possible) misunderstandings about closures that appear in some of the other answers. I wrote a blog post a while back explaining closures. Here's what I said about closures in terms of why you'd want one. Closures are a way to let a function   have persistent, private variables -   that is, variables that only one   function knows about, where it can   keep track of info from previous times   that it was run. In that sense, they let a function act a bit like an object with private attributes. Full post: So what are these closure thingys? The following simple example covers all the main points of JavaScript closures.*   Here is a factory that produces calculators that can add and multiply: The key point: Each call to make_calculator creates a new local variable n, which continues to be usable by that calculator's add and multiply functions long after make_calculator returns. If you are familiar with stack frames, these calculators seem strange: How can they keep accessing n after make_calculator returns?  The answer is to imagine that JavaScript doesn't use "stack frames", but instead uses "heap frames", which can persist after the function call that made them returns. Inner functions like add and multiply, which access variables declared in an outer function**, are called closures. That is pretty much all there is to closures.  * For example, it covers all the points in the "Closures for Dummies" article given in another answer, except example 6, which simply shows that variables can be used before they are declared, a nice fact to know but completely unrelated to closures. It also covers all the points in the accepted answer, except for the points (1) that functions copy their arguments into local variables (the named function arguments), and (2) that copying numbers creates a new number, but copying an object reference gives you another reference to the same object. These are also good to know but again completely unrelated to closures. It is also very similar to the example in this answer but a bit shorter and less abstract. It does not cover the point of this answer or this comment, which is that JavaScript makes it difficult to plug the current value of a loop variable into your inner function: The "plugging in" step can only be done with a helper function that encloses your inner function and is invoked on each loop iteration. (Strictly speaking, the inner function accesses the helper function's copy of the variable, rather than having anything plugged in.) Again, very useful when creating closures, but not part of what a closure is or how it works. There is additional confusion due to closures working differently in functional languages like ML, where variables are bound to values rather than to storage space, providing a constant stream of people who understand closures in a way (namely the "plugging in" way) that is simply incorrect for JavaScript, where variables are always bound to storage space, and never to values.  ** Any outer function, if several are nested, or even in the global context, as this answer points out clearly. How I'd explain it to a six-year-old: You know how grown-ups can own a house, and they call it home? When a mom has a child, the child doesn't really own anything, right? But its parents own a house, so whenever someone asks the child "Where's your home?", he/she can answer "that house!", and point to the house of its parents. A "Closure" is the ability of the child to always (even if abroad) be able to say it has a home, even though it's really the parent's who own the house. I still think Google's explanation works very well and is concise:  *A C# question I tend to learn better by GOOD/BAD comparisons. I like to see working code followed by non-working code that someone is likely to encounter. I put together a jsFiddle that does a comparison and tries to boil down the differences to the simplest explanations I could come up with. In the above code createClosure(n) is invoked in every iteration of the loop. Note that I named the variable n to highlight that it is a new variable created in a new function scope and is not the same variable as index which is bound to the outer scope. This creates a new scope and n is bound to that scope; this means we have 10 separate scopes, one for each iteration. createClosure(n) returns a function that returns the n within that scope. Within each scope n is bound to whatever value it had when createClosure(n) was invoked so the nested function that gets returned will always return the value of n that it had when createClosure(n) was invoked. In the above code the loop was moved within the createClosureArray() function and the function now just returns the completed array, which at first glance seems more intuitive. What might not be obvious is that since createClosureArray() is only invoked once only one scope is created for this function instead of one for every iteration of the loop. Within this function a variable named index is defined. The loop runs and adds functions to the array that return index. Note that index is defined within the createClosureArray function which only ever gets invoked one time. Because there was only one scope within the createClosureArray() function, index is only bound to a value within that scope. In other words, each time the loop changes the value of index, it changes it for everything that references it within that scope. All of the functions added to the array return the SAME index variable from the parent scope where it was defined instead of 10 different ones from 10 different scopes like the first example. The end result is that all 10 functions return the same variable from the same scope. After the loop finished and index was done being modified the end value was 10, therefore every function added to the array returns the value of the single index variable which is now set to 10. CLOSURES DONE RIGHT n = 0 n = 1 n = 2 n = 3 n = 4 n = 5 n = 6 n = 7 n = 8 n = 9 CLOSURES DONE WRONG n = 10 n = 10 n = 10 n = 10 n = 10 n = 10 n = 10 n = 10 n = 10 n = 10 Wikipedia on closures: In computer science, a closure is a function together with a referencing environment for the nonlocal names (free variables) of that function. Technically, in JavaScript, every function is a closure. It always has an access to variables defined in the surrounding scope. Since scope-defining construction in JavaScript is a function, not a code block like in many other languages, what we usually mean by closure in JavaScript is a function working with nonlocal variables defined in already executed surrounding function. Closures are often used for creating functions with some hidden private data (but it's not always the case). ems The example above is using an anonymous function, which was executed once. But it does not have to be. It can be named (e.g. mkdb) and executed later, generating a database function each time it is invoked. Every generated function will have its own hidden database object. Another usage example of closures is when we don't return a function, but an object containing multiple functions for different purposes, each of those function having access to the same data. I put together an interactive JavaScript tutorial to explain how closures work. What's a Closure? Here's one of the examples: The children will always remember the secrets they have shared with their parents, even after their parents are   gone. This is what closures are for functions. The secrets for JavaScript functions are the private variables Every time you call it, local variable "name" is created and given name "Mary". And every time the function exits the variable is lost and the name is forgotten. As you may guess, because the variables are re-created every time the function is called, and nobody else will know them, there must be a secret place where they are stored. It could be called Chamber of Secrets or stack or local scope but it doesn't really matter. We know they are there, somewhere, hidden in the memory. But, in JavaScript there is this very special thing that functions which are created inside other functions, can also know the local variables of their parents and keep them as long as they live. So, as long as we are in the parent -function, it can create one or more child functions which do share the secret variables from the secret place. But the sad thing is, if the child is also a private variable of its parent function, it would also die when the parent ends, and the secrets would die with them. So to live, the child has to leave before it's too late And now, even though Mary is "no longer running", the memory of her is not lost and her child will always remember her name and other secrets they shared during their time together. So, if you call the child "Alice", she will respond That's all there is to tell. I do not understand why the answers are so complex here. Here is a closure: Yes. You probably use that many times a day. There is no reason to believe closures are a complex design hack to address specific problems. No, closures are just about using a variable that comes from a higher scope from the perspective of where the function was declared (not run). Now what it allows you to do can be more spectacular, see other answers. Example for the first point by dlaliberte: A closure is not only created when you return an inner function. In fact, the enclosing function does not need to return at all. You might instead assign your inner function to a variable in an outer scope, or pass it as an argument to another function where it could be used immediately. Therefore, the closure of the enclosing function probably already exists at the time that enclosing function was called since any inner function has access to it as soon as it is called. A closure is where an inner function has access to variables in its outer function. That's probably the simplest one-line explanation you can get for closures. I know there are plenty of solutions already, but I guess that this small and simple script can be useful to demonstrate the concept: You're having a sleep over and you invite Dan. You tell Dan to bring one XBox controller. Dan invites Paul. Dan asks Paul to bring one controller. How many controllers were brought to the party? The author of Closures has explained closures pretty well, explaining the reason why we need them and also explaining LexicalEnvironment which is necessary to understanding closures.  Here is the summary: What if a variable is accessed, but it isn’t local? Like here:  In this case, the interpreter finds the variable in the outer LexicalEnvironment object. The process consists of two steps:  When a function is created, it gets a hidden property, named [[Scope]], which references the current LexicalEnvironment.  If a variable is read, but can not be found anywhere, an error is generated. Nested functions Functions can be nested one inside another, forming a chain of LexicalEnvironments which can also be called a scope chain.  So, function g has access to g, a and f. Closures A nested function may continue to live after the outer function has finished:  Marking up LexicalEnvironments:  As we see, this.say is a property in the user object, so it continues to live after User completed. And if you remember, when this.say is created, it (as every function) gets an internal reference this.say.[[Scope]] to the current LexicalEnvironment. So, the LexicalEnvironment of the current User execution stays in memory. All variables of User also are its properties, so they are also carefully kept, not junked as usually. The whole point is to ensure that if the inner function wants to access an outer variable in the future, it is able to do so. To summarize: This is called a closure. JavaScript functions can access their: If a function accesses its environment, then the function is a closure. Note that outer functions are not required, though they do offer benefits I don't discuss here. By accessing data in its environment, a closure keeps that data alive. In the subcase of outer/inner functions, an outer function can create local data and eventually exit, and yet, if any inner function(s) survive after the outer function exits, then the inner function(s) keep the outer function's local data alive. Example of a closure that uses the global environment: Imagine that the Stack Overflow Vote-Up and Vote-Down button events are implemented as closures, voteUp_click and voteDown_click, that have access to external variables isVotedUp and isVotedDown, which are defined globally. (For simplicity's sake, I am referring to StackOverflow's Question Vote buttons, not the array of Answer Vote buttons.) When the user clicks the VoteUp button, the voteUp_click function checks whether isVotedDown == true to determine whether to vote up or merely cancel a down vote. Function voteUp_click is a closure because it is accessing its environment. All four of these functions are closures as they all access their environment. As a father of a 6-year-old, currently teaching young children (and a relative novice to coding with no formal education so corrections will be required), I think the lesson would stick best through hands-on play. If the 6-year-old is ready to understand what a closure is, then they are old enough to have a go themselves. I'd suggest pasting the code into jsfiddle.net, explaining a bit, and leaving them alone to concoct a unique song. The explanatory text below is probably more appropriate for a 10 year old. INSTRUCTIONS DATA: Data is a collection of facts. It can be numbers, words, measurements, observations or even just descriptions of things. You can't touch it, smell it or taste it. You can write it down, speak it and hear it. You could use it to create touch smell and taste using a computer. It can be made useful by a computer using code. CODE: All the writing above is called code. It is written in JavaScript. JAVASCRIPT: JavaScript is a language. Like English or French or Chinese are languages. There are lots of languages that are understood by computers and other electronic processors. For JavaScript to be understood by a computer it needs an interpreter. Imagine if a teacher who only speaks Russian comes to teach your class at school. When the teacher says "все садятся", the class would not understand. But luckily you have a Russian pupil in your class who tells everyone this means "everybody sit down" - so you all do. The class is like a computer and the Russian pupil is the interpreter. For JavaScript the most common interpreter is called a browser. BROWSER: When you connect to the Internet on a computer, tablet or phone to visit a website, you use a browser. Examples you may know are Internet Explorer, Chrome, Firefox and Safari. The browser can understand JavaScript and tell the computer what it needs to do. The JavaScript instructions are called functions. FUNCTION: A function in JavaScript is like a factory. It might be a little factory with only one machine inside. Or it might contain many other little factories, each with many machines doing different jobs. In a real life clothes factory you might have reams of cloth and bobbins of thread going in and T-shirts and jeans coming out. Our JavaScript factory only processes data, it can't sew, drill a hole or melt metal. In our JavaScript factory data goes in and data comes out. All this data stuff sounds a bit boring, but it is really very cool; we might have a function that tells a robot what to make for dinner. Let's say I invite you and your friend to my house. You like chicken legs best, I like sausages, your friend always wants what you want and my friend does not eat meat. I haven't got time to go shopping, so the function needs to know what we have in the fridge to make decisions. Each ingredient has a different cooking time and we want everything to be served hot by the robot at the same time. We need to provide the function with the data about what we like, the function could 'talk' to the fridge, and the function could control the robot. A function normally has a name, parentheses and braces. Like this: Note that /*...*/ and // stop code being read by the browser. NAME: You can call a function just about whatever word you want. The example "cookMeal" is typical in joining two words together and giving the second one a capital letter at the beginning - but this is not necessary. It can't have a space in it, and it can't be a number on its own. PARENTHESES: "Parentheses" or () are the letter box on the JavaScript function factory's door or a post box in the street for sending packets of information to the factory. Sometimes the postbox might be marked for example cookMeal(you, me, yourFriend, myFriend, fridge, dinnerTime), in which case you know what data you have to give it. BRACES: "Braces" which look like this {} are the tinted windows of our factory. From inside the factory you can see out, but from the outside you can't see in. THE LONG CODE EXAMPLE ABOVE Our code begins with the word function, so we know that it is one! Then the name of the function sing - that's my own description of what the function is about. Then parentheses (). The parentheses are always there for a function. Sometimes they are empty, and sometimes they have something in. This one has a word in: (person). After this there is a brace like this { . This marks the start of the function sing(). It has a partner which marks the end of sing() like this } So this function might have something to do with singing, and might need some data about a person. It has instructions inside to do something with that data. Now, after the function sing(), near the end of the code is the line VARIABLE: The letters var stand for "variable". A variable is like an envelope. On the outside this envelope is marked "person". On the inside it contains a slip of paper with the information our function needs, some letters and spaces joined together like a piece of string (it's called a string) that make a phrase reading "an old lady". Our envelope could contain other kinds of things like numbers (called integers), instructions (called functions), lists (called arrays). Because this variable is written outside of all the braces {}, and because you can see out through the tinted windows when you are inside the braces, this variable can be seen from anywhere in the code. We call this a 'global variable'. GLOBAL VARIABLE: person is a global variable, meaning that if you change its value from "an old lady" to "a young man", the person will keep being a young man until you decide to change it again and that any other function in the code can see that it's a young man. Press the F12 button or look at the Options settings to open the developer console of a browser and type "person" to see what this value is. Type person="a young man" to change it and then type "person" again to see that it has changed. After this we have the line This line is calling the function, as if it were calling a dog "Come on sing, Come and get person!" When the browser has loaded the JavaScript code an reached this line, it will start the function. I put the line at the end to make sure that the browser has all the information it needs to run it. Functions define actions  - the main function is about singing. It contains a variable called firstPart which applies to the singing about the person that applies to each of the verses of the song: "There was " + person + " who swallowed". If you type firstPart into the console, you won't get an answer because the variable is locked up in a function - the browser can't see inside the tinted windows of the braces. CLOSURES: The closures are the smaller functions that are inside the big sing() function. The little factories inside the big factory. They each have their own braces which mean that the variables inside them can't be seen from the outside. That's why the names of the variables (creature and result) can be repeated in the closures but with different values. If you type these variable names in the console window, you won't get its value because it's hidden by two layers of tinted windows. The closures all know what the sing() function's variable called firstPart is, because they can see out from their tinted windows. After the closures come the lines The sing() function will call each of these functions in the order they are given. Then the sing() function's work will be done. Okay, talking with a 6-year old child, I would possibly use following associations. Imagine - you are playing with your little brothers and sisters in the entire house, and you are moving around with your toys and brought some of them into your older brother's room. After a while your brother returned from the school and went to his room, and he locked inside it, so now you could not access toys left there anymore in a direct way. But you could knock the door and ask your brother for that toys. This is called toy's closure; your brother made it up for you, and he is now into outer scope. Compare with a situation when a door was locked by draft and nobody inside (general function execution), and then some local fire occur and burn down the room (garbage collector:D), and then a new room was build and now you may leave another toys there (new function instance), but never get the same toys which were left in the first room instance. For an advanced child I would put something like the following. It is not perfect, but it makes you feel about what it is: As you can see, the toys left in the room are still accessible via the brother and no matter if the room is locked. Here is a jsbin to play around with it. An answer for a six-year-old (assuming he knows what a function is and what a variable is, and what data is): Functions can return data. One kind of data you can return from a function is another function. When that new function gets returned, all the variables and arguments used in the function that created it don't go away. Instead, that parent function "closes." In other words, nothing can look inside of it and see the variables it used except for the function it returned. That new function has a special ability to look back inside the function that created it and see the data inside of it. Another really simple way to explain it is in terms of scope: Any time you create a smaller scope inside of a larger scope, the smaller scope will always be able to see what is in the larger scope. A function in JavaScript is not just a reference to a set of instructions (as in C language), but it also includes a hidden data structure which is composed of references to all nonlocal variables it uses (captured variables). Such two-piece functions are called closures. Every function in JavaScript can be considered a closure. Closures are functions with a state. It is somewhat similar to "this" in the sense that "this" also provides state for a function but function and "this" are separate objects ("this" is just a fancy parameter, and the only way to bind it permanently to a function is to create a closure). While "this" and function always live separately, a function cannot be separated from its closure and the language provides no means to access captured variables. Because all these external variables referenced by a lexically nested function are actually local variables in the chain of its lexically enclosing functions (global variables can be assumed to be local variables of some root function), and every single execution of a function creates new instances of its local variables, it follows that every execution of a function returning (or otherwise transferring it out, such as registering it as a callback) a nested function creates a new closure (with its own potentially unique set of referenced nonlocal variables which represent its execution context). Also, it must be understood that local variables in JavaScript are created not on the stack frame, but on the heap and destroyed only when no one is referencing them. When a function returns, references to its local variables are decremented, but they can still be non-null if during the current execution they became part of a closure and are still referenced by its lexically nested functions (which can happen only if the references to these nested functions were returned or otherwise transferred to some external code). An example: Perhaps a little beyond all but the most precocious of six-year-olds, but a few examples that helped make the concept of closure in JavaScript click for me. A closure is a function that has access to another function's scope (its variables and functions). The easiest way to create a closure is with a function within a function; the reason being that in JavaScript a function always has access to its containing function’s scope.   function outerFunction() {     var outerVar = "monkey";          function innerFunction() {         alert(outerVar);     }          innerFunction(); }  outerFunction();    ALERT: monkey In the above example, outerFunction is called which in turn calls innerFunction. Note how outerVar is available to innerFunction, evidenced by its correctly alerting the value of outerVar. Now consider the following:   function outerFunction() {     var outerVar = "monkey";          function innerFunction() {         return outerVar;     }          return innerFunction; }  var referenceToInnerFunction = outerFunction(); alert(referenceToInnerFunction());    ALERT: monkey referenceToInnerFunction is set to outerFunction(), which simply returns a reference to innerFunction. When referenceToInnerFunction is called, it returns outerVar. Again, as above, this demonstrates that innerFunction has access to outerVar, a variable of outerFunction. Furthermore, it is interesting to note that it retains this access even after outerFunction has finished executing. And here is where things get really interesting. If we were to get rid of outerFunction, say set it to null, you might think that referenceToInnerFunction would loose its access to the value of outerVar. But this is not the case.    function outerFunction() {     var outerVar = "monkey";          function innerFunction() {         return outerVar;     }          return innerFunction; }  var referenceToInnerFunction = outerFunction(); alert(referenceToInnerFunction());  outerFunction = null; alert(referenceToInnerFunction());    ALERT: monkey ALERT: monkey But how is this so? How can referenceToInnerFunction still know the value of outerVar now that outerFunction has been set to null? The reason that referenceToInnerFunction can still access the value of outerVar is because when the closure was first created by placing innerFunction inside of outerFunction, innerFunction added a reference to outerFunction’s scope (its variables and functions) to its scope chain. What this means is that innerFunction has a pointer or reference to all of outerFunction’s variables, including outerVar. So even when outerFunction has finished executing, or even if it is deleted or set to null, the variables in its scope, like outerVar, stick around in memory because of the outstanding reference to them on the part of the innerFunction that has been returned to referenceToInnerFunction. To truly release outerVar and the rest of outerFunction’s variables from memory you would have to get rid of this outstanding reference to them, say by setting referenceToInnerFunction to null as well. ////////// Two other things about closures to note. First, the closure will always have access to the last values of its containing function.   function outerFunction() {     var outerVar = "monkey";          function innerFunction() {         alert(outerVar);     }          outerVar = "gorilla";      innerFunction(); }  outerFunction();    ALERT: gorilla Second, when a closure is created, it retains a reference to all of its enclosing function’s variables and functions; it doesn’t get to pick and choose. And but so, closures should be used sparingly, or at least carefully, as they can be memory intensive; a lot of variables can be kept in memory long after a containing function has finished executing. I'd simply point them to the Mozilla Closures page. It's the best, most concise and simple explanation of closure basics and practical usage that I've found. It is highly recommended to anyone learning JavaScript. And yes, I'd even recommend it to a 6-year old -- if the 6-year old is learning about closures, then it's logical they're ready to comprehend the concise and simple explanation provided in the article.
__label__git __label__git-checkout __label__git-reset __label__git-revert How do I revert from my current state to a snapshot made on a certain commit? If I do git log, then I get the following output: How do I revert to the commit from November 3, i.e. commit 0d1d7fc? This depends a lot on what you mean by "revert". If you want to temporarily go back to it, fool around, then come back to where you are, all you have to do is check out the desired commit: Or if you want to make commits while you're there, go ahead and make a new branch while you're at it: To go back to where you were, just check out the branch you were on again. (If you've made changes, as always when switching branches, you'll have to deal with them as appropriate. You could reset to throw them away; you could stash, checkout, stash pop to take them with you; you could commit them to a branch there if you want a branch there.) If, on the other hand, you want to really get rid of everything you've done since then, there are two possibilities. One, if you haven't published any of these commits, simply reset: If you mess up, you've already thrown away your local changes, but you can at least get back to where you were before by resetting again. On the other hand, if you've published the work, you probably don't want to reset the branch, since that's effectively rewriting history. In that case, you could indeed revert the commits. With Git, revert has a very specific meaning: create a commit with the reverse patch to cancel it out. This way you don't rewrite any history. The git-revert manpage actually covers a lot of this in its description. Another useful link is this git-scm.com section discussing git-revert. If you decide you didn't want to revert after all, you can revert the revert (as described here) or reset back to before the revert (see the previous section). You may also find this answer helpful in this case: How can I move HEAD back to a previous location? (Detached head) & Undo commits Lots of complicated and dangerous answers here, but it's actually easy: This will revert everything from the HEAD back to the commit hash, meaning it will recreate that commit state in the working tree as if every commit after 0766c053 had been walked back. You can then commit the current tree, and it will create a brand new commit essentially equivalent to the commit you "reverted" to.   (The --no-commit flag lets git revert all the commits at once- otherwise you'll be prompted for a message for each commit in the range, littering your history with unnecessary new commits.) This is a safe and easy way to rollback to a previous state. No history is destroyed, so it can be used for commits that have already been made public. Working on your own and just want it to work? Follow these instructions below, they’ve worked reliably for me and many others for years. Working with others? Git is complicated. Read the comments below this answer before you do something rash. To revert to a previous commit, ignoring any changes: where HEAD is the last commit in your current branch To revert to a commit that's older than the most recent commit: Credits go to a similar Stack Overflow question, Revert to a commit by a SHA hash in Git?. The best option for me and probably others is the Git reset option: This has been the best option for me! It is simple, fast and effective! ** Note:** As mentioned in comments don't do this if you're sharing your branch with other people who have copies of the old commits Also from the comments, if you wanted a less 'ballzy' method you could use  Before answering let's add some background, explaining what this HEAD is. HEAD is simply a reference to the current commit (latest) on the current branch. There can only be a single HEAD at any given time (excluding git worktree). The content of HEAD is stored inside .git/HEAD, and it contains the 40 bytes SHA-1 of the current commit. If you are not on the latest commit - meaning that HEAD is pointing to a prior commit in history it's called detached HEAD.  On the command line it will look like this - SHA-1 instead of the branch name since the HEAD is not pointing to the the tip of the current branch:  This will checkout new branch pointing to the desired commit. This command will checkout to a given commit. At this point you can create a branch and start to work from this point on: You can always use the reflog as well. git reflog  will display any change which updated the HEAD and checking out the desired reflog entry will set the HEAD back to this commit. Every time the HEAD is modified there will be a new entry in the reflog This will get you back to your desired commit  "Move" your head back to the desired commit. This schema illustrates which command does what. As you can see there reset && checkout modify the HEAD.  If you want to "uncommit", erase the last commit message, and put the modified files back in staging, you would use the command: This is an extremely useful command in situations where you committed the wrong thing and you want to undo that last commit. Source: http://nakkaya.com/2009/09/24/git-delete-last-commit/ You can do this by the following two commands: It will remove your previous Git commit. If you want to keep your changes, you can also use: Then it will save your changes. I have tried a lot of ways to revert local changes in Git, and it seems that this works the best if you just want to revert to the latest commit state.  Short description:  I found a much more convenient and simple way to achieve the results above:  where HEAD points to the latest commit at you current branch. It is the same code code as boulder_ruby suggested, but I have added git add . before  git reset --hard HEAD to erase all new files created since the last commit since this is what most people expect I believe when reverting to the latest commit. OK, going back to a previous commit in Git is quite easy... Revert back without keeping the changes: Revert back with keeping the changes: Explanation: using git reset, you can reset to a specific state. It's common using it with a commit hash as you see above. But as you see the difference is using the two flags --soft and --hard, by default git reset using --soft flag, but it's a good practice always using the flag, I explain each flag: The default flag as explained, not need to provide it, does not change the working tree, but it adds all changed files ready to commit, so you go back to the commit status which changes to files get unstaged. Be careful with this flag. It resets the working tree and all changes to tracked files and all will be gone! I also created the image below that may happen in a real life working with Git:  The best way is: This will reset the branch to the specific commit and then will upload the remote server with the same commits as you have in local. Be careful with the --force flag as it removes all the subsequent commits after the selected commit without the option to recover them. Assuming you're talking about master and on that respective branch (that said, this could be any working branch you're concerned with): I found the answer from in a blog post (now no longer exists) Note that this is Resetting and Forcing the change to the remote, so that if others on your team have already git pulled, you will cause problems for them. You are destroying the change history, which is an important reason why people use git in the first place. Better to use revert (see other answers) than reset.  If you're a one man team then it probably doesn't matter. Say you have the following commits in a text file named ~/commits-to-revert.txt (I used git log --pretty=oneline to get them) Create a Bash shell script to revert each of them: This reverts everything back to the previous state, including file and directory creations, and deletions, commit it to your branch and you retain the history, but you have it reverted back to the same file structure. Why Git doesn't have a git revert --to <hash> is beyond me. Jefromi's solutions are definitely the best ones, and you should definitely use them. However, for the sake of completeness, I also wanted to show these other alternative solutions that can also be used to revert a commit (in the sense that you create a new commit that undoes changes in previous commit, just like what git revert does). To be clear, these alternatives are not the best way to revert commits, Jefromi's solutions are, but I just want to point out that you can also use these other methods to achieve the same thing as git revert. This is a very slightly modified version of Charles Bailey's solution to Revert to a commit by a SHA hash in Git?: This basically works by using the fact that soft resets will leave the state of the previous commit staged in the index/staging-area, which you can then commit. This solution comes from svick's solution to Checkout old commit and make it a new commit: Similarly to alternative #1, this reproduces the state of <commit> in the current working copy. It is necessary to do git rm first because git checkout won't remove files that have been added since <commit>. Here is a much simpler way to go back to a previous commit (and have it in an uncommited state, to do with it whatever you like): So, no need for commit ids and so on :) There is a command (not a part of core Git, but it is in the git-extras package) specifically for reverting and staging old commits: Per the man page, it can also be used as such: After all the changes, when you push all these commands, you might have to use: And not only git push. You can complete all these initial steps yourself and push back to the Git repository. Pull the latest version of your repository from Bitbucket using the git pull --all command. Run the Git log command with -n 4 from your terminal. The number after the -n determines the number of commits in the log starting from the most recent commit in your local history. Reset the head of your repository's history using the git reset --hard HEAD~N where N is the number of commits you want to take the head back. In the following example the head would be set back one commit, to the last commit in the repository history: Push the change to Git repository using git push --force to force push the change. If you want the Git repository to a previous commit: Revert to most recent commit and ignoring all local changes: Select your required commit, and check it by till you get the required commit. To make the HEAD point to that, do or git reset --hard HEAD~2 or whatever. If the situation is an urgent one, and you just want to do what the questioner asked in a quick and dirty way, assuming your project is under a directory called, for example, "my project": QUICK AND DIRTY: depending on the circumstances, quick and dirty may in fact be very GOOD. What my solution here does is NOT replace irreversibly the files you have in your working directory with files hauled up/extracted from the depths of the git repository lurking beneath your .git/ directory using fiendishly clever and diabolically powerful git commands, of which there are many. YOU DO NOT HAVE TO DO SUCH DEEP-SEA DIVING TO RECOVER what may appear to be a disastrous situation, and attempting to do so without sufficient expertise may prove fatal. Copy the whole directory and call it something else, like "my project - copy". Assuming your git repository ("repo") files are under the "my project" directory (the default place for them, under a directory called ".git"), you will now have copied both your work files and your repo files. Do this in the directory "my project": This will return the state of the repo under "my project" to what it was when you made that commit (a "commit" means a snapshot of your working files). All commits since then will be lost forever under "my project", BUT... they will still be present in the repo under "my project - copy" since you copied all those files - including the ones under .../.git/. You then have two versions on your system... you can examine or copy or modify files of interest, or whatever, from the previous commit. You can completely discard the files under "my project - copy", if you have decided the new work since the restored commit was going nowhere...  The obvious thing if you want to carry on with the state of the project without actually discarding the work since this retrieved commit is to rename your directory again: Delete the project containing the retrieved commit (or give it a temporary name) and rename your "my project - copy" directory back to "my project". Then maybe try to understand some of the other answers here, and probably do another commit fairly soon. Git is a brilliant creation but absolutely no-one is able to just "pick it up on the fly": also people who try to explain it far too often assume prior knowledge of other VCS [Version Control Systems] and delve far too deep far too soon, and commit other crimes, like using interchangeable terms for "checking out" - in ways which sometimes appear almost calculated to confuse a beginner.   To save yourself much stress, learn from my scars. You have to pretty much have to read a book on Git - I'd recommend "Version Control with Git". Do it sooner rather than later. If you do, bear in mind that much of the complexity of Git comes from branching and then remerging: you can skip those parts in any book. From your question there's no reason why people should be blinding you with science.   Especially if, for example, this is a desperate situation and you're a newbie with Git! PS: One other thought: It is (now) actually quite simple to keep the Git repo in a directory other than the one with the working files. This would mean you would not have to copy the entire Git repository using the above quick & dirty solution. See the answer by Fryer using --separate-git-dir here. Be warned, though: If you have a "separate-directory" repository which you don't copy, and you do a hard reset, all versions subsequent to the reset commit will be lost forever, unless you have, as you absolutely should, regularly backed up your repository, preferably to the Cloud (e.g. Google Drive) among other places. On this subject of "backing up to the Cloud", the next step is to open an account (free of course) with GitHub or (better in my view) GitLab. You can then regularly do a git push command to make your Cloud repo up-to-date "properly". But again, talking about this may be too much too soon. It directly clears all the changes that you have been making since the last commit. PS: It has a little problem; it also deletes all you recently stored stash changes. Which I guess in most cases should not matter. To completely clean a coder's directory up from some accidental changes, we used: Just git reset --hard HEAD will get rid of modifications, but it won't get rid of "new" files. In their case they'd accidentally dragged an important folder somewhere random, and all those files were being treated as new by Git, so a reset --hard didn't fix it. By running the git add -A . beforehand, it explicitly tracked them all with git, to be wiped out by the reset. To keep the changes from the previous commit to HEAD and move to the previous commit, do: If changes are not required from the previous commit to HEAD and just discard all changes, do: I believe some people may come to this question wanting to know how to rollback committed changes they've made in their master - ie throw everything away and go back to origin/master, in which case, do this: https://superuser.com/questions/273172/how-to-reset-master-to-origin-master Revert is the command to rollback the commits. Sample: git revert 2h3h23233 It is capable of taking range from the HEAD like below. Here 1 says "revert last commit." git revert HEAD~1..HEAD and then do git push Caution! This command can cause losing commit history, if user put the wrong commit mistakenly. Always have en extra backup of your git some   where else just in case if you do mistakes, than you are a bit safer.   :) I have had a similar issue and wanted to revert back to an earlier commit. In my case I was not interested to keep the newer commit, hence I used Hard. This is how I did it: This will revert on the local repository, and here after using git push -f will update the remote repository. Try resetting to the desired commit - git reset <COMMIT_ID> (to check COMMIT_ID use git log) This will reset all changed files to un-added state. Now you can checkout all un-added files by  git checkout . Check git log to verify your changes. UPDATE If you have one and only commit in your repo, try git update-ref -d HEAD As your commits are pushed remotely, you need to remove them. Let me assume your branch is develop and it is pushed over origin. You first need to remove develop from origin: Then you need to get develop to the status you want, let me assume the commit hash is EFGHIJK: Lastly, push develop again: For rollback (or to revert): Try the above two steps, and if you find this is what you want then git push. If you find something wrong, do: If you want to correct some error in the last commit a good alternative would be using git commit --amend command. If the last commit is not pointed by any reference, this will do the trick, as it create a commit with the same parent as the last commit. If there is no reference to the last commit, it will simply be discarded and this commit will be the last commit. This is a good way of correcting commits without reverting commits. However it has its own limitations. 
__label__javascript __label__substring __label__string-matching __label__string Usually I would expect a String.contains() method, but there doesn't seem to be one.  What is a reasonable way to check for this? ECMAScript 6  introduced String.prototype.includes:   const string = "foo"; const substring = "oo";  console.log(string.includes(substring));    includes doesn’t have Internet Explorer support, though. In ECMAScript 5 or older environments, use String.prototype.indexOf, which returns -1 when a substring cannot be found:   var string = "foo"; var substring = "oo";  console.log(string.indexOf(substring) !== -1);    There is a String.prototype.includes in ES6: Note that this does not work in Internet Explorer or some other old browsers with no or incomplete ES6 support. To make it work in old browsers, you may wish to use a transpiler like Babel, a shim library like es6-shim, or this polyfill from MDN: Another alternative is KMP (Knuth–Morris–Pratt). The KMP algorithm searches for a length-m substring in a length-n string in worst-case O(n+m) time, compared to a worst-case of O(n⋅m) for the naive algorithm, so using KMP may be reasonable if you care about worst-case time complexity. Here's a JavaScript implementation by Project Nayuki, taken from https://www.nayuki.io/res/knuth-morris-pratt-string-matching/kmp-string-matcher.js:   function kmpSearch(pattern, text) {   if (pattern.length == 0)     return 0; // Immediate match    // Compute longest suffix-prefix table   var lsp = [0]; // Base case   for (var i = 1; i < pattern.length; i++) {     var j = lsp[i - 1]; // Start by assuming we're extending the previous LSP     while (j > 0 && pattern.charAt(i) != pattern.charAt(j))       j = lsp[j - 1];     if (pattern.charAt(i) == pattern.charAt(j))       j++;     lsp.push(j);   }    // Walk through text string   var j = 0; // Number of chars matched in pattern   for (var i = 0; i < text.length; i++) {     while (j > 0 && text.charAt(i) != pattern.charAt(j))       j = lsp[j - 1]; // Fall back in the pattern     if (text.charAt(i) == pattern.charAt(j)) {       j++; // Next char matched, increment position       if (j == pattern.length)         return i - (j - 1);     }   }   return -1; // Not found }  console.log(kmpSearch('ays', 'haystack') != -1) // true console.log(kmpSearch('asdf', 'haystack') != -1) // false   
__label__git __label__git-branch __label__branch How do you delete untracked local files from your current working tree? Cleans the working tree by recursively removing files that are not under version control, starting from the current directory. Normally, only files unknown to Git are removed, but if the -x option is specified, ignored files are also removed. This can, for example, be useful to remove all build products. If any optional <path>... arguments are given, only those paths are affected. Step 1 is to show what will be deleted by using the -n option: Clean Step - beware: this will delete files: Note the case difference on the X for the two latter commands. If clean.requireForce is set to "true" (the default) in your configuration, one needs to specify -f otherwise nothing will actually happen. Again see the git-clean docs for more information. -f, --force If the Git configuration variable clean.requireForce is not set to false, git clean will refuse to run unless given -f, -n or -i. -x Don’t use the standard ignore rules read from .gitignore (per directory) and $GIT_DIR/info/exclude, but do still use the ignore rules given with -e options. This allows removing all untracked files, including build products. This can be used (possibly in conjunction with git reset) to create a pristine working directory to test a clean build. -X Remove only files ignored by Git. This may be useful to rebuild everything from scratch, but keep manually created files. -n, --dry-run Don’t actually remove anything, just show what would be done. -d Remove untracked directories in addition to untracked files. If an untracked directory is managed by a different Git repository, it is not removed by default. Use -f option twice if you really want to remove such a directory. Use git clean -f -d to make sure that directories are also removed. Don’t actually remove anything, just show what would be done. or  Remove untracked directories in addition to untracked files. If an untracked directory is managed by a different Git repository, it is not removed by default. Use the -f option twice if you really want to remove such a directory. You can then check if your files are really gone with git status. I am surprised nobody mentioned this before: That stands for interactive and you will get a quick overview of what is going to be deleted offering you the possibility to include/exclude the affected files. Overall, still faster than running the mandatory --dry-run before the real cleaning. You will have to toss in a -d if you also want to take care of empty folders. At the end, it makes  for a nice alias: That being said, the extra hand holding of interactive commands can be tiring for experienced users.  These days I just use the already mentioned git clean -fd  git-clean - Remove untracked files from the working tree To remove all untracked files, The simple way is to add all of them first and reset the repo as below If untracked directory is a git repository of its own (e.g. submodule), you need to use -f twice: git clean -d -f -f I like git stash push -u because you can undo them all with git stash pop. EDIT: Also I found a way to show untracked file in a stash (e.g. git show stash@{0}^3) https://stackoverflow.com/a/12681856/338986 EDIT2: git stash save is deprecated in favor of push. Thanks @script-wolf. This is what I always use: For a very large project you might want to run it a couple of times. git-clean is what you are looking for. It is used to remove untracked files from the working tree. If needed to remove untracked files from particular subdirectory, And combined way to delete untracked dir/files and ignored files.  after this you will have modified files only in git status. Remove all extra folders and files in this repo + submodules This gets you in same state as fresh clone. Remove all extra folders and files in this repo but not its submodules Remove extra folders but not files (ex. build or logs folder) Remove extra folders + ignored files (but not newly added files) If file wasn't ignored and not yet checked-in then  it stays. Note the capital X. New interactive mode git clean -fd removes directory git clean -fX removes ignored files git clean -fx removes ignored and un-ignored files can be used all above options in combination as  git clean -fdXx  check git manual for more help OK, deleting unwanted untracked files and folders are easy using git in command line, just do it like this: Double check before doing it as it will delete the files and folders without making any history... Also in this case, -f stands for force and -d stands for directory... So, if you want to delete files only, you can use -f only: If you want to delete(directories) and files, you can delete only untracked directories and files like this: Also, you can use -x flag for including the files which are ignored by git. This would be helpful if you want to delete everything. And adding -i flag, makes git asking you for permission for deleting files one by one on the go. If you not sure and want to check things first, add -n flag. Use -q if you don't want to see any report after successful deletion. I also create the image below to make it more memorable, especially I have seen many people confuse -f for cleaning folder sometimes or mix it up somehow!    A better way is to use: git clean This removes untracked files, including directories (-d) and files ignored by git (-x). Also, replace the -f argument with -n to perform a dry-run or -i for interactive mode and it will tell you what will be removed. User interactive approach:  -i for interactive -f for force -d for directory -x for ignored files(add if required) Note: Add -n or --dry-run to just check what it will do. A lifehack for such situation I just invented and tried (that works perfectly): Beware! Be sure to commit any needed changes (even in non-untracked files) before performing this. git clean -f -d -x $(git rev-parse --show-cdup) applies clean to the root directory, no matter where you call it within a repository directory tree. I use it all the time as it does not force you to leave the folder where you working now and allows to clean & commit right from the place where you are. Be sure that flags -f, -d, -x match your needs: There are other flags as well available, just check git clean --help. For me only following worked: In all other cases, I was getting message "Skipping Directory" for some subdirectories.  If you just want to delete the files listed as untracked by 'git status' I prefer this to 'git clean' because 'git clean' will delete files ignored by git, so your next build will have to rebuild everything and you may lose your IDE settings too. To know what will be deleted before actually deleting: git clean -d -n It will output something like: Would remove sample.txt To delete everything listed in the output of the previous command: git clean -d -f It will output something like: Removing sample.txt To remove Untracked files : To remove the untracked files you should first use command to view the files that will be affected by cleaning  This will show you the list of files that will be deleted. Now to actually delete those files use this command: Always use -n before running the actual command as it will show you what files would get removed.  By default, git clean will only remove untracked files that are not ignored. Any file that matches a pattern in your .gitignore or other ignore files will not be removed. If you want to remove those files too, you can add a -x to the clean command. There is also interactive mode available -i with the clean command If you are not 100% sure that deleting your uncommitted work is safe, you could use stashing instead It will also clear your directory but give you flexibility to retrieve the files at any point in time using stash with apply or pop. Then at later point you could clear your stash using: uggested Command for Removing Untracked Files from git docs is git clean  git clean - Remove untracked files from the working tree Suggested Method:  Interative Mode by using git clean -i so we can have control over it. let see remaining available options. Available Options: Explanation: 1. -d Remove untracked directories in addition to untracked files. If an untracked directory is managed by a different Git repository,    it is not removed by default. Use -f option twice if you really want to remove such a directory. 2. -f, --force If the Git configuration variable clean.requireForce is not set to false, git clean will refuse to run unless given -f, -n or    -i. 3. -i, --interactive Show what would be done and clean files interactively. See “Interactive mode” for details. 4. -n, --dry-run Don’t actually remove anything, just show what would be done. 5. -q, --quiet Be quiet, only report errors, but not the files that are successfully removed. 6. -e , --exclude= In addition to those found in .gitignore (per directory) and $GIT_DIR/info/exclude, also consider these patterns to be in the    set of the ignore rules in effect. 7. -x Don’t use the standard ignore rules read from .gitignore (per directory) and $GIT_DIR/info/exclude, but do still use the ignore    rules given with -e options. This allows removing all untracked files, including build products. This can be used (possibly in    conjunction with git reset) to create a pristine working directory to test a clean build. 8. -X Remove only files ignored by Git. This may be useful to rebuild everything from scratch, but keep manually created files. Normal git clean command doesn't remove untracked files with my git version 2.9.0.windows.1. git clean -f to remove untracked files from working directory. I have covered some basics here in my blog, git-intro-basic-commands We can easily removed local untracked files from the current git working tree by using below git comments. Example: Links : The following command will clean out   the current git repository and all its submodules recursively: will remove the untracked files from the current git  when you want to remove directories and files, this will delete only untracked directories and files oh-my-zsh with zsh provides those great aliases via the git plugin. They can be used in bash as well. gclean='git clean -fd' gpristine='git reset --hard && git clean -dfx'
__label__git __label__git-checkout __label__remote-branch Somebody pushed a branch called test with git push origin test to a shared repository. I can see the branch with git branch -r. Now I'm trying to check out the remote test branch. I've tried: git checkout test which does nothing git checkout origin/test gives * (no branch). Which is confusing. How can I be on "no branch"? How do I check out a remote Git branch? Jakub's answer actually improves on this. With Git versions ≥ 1.6.6, with only one remote, you can do: As user masukomi points out in a comment, git checkout test will NOT work in modern git if you have multiple remotes. In this case use or the shorthand Before you can start working locally on a remote branch, you need to fetch it as called out in answers below. To fetch a branch, you simply need to: This will fetch all of the remote branches for you. You can see the branches available for checkout with: With the remote branches in hand, you now  need to check out the branch you are interested in, giving you a local working copy: Sidenote: With modern Git (>= 1.6.6), you are able to use just (note that it is 'test' not 'origin/test') to perform magical DWIM-mery and create local branch 'test' for you, for which upstream would be remote-tracking branch 'origin/test'. The * (no branch) in git branch output means that you are on unnamed branch, in so called "detached HEAD" state (HEAD points directly to commit, and is not symbolic reference to some local branch).  If you made some commits on this unnamed branch, you can always create local branch off current commit: I found a comment buried below which seems to modernize this answer: @Dennis: git checkout <non-branch>, for example git checkout origin/test results in detached HEAD / unnamed branch, while git checkout test or git checkout -b test origin/test results in local branch test (with remote-tracking branch origin/test as upstream) – Jakub Narębski Jan 9 '14 at 8:17 emphasis on git checkout origin/test In this case, you probably want to create a local test branch which is tracking the remote test branch: In earlier versions of git, you needed an explicit --track option, but that is the default now when you are branching off a remote branch. While the first and selected answer is technically correct, there's the possibility you have not yet retrieved all objects and refs from the remote repository. If that is the case, you'll receive the following error: fatal: git checkout: updating paths is incompatible with switching branches.   Did you intend to checkout 'origin/remote_branch' which can not be resolved as commit? If you receive this message, you must first do a git fetch origin where origin is the name of the remote repository prior to running git checkout remote_branch. Here's a full example with responses: As you can see, running git fetch origin retrieved any remote branches we were not yet setup to track on our local machine. From there, since we now have a ref to the remote branch, we can simply run git checkout remote_branch and we'll gain the benefits of remote tracking. I tried the above solution, but it didn't work. Try this, it works: This will fetch the remote branch and create a new local branch (if not exists already) with name local_branch_name and track the remote one in it. This will DWIM for a remote not named origin (documentation): To add a new remote, you will need to do the following first: The first tells Git the remote exists, the second gets the commits. Use: Other answers do not work with modern Git in my benign case. You might need to pull first if the remote branch is new, but I haven't checked that case. OK, the answer is easy... You basically see the branch, but you don't have a local copy yet!... You need to fetch the branch...  You can simply fetch and then checkout to the branch, use the one line command below to do that: I also created the image below for you to share the differences, look at how fetch works and also how it's different to pull:  To clone a Git repository, do: The above command checks out all of the branches, but only the master branch will be initialized. If you want to checkout the other branches, do: This command checks out the remote branch, and your local branch name will be same as the remote branch. If you want to override your local branch name on checkout: Now your local branch name is enhancement, but your remote branch name is future_branch. You can try or First, you need to do: git fetch # If you don't know about branch name Second, you can check out remote branch into your local by: -b will create new branch in specified name from your selected remote branch. I use the following command: Commands are equal to and then Both will create a latest fixes_for_dev from development If the branch is on something other than the origin remote I like to do the following: This will checkout the next branch on the upstream remote in to a local branch called second/next. Which means if you already have a local branch named next it will not conflict. Simply run git checkout with the name of the remote branch. Git will automatically create a local branch that tracks the remote one: However, if that branch name is found in more than one remote, this won't work as Git doesn't know which to use. In that case you can use either: or In 2.19, Git learned the checkout.defaultRemote configuration, which specifies a remote to default to when resolving such an ambiguity. git fetch && git checkout your-branch-name none of these answers worked for me. this worked:  git checkout -b feature/branch remotes/origin/feature/branch I was stuck in a situation seeing error: pathspec 'desired-branch' did not match any file(s) known to git. for all of the suggestions above. I'm on git version 1.8.3.1.   So this worked for me:   The explanation behind is that I've noticed that when fetching the remote branch, it was fetched to FETCH_HEAD: git branch -r says the object name is invalid, because that branch name isn't in Git's local branch list. Update your local branch list from origin with: And then try checking out your remote branch again. This worked for me. I believe git fetch pulls in all remote branches, which is not what the original poster wanted. The git remote show <origin name> command will list all branches (including un-tracked branches). Then you can find the remote branch name that you need to fetch. Example: Use these steps to fetch remote branches: Example: Other guys and gals give the solutions, but maybe I can tell you why. git checkout test which does nothing Does nothing doesn't equal doesn't work, so I guess when you type 'git checkout test' in your terminal and press enter key, no message appears and no error occurs. Am I right? If the answer is 'yes', I can tell you the cause. The cause is that there is a file (or folder) named 'test' in your work tree. When git checkout xxx parsed, Fetch from the remote and checkout the branch. E.g.: git fetch origin && git checkout feature/XYZ-1234-Add-alerts  To get newly created branches  To switch into another branch git checkout -b "Branch_name" [ B means Create local branch] git branch --all git checkout -b "Your Branch name" git branch  successfully checkout from the master branch to dev branch   You can start tracking all remote branches with the following Bash script: Here is also a single-line version: to get all remote branches use this :  then checkout to the branch : If the remote branch name begins with special characteres you need to use single quotes around it in the checkout command, or else git won't know which branch you are talking about. For example, I tried to checkout a remote branch named as #9773 but the command didn't work properly, as shown in the picture below:  For some reason I wondered if the sharp symbol (#) could have something to do with it, and then I tried surrounding the branch name with single quotes, like '#9773' rathen than just #9773, and fortunately it worked fine. For us, it seems the remote.origin.fetch configuration gave a problem. Therefore, we could not see any other remote branches than master, so git fetch [--all] did not help. Neither git checkout mybranch nor git checkout -b mybranch --track origin/mybranch did work, although it certainly was at remote.  The previous configuration only allowed master to be fetched: Fix it by using * and fetch the new information from origin: Now we could git checkout the remote branch locally. No idea how this config ended up in our local repo. Use fetch to pull all your remote To list remote branches: For list all your branches To checkout/change a branch Please follow the command to create an empty folder. Enter that and use this command:
__label__javascript __label__syntax __label__function __label__idioms I've recently started maintaining someone else's JavaScript code. I'm fixing bugs, adding features and also trying to tidy up the code and make it more consistent. The previous developer used two ways of declaring functions and I can't work out if there is a reason behind it or not. The two ways are: What are the reasons for using these two different methods and what are the pros and cons of each? Is there anything that can be done with one method that can't be done with the other? The difference is that functionOne is a function expression and so only defined when that line is reached, whereas functionTwo is a function declaration and is defined as soon as its surrounding function or script is executed (due to hoisting).   For example, a function expression:   // TypeError: functionOne is not a function functionOne();  var functionOne = function() {   console.log("Hello!"); };    And, a function declaration:      // Outputs: "Hello!" functionTwo();  function functionTwo() {   console.log("Hello!"); }    Historically, function declarations defined within blocks were handled inconsistently between browsers. Strict mode (introduced in ES5) resolved this by scoping function declarations to their enclosing block.   'use strict';     { // note this block!   function functionThree() {     console.log("Hello!");   } } functionThree(); // ReferenceError    First I want to correct Greg: function abc(){} is scoped too — the name abc is defined in the scope where this definition is encountered. Example: Secondly, it is possible to combine both styles: xyz is going to be defined as usual, abc is undefined in all browsers but Internet Explorer — do not rely on it being defined. But it will be defined inside its body: If you want to alias functions on all browsers, use this kind of declaration: In this case, both xyz and abc are aliases of the same object: One compelling reason to use the combined style is the "name" attribute of function objects (not supported by Internet Explorer). Basically when you define a function like its name is automatically assigned. But when you define it like its name is empty — we created an anonymous function and assigned it to some variable. Another good reason to use the combined style is to use a short internal name to refer to itself, while providing a long non-conflicting name for external users: In the example above we can do the same with an external name, but it'll be too unwieldy (and slower). (Another way to refer to itself is to use arguments.callee, which is still relatively long, and not supported in the strict mode.) Deep down, JavaScript treats both statements differently. This is a function declaration: abc here is defined everywhere in the current scope: Also, it hoisted through a return statement: This is a function expression: xyz here is defined from the point of assignment: Function declaration vs. function expression is the real reason why there is a difference demonstrated by Greg. Fun fact: Personally, I prefer the "function expression" declaration because this way I can control the visibility. When I define the function like I know that I defined the function locally. When I define the function like I know that I defined it globally providing that I didn't define abc anywhere in the chain of scopes. This style of definition is resilient even when used inside eval(). While the definition depends on the context and may leave you guessing where it is actually defined, especially in the case of eval() — the answer is: It depends on the browser. Here's the rundown on the standard forms that create functions: (Originally written for another question, but adapted after being moved into the canonical question.) Terms: The quick list: Function Declaration "Anonymous" function Expression (which despite the term, sometimes create functions with names) Named function Expression Accessor Function Initializer (ES5+) Arrow Function Expression (ES2015+) (which, like anonymous function expressions, don't involve an explicit name, and yet can create functions with names) Method Declaration in Object Initializer (ES2015+) Constructor and Method Declarations in class (ES2015+) The first form is a function declaration, which looks like this: A function declaration is a declaration; it's not a statement or expression. As such, you don't follow it with a ; (although doing so is harmless). A function declaration is processed when execution enters the context in which it appears, before any step-by-step code is executed. The function it creates is given a proper name (x in the example above), and that name is put in the scope in which the declaration appears. Because it's processed before any step-by-step code in the same context, you can do things like this: Until ES2015, the spec didn't cover what a JavaScript engine should do if you put a function declaration inside a control structure like try, if, switch, while, etc., like this: And since they're processed before step-by-step code is run, it's tricky to know what to do when they're in a control structure. Although doing this wasn't specified until ES2015, it was an allowable extension to support function declarations in blocks. Unfortunately (and inevitably), different engines did different things. As of ES2015, the specification says what to do. In fact, it gives three separate things to do: The rules for the loose modes are tricky, but in strict mode, function declarations in blocks are easy: They're local to the block (they have block scope, which is also new in ES2015), and they're hoisted to the top of the block. So: The second common form is called an anonymous function expression: Like all expressions, it's evaluated when it's reached in the step-by-step execution of the code. In ES5, the function this creates has no name (it's anonymous). In ES2015, the function is assigned a name if possible by inferring it from context. In the example above, the name would be y. Something similar is done when the function is the value of a property initializer. (For details on when this happens and the rules, search for SetFunctionName in the the specification — it appears all over the place.) The third form is a named function expression ("NFE"): The function this creates has a proper name (w in this case). Like all expressions, this is evaluated when it's reached in the step-by-step execution of the code. The name of the function is not added to the scope in which the expression appears; the name is in scope within the function itself: Note that NFEs have frequently been a source of bugs for JavaScript implementations. IE8 and earlier, for instance, handle NFEs completely incorrectly, creating two different functions at two different times. Early versions of Safari had issues as well. The good news is that current versions of browsers (IE9 and up, current Safari) don't have those issues any more. (But as of this writing, sadly, IE8 remains in widespread use, and so using NFEs with code for the web in general is still problematic.) Sometimes functions can sneak in largely unnoticed; that's the case with accessor functions. Here's an example: Note that when I used the function, I didn't use ()! That's because it's an accessor function for a property. We get and set the property in the normal way, but behind the scenes, the function is called. You can also create accessor functions with Object.defineProperty, Object.defineProperties, and the lesser-known second argument to Object.create. ES2015 brings us the arrow function. Here's one example: See that n => n * 2 thing hiding in the map() call? That's a function. A couple of things about arrow functions: They don't have their own this. Instead, they close over the this of the context where they're defined. (They also close over arguments and, where relevant, super.) This means that the this within them is the same as the this where they're created, and cannot be changed. As you'll have noticed with the above, you don't use the keyword function; instead, you use =>. The n => n * 2 example above is one form of them. If you have multiple arguments to pass the function, you use parens: (Remember that Array#map passes the entry as the first argument, and the index as the second.) In both cases, the body of the function is just an expression; the function's return value will automatically be the result of that expression (you don't use an explicit return). If you're doing more than just a single expression, use {} and an explicit return (if you need to return a value), as normal: The version without { ... } is called an arrow function with an expression body or concise body. (Also: A concise arrow function.) The one with { ... } defining the body is an arrow function with a function body. (Also: A verbose arrow function.) ES2015 allows a shorter form of declaring a property that references a function called a method definition; it looks like this: the almost-equivalent in ES5 and earlier would be: the difference (other than verbosity) is that a method can use super, but a function cannot. So for instance, if you had an object that defined (say) valueOf using method syntax, it could use super.valueOf() to get the value Object.prototype.valueOf would have returned (before presumably doing something else with it), whereas the ES5 version would have to do Object.prototype.valueOf.call(this) instead. That also means that the method has a reference to the object it was defined on, so if that object is temporary (for instance, you're passing it into Object.assign as one of the source objects), method syntax could mean that the object is retained in memory when otherwise it could have been garbage collected (if the JavaScript engine doesn't detect that situation and handle it if none of the methods uses super). ES2015 brings us class syntax, including declared constructors and methods: There are two function declarations above: One for the constructor, which gets the name Person, and one for getFullName, which is a function assigned to Person.prototype. Speaking about the global context, both, the var statement and a FunctionDeclaration at the end will create a non-deleteable property on the global object, but the value of both can be overwritten. The subtle difference between the two ways is that when the Variable Instantiation process runs (before the actual code execution) all identifiers declared with var will be initialized with undefined, and the ones used by the FunctionDeclaration's will be available since that moment, for example: The assignment of the bar FunctionExpression takes place until runtime. A global property created by a FunctionDeclaration can be overwritten without any problems just like a variable value, e.g.: Another obvious difference between your two examples is that the first function doesn't have a name, but the second has it, which can be really useful when debugging (i.e. inspecting a call stack). About your edited first example (foo = function() { alert('hello!'); };), it is an undeclared assignment, I would highly encourage you to always use the var keyword. With an assignment, without the var statement, if the referenced identifier is not found in the scope chain, it will become a deleteable property of the global object. Also, undeclared assignments throw a ReferenceError on ECMAScript 5 under Strict Mode. A must read: Note: This answer has been merged from another question, in which the major doubt and misconception from the OP was that identifiers declared with a FunctionDeclaration, couldn't be overwritten which is not the case. The two code snippets you've posted there will, for almost all purposes, behave the same way. However, the difference in behaviour is that with the first variant (var functionOne = function() {}), that function can only be called after that point in the code. With the second variant (function functionTwo()), the function is available to code that runs above where the function is declared. This is because with the first variant, the function is assigned to the variable foo at run time. In the second, the function is assigned to that identifier, foo, at parse time. More technical information JavaScript has three ways of defining functions. A better explanation to Greg's answer Why no error? We were always taught that expressions are executed from top to bottom(??) Function declarations and variable declarations are always moved (hoisted) invisibly to the top of their containing scope by the JavaScript interpreter. Function parameters and language-defined names are, obviously, already there. ben cherry This means that code like this: Notice that the assignment portion of the declarations were not hoisted. Only the name is hoisted. But in the case with function declarations, the entire function body will be hoisted as well: Other commenters have already covered the semantic difference of the two variants above. I wanted to note a stylistic difference: Only the "assignment" variation can set a property of another object. I often build JavaScript modules with a pattern like this: With this pattern, your public functions will all use assignment, while your private functions use declaration. (Note also that assignment should require a semicolon after the statement, while declaration prohibits it.) An illustration of when to prefer the first method to the second one is when you need to avoid overriding a function's previous definitions. With , this definition of myfunction will override any previous definition, since it will be done at parse-time. While does the correct job of defining myfunction only when condition is met. An important reason is to add one and only one variable as the "Root" of your namespace... or There are many techniques for namespacing. It's become more important with the plethora of JavaScript modules available. Also see How do I declare a namespace in JavaScript? Hoisting is the JavaScript interpreter’s action of moving all variable and function declarations to the top of the current scope.  However, only the actual declarations are hoisted. by leaving assignments where they are. Variable Javascript is called loosely typed language. Which means Javascript variables can hold value of any Data-Type. Javascript automatically takes care of changing the variable-type based on the value/literal provided during runtime. Function Default return value of function is 'undefined', Variable declaration default value also 'undefined' Function Declaration Function Expression Function assigned to variable Example: javascript interpreted as You can check function declaration, expression test over different browser's using jsperf Test Runner ES5 Constructor Function Classes: Function objects created using Function.prototype.bind JavaScript treats functions as first-class objects, so being an object, you can assign properties to a function. ES6 introduced Arrow function: An arrow function expression has a shorter syntax, they are best suited for non-method functions, and they cannot be used as constructors. ArrowFunction : ArrowParameters => ConciseBody. I'm adding my own answer just because everyone else has covered the hoisting part thoroughly. I've wondered about which way is better for a long while now, and thanks to http://jsperf.com now I know :)  Function declarations are faster, and that's what really matters in web dev right? ;) A function declaration and a function expression assigned to a variable behave the same once the binding is established. There is a difference however at how and when the function object is actually associated with its variable. This difference is due to the mechanism called variable hoisting in JavaScript. Basically, all function declarations and variable declarations are hoisted to the top of the function in which the declaration occurs (this is why we say that JavaScript has function scope). When a function declaration is hoisted, the function body "follows" so when the function body is evaluated, the variable will immediately be bound to a function object. When a variable declaration is hoisted, the initialization does not follow, but is "left behind". The variable is initialized to undefined at the start of the function body, and will be assigned a value at its original location in the code. (Actually, it will be assigned a value at every location where a declaration of a variable with the same name occurs.) The order of hoisting is also important: function declarations take precedence over variable declarations with the same name, and the last function declaration takes precedence over previous function declarations with the same name. Some examples... Variable foo is hoisted to the top of the function, initialized to undefined, so that !foo is true, so foo is assigned 10. The foo outside of bar's scope plays no role and is untouched.  Function declarations take precedence over variable declarations, and the last function declaration "sticks". In this example a is initialized with the function object resulting from evaluating the second function declaration, and then is assigned 4. Here the function declaration is hoisted first, declaring and initializing variable a. Next, this variable is assigned 10. In other words: the assignment does not assign to outer variable a. The first example is a function declaration: The second example is a function expression: The main difference is how they are hoisted (lifted and declared). In the first example, the whole function declaration is hoisted. In the second example only the var 'abc' is hoisted, its value (the function) will be undefined, and the function itself remains at the position that it is declared. To put it simply: To study more about this topic I strongly recommend you this link In terms of code maintenance cost, named functions are more preferable: I suspect more PROS for named functions are follow. And what is listed as an advantage of named functions is a disadvantage for anonymous ones. Historically, anonymous functions appeared from the inability of JavaScript as a language to list members with named functions: The following works because function add() is scoped to the nearest block:   try {   console.log("Success: ", add(1, 1)); } catch(e) {   console.log("ERROR: " + e); }  function add(a, b){   return a + b; }    The following does not work because the variable is called before a function value is assigned to the variable add.   try {   console.log("Success: ", add(1, 1)); } catch(e) {   console.log("ERROR: " + e); }  var add=function(a, b){   return a + b; }    The above code is identical in functionality to the code below. Note that explicitly assigning add = undefined is superfluous because simply doing var add; is the exact same as var add=undefined.   var add = undefined;  try {   console.log("Success: ", add(1, 1)); } catch(e) {   console.log("ERROR: " + e); }  add = function(a, b){   return a + b; }    The following does not work because var add= begins an expression and causes the following function add() to be an expression instead of a block. Named functions are only visible to themselves and their surrounding block. As function add() is an expression here, it has no surrounding block, so it is only visible to itself.   try {   console.log("Success: ", add(1, 1)); } catch(e) {   console.log("ERROR: " + e); }  var add=function add(a, b){   return a + b; }    The name of a function function thefuncname(){} is thefuncname when it is declared this way.   function foobar(a, b){}  console.log(foobar.name);      var a = function foobar(){};  console.log(a.name);    Otherwise, if a function is declared as function(){}, the function.name is the first variable used to store the function.   var a = function(){}; var b = (function(){ return function(){} });  console.log(a.name); console.log(b.name);    If there are no variables set to the function, then the functions name is the empty string ("").   console.log((function(){}).name === "");    Lastly, while the variable the function is assigned to initially sets the name, successive variables set to the function do not change the name.   var a = function(){}; var b = a; var c = b;  console.log(a.name); console.log(b.name); console.log(c.name);    In Google's V8 and Firefox's Spidermonkey there might be a few microsecond JIST compilation difference, but ultimately the result is the exact same. To prove this, let's examine the efficiency of JSPerf at microbenchmarks by comparing the speed of two blank code snippets. The JSPerf tests are found here. And, the jsben.ch testsare  found here. As you can see, there is a noticable difference when there should be none. If you are really a performance freak like me, then it might be more worth your while trying to reduce the number of variables and functions in the scope and especially eliminating polymorphism (such as using the same variable to store two different types). When you use the var keyword to declare a variable, you can then reassign a different value to the variable like so.   (function(){     "use strict";     var foobar = function(){}; // initial value     try {         foobar = "Hello World!"; // new value         console.log("[no error]");     } catch(error) {         console.log("ERROR: " + error.message);     }     console.log(foobar, window.foobar); })();    However, when we use the const-statement, the variable reference becomes immutable. This means that we cannot assign a new value to the variable. Please note, however, that this does not make the contents of the variable immutable: if you do const arr = [], then you can still do arr[10] = "example". Only doing something like arr = "new value" or arr = [] would throw an error as seen below.   (function(){     "use strict";     const foobar = function(){}; // initial value     try {         foobar = "Hello World!"; // new value         console.log("[no error]");     } catch(error) {         console.log("ERROR: " + error.message);     }     console.log(foobar, window.foobar); })();    Interestingly, if we declare the variable as function funcName(){}, then the immutability of the variable is the same as declaring it with var.   (function(){     "use strict";     function foobar(){}; // initial value     try {         foobar = "Hello World!"; // new value         console.log("[no error]");     } catch(error) {         console.log("ERROR: " + error.message);     }     console.log(foobar, window.foobar); })();    The "nearest block" is the nearest "function," (including asynchronous functions, generator functions, and asynchronous generator functions). However, interestingly, a function functionName() {} behaves like a var functionName = function() {} when in a non-closure block to items outside said closure. Observe.   try {   // typeof will simply return "undefined" if the variable does not exist   if (typeof add !== "undefined") {     add(1, 1); // just to prove it     console.log("Not a block");   }else if(add===undefined){ // this throws an exception if add doesn't exist     console.log('Behaves like var add=function(a,b){return a+b}');   } } catch(e) {   console.log("Is a block"); } var add=function(a, b){return a + b}      try {   // typeof will simply return "undefined" if the variable does not exist   if (typeof add !== "undefined") {     add(1, 1); // just to prove it     console.log("Not a block");   }else if(add===undefined){ // this throws an exception if add doesn't exist     console.log('Behaves like var add=function(a,b){return a+b}')   } } catch(e) {   console.log("Is a block"); } function add(a, b){   return a + b; }      try {   // typeof will simply return "undefined" if the variable does not exist   if (typeof add !== "undefined") {     add(1, 1); // just to prove it     console.log("Not a block");   }else if(add===undefined){ // this throws an exception if add doesn't exist     console.log('Behaves like var add=function(a,b){return a+b}')   } } catch(e) {   console.log("Is a block"); } (function () {     function add(a, b){       return a + b;     } })();      try {   // typeof will simply return "undefined" if the variable does not exist   if (typeof add !== "undefined") {     add(1, 1); // just to prove it     console.log("Not a block");   }else if(add===undefined){ // this throws an exception if add doesn't exist     console.log('Behaves like var add=function(a,b){return a+b}')   } } catch(e) {   console.log("Is a block"); } {     function add(a, b){       return a + b;     } }      try {   // typeof will simply return "undefined" if the variable does not exist   if (typeof add !== "undefined") {     add(1, 1); // just to prove it     console.log("Not a block");   }else if(add===undefined){ // this throws an exception if add doesn't exist     console.log('Behaves like var add=function(a,b){return a+b}')   } } catch(e) {   console.log("Is a block"); } (() => {     var add=function(a, b){       return a + b;     } })();      try {   // typeof will simply return "undefined" if the variable does not exist   if (typeof add !== "undefined") {     add(1, 1); // just to prove it     console.log("Not a block");   }else if(add===undefined){ // this throws an exception if add doesn't exist     console.log('Behaves like var add=function(a,b){return a+b}')   } } catch(e) {   console.log("Is a block"); } (() => {     function add(a, b){       return a + b;     } })();    In computer science terms, we talk about anonymous functions and named functions. I think the most important difference is that an anonymous function is not bound to an name, hence the name anonymous function. In JavaScript it is a first class object dynamically declared at runtime. For more information on anonymous functions and lambda calculus, Wikipedia is a good start (http://en.wikipedia.org/wiki/Anonymous_function). I use the variable approach in my code for a very specific reason, the theory of which has been covered in an abstract way above, but an example might help some people like me, with limited JavaScript expertise. I have code that I need to run with 160 independently-designed brandings. Most of the code is in shared files, but branding-specific stuff is in a separate file, one for each branding. Some brandings require specific functions, and some do not. Sometimes I have to add new functions to do new branding-specific things. I am happy to change the shared coded, but I don't want to have to change all 160 sets of branding files. By using the variable syntax, I can declare the variable (a function pointer essentially) in the shared code and either assign a trivial stub function, or set to null. The one or two brandings that need a specific implementation of the function can then define their version of the function and assign this to the variable if they want, and the rest do nothing. I can test for a null function before I execute it in the shared code. From people's comments above, I gather it may be possible to redefine a static function too, but I think the variable solution is nice and clear. Greg's Answer is good enough, but I still would like to add something to it that I learned just now watching Douglas Crockford's videos. Function expression: Function statement: The function statement is just a shorthand for var statement with a function value. So expands to Which expands further to: And they are both hoisted to the top of the code.  @EugeneLazutkin gives an example where he names an assigned function to be able to use shortcut() as an internal reference to itself. John Resig gives another example - copying a recursive function assigned to another object in his Learning Advanced Javascript tutorial. While assigning functions to properties isn't strictly the question here, I recommend actively trying the tutorial out - run the code by clicking the button in the upper right corner, and double click the code to edit to your liking. Examples from the tutorial: recursive calls in yell(): Tests fail when the original ninja object is removed. (page 13) If you name the function that will be called recursively, the tests will pass. (page 14) Another difference that is not mentioned in the other answers is that if you use the anonymous function and use that as a constructor as in then one.constructor.name will not be defined. Function.name is non-standard but is supported by Firefox, Chrome, other Webkit-derived browsers and IE 9+. With  it is possible to retrieve the name of the constructor as a string with two.constructor.name. The first one (function doSomething(x)) should be part of an object notation. The second one (var doSomething = function(x){ alert(x);}) is simply creating an anonymous function and assigning it to a variable, doSomething. So doSomething() will call the function. You may want to know what a function declaration and function expression is. A function declaration defines a named function variable without requiring variable assignment. Function declarations occur as standalone constructs and cannot be nested within non-function blocks. ECMA 5 (13.0) defines the syntax as    function Identifier ( FormalParameterListopt ) { FunctionBody } In above condition the function name is visible within its scope and the scope of its parent (otherwise it would be unreachable). And in a function expression A function expression defines a function as a part of a larger expression syntax (typically a variable assignment ). Functions defined via functions expressions can be named or anonymous. Function expressions should not start with “function”. ECMA 5 (13.0) defines the syntax as    function Identifieropt ( FormalParameterListopt ) { FunctionBody } I'm listing out the differences below: A function declaration can be placed anywhere in the code. Even if it is invoked before the definition appears in code, it gets executed as function declaration is committed to memory or in a way it is hoisted up, before any other code in the page starts execution. Take a look at the function below: This is because, during execution, it looks like:- A function expression, if not defined before calling it, will result in an error. Also, here the function definition itself is not moved to the top or committed to memory like in the function declarations. But the variable to which we assign the function gets hoisted up and undefined gets assigned to it. Same function using function expressions: This is because during execution, it looks like: It is not safe to write function declarations in non-function blocks like if because they won't be accessible. Named function expression like the one below, may not work in Internet Explorer browsers prior to version 9. If you would use those functions to create objects, you would get: In light of the "named functions show up in stack traces" argument, modern JavaScript engines are actually quite capable of representing anonymous functions. As of this writing, V8, SpiderMonkey, Chakra and Nitro always refer to named functions by their names. They almost always refer to an anonymous function by its identifier if it has one. SpiderMonkey can figure out the name of an anonymous function returned from another function. The rest can't. If you really, really wanted your iterator and success callbacks to show up in the trace, you could name those too... But for the most part it's not worth stressing over. In JavaScript there are two ways to create functions: Function declaration: This is very basic, self-explanatory, used in many languages and standard across C family of languages. We declared a function defined it and executed it by calling it. What you should be knowing is that functions are actually objects in JavaScript; internally we have created an object for above function and given it a name called fn or the reference to the object is stored in fn. Functions are objects in JavaScript; an instance of function is actually an object instance. Function expression: JavaScript has first-class functions, that is, create a function and assign it to a variable just like you create a string or number and assign it to a variable. Here, the fn variable is assigned to a function. The reason for this concept is functions are objects in JavaScript; fn is pointing to the object instance of the above function. We have initialized a function and assigned it to a variable. It's not executing the function and assigning the result. Reference: JavaScript function declaration syntax: var fn = function() {} vs function fn() {} Both are different ways of defining a function. The difference is how the browser interprets and loads them into an execution context.  The first case is of function expressions which loads only when the interpreter reaches that line of code. So if you do it like the following, you will get an error that the functionOne is not a function. The reason is that on the first line no value is assigned to functionOne, and hence it is undefined. We are trying to call it as a function, and hence we are getting an error. On the second line we are assigning the reference of an anonymous function to functionOne. The second case is of function declarations that loads before any code is executed. So if you do like the following you won't get any error as the declaration loads before code execution. About performance: New versions of V8 introduced several under-the-hood optimizations and so did SpiderMonkey. There is almost no difference now between expression and declaration. Function expression appears to be faster now. Chrome 62.0.3202  FireFox 55  Chrome Canary 63.0.3225   Anonymous function expressions appear to have better performance   against Named function expression.  Firefox  Chrome Canary  Chrome  They are pretty similar with some small differences, first one is a variable which assigned to an anonymous function (Function Declaration) and second one is the normal way to create a function in JavaScript(Anonymous function Declaration), both has usage, cons and pros: 1. Function Expression A Function Expression defines a function as a part of a larger   expression syntax (typically a variable assignment ). Functions   defined via Functions Expressions can be named or anonymous. Function   Expressions must not start with “function” (hence the parentheses   around the self invoking example below). Assign a variable to a function, means no Hoisting, as we know functions in JavaScript can Hoist, means they can be called before they get declared, while variables need to be declared before getting access to them, so means in this case we can not access the function before where it's declared, also it could be a way that you write your functions, for the functions which return another function, this kind of declaration could make sense, also in ECMA6 & above you can assign this to an arrow function which can be used to call anonymous functions, also this way of declaring is a better way to create Constructor functions in JavaScript. 2. Function Declaration A Function Declaration defines a named function variable without   requiring variable assignment. Function Declarations occur as   standalone constructs and cannot be nested within non-function blocks.   It’s helpful to think of them as siblings of Variable Declarations.   Just as Variable Declarations must start with “var”, Function   Declarations must begin with “function”. This is the normal way of calling a function in JavaScript, this function can be called before you even declare it as in JavaScript all functions get Hoisted, but if you have 'use strict' this won't Hoist as expected, it's a good way to call all normal functions which are not big in lines and neither are a  constructor function. Also, if you need more info about how hoisting works in JavaScript, visit the link below: https://developer.mozilla.org/en-US/docs/Glossary/Hoisting This is just two possible ways of declaring functions, and in the second way, you can use the function before declaration. new Function() can be used to pass the function's body in a string. And hence this can be used to create dynamic functions. Also passing the script without executing the script.
__label__java __label__timezone __label__date If I run the following program, which parses two date strings referencing times 1 second apart and compares them: The output is: Why is ld4-ld3, not 1 (as I would expect from the one-second difference in the times), but 353? If I change the dates to times 1 second later: Then ld4-ld3 will be 1. Java version: It's a time zone change on December 31st in Shanghai. See this page for details of 1927 in Shanghai. Basically at midnight at the end of 1927, the clocks went back 5 minutes and 52 seconds. So "1927-12-31 23:54:08" actually happened twice, and it looks like Java is parsing it as the later possible instant for that local date/time - hence the difference. Just another episode in the often weird and wonderful world of time zones. EDIT: Stop press! History changes... The original question would no longer demonstrate quite the same behaviour, if rebuilt with version 2013a of TZDB. In 2013a, the result would be 358 seconds, with a transition time of 23:54:03 instead of 23:54:08. I only noticed this because I'm collecting questions like this in Noda Time, in the form of unit tests... The test has now been changed, but it just goes to show - not even historical data is safe. EDIT: History has changed again... In TZDB 2014f, the time of the change has moved to 1900-12-31, and it's now a mere 343 second change (so the time between t and t+1 is 344 seconds, if you see what I mean). EDIT: To answer a question around a transition at 1900... it looks like the Java timezone implementation treats all time zones as simply being in their standard time for any instant before the start of 1900 UTC: The code above produces no output on my Windows machine. So any time zone which has any offset other than its standard one at the start of 1900 will count that as a transition. TZDB itself has some data going back earlier than that, and doesn't rely on any idea of a "fixed" standard time (which is what getRawOffset assumes to be a valid concept) so other libraries needn't introduce this artificial transition. You've encountered a local time discontinuity: When local standard time was about to reach Sunday, 1. January 1928,   00:00:00 clocks were turned backward 0:05:52 hours to Saturday, 31.   December 1927, 23:54:08 local standard time instead This is not particularly strange and has happened pretty much everywhere at one time or another as timezones were switched or changed due to political or administrative actions. The moral of this strangeness is: When incrementing time you should convert back to UTC and then add or subtract. Use the local time only for display. This way you will be able to walk through any periods where hours or minutes happen twice. If you converted to UTC, add each second, and convert to local time for display. You would go through 11:54:08 p.m. LMT - 11:59:59 p.m. LMT and then 11:54:08 p.m. CST - 11:59:59 p.m. CST. Instead of converting each date, you can use the following code:  And then see that the result is: I'm sorry to say, but the time discontinuity has moved a bit in JDK 6 two years ago, and in JDK 7 just recently in update 25. Lesson to learn: avoid non-UTC times at all costs, except maybe for display. As explained by others, there's a time discontinuity there. There are two possible timezone offsets for 1927-12-31 23:54:08 at Asia/Shanghai, but only one offset for 1927-12-31 23:54:07. So, depending on which offset is used, there's either a one second difference or a 5 minutes and 53 seconds difference. This slight shift of offsets, instead of the usual one-hour daylight savings (summer time) we are used to, obscures the problem a bit. Note that the 2013a update of the timezone database moved this discontinuity a few seconds earlier, but the effect would still be observable. The new java.time package on Java 8 let use see this more clearly, and provide tools to handle it. Given: Then durationAtEarlierOffset will be one second, while durationAtLaterOffset will be five minutes and 53 seconds. Also, these two offsets are the same: But these two are different: You can see the same problem comparing 1927-12-31 23:59:59 with 1928-01-01 00:00:00, though, in this case, it is the earlier offset that produces the longer divergence, and it is the earlier date that has two possible offsets. Another way to approach this is to check whether there's a transition going on. We can do this like this: You can check whether the transition is an overlap where there's more than one valid offset for that date/time or a gap where that date/time is not valid for that zone id - by using the isOverlap() and isGap() methods on zot4. I hope this helps people handle this sort of issue once Java 8 becomes widely available, or to those using Java 7 who adopt the JSR 310 backport. IMHO the pervasive, implicit localization in Java is its single largest design flaw. It may be intended for user interfaces, but frankly, who really uses Java for user interfaces today except for some IDEs where you can basically ignore localization because programmers aren't exactly the target audience for it. You can fix it (especially on Linux servers) by: To the Java Community Process members I recommend: I mean, come on, aren't global static variables an anti-OO pattern? Nothing else is those pervasive defaults given by some rudimentary environment variables....... As others said, it's a time change in 1927 in Shanghai. It was 23:54:07 in Shanghai, in the local standard time, but then after 5 minutes and 52 seconds, it turned to the next day at 00:00:00, and then local standard time changed back to 23:54:08. So, that's why the difference between the two times is 343 seconds, not 1 second, as you would have expected. The time can also mess up in other places like the US. The US has Daylight Saving Time. When the Daylight Saving Time starts the time goes forward 1 hour. But after a while, the Daylight Saving Time ends, and it goes backward 1 hour back to the standard time zone. So sometimes when comparing times in the US the difference is about 3600 seconds not 1 second. But there is something different about these two-time changes. The latter changes continuously and the former was just a change. It didn't change back or change again by the same amount. It's better to use UTC unless if needed to use non-UTC time like in display. This happens because that's the timezone rules for the year 31st DEC 1927 in Shanghai. If you go to this page and choose "Time zone changes for 1900 - 1924", you'll see that in 1900 the date and time are "UTC +8:05:43 hours all of the period". So Java is just showing the time configured for that timezone, at that year. if you change your default timezone to Hong Kong it will show correct results. Note that the timezone changed from CST (China Standard Time, the "3-letter equivalent" to Asia/Shanghai) to HKT (the 3-letter name for Hong Kong's timezone). But changing time zone is not good solution. So instead use system UTC time whenever possible. It will always give local time after conversion. To avoid that issue, when incrementing time you should convert back to UTC and then add or subtract. This way you will be able to walk through any periods where hours or minutes happen twice. If you converted to UTC, add each second, and convert to local time for display. You would go through 11:54:08 p.m. LMT - 11:59:59 p.m. LMT and then 11:54:08 p.m. CST - 11:59:59 p.m. CST.
__label__methods __label__pass-by-value __label__java __label__pass-by-reference __label__parameter-passing I always thought Java uses pass-by-reference. However, I've seen a couple of blog posts (for example, this blog) that claim that it isn't (the blog post says that Java uses pass-by-value). I don't think I understand the distinction they're making. What is the explanation? Java is always pass-by-value. Unfortunately, we never handle an object at all, instead juggling object-handles called references (which are passed by value of course).  The chosen terminology and semantics easily confuse many beginners. It goes like this: In the example above aDog.getName() will still return "Max". The value aDog within main is not changed in the function foo with the Dog "Fifi" as the object reference is passed by value. If it were passed by reference, then the aDog.getName() in main would return "Fifi" after the call to foo. Likewise: In the above example, Fifi is the dog's name after call to foo(aDog) because the object's name was set inside of foo(...). Any operations that foo performs on d are such that, for all practical purposes, they are performed on aDog, but it is not possible to change the value of the variable aDog itself. I just noticed you referenced my article. The Java Spec says that everything in Java is pass-by-value. There is no such thing as "pass-by-reference" in Java. The key to understanding this is that something like is not a Dog; it's actually a pointer to a Dog. What that means, is when you have you're essentially passing the address of the created Dog object to the foo method. (I say essentially because Java pointers aren't direct addresses, but it's easiest to think of them that way) Suppose the Dog object resides at memory address 42. This means we pass 42 to the method. if the Method were defined as let's look at what's happening. Now let's think about what happens outside the method: Did myDog change? There's the key. Keeping in mind that myDog is a pointer, and not an actual Dog, the answer is NO. myDog still has the value 42; it's still pointing to the original Dog (but note that because of line "AAA", its name is now "Max" - still the same Dog; myDog's value has not changed.) It's perfectly valid to follow an address and change what's at the end of it; that does not change the variable, however. Java works exactly like C. You can assign a pointer, pass the pointer to a method, follow the pointer in the method and change the data that was pointed to. However, you cannot change where that pointer points. In C++, Ada, Pascal and other languages that support pass-by-reference, you can actually change the variable that was passed. If Java had pass-by-reference semantics, the foo method we defined above would have changed where myDog was pointing when it assigned someDog on line BBB. Think of reference parameters as being aliases for the variable passed in. When that alias is assigned, so is the variable that was passed in. Java always passes arguments by value, NOT by reference. Let me explain this through an example: I will explain this in steps: Declaring a reference named f of type Foo and assign it a new object of type Foo with an attribute "f".  From the method side, a reference of type Foo with a name a is declared and it's initially assigned null.  As you call the method changeReference, the reference a will be assigned the object which is passed as an argument.  Declaring a reference named b of type Foo and assign it a new object of type Foo with an attribute "b".  a = b makes a new assignment to the reference a, not f, of the object whose attribute is "b".  As you call modifyReference(Foo c) method, a reference c is created and assigned the object with attribute "f".  c.setAttribute("c"); will change the attribute of the object that reference c points to it, and it's the same object that reference f points to it.  I hope you understand now how passing objects as arguments works in Java :) This will give you some insights of how Java really works to the point that in your next discussion about Java passing by reference or passing by value you'll just smile :-) Step one please erase from your mind that word that starts with 'p' "_ _ _ _ _ _ _", especially if you come from other programming languages. Java and 'p' cannot be written in the same book, forum, or even txt. Step two remember that when you pass an Object into a method you're passing the Object reference and not the Object itself. Now think of what an Object's reference/variable does/is: In the following (please don't try to compile/execute this...): What happens? A picture is worth a thousand words:  Note that the anotherReferenceToTheSamePersonObject arrows is directed towards the Object and not towards the variable person! If you didn't get it then just trust me and remember that it's better to say that Java is pass by value. Well, pass by reference value. Oh well, even better is pass-by-copy-of-the-variable-value! ;) Now feel free to hate me but note that given this there is no difference between passing primitive data types and Objects when talking about method arguments. You always pass a copy of the bits of the value of the reference! Java is pass-by-value because inside a method you can modify the referenced Object as much as you want but no matter how hard you try you'll never be able to modify the passed variable that will keep referencing (not p _ _ _ _ _ _ _) the same Object no matter what! The changeName function above will never be able to modify the actual content (the bit values) of the passed reference. In other word changeName cannot make Person person refer to another Object. Of course you can cut it short and just say that  Java is pass-by-value! Java is always pass by value, with no exceptions, ever. So how is it that anyone can be at all confused by this, and believe that Java is pass by reference, or think they have an example of Java acting as pass by reference? The key point is that Java never provides direct access to the values of objects themselves, in any circumstances. The only access to objects is through a reference to that object. Because Java objects are always accessed through a reference, rather than directly, it is common to talk about fields and variables and method arguments as being objects, when pedantically they are only references to objects. The confusion stems from this (strictly speaking, incorrect) change in nomenclature. So, when calling a method So if you have doSomething(foo) and public void doSomething(Foo foo) { .. } the two Foos have copied references that point to the same objects. Naturally, passing by value a reference to an object looks very much like (and is indistinguishable in practice from) passing an object by reference. Java passes references by value. So you can't change the reference that gets passed in. I feel like arguing about "pass-by-reference vs pass-by-value" is not super-helpful. If you say, "Java is pass-by-whatever (reference/value)", in either case, you're not provide a complete answer. Here's some additional information that will hopefully aid in understanding what's happening in memory. Crash course on stack/heap before we get to the Java implementation: Values go on and off the stack in a nice orderly fashion, like a stack of plates at a cafeteria. Memory in the heap (also known as dynamic memory) is haphazard and disorganized. The JVM just finds space wherever it can, and frees it up as the variables that use it are no longer needed. Okay. First off, local primitives go on the stack. So this code: results in this:  When you declare and instantiate an object. The actual object goes on the heap. What goes on the stack? The address of the object on the heap. C++ programmers would call this a pointer, but some Java developers are against the word "pointer". Whatever. Just know that the address of the object goes on the stack. Like so:  An array is an object, so it goes on the heap as well. And what about the objects in the array? They get their own heap space, and the address of each object goes inside the array.  So, what gets passed in when you call a method? If you pass in an object, what you're actually passing in is the address of the object. Some might say the "value" of the address, and some say it's just a reference to the object. This is the genesis of the holy war between "reference" and "value" proponents. What you call it isn't as important as that you understand that what's getting passed in is the address to the object. One String gets created and space for it is allocated in the heap, and the address to the string is stored on the stack and given the identifier hisName, since the address of the second String is the same as the first, no new String is created and no new heap space is allocated, but a new identifier is created on the stack. Then we call shout(): a new stack frame is created and a new identifier, name is created and assigned the address of the already-existing String.  So, value, reference? You say "potato". Just to show the contrast, compare the following C++ and Java snippets: In C++: Note: Bad code - memory leaks!  But it demonstrates the point. In Java,  Java only has the two types of passing: by value for built-in types, and by value of the pointer for object types. Java passes references to objects by value. Basically, reassigning Object parameters doesn't affect the argument, e.g., will print out "Hah!" instead of null. The reason this works is because bar is a copy of the value of baz, which is just a reference to "Hah!". If it were the actual reference itself, then foo would have redefined baz to null. I can't believe that nobody mentioned Barbara Liskov yet. When she designed CLU in 1974, she ran into this same terminology problem, and she invented the term call by sharing (also known as call by object-sharing and call by object) for this specific case of "call by value where the value is a reference". The crux of the matter is that the word reference in the expression "pass by reference" means something completely different from the usual meaning of the word reference in Java.  Usually in Java reference means a a reference to an object. But the technical terms pass by reference/value from programming language theory is talking about a reference to the memory cell holding the variable, which is something completely different. In java everything is reference, so when you have something like:     Point pnt1 = new Point(0,0); Java does following:  Java doesn't pass method arguments by reference; it passes them by value. I will use example from this site:  Flow of the program: Creating two different Point object with two different reference associated.  As expected output will be: On this line 'pass-by-value' goes into the play...  References pnt1 and pnt2 are passed by value to the tricky method, which means that now yours references pnt1 and pnt2 have their copies named arg1 and arg2.So pnt1 and arg1 points to the same object. (Same for the pnt2 and arg2)  In the tricky method:  Next in the tricky method Here, you first create new temp Point reference which will point on same place like arg1 reference. Then you move reference arg1 to point to the same place like arg2 reference. Finally arg2 will point to the same place like temp.  From here scope of tricky method is gone and you don't have access any more to the references: arg1, arg2, temp. But important note is that everything you do with these references when they are 'in life' will permanently affect object on which they are point to.  So after executing method tricky, when you return to main, you have this situation:  So now, completely execution of program will be: Java is always pass by value, not pass by reference First of all, we need to understand what pass by value and pass by reference are. Pass by value means that you are making a copy in memory of the actual parameter's value that is passed in. This is a copy of the contents of the actual parameter. Pass by reference (also called pass by address) means that a copy of the address of the actual parameter is stored. Sometimes Java can give the illusion of pass by reference. Let's see how it works by using the example below: The output of this program is: Let's understand step by step: As we all know it will create an object in the heap and return the reference value back to t. For example, suppose the value of t is 0x100234 (we don't know the actual JVM internal value, this is just an example) .  When passing reference t to the function it will not directly pass the actual reference value of object test,  but it will create a copy of t and then pass it to the function. Since it is passing by value, it passes a copy of the variable rather than the actual reference of it. Since we said the value of t was 0x100234, both t and f will have the same value and hence they will point to the same object.  If you change anything in the function using reference f it will modify the existing contents of the object. That is why we got the output changevalue,   which is updated in the function. To understand this more clearly, consider the following example: Will this throw a NullPointerException? No, because it only passes a copy of the reference. In the case of passing by reference, it could have thrown a NullPointerException, as seen below:  Hopefully this will help. There are already great answers that cover this. I wanted to make a small contribution by sharing a very simple example (which will compile) contrasting the behaviors between Pass-by-reference in c++ and Pass-by-value in Java. A few points: C++ pass by reference example: Java pass "a Java reference" by value example EDIT Several people have written comments which seem to indicate that either they are not looking at my examples or they don't get the c++ example. Not sure where the disconnect is, but guessing the c++ example is not clear. I'm posting the same example in pascal because I think pass-by-reference looks cleaner in pascal, but I could be wrong. I might just be confusing people more; I hope not. In pascal, parameters passed-by-reference are called "var parameters". In the procedure setToNil below, please note the keyword 'var' which precedes the parameter 'ptr'. When a pointer is passed to this procedure, it will be passed by reference. Note the behavior: when this procedure sets ptr to nil (that's pascal speak for NULL), it will set the argument to nil--you can't do that in Java. EDIT 2 Some excerpts from "THE Java Programming Language" by Ken Arnold, James Gosling (the guy who invented Java), and David Holmes, chapter 2, section 2.6.5 All parameters to methods are passed "by value". In other words,   values of parameter variables in a method are copies of the invoker   specified as arguments. He goes on to make the same point regarding objects . . .  You should note that when the parameter is an object reference, it is   the object reference-not the object itself-that is passed "by value". And towards the end of the same section he makes a broader statement about java being only pass by value and never pass by reference. The Java programming language does not pass objects by reference; it passes object references by value. Because two copies of the same   reference refer to the same actual object, changes made through one   reference variable are visible through the other. There is exactly one   parameter passing mode-pass by value-and that helps keep things   simple. This section of the book has a great explanation of parameter passing in Java and of the distinction between pass-by-reference and pass-by-value and it's by the creator of Java. I would encourage anyone to read it, especially if you're still not convinced. I think the difference between the two models is very subtle and unless you've done programming where you actually used pass-by-reference, it's easy to miss where two models differ. I hope this settles the debate, but probably won't. EDIT 3 I might be a little obsessed with this post. Probably because I feel that the makers of Java inadvertently spread misinformation. If instead of using the word "reference" for pointers they had used something else, say  dingleberry, there would've been no problem. You could say, "Java passes dingleberries by value and not by reference", and nobody would be confused. That's the reason only Java developers have issue with this. They look at the word "reference" and think they know exactly what that means, so they don't even bother to consider the opposing argument. Anyway, I noticed a comment in an older post, which made a balloon analogy which I really liked. So much so that I decided to glue together some clip-art to make a set of cartoons to illustrate the point. Passing a reference by value--Changes to the reference are not reflected in the caller's scope, but the changes to the object are. This is because the reference is copied, but the both the original and the copy refer to the same object.  Pass by reference--There is no copy of the reference. Single reference is shared by both the caller and the function being called. Any changes to the reference or the Object's data are reflected in the caller's scope.  EDIT 4 I have seen posts on this topic which describe the low level implementation of parameter passing in Java, which I think is great and very helpful because it makes an abstract idea concrete. However, to me the question is more about the behavior described in the language specification than about the technical implementation of the behavior. This is an exerpt from the Java Language Specification, section 8.4.1 : When the method or constructor is invoked (§15.12), the values of the   actual argument expressions initialize newly created parameter   variables, each of the declared type, before execution of the body of   the method or constructor. The Identifier that appears in the   DeclaratorId may be used as a simple name in the body of the method or   constructor to refer to the formal parameter. Which means, java creates a copy of the passed parameters before executing a method. Like most people who studied compilers in college, I used "The Dragon Book" which is THE compilers book. It has a good description of "Call-by-value" and "Call-by-Reference" in Chapter 1. The Call-by-value description matches up with Java Specs exactly. Back when I studied compilers-in the 90's, I used the first edition of the book from 1986 which pre-dated Java by about 9 or 10 years. However, I just ran across a copy of the 2nd Eddition from 2007 which actually mentions Java! Section 1.6.6 labeled "Parameter Passing Mechanisms" describes parameter passing pretty nicely. Here is an excerpt under the heading "Call-by-value" which mentions Java: In call-by-value, the actual parameter is evaluated (if it is an   expression) or copied (if it is a variable). The value is placed in   the location belonging to the corresponding formal parameter of the   called procedure. This method is used in C and Java, and is a common   option in C++ , as well as in most other languages. Java is a call by value How it works You always pass a copy of the bits of the value of the reference! If it's a primitive data type these bits contain the value of the primitive data type itself, That's why if we change the value of header inside the method then it does not reflect the changes outside. If it's an object data type like Foo foo=new Foo() then in this case copy of the address of the object passes like file shortcut  , suppose we have a text file abc.txt at C:\desktop and suppose we make shortcut of the same file and put this inside C:\desktop\abc-shortcut so when you access the file from C:\desktop\abc.txt and write 'Stack Overflow' and close the file and again you open the file from shortcut then you write ' is the largest online community for programmers to learn' then total file change will be 'Stack Overflow is the largest online community for programmers to learn' which means it doesn't matter from where you open the file , each time we were accessing the same file , here we can assume Foo as a file and suppose foo stored at 123hd7h(original address like C:\desktop\abc.txt ) address and 234jdid(copied address like C:\desktop\abc-shortcut which actually contains the original address of the file inside) .. So for better understanding make shortcut file and feel. Getting an outside of the box view, let's look at Assembly or some low level memory management. At the CPU level a reference to anything immediately becomes a value if it gets written to memory or to one of the CPU registers. (That is why pointer is a good definition. It is a value, which has a purpose at the same time). Data in memory has a Location and at that location there is a value (byte,word, whatever). In Assembly we have a convenient solution to give a Name to certain Location (aka variable), but when compiling the code, the assembler simply replaces Name with the designated location just like your browser replaces domain names with IP addresses. Down to the core it is technically impossible to pass a reference to anything in any language without representing it (when it immediately becomes a value). Lets say we have a variable Foo, its Location is at the 47th byte in memory and its Value is 5. We have another variable Ref2Foo which is at 223rd byte in memory, and its value will be 47. This Ref2Foo might be a technical variable, not explicitly created by the program. If you just look at 5 and 47 without any other information, you will see just two Values. If you use them as references then to reach to 5 we have to travel: This is how jump-tables work.  If we want to call a method/function/procedure with Foo's value, there are a few possible way to pass the variable to the method, depending on the language and its several method invocation modes: In every cases above a value - a copy of an existing value - has been created, it is now upto the receiving method to handle it. When you write "Foo" inside the method, it is either read out from EAX, or automatically  dereferenced, or double dereferenced, the process depends on how the language works and/or what the type of Foo dictates. This is hidden from the developer until she circumvents the dereferencing process. So a reference is a value when represented, because a reference is a value that has to be processed (at language level). Now we have passed Foo to the method: Nitpicking on insignificant details, even languages that do pass-by-reference will pass values to functions, but those functions know that they have to use it for dereferencing purposes. This pass-the-reference-as-value is just hidden from the programmer because it is practically useless and the terminology is only pass-by-reference. Strict pass-by-value is also useless, it would mean that a 100 Mbyte array should have to be copied every time we call a method with the array as argument, therefore Java cannot be stricly pass-by-value. Every language would pass a reference to this huge array (as a value) and either employs copy-on-write mechanism if that array can be changed locally inside the method or allows the method (as Java does) to modify the array globally (from the caller's view) and a few languages allows to modify the Value of the reference itself. So in short and in Java's own terminology, Java is pass-by-value where value can be: either a real value or a value that is a representation of a reference.  As far as I know, Java only knows call by value. This means for primitive datatypes you will work with an copy and for objects you will work with an copy of the reference to the objects. However I think there are some pitfalls; for example, this will not work: This will populate Hello World and not World Hello because in the swap function you use copys which have no impact on the references in the main. But if your objects are not immutable you can change it for example: This will populate Hello World on the command line. If you change StringBuffer into String it will produce just Hello because String is immutable. For example: However you could make a wrapper for String like this which would make it able to use it with Strings: edit: i believe this is also the reason to use StringBuffer when it comes to "adding" two Strings because you can modifie the original object which u can't with immutable objects like String is. No, it's not pass by reference. Java is pass by value according to the Java Language Specification: When the method or constructor is invoked (§15.12), the values of the actual argument expressions initialize newly created parameter variables, each of the declared type, before execution of the body of the method or constructor. The Identifier that appears in the DeclaratorId may be used as a simple name in the body of the method or constructor to refer to the formal parameter.  Let me try to explain my understanding with the help of four examples. Java is pass-by-value, and not pass-by-reference /** Pass By Value In Java, all parameters are passed by value, i.e. assigning a method argument is not visible to the caller. */ Example 1: Result Example 2: /**  *   * Pass By Value  *  */ Result Example 3: /**   This 'Pass By Value has a feeling of 'Pass By Reference' Some people say primitive types and 'String' are 'pass by value'   and objects are 'pass by reference'. But from this example, we can understand that it is infact pass by value only,   keeping in mind that here we are passing the reference as the value.   ie: reference is passed by value.   That's why are able to change and still it holds true after the local scope.   But we cannot change the actual reference outside the original scope.   what that means is demonstrated by next example of PassByValueObjectCase2. */ Result Example 4: /** In addition to what was mentioned in Example3 (PassByValueObjectCase1.java),  we cannot change the actual reference outside the original scope." Note: I am not pasting the code for private class Student. The class definition for Student is same as Example3. */ Result You can never pass by reference in Java, and one of the ways that is obvious is when you want to return more than one value from a method call. Consider the following bit of code in C++: Sometimes you want to use the same pattern in Java, but you can't; at least not directly. Instead you could do something like this: As was explained in previous answers, in Java you're passing a pointer to the array as a value into getValues. That is enough, because the method then modifies the array element, and by convention you're expecting element 0 to contain the return value. Obviously you can do this in other ways, such as structuring your code so this isn't necessary, or constructing a class that can contain the return value or allow it to be set. But the simple pattern available to you in C++ above is not available in Java. I thought I'd contribute this answer to add more details from the Specifications. First, What's the difference between passing by reference vs. passing by value? Passing by reference means the called functions' parameter will be the   same as the callers' passed argument (not the value, but the identity   - the variable itself).  Pass by value means the called functions' parameter will be a copy of   the callers' passed argument. Or from wikipedia, on the subject of pass-by-reference In call-by-reference evaluation (also referred to as   pass-by-reference), a function receives an implicit reference to a   variable used as argument, rather than a copy of its value. This   typically means that the function can modify (i.e. assign to) the   variable used as argument—something that will be seen by its caller. And on the subject of pass-by-value In call-by-value, the argument expression is evaluated, and the   resulting value is bound to the corresponding variable in the function [...].    If the function or procedure is able to assign values to its   parameters, only its local copy is assigned [...]. Second, we need to know what Java uses in its method invocations. The Java Language Specification states When the method or constructor is invoked (§15.12), the values of the   actual argument expressions initialize newly created parameter   variables, each of the declared type, before execution of the body of   the method or constructor. So it assigns (or binds) the value of the argument to the corresponding parameter variable.  What is the value of the argument? Let's consider reference types, the Java Virtual Machine Specification states There are three kinds of reference types: class types, array types,   and interface types. Their values are references to dynamically   created class instances, arrays, or class instances or arrays that   implement interfaces, respectively. The Java Language Specification also states The reference values (often just references) are pointers to these objects, and a special null reference, which refers to no object. The value of an argument (of some reference type) is a pointer to an object. Note that a variable, an invocation of a method with a reference type return type, and an instance creation expression (new ...) all resolve to a reference type value. So all bind the value of a reference to a String instance to the method's newly created parameter, param. This is exactly what the definition of pass-by-value describes. As such, Java is pass-by-value. The fact that you can follow the reference to invoke a method or access a field of the referenced object is completely irrelevant to the conversation. The definition of pass-by-reference was This typically means that the function can modify (i.e. assign to) the   variable used as argument—something that will be seen by its caller. In Java, modifying the variable means reassigning it. In Java, if you reassigned the variable within the method, it would go unnoticed to the caller. Modifying the object referenced by the variable is a different concept entirely.  Primitive values are also defined in the Java Virtual Machine Specification, here. The value of the type is the corresponding integral or floating point value, encoded appropriately (8, 16, 32, 64, etc. bits). In Java only references are passed and are passed by value: Java arguments are all passed by value (the reference is copied when used by the method) : In the case of primitive types, Java behaviour is simple:  The value is copied in another instance of the primitive type. In case of Objects, this is the same:  Object variables are pointers (buckets) holding only Object’s address that was created using the "new" keyword, and are copied like primitive types. The behaviour can appear different from primitive types: Because the copied object-variable contains the same address (to the same Object). Object's content/members might still be modified within a method and later access outside, giving the illusion that the (containing) Object itself was passed by reference.  "String" Objects appear to be a good counter-example to the urban legend saying that "Objects are passed by reference": In effect, using a method, you will never be able, to update the value of a String passed as argument: A String Object, holds characters by an array declared final that can't be modified. Only the address of the Object might be replaced by another using "new".  Using "new" to update the variable, will not let the Object be accessed from outside, since the variable was initially passed by value and copied. The distinction, or perhaps just the way I remember as I used to be under the same impression as the original poster is this: Java is always pass by value. All objects( in Java, anything except for primitives) in Java are references. These references are passed by value. As many people mentioned it before, Java is always pass-by-value Here is another example that will help you understand the difference (the classic swap example): Prints:   Before: a = 2, b = 3   After: a = 2, b = 3 This happens because iA and iB are new local reference variables that have the same value of the passed references (they point to a and b respectively). So, trying to change the references of iA or iB will only change in the local scope and not outside of this method. Unlike some other languages, Java does not allow you to choose between pass-by-value and pass-by-reference—all arguments are passed by value. A method call can pass two types of values to a method—copies of primitive values (e.g., values of int and double) and copies of references to objects. When a method modifies a primitive-type parameter, changes to the parameter have no effect on the original argument value in the calling method. When it comes to objects, objects themselves cannot be passed to methods. So we pass the reference(address) of the object. We can manipulate the original object using this reference. How Java creates and stores objects: When we create an object we store the object’s address in a reference variable. Let's analyze the following statement. “Account account1” is the type and name of the reference variable, “=” is the assignment operator, “new” asks for the required amount of space from the system. The constructor to the right of keyword new which creates the object is called implicitly by the keyword new. Address of the created object(result of right value, which is an expression called "class instance creation expression") is assigned to the left value (which is a reference variable with a name and a type specified) using the assign operator. Although an object’s reference is passed by value, a method can still interact with the referenced object by calling its public methods using the copy of the object’s reference. Since the reference stored in the parameter is a copy of the reference that was passed as an argument, the parameter in the called method and the argument in the calling method refer to the same object in memory. Passing references to arrays, instead of the array objects themselves, makes sense for performance reasons. Because everything in Java is passed by value, if array objects were passed, a copy of each element would be passed. For large arrays, this would waste time and consume considerable storage for the copies of the elements. In the image below you can see we have two reference variables(These are called pointers in C/C++, and I think that term makes it easier to understand this feature.) in the main method. Primitive and reference variables are kept in stack memory(left side in images below). array1 and array2 reference variables "point" (as C/C++ programmers call it) or reference to a and b arrays respectively, which are objects (values these reference variables hold are addresses of objects) in heap memory (right side in images below).  If we pass the value of array1 reference variable as an argument to the reverseArray method, a reference variable is created in the method and that reference variable starts pointing to the same array (a).   So, if we say  in reverseArray method, it will make a change in array a. We have another reference variable in reverseArray method (array2) that points to an array c. If we were to say  in reverseArray method, then the reference variable array1 in method reverseArray would stop pointing to array a and start pointing to array c (Dotted line in second image). If we return value of reference variable array2 as the return value of method reverseArray and assign this value to reference variable array1 in main method, array1 in main will start pointing to array c. So let's write all the things we have done at once now.  And now that reverseArray method is over, its reference variables(array1 and array2) are gone. Which means we now only have the two reference variables in main method array1 and array2 which point to c and b arrays respectively. No reference variable is pointing to object (array) a. So it is eligible for garbage collection. You could also assign value of array2 in main to array1. array1 would start pointing to b. I always think of it as "pass by copy". It is a copy of the value be it primitive or reference. If it is a primitive it is a copy of the bits that are the value and if it is an Object it is a copy of the reference. output of java PassByCopy: name= Maxx   name= Fido Primitive wrapper classes and Strings are immutable so any example using those types will not work the same as other types/objects. Java has only pass by value. A very simple example to validate this. I have created a thread devoted to these kind of questions for any programming languages here. Java is also mentioned. Here is the short summary: A few corrections to some posts. C does NOT support pass by reference. It is ALWAYS pass by value. C++ does support pass by reference, but is not the default and is quite dangerous. It doesn't matter what the value is in Java: primitive or address(roughly) of object, it is ALWAYS passed by value. If a Java object "behaves" like it is being passed by reference, that is a property of mutability and has absolutely nothing to do with passing mechanisms. I am not sure why this is so confusing, perhaps because so many Java "programmers" are not formally trained, and thus do not understand what is really going on in memory?
__label__c# __label__string __label__types __label__alias __label__.net Example (note the case): What are the guidelines for the use of each? And what are the differences? string is an alias in C# for System.String. So technically, there is no difference.  It's like int vs. System.Int32. As far as guidelines, it's generally recommended to use string any time you're referring to an object. e.g.  Likewise, I think it's generally recommended to use String if you need to refer specifically to the class. e.g. It appears that the guidance in this area may have changed, as StyleCop now enforces the use of the C# specific aliases. Just for the sake of completeness, here's a brain dump of related information... As others have noted, string is an alias for System.String. Assuming your code using String compiles to System.String (i.e. you haven't got a using directive for some other namespace with a different String type), they compile to the same code, so at execution time there is no difference whatsoever. This is just one of the aliases in C#. The complete list is: Apart from string and object, the aliases are all to value types. decimal is a value type, but not a primitive type in the CLR. The only primitive type which doesn't have an alias is System.IntPtr. In the spec, the value type aliases are known as "simple types". Literals can be used for constant values of every simple type; no other value types have literal forms available. (Compare this with VB, which allows DateTime literals, and has an alias for it too.) There is one circumstance in which you have to use the aliases: when explicitly specifying an enum's underlying type. For instance: That's just a matter of the way the spec defines enum declarations - the part after the colon has to be the integral-type production, which is one token of sbyte, byte, short, ushort, int, uint, long, ulong, char... as opposed to a type production as used by variable declarations for example. It doesn't indicate any other difference. Finally, when it comes to which to use: personally I use the aliases everywhere for the implementation, but the CLR type for any APIs. It really doesn't matter too much which you use in terms of implementation - consistency among your team is nice, but no-one else is going to care. On the other hand, it's genuinely important that if you refer to a type in an API, you do so in a language-neutral way. A method called ReadInt32 is unambiguous, whereas a method called ReadInt requires interpretation. The caller could be using a language that defines an int alias for Int16, for example. The .NET framework designers have followed this pattern, good examples being in the BitConverter, BinaryReader and Convert classes. String stands for System.String and it is a .NET Framework type. string is an alias in the C# language for  System.String. Both of them are compiled to System.String in IL (Intermediate Language), so there is no difference. Choose what you like and use that. If you code in C#, I'd prefer string as it's a C# type alias and well-known by C# programmers. I can say the same about (int, System.Int32) etc.. The best answer I have ever heard about using the provided type aliases in C# comes from Jeffrey Richter in his book CLR Via C#. Here are his 3 reasons: So there you have it. I think these are all really good points. I however, don't find myself using Jeffrey's advice in my own code. Maybe I am too stuck in my C# world but I end up trying to make my code look like the framework code. string is a reserved word, but String is just a class name.  This means that string cannot be used as a variable name by itself. If for some reason you wanted a variable called string, you'd see only the first of these compiles: If you really want a variable name called string you can use @ as a prefix: Another critical difference: Stack Overflow highlights them differently.  There is one difference - you can't use String without using System; beforehand. It's been covered above; however, you can't use string in reflection; you must use String. System.String is the .NET string class - in C# string is an alias for System.String - so in use they are the same. As for guidelines I wouldn't get too bogged down and just use whichever you feel like - there are more important things in life and the code is going to be the same anyway. If you find yourselves building systems where it is necessary to specify the size of the integers you are using and so tend to use Int16, Int32, UInt16, UInt32 etc. then it might look more natural to use String - and when moving around between different .net languages it might make things more understandable - otherwise I would use string and int.  I prefer the capitalized .NET types (rather than the aliases) for formatting reasons. The .NET types are colored the same as other object types (the value types are proper objects, after all). Conditional and control keywords (like if, switch, and return) are lowercase and colored dark blue (by default). And I would rather not have the disagreement in use and format. Consider: string and String are identical in all ways (except the uppercase "S").  There are no performance implications either way. Lowercase string is preferred in most projects due to the syntax highlighting C# is a language which is used together with the CLR. string is a type in C#. System.String is a type in the CLR. When you use C# together with the CLR string will be mapped to System.String. Theoretically, you could implement a C#-compiler that generated Java bytecode. A sensible implementation of this compiler would probably map string to java.lang.String in order to interoperate with the Java runtime library. This YouTube video demonstrates practically how they differ. But now for a long textual answer. When we talk about .NET there are two different things one there is .NET framework and the other there are languages ( C#, VB.NET etc) which use that framework.  "System.String" a.k.a "String" ( capital "S") is a .NET framework data type while "string" is a C# data type.  In short "String" is an alias ( the same thing called with different names) of "string". So technically both the below code statements will give the same output. or In the same way, there are aliases for other c# data type as shown below:- object: System.Object, string: System.String, bool: System.Boolean, byte: System.Byte, sbyte: System.SByte, short: System.Int16 and so on Now the million-dollar question from programmer's point of view So when to use "String" and "string"? The first thing to avoid confusion use one of them consistently. But from best practices perspective when you do variable declaration it's good to use "string" ( small "s") and when you are using it as a class name then "String" ( capital "S") is preferred. In the below code the left-hand side is a variable declaration and it declared using "string". On the right-hand side, we are calling a method so "String" is more sensible. Lower case string is an alias for System.String. They are the same in C#. There's a debate over whether you should use the System types (System.Int32, System.String, etc.) types or the C# aliases (int, string, etc). I personally believe you should use the C# aliases, but that's just my personal preference. string is just an alias for System.String. The compiler will treat them identically. The only practical difference is the syntax highlighting as you mention, and that you have to write using System if you use String. Both are same. But from coding guidelines perspective it's better to use string instead of String. This is what generally developers use. e.g. instead of using Int32 we use int as int is alias to Int32 FYI “The keyword string is simply an alias for the predefined class System.String.” - C# Language Specification 4.2.3 http://msdn2.microsoft.com/En-US/library/aa691153.aspx As the others are saying, they're the same.  StyleCop rules, by default, will enforce you to use string as a C# code style best practice, except when referencing System.String static functions, such as String.Format, String.Join, String.Concat, etc... New answer after 6 years and 5 months (procrastination). While string is a reserved C# keyword that always has a fixed meaning, String is just an ordinary identifier which could refer to anything. Depending on members of the current type, the current namespace and the applied using directives and their placement, String could be a value or a type distinct from global::System.String. I shall provide two examples where using directives will not help. First, when String is a value of the current type (or a local variable): The above will not compile because IEnumerable<> does not have a non-static member called Format, and no extension methods apply. In the above case, it may still be possible to use String in other contexts where a type is the only possibility syntactically. For example String local = "Hi mum!"; could be OK (depending on namespace and using directives). Worse: Saying String.Concat(someSequence) will likely (depending on usings) go to the Linq extension method Enumerable.Concat. It will not go to the static method string.Concat. Secondly, when String is another type, nested inside the current type: Neither statement in the Example method compiles. Here String is always a piano string, MyPiano.String. No member (static or not) Format exists on it (or is inherited from its base class). And the value "Goodbye" cannot be converted into it. Using System types makes it easier to port between C# and VB.Net, if you are into that sort of thing. Against what seems to be common practice among other programmers, I prefer String over string, just to highlight the fact that String is a reference type, as Jon Skeet mentioned. string is an alias (or shorthand) of System.String. That means, by typing string we meant System.String. You can read more in think link: 'string' is an alias/shorthand of System.String. I'd just like to add this to lfousts answer, from Ritchers book: The C# language specification states, “As a matter of style, use of the keyword is favored over   use of the complete system type name.” I disagree with the language specification; I prefer   to use the FCL type names and completely avoid the primitive type names. In fact, I wish that   compilers didn’t even offer the primitive type names and forced developers to use the FCL   type names instead. Here are my reasons: I’ve seen a number of developers confused, not knowing whether to use string   or String in their code. Because in C# string (a keyword) maps exactly to   System.String (an FCL type), there is no difference and either can be used. Similarly,   I’ve heard some developers say that int represents a 32-bit integer when the application   is running on a 32-bit OS and that it represents a 64-bit integer when the application   is running on a 64-bit OS. This statement is absolutely false: in C#, an int always maps   to System.Int32, and therefore it represents a 32-bit integer regardless of the OS the   code is running on. If programmers would use Int32 in their code, then this potential   confusion is also eliminated. In C#, long maps to System.Int64, but in a different programming language, long   could map to an Int16 or Int32. In fact, C++/CLI does treat long as an Int32.   Someone reading source code in one language could easily misinterpret the code’s   intention if he or she were used to programming in a different programming language.   In fact, most languages won’t even treat long as a keyword and won’t compile code   that uses it. The FCL has many methods that have type names as part of their method names. For   example, the BinaryReader type offers methods such as ReadBoolean, ReadInt32,   ReadSingle, and so on, and the System.Convert type offers methods such as   ToBoolean, ToInt32, ToSingle, and so on. Although it’s legal to write the following   code, the line with float feels very unnatural to me, and it’s not obvious that the line is   correct: Many programmers that use C# exclusively tend to forget that other programming   languages can be used against the CLR, and because of this, C#-isms creep into the   class library code. For example, Microsoft’s FCL is almost exclusively written in C# and   developers on the FCL team have now introduced methods into the library such as   Array’s GetLongLength, which returns an Int64 value that is a long in C# but not   in other languages (like C++/CLI). Another example is System.Linq.Enumerable’s   LongCount method. I didn't get his opinion before I read the complete paragraph.   String (System.String) is a class in the base class library. string (lower case) is a reserved work in C# that is an alias for System.String. Int32 vs int is a similar situation as is Boolean vs. bool. These C# language specific keywords enable you to declare primitives in a style similar to C.  It's a matter of convention, really.  string just looks more like C/C++ style.  The general convention is to use whatever shortcuts your chosen language has provided (int/Int for Int32).  This goes for "object" and decimal as well. Theoretically this could help to port code into some future 64-bit standard in which "int" might mean Int64, but that's not the point, and I would expect any upgrade wizard to change any int references to Int32 anyway just to be safe. String is not a keyword and it can be used as Identifier whereas string is a keyword and cannot be used as Identifier. And in function point of view both are same. Coming late to the party: I use the CLR types 100% of the time (well, except if forced to use the C# type, but I don't remember when the last time that was).  I originally started doing this years ago, as per the CLR books by Ritchie. It made sense to me that all CLR languages ultimately have to be able to support the set of CLR types, so using the CLR types yourself provided clearer, and possibly more "reusable" code. Now that I've been doing it for years, it's a habit and I like the coloration that VS shows for the CLR types. The only real downer is that auto-complete uses the C# type, so I end up re-typing automatically generated types to specify the CLR type instead. Also, now, when I see "int" or "string", it just looks really wrong to me, like I'm looking at 1970's C code. There is no difference. The C# keyword string maps to the .NET type System.String - it is an alias that keeps to the naming conventions of the language. Similarly, int maps to System.Int32. @JaredPar (a developer on the C# compiler and prolific SO user!) wrote a great blog post on this issue. I think it is worth sharing here. It is a nice perspective on our subject. [...] The keyword string has concrete meaning in C#. It is the type System.String which exists in the core runtime assembly. The runtime intrinsically understands this type and provides the capabilities developers expect for strings in .NET. Its presence is so critical to C# that if that type doesn’t exist the compiler will exit before attempting to even parse a line of code. Hence string has a precise, unambiguous meaning in C# code. The identifier String though has no concrete meaning in C#. It is an identifier that goes through all the name lookup rules as Widget, Student, etc … It could bind to string or it could bind to a type in another assembly entirely whose purposes may be entirely different than string. Worse it could be defined in a way such that code like String s = "hello"; continued to compile. The actual meaning of String will always depend on name resolution. That means it depends on all the source files in the project and all the types defined in all the referenced assemblies. In short it requires quite a bit of context to know what it means. True that in the vast majority of cases String and string will bind to the same type. But using String still means developers are leaving their program up to interpretation in places where there is only one correct answer. When String does bind to the wrong type it can leave developers debugging for hours, filing bugs on the compiler team, and generally wasting time that could’ve been saved by using string. Another way to visualize the difference is with this sample: Many will argue that while this is information technically accurate using String is still fine because it’s exceedingly rare that a codebase would define a type of this name. Or that when String is defined it’s a sign of a bad codebase. [...] You’ll see that String is defined for a number of completely valid purposes: reflection helpers, serialization libraries, lexers, protocols, etc … For any of these libraries String vs. string has real consequences depending on where the code is used. So remember when you see the String vs. string debate this is about semantics, not style. Choosing string gives crisp meaning to your codebase. Choosing String isn’t wrong but it’s leaving the door open for surprises in the future. Note: I copy/pasted most of the blog posts for archive reasons. I ignore some parts, so I recommend skipping and reading the blog post if you can. There's a quote on this issue from Daniel Solis' book. All the predefined types  are mapped directly to   underlying .NET types. The C# type names (string) are simply aliases for the   .NET types (String or System.String), so using the .NET names works fine syntactically, although   this is discouraged. Within a C# program, you should use the C# names   rather than the .NET names. Yes, that's no difference between them, just like the bool and Boolean. string is a keyword, and you can't use string as an identifier.  String is not a keyword, and you can use it as an identifier: Example The keyword string  is an alias for  System.String aside from the keyword issue, the two are exactly  equivalent.
__label__main __label__python-module __label__idioms __label__namespaces __label__python Given the following code, what does the if __name__ == "__main__": do? Whenever the Python interpreter reads a source file, it does two things: it sets a few special variables like __name__, and then it executes all of the code found in the file. Let's see how this works and how it relates to your question about the __name__ checks we always see in Python scripts. Let's use a slightly different code sample to explore how imports and scripts work.  Suppose the following is in a file called foo.py. When the Python interpreter reads a source file, it first defines a few special variables. In this case, we care about the __name__ variable. When Your Module Is the Main Program If you are running your module (the source file) as the main program, e.g. the interpreter will assign the hard-coded string "__main__" to the __name__ variable, i.e. When Your Module Is Imported By Another On the other hand, suppose some other module is the main program and it imports your module. This means there's a statement like this in the main program, or in some other module the main program imports: The interpreter will search for your foo.py file (along with searching for a few other variants), and prior to executing that module, it will assign the name "foo" from the import statement to the __name__ variable, i.e. After the special variables are set up, the interpreter executes all the code in the module, one statement at a time. You may want to open another window on the side with the code sample so you can follow along with this explanation. Always It prints the string "before import" (without quotes). It loads the math module and assigns it to a variable called math. This is equivalent to replacing import math with the following (note that __import__ is a low-level function in Python that takes a string and triggers the actual import): It prints the string "before functionA". It executes the def block, creating a function object, then assigning that function object to a variable called functionA. It prints the string "before functionB". It executes the second def block, creating another function object, then assigning it to a variable called functionB. It prints the string "before __name__ guard". Only When Your Module Is the Main Program Only When Your Module Is Imported by Another Always Summary In summary, here's what'd be printed in the two cases: You might naturally wonder why anybody would want this.  Well, sometimes you want to write a .py file that can be both used by other programs and/or modules as a module, and can also be run as the main program itself.  Examples: Your module is a library, but you want to have a script mode where it runs some unit tests or a demo. Your module is only used as a main program, but it has some unit tests, and the testing framework works by importing .py files like your script and running special test functions. You don't want it to try running the script just because it's importing the module. Your module is mostly used as a main program, but it also provides a programmer-friendly API for advanced users. Beyond those examples, it's elegant that running a script in Python is just setting up a few magic variables and importing the script. "Running" the script is a side effect of importing the script's module. Question: Can I have multiple __name__ checking blocks?  Answer: it's strange to do so, but the language won't stop you. Suppose the following is in foo2.py.  What happens if you say python foo2.py on the command-line? Why? When your script is run by passing it as a command to the Python interpreter, all of the code that is at indentation level 0 gets executed.  Functions and classes that are defined are, well, defined, but none of their code gets run.  Unlike other languages, there's no main() function that gets run automatically - the main() function is implicitly all the code at the top level. In this case, the top-level code is an if block.  __name__ is a built-in variable which evaluates to the name of the current module.  However, if a module is being run directly (as in myscript.py above), then __name__ instead is set to the string "__main__".  Thus, you can test whether your script is being run directly or being imported by something else by testing If your script is being imported into another module, its various function and class definitions will be imported and its top-level code will be executed, but the code in the then-body of the if clause above won't get run as the condition is not met. As a basic example, consider the following two scripts: Now, if you invoke the interpreter as The output will be If you run two.py instead: You get Thus, when module one gets loaded, its __name__ equals "one" instead of "__main__". The simplest explanation for the __name__ variable (imho) is the following: Create the following files. and Running them will get you this output: As you can see, when a module is imported, Python sets globals()['__name__'] in this module to the module's name. Also, upon import all the code in the module is being run. As the if statement evaluates to False this part is not executed. As you can see, when a file is executed, Python sets globals()['__name__'] in this file to "__main__". This time, the if statement evaluates to True and is being run. To outline the basics: The global variable, __name__, in the module that is the entry point to your program, is '__main__'. Otherwise, it's the name you import the module by. So, code under the if block will only run if the module is the entry point to your program. It allows the code in the module to be importable by other modules, without executing the code block beneath on import. Why do we need this? Say you're writing a Python script designed to be used as a module: You could test the module by adding this call of the function to the bottom: and running it (on a command prompt) with something like: However, if you want to import the module to another script: On import, the do_important function would be called, so you'd probably comment out your function call, do_important(), at the bottom.  And then you'll have to remember whether or not you've commented out your test function call. And this extra complexity would mean you're likely to forget, making your development process more troublesome. The __name__ variable points to the namespace wherever the Python interpreter happens to be at the moment.  Inside an imported module, it's the name of that module.  But inside the primary module (or an interactive Python session, i.e. the interpreter's Read, Eval, Print Loop, or REPL) you are running everything from its "__main__". So if you check before executing: With the above, your code will only execute when you're running it as the primary module (or intentionally call it from another script).  There's a Pythonic way to improve on this, though.  What if we want to run this business process from outside the module? If we put the code we want to exercise as we develop and test in a function like this and then do our check for '__main__' immediately after: We now have a final function for the end of our module that will run if we run the module as the primary module.  It will allow the module and its functions and classes to be imported into other scripts without running the main function, and will also allow the module (and its functions and classes) to be called when running from a different '__main__' module, i.e. This idiom can also be found in the Python documentation in an explanation of the __main__ module. That text states: This module represents the (otherwise anonymous) scope in which the   interpreter’s main program executes — commands read either from   standard input, from a script file, or from an interactive prompt. It   is this environment in which the idiomatic “conditional script” stanza   causes a script to run: if __name__ == "__main__" is the part that runs when the script is run from (say) the command line using a command like python myscript.py. __name__ is a global variable (in Python, global actually means on the module level) that exists in all namespaces. It is typically the module's name (as a str type). As the only special case, however, in whatever Python process you run, as in mycode.py: the otherwise anonymous global namespace is assigned the value of '__main__' to its __name__.  Thus, including the final lines will cause your script's uniquely defined main function to run.  Another benefit of using this construct: you can also import your code as a module in another script and then run the main function if and when your program decides: There are lots of different takes here on the mechanics of the code in question, the "How", but for me none of it made sense until I understood the "Why". This should be especially helpful for new programmers. Take file "ab.py": And a second file "xy.py": What is this code actually doing? When you execute xy.py, you import ab. The import statement runs the module immediately on import, so ab's operations get executed before the remainder of xy's. Once finished with ab, it continues with xy. The interpreter keeps track of which scripts are running with __name__. When you run a script - no matter what you've named it - the interpreter calls it "__main__", making it the master or 'home' script that gets returned to after running an external script. Any other script that's called from this "__main__" script is assigned its filename as its __name__ (e.g., __name__ == "ab.py"). Hence, the line if __name__ == "__main__": is the interpreter's test to determine if it's interpreting/parsing the 'home' script that was initially executed, or if it's temporarily peeking into another (external) script. This gives the programmer flexibility to have the script behave differently if it's executed directly vs. called externally. Let's step through the above code to understand what's happening, focusing first on the unindented lines and the order they appear in the scripts. Remember that function - or def - blocks don't do anything by themselves until they're called. What the interpreter might say if mumbled to itself: The bottom two lines mean: "If this is the "__main__" or 'home' script, execute the function called main()". That's why you'll see a def main(): block up top, which contains the main flow of the script's functionality. Why implement this? Remember what I said earlier about import statements? When you import a module it doesn't just 'recognize' it and wait for further instructions - it actually runs all the executable operations contained within the script. So, putting the meat of your script into the main() function effectively quarantines it, putting it in isolation so that it won't immediately run when imported by another script. Again, there will be exceptions, but common practice is that main() doesn't usually get called externally. So you may be wondering one more thing: if we're not calling main(), why are we calling the script at all? It's because many people structure their scripts with standalone functions that are built to be run independent of the rest of the code in the file. They're then later called somewhere else in the body of the script. Which brings me to this: But the code works without it Yes, that's right. These separate functions can be called from an in-line script that's not contained inside a main() function. If you're accustomed (as I am, in my early learning stages of programming) to building in-line scripts that do exactly what you need, and you'll try to figure it out again if you ever need that operation again ... well, you're not used to this kind of internal structure to your code, because it's more complicated to build and it's not as intuitive to read. But that's a script that probably can't have its functions called externally, because if it did it would immediately start calculating and assigning variables. And chances are if you're trying to re-use a function, your new script is related closely enough to the old one that there will be conflicting variables. In splitting out independent functions, you gain the ability to re-use your previous work by calling them into another script. For example, "example.py" might import "xy.py" and call x(), making use of the 'x' function from "xy.py". (Maybe it's capitalizing the third word of a given text string; creating a NumPy array from a list of numbers and squaring them; or detrending a 3D surface. The possibilities are limitless.) (As an aside, this question contains an answer by @kindall that finally helped me to understand - the why, not the how. Unfortunately it's been marked as a duplicate of this one, which I think is a mistake.) When there are certain statements in our module (M.py) we want to be executed when it'll be running as main (not imported), we can place those statements (test-cases, print statements) under this if block. As by default (when module running as main, not imported) the __name__ variable is set to "__main__", and when it'll be imported the __name__ variable will get a different value, most probably the name of the module ('M'). This is helpful in running different variants of a modules together, and separating their specific input & output statements and also if there are any test-cases. In short, use this 'if __name__ == "main" ' block to prevent (certain) code from being run when the module is imported. Put simply, __name__ is a variable defined for each script that defines whether the script is being run as the main module or it is being run as an imported module. So if we have two scripts; and The output from executing script1 is And the output from executing script2 is: As you can see, __name__ tells us which code is the 'main' module. This is great, because you can just write code and not have to worry about structural issues like in C/C++, where, if a file does not implement a 'main' function then it cannot be compiled as an executable and if it does, it cannot then be used as a library. Say you write a Python script that does something great and you implement a boatload of functions that are useful for other purposes. If I want to use them I can just import your script and use them without executing your program (given that your code only executes within the  if __name__ == "__main__": context). Whereas in C/C++ you would have to portion out those pieces into a separate module that then includes the file. Picture the situation below;  The arrows are import links. For three modules each trying to include the previous modules code there are six files (nine, counting the implementation files) and five links. This makes it difficult to include other code into a C project unless it is compiled specifically as a library. Now picture it for Python:  You write a module, and if someone wants to use your code they just import it and the __name__ variable can help to separate the executable portion of the program from the library part. Let's look at the answer in a more abstract way: Suppose we have this code in x.py: Blocks A and B are run when we are running x.py. But just block A (and not B) is run when we are running another module, y.py for example, in which x.py is imported and the code is run from there (like when a function in x.py is called from y.py). When you run Python interactively the local __name__ variable is assigned a value of __main__. Likewise, when you execute a Python module from the command line, rather than importing it into another module, its __name__ attribute is assigned a value of __main__, rather than the actual name of the module. In this way, modules can look at their own __name__ value to determine for themselves how they are being used, whether as support for another program or as the main application executed from the command line. Thus, the following idiom is quite common in Python modules: Consider: It checks if the __name__ attribute of the Python script is "__main__". In other words, if the program itself is executed, the attribute will be __main__, so the program will be executed (in this case the main() function). However, if your Python script is used by a module, any code outside of the if statement will be executed, so if \__name__ == "\__main__" is used just to check if the program is used as a module or not, and therefore decides whether to run the code. I've been reading so much throughout the answers on this page. I would say, if you know the thing, for sure you will understand those answers, otherwise, you are still confused. To be short, you need to know several points: import a action actually runs all that can be ran in a.py, meaning each line in a.py Because of point 1, you may not want everything to be run in a.py when importing it To solve the problem in point 2, python allows you to put a condition check __name__ is an implicit variable in all .py modules: The important thing that python is special at is point 4! The rest is just basic logic. Before explaining anything about if __name__ == '__main__' it is important to understand what __name__ is and what it does. What is __name__? __name__ is a DunderAlias - can be thought of as a global variable (accessible from modules) and works in a similar way to global. It is a string (global as mentioned above) as indicated by type(__name__) (yielding <class 'str'>), and is an inbuilt standard for both Python 3 and Python 2 versions. Where: It can not only be used in scripts but can also be found in both the interpreter and modules/packages.   Interpreter: Script: test_file.py: Resulting in __main__ Module or package: somefile.py: test_file.py: Resulting in somefile Notice that when used in a package or module, __name__ takes the name of the file.  The path of the actual module or package path is not given, but has its own DunderAlias __file__, that allows for this. You should see that, where __name__, where it is the main file (or program) will always return __main__, and if it is a module/package, or anything that is running off some other Python script, will return the name of the file where it has originated from. Practice: Being a variable means that it's value can be overwritten ("can" does not mean "should"), overwriting the value of __name__ will result in a lack of readability.  So do not do it, for any reason.  If you need a variable define a new variable. It is always assumed that the value of __name__ to be __main__ or the name of the file.  Once again changing this default value will cause more confusion that it will do good, causing problems further down the line. example: It is considered good practice in general to include the if __name__ == '__main__' in scripts. Now to answer if __name__ == '__main__': Now we know the behaviour of __name__ things become clearer: An if is a flow control statement that contains the block of code will execute if the value given is true. We have seen that __name__ can take either  __main__ or the file name it has been imported from.   This means that if __name__ is equal to __main__ then the file must be the main file and must actually be running (or it is the interpreter), not a module or package imported into the script. If indeed __name__ does take the value of __main__ then whatever is in that block of code will execute. This tells us that if the file running is the main file (or you are running from the interpreter directly) then that condition must execute.  If it is a package then it should not, and the value will not be __main__. Modules: __name__ can also be used in modules to define the name of a module Variants:  It is also possible to do other, less common but useful things with __name__, some I will show here: Executing only if the file is a module or package: Running one condition if the file is the main one and another if it is not: You can also use it to provide runnable help functions/utilities on packages and modules without the elaborate use of libraries. It also allows modules to be run from the command line as main scripts, which can be also very useful. I think it's best to break the answer in depth and in simple words: __name__: Every module in Python has a special attribute called __name__. It is a built-in variable that returns the name of the module. __main__: Like other programming languages, Python too has an execution entry point, i.e., main. '__main__' is the name of the scope in which top-level code executes. Basically you have two ways of using a Python module: Run it directly as a script, or import it. When a module is run as a script, its __name__ is set to __main__. Thus, the value of the __name__ attribute is set to __main__ when the module is run as the main program. Otherwise the value of __name__  is set to contain the name of the module. It is a special for when a Python file is called from the command line. This is typically used to call a "main()" function or execute other appropriate startup code, like commandline arguments handling for instance. It could be written in several ways. Another is: I am not saying you should use this in production code, but it serves to illustrate that there is nothing "magical" about if __name__ == '__main__'. It is a good convention for invoking a main function in Python files. There are a number of variables that the system (Python interpreter) provides for source files (modules).  You can get their values anytime you want, so, let us focus on the __name__ variable/attribute: When Python loads a source code file, it executes all of the code found in it. (Note that it doesn't call all of the methods and functions defined in the file, but it does define them.) Before the interpreter executes the source code file though, it defines a few special variables for that file; __name__ is one of those special variables that Python automatically defines for each source code file. If Python is loading this source code file as the main program (i.e. the file you run), then it sets the special __name__ variable for this file to have a value "__main__". If this is being imported from another module, __name__ will be set to that module's name. So, in your example in part: means that the code block: will be executed only when you run the module directly; the code block will not execute if another module is calling/importing it because the value of __name__ will not equal to "main" in that particular instance. Hope this helps out. The code under if __name__ == '__main__': will be executed only if the module is invoked as a script. As an example consider the following module my_test_module.py: 1st possibility: Import my_test_module.py in another module Now if you invoke main.py: Note that only the top-level print() statement in my_test_module is executed. 2nd possibility: Invoke my_test_module.py as a script Now if you run my_test_module.py as a Python script, both print() statements will be exectued: For a more comprehensive explanation you can read this blog post. if __name__ == "__main__": is basically the top-level script environment, and it specifies the interpreter that ('I have the highest priority to be executed first'). '__main__' is the name of the scope in which top-level code executes. A module’s __name__ is set equal to '__main__' when read from standard input, a script, or from an interactive prompt. You can make the file usable as a script as well as an importable module. fibo.py (a module named fibo) Reference: https://docs.python.org/3.5/tutorial/modules.html Consider: The output for the above is __main__. The above statement is true and prints "direct method". Suppose if they imported this class in another class it doesn't print "direct method" because, while importing, it will set __name__ equal to "first model name". The reason for is primarily to avoid the import lock problems that would arise from having code directly imported. You want main() to run if your file was directly invoked (that's the __name__ == "__main__" case), but if your code was imported then the importer has to enter your code from the true main module to avoid import lock problems. A side-effect is that you automatically sign on to a methodology that supports multiple entry points. You can run your program using main() as the entry point, but you don't have to. While setup.py expects main(), other tools use alternate entry points. For example, to run your file as a gunicorn process, you define an app() function instead of a main(). Just as with setup.py, gunicorn imports your code so you don't want it do do anything while it's being imported (because of the import lock issue). Every module in python has a attribute called __name__. The value of __name__  attribute is  __main__  when the module is run directly, like python my_module.py. Otherwise (like when you say import my_module) the value of __name__  is the name of the module. Small example to explain in short. We can execute this directly as Output Now suppose we call above script from other script When you execute this Output So, above is self explanatory that when you call test from other script, if loop __name__ in test.py will not execute. This answer is for Java programmers learning Python. Every Java file typically contains one public class. You can use that class in two ways:  Call the class from other files. You just have to import it in the calling program. Run the class stand alone, for testing purposes.  For the latter case, the class should contain a public static void main() method. In Python this purpose is served by the globally defined label '__main__'. If this .py file are imported by other .py files, the code under "the if statement" will not be executed. If this .py are run by python this_py.py under shell, or double clicked in Windows. the code under "the if statement" will be executed. It is usually written for testing. We see if __name__ == '__main__': quite often. It checks if a module is being imported or not. In other words, the code within the if block will be executed only when the code runs directly. Here directly means not imported. Let's see what it does using a simple code that prints the name of the module: If we run the code directly via python test.py, the module name is __main__: If the python interpreter is running a particular module then __name__ global  variable will have value "__main__" When you run this script prints you can see me  a If you import this file say A to file B  and execute the file B then if __name__ == "__main__" in file A becomes false, so it prints  You can't see me b All the answers have pretty much explained the functionality. But I will provide one example of its usage which might help clearing out the concept further. Assume that you have two Python files, a.py and b.py. Now, a.py imports b.py. We run the a.py file, where the "import b.py" code is executed first. Before the rest of the a.py code runs, the code in the file b.py must run completely. In the b.py code there is some code that is exclusive to that file b.py and we don't want any other file (other than b.py file), that has imported the b.py file, to run it. So that is what this line of code checks. If it is the main file (i.e., b.py) running the code, which in this case it is not (a.py is the main file running), then only the code gets executed. Create a file, a.py: __name__ is always equal to __main__ whenever that file is run directly showing that this is the main file. Create another file, b.py, in the same directory: Run it. It will print a, i.e., the name of the file which is imported. So, to show two different behavior of the same file, this is a commonly used trick: Every module in Python has a special attribute called name. The value of name  attribute is set to 'main'  when the module is executed as  the main program (e.g. running python foo.py). Otherwise, the value of name  is set to the name of the module that it was called from.
__label__conditional-operator __label__operators __label__ternary-operator __label__python If Python does not have a ternary conditional operator, is it possible to simulate one using other language constructs? Yes, it was added in version 2.5. The expression syntax is: First condition is evaluated, then exactly one of either a or b is evaluated and returned based on the Boolean value of condition. If condition evaluates to True, then a is evaluated and returned but b is ignored, or else when b is evaluated and returned but a is ignored. This allows short-circuiting because when condition is true only a is evaluated and b is not evaluated at all, but when condition is false only b is evaluated and a is not evaluated at all. For example: Note that conditionals are an expression, not a statement. This means you can't use assignment statements or pass or other statements within a conditional expression: You can, however, use conditional expressions to assign a variable like so: Think of the conditional expression as switching between two values. It is very useful when you're in a 'one value or another' situation, it but doesn't do much else. If you need to use statements, you have to use a normal if statement instead of a conditional expression. Keep in mind that it's frowned upon by some Pythonistas for several reasons: If you're having trouble remembering the order, then remember that when read aloud, you (almost) say what you mean. For example, x = 4 if b > 8 else 9 is read aloud as x will be 4 if b is greater than 8 otherwise 9. Official documentation: You can index into a tuple: test needs to return True or False. It might be safer to always implement it as: or you can use the built-in bool() to assure a Boolean value: For versions prior to 2.5, there's the trick: It can give wrong results when on_true   has a false boolean value.1 Although it does have the benefit of evaluating expressions left to right, which is clearer in my opinion. 1. Is there an equivalent of C’s ”?:” ternary operator?  <expression 1> if <condition> else <expression 2>  From the documentation: Conditional expressions (sometimes called a “ternary operator”) have the lowest priority of all Python operations. The expression x if C else y first evaluates the condition, C (not x); if C is true, x is evaluated and its value is returned; otherwise, y is evaluated and its value is returned. See PEP 308 for more details about conditional expressions. New since version 2.5. An operator for a conditional expression in Python was added in 2006 as part of Python Enhancement Proposal 308. Its form differ from common ?: operator and it's: which is equivalent to: Here is an example: Another syntax which can be used (compatible with versions before 2.5): where operands are lazily evaluated. Another way is by indexing a tuple (which isn't consistent with the conditional operator of most other languages): or explicitly constructed dictionary: Another (less reliable), but simpler method is to use and and or operators: however this won't work if x would be False. A possible workaround is to make x and y lists or tuples as in the following: or: If you're working with dictionaries, instead of using a ternary conditional, you can take advantage of get(key, default), for example: Source: ?: in Python at Wikipedia Unfortunately, the solution doesn't have short-circuit behaviour; thus both falseValue and trueValue are evaluated regardless of the condition. This could be suboptimal or even buggy (i.e. both trueValue and falseValue could be methods and have side-effects). One solution to this would be (execution delayed until the winner is known ;)), but it introduces inconsistency between callable and non-callable objects. In addition, it doesn't solve the case when using properties. And so the story goes - choosing between 3 mentioned solutions is a trade-off between having the short-circuit feature, using at least Зython 2.5 (IMHO not a problem anymore) and not being prone to "trueValue-evaluates-to-false" errors. Here I just try to show some important difference in ternary operator between a couple of programming languages. Ternary Operator in Javascript Ternary Operator in Ruby Ternary operator in Scala Ternary operator in R programming Ternary operator in Python For Python 2.5 and newer there is a specific syntax: In older Pythons a ternary operator is not implemented but it's possible to simulate it. Though, there is a potential problem, which if cond evaluates to True and on_true evaluates to False then on_false is returned instead of on_true. If you want this behavior the method is OK, otherwise use this: which can be wrapped by: and used this way: It is compatible with all Python versions. You might often find but this lead to problem when on_true == 0 where you would expect for a  normal ternary operator this result Yes. From the grammar file: The part of interest is: So, a ternary conditional operation is of the form: expression3 will be lazily evaluated (that is, evaluated only if expression2 is false in a boolean context). And because of the recursive definition, you can chain them indefinitely (though it may considered bad style.) Note that every if must be followed with an else. People learning list comprehensions and generator expressions may find this to be a difficult lesson to learn - the following will not work, as Python expects a third expression for an else: which raises a SyntaxError: invalid syntax. So the above is either an incomplete piece of logic (perhaps the user expects a no-op in the false condition) or what may be intended is to use expression2 as a filter - notes that the following is legal Python: expression2 works as a filter for the list comprehension, and is not a ternary conditional operator. You may find it somewhat painful to write the following: expression1 will have to be evaluated twice with the above usage. It can limit redundancy if it is simply a local variable. However, a common and performant Pythonic idiom for this use-case is to use or's shortcutting behavior: which is equivalent in semantics. Note that some style-guides may limit this usage on the grounds of clarity - it does pack a lot of meaning into very little syntax. Simulating the python ternary operator. For example output: Ternary conditional operator simply allows testing a condition in a single line replacing the multiline if-else making the code compact. [on_true] if [expression] else [on_false]  Above approach can be written as: Just memorize this pyramid if you have trouble remembering: you can do this :- Example:- This would print "odd" if the number is odd or "even" if the number is even. Note :- 0 , None , False , emptylist , emptyString evaluates as False. And any data other than 0 evaluates to True. if the condition [condition] becomes "True" then , expression_1 will be evaluated but not expression_2 . If we "and" something with 0 (zero) , the result will always to be fasle .So in the below statement , The expression exp won't be evaluated at all since "and" with 0 will always evaluate to zero and there is no need to evaluate the expression . This is how the compiler itself works , in all languages. In the expression exp won't be evaluated at all since "or" with 1 will always be 1. So it won't bother to evaluate the expression exp since the result will be 1 anyway . (compiler optimization methods). But in case of The second expression exp2 won't be evaluated since True and exp1 would be True when exp1 isn't false . Similarly in The expression exp1 won't be evaluated since False is equivalent to writing 0 and doing "and" with 0 would be 0 itself but after exp1 since "or" is used, it will evaluate the expression exp2 after "or" . Note:- This kind of branching using "or" and "and" can only be used when the expression_1 doesn't have a Truth value of False (or 0 or None or emptylist [ ] or emptystring ' '.) since if expression_1 becomes False , then the expression_2 will be evaluated because of the presence "or" between exp_1 and exp_2. In case you still want to make it work for all the cases regardless of what exp_1 and exp_2 truth values are, do this :- One of the alternatives to Python's conditional expression is the following: which has the following nice extension: The shortest alternative remains: but there is no alternative to if you want to avoid the evaluation of yes() and no(), because in both no() and yes() are evaluated. More a tip than an answer (don't need to repeat the obvious for the hundreth time), but I sometimes use it as a oneliner shortcut in such constructs: , becomes: Some (many :) may frown upon it as unpythonic (even, ruby-ish :), but I personally find it more natural - i.e. how you'd express it normally, plus a bit more visually appealing in large blocks of code. As already answered, yes there is a ternary operator in python: Additional information: If <expression 1> is the condition you can use Short-cirquit evaluation: PS: Of course, a Short-cirquit evaluation is not a ternary operator but often the ternary is used in cases where the short circuit would be enough. Many programming languages derived from C usually have the following syntax of ternary conditional operator: At first, the Python Benevolent Dictator For Life (I mean Guido van Rossum, of course) rejected it (as non-Pythonic style), since it's quite hard to understand for people not used to C language. Also, the colon sign : already has many uses in Python. After PEP 308 was approved, Python finally received its own shortcut conditional expression (what we use now): So, firstly it evaluates the condition. If it returns True, expression1 will be evaluated to give the result, otherwise expression2 will be evaluated. Due to Lazy Evaluation mechanics – only one expression will be executed. Here are some examples (conditions will be evaluated from left to right): Ternary operators can be chained in series:      The following one is the same as previous one: Hope this helps. YES, python have a ternary operator, here is the syntax and an example code to demonstrate the same :) Python has a ternary form for assignments; however there may be even a shorter form that people should be aware of. It's very common to need to assign to a variable one value or another depending on a condition. ^ This is the long form for doing such assignments. Below is the ternary form. But this isn't most succinct way - see last example. With Python, you can simply use or for alternative assignments. The above works since li1 is None and the interp treats that as False in logic expressions. The interp then moves on and evaluates the second expression, which is not None and it's not an empty list - so it gets assigned to a. This also works with empty lists. For instance, if you want to assign a whichever list has items. Knowing this, you can simply such assignments whenever you encounter them. This also works with strings and other iterables. You could assign a whichever string isn't empty. I always liked the C ternary syntax, but Python takes it a step further! I understand that some may say this isn't a good stylistic choice because it relies on mechanics that aren't immediately apparent to all developers. I personally disagree with that viewpoint. Python is a syntax rich language with lots of idiomatic tricks that aren't immediately apparent to the dabler. But the more you learn and understand the mechanics of the underlying system, the more you appreciate it. Other answers correctly talk about the Python ternary operator. I would like to complement by mentioning a scenario for which the ternary operator is often used but for which there is a better idiom. This is the scenario of using a default value. Suppose we want to use option_value with a default value if it is not set: or, if option_value is never set to a falsy value (0, "", etc), simply However, in this case an ever better solution is simply to write Vinko Vrsalovic's answer is good enough. There is only one more thing: Note that conditionals are an expression, not a statement. This means you can't use assignment statements or pass or other statements within a conditional expression After that walrus operator was introduced in Python 3.8, there is something changed. gives a = 3 and b is not defined, gives a is not defined and b = 5, and gives c = 5, a is not defined and b = 5. Even if this may be ugly, assignments can be done inside conditional expressions after Python 3.8. Anyway, it is still better to use normal if statement instead in this case. The ternary operator is a way of writing conditional statements in Python. As the name ternary suggests, this Python operator consists of three operands. Syntax: The three operands in a ternary operator include: condition: A boolean expression that evaluates to either true or false. true_val: A value to be assigned if the expression is evaluated to true. false_val: A value to be assigned if the expression is evaluated to false. The variable var on the left-hand side of the = (assignment) operator will be assigned: value1 if the booleanExpression evaluates to true. value2 if the booleanExpression evaluates to false. Example: A neat way to chain multiple operators: I find cumbersome the default python syntax val = a if cond else b, so sometimes I do this: Of course, it has the downside of always evaluating both sides (a and b), but the syntax it's way clearer to me if variable is defined and you want to check if it has value you can just a or b will output ** it can be nested as your need. best of luck **
__label__javascript __label__javascript-objects Say I create an object as follows: What is the best way to remove the property regex to end up with new myObject as follows? To remove a property from an object (mutating the object), you can do it like this: Demo   var myObject = {     "ircEvent": "PRIVMSG",     "method": "newURI",     "regex": "^http://.*" }; delete myObject.regex;  console.log(myObject);    For anyone interested in reading more about it, Stack Overflow user kangax has written an incredibly in-depth blog post about the delete statement on their blog, Understanding delete. It is highly recommended. If you'd like a new object with all the keys of the original except some, you could use the destructuring. Demo   let myObject = {   "ircEvent": "PRIVMSG",   "method": "newURI",   "regex": "^http://.*" };  const {regex, ...newObj} = myObject;  console.log(newObj);   // has no 'regex' key console.log(myObject); // remains unchanged    Objects in JavaScript can be thought of as maps between keys and values. The delete operator is used to remove these keys, more commonly known as object properties, one at a time.   var obj = {   myProperty: 1     } console.log(obj.hasOwnProperty('myProperty')) // true delete obj.myProperty console.log(obj.hasOwnProperty('myProperty')) // false    The delete operator does not directly free memory, and it differs from simply assigning the value of null or undefined to a property, in that the property itself is removed from the object. Note that if the value of a deleted property was a reference type (an object), and another part of your program still holds a reference to that object, then that object will, of course, not be garbage collected until all references to it have disappeared. delete will only work on properties whose descriptor marks them as configurable.   var myObject = {"ircEvent": "PRIVMSG", "method": "newURI", "regex": "^http://.*"};      delete myObject.regex;  console.log ( myObject.regex); // logs: undefined    This works in Firefox and Internet Explorer, and I think it works in all others. The delete operator is used to remove properties from objects. Note that, for arrays, this is not the same as removing an element. To remove an element from an array, use Array#splice or Array#pop. For example: delete in JavaScript has a different function to that of the keyword in C and C++: it does not directly free memory. Instead, its sole purpose is to remove properties from objects. For arrays, deleting a property corresponding to an index, creates a sparse array (ie. an array with a "hole" in it). Most browsers represent these missing array indices as "empty". Note that delete does not relocate array[3] into array[2]. Different built-in functions in JavaScript handle sparse arrays differently. for...in will skip the empty index completely.  A traditional for loop will return undefined for the value at the index. Any method using Symbol.iterator will return undefined for the value at the index. forEach, map and reduce will simply skip the missing index. So, the delete operator should not be used for the common use-case of removing elements from an array. Arrays have a dedicated methods for removing elements and reallocating memory: Array#splice() and Array#pop. Array#splice mutates the array, and returns any removed indices. deleteCount elements are removed from index start, and item1, item2... itemN are inserted into the array from index start. If deleteCount is omitted then elements from startIndex are removed to the end of the array. There is also a similarly named, but different, function on Array.prototype: Array#slice. Array#slice is non-destructive, and returns a new array containing the indicated indices from start to end. If end is left unspecified, it defaults to the end of the array. If end is positive, it specifies the zero-based non-inclusive index to stop at. If end is negative it, it specifies the index to stop at by counting back from the end of the array (eg. -1 will omit the final index). If end <= start, the result is an empty array. Array#pop removes the last element from an array, and returns that element. This operation changes the length of the array. Old question, modern answer. Using object destructuring, an ECMAScript 6 feature, it's as simple as: Or with the questions sample: You can see it in action in the Babel try-out editor. Edit: To reassign to the same variable, use a let: To whoever needs it... To complete @Koen answer in this thread, in case you want to remove dynamic variable using the spread syntax, you can do it like so:   const key = 'a';          const { [key]: foo, ...rest } = { a: 1, b: 2, c: 3 };  console.log(foo);  // 1 console.log(rest); // { b: 2, c: 3 }    * foo will be a new variable with the value of a (which is 1).  EXTENDED ANSWER 😇 There are few common ways to remove a property from an object.Each one has it's own pros and cons (check this performance comparison): Delete Operator Readable and short, however, it might not be the best choice if you are operating on a large number of objects as its performance is not optimized.  Reassignment More than 2X faster than delete, however the property is not deleted and can be iterated.  Spread Operator This ES6 operator allows us to return a brand new object, excluding any properties, without mutating the existing object. The downside is that it has the worse performance out of the above and not suggested to be used when you need to remove many properties at a time. Another alternative is to use the Underscore.js library.  Note that _.pick() and _.omit() both return a copy of the object and don't directly modify the original object. Assigning the result to the original object should do the trick (not shown). Reference: link _.pick(object, *keys) Return a copy of the object, filtered to only have values for the  whitelisted keys (or array of valid keys). Reference: link _.omit(object, *keys) Return a copy of the object, filtered to omit the  blacklisted keys (or array of keys). For arrays, _.filter() and _.reject() can be used in a similar manner.  The term you have used in your question title Remove a property from a JavaScript object, can be interpreted in some different ways. The one is to remove it for whole the memory and the list of object keys or the other is just to remove it from your object. As it has been mentioned in some other answers, the delete keyword is the main part. Let's say you have your object like: If you do: the result would be: You can delete that specific key from your object keys like: Then your objects key using Object.keys(myJSONObject) would be: But the point is if you care about memory and you want to whole the object gets removed from the memory, it is recommended to set it to null before you delete the key: The other important point here is to be careful about your other references to the same object. For instance, if you create a variable like: Or add it as a new pointer to another object like: Then even if you remove it from your object myJSONObject, that specific object won't get deleted from the memory, since the regex variable and myOtherObject["regex"] still have their values. Then how could we remove the object from the memory for sure? The answer would be to delete all the references you have in your code, pointed to that very object and also not use var statements to create new references to that object. This last point regarding var statements, is one of the most crucial issues that we are usually faced with, because using var statements would prevent the created object from getting removed. Which means in this case you won't be able to remove that object because you have created the regex variable via a var statement, and if you do: The result would be false, which means that your delete statement haven't been executed as you expected. But if you had not created that variable before, and you only had myOtherObject["regex"] as your last existing reference, you could have done this just by removing it like: In other words, a JavaScript object gets killed as soon as there is no reference left in your code pointed to that object. Update: Thanks to @AgentME: Setting a property to null before deleting it doesn't accomplish   anything (unless the object has been sealed by Object.seal and the   delete fails. That's not usually the case unless you specifically   try). To get more info on Object.seal: Object.seal() To clone object without property: For example: And we need to delete a. Usage Or ECMAScript 2015 (or ES6) came with built-in Reflect object. It is possible to delete object property by calling Reflect.deleteProperty() function with target object and property key as parameters: which is equivalent to: But if the property of the object is not configurable it cannot be deleted neither with deleteProperty function nor delete operator: Object.freeze() makes all properties of object not configurable (besides other things). deleteProperty function (as well as delete operator) returns false when tries to delete any of it's properties. If property is configurable it returns true, even if property does not exist. The difference between delete and deleteProperty is when using strict mode: Suppose you have an object that looks like this: If you want to use the entire staff array, the proper way to do this, would be to do this: Alternatively, you could also do this: Similarly, removing the entire students array would be done by calling delete Hogwarts.students; or delete Hogwarts['students'];. Now, if you want to remove a single staff member or student, the procedure is a bit different, because both properties are arrays themselves. If you know the index of your staff member, you could simply do this: If you do not know the index, you'll also have to do an index search: While you technically can use delete for an array, using it would result in getting incorrect results when calling for example Hogwarts.staff.length later on. In other words, delete would remove the element, but it wouldn't update the value of length property. Using delete would also mess up your indexing. So, when deleting values from an object, always first consider whether you're dealing with object properties or whether you're dealing with array values, and choose the appropriate strategy based on that. If you want to experiment with this, you can use this Fiddle as a starting point. I personally use Underscore.js or Lodash for object and array manipulation: Using delete method is the best way to do that, as per MDN description, the delete operator removes a property from an object. So you can simply write: The delete operator removes a given property from an object. On   successful deletion, it will return true, else false will be returned.   However, it is important to consider the following scenarios: If the property which you are trying to delete does not exist, delete   will not have any effect and will return true If a property with the same name exists on the object's prototype   chain, then, after deletion, the object will use the property from the   prototype chain (in other words, delete only has an effect on own   properties). Any property declared with var cannot be deleted from the global scope   or from a function's scope. As such, delete cannot delete any functions in the global scope (whether this is part from a function definition or a function  (expression).  Functions which are part of an object (apart from the   global scope) can be deleted with delete. Any property declared with let or const cannot be deleted from the scope  within which they were defined. Non-configurable properties cannot be removed. This includes properties of built-in objects like Math, Array, Object and properties that are created as non-configurable with methods like Object.defineProperty(). The following snippet gives another simple example:   var Employee = {       age: 28,       name: 'Alireza',       designation: 'developer'     }          console.log(delete Employee.name);   // returns true     console.log(delete Employee.age);    // returns true          // When trying to delete a property that does      // not exist, true is returned      console.log(delete Employee.salary); // returns true    For more info about and seeing more example, visit the link below: https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Operators/delete Another solution, using Array#reduce.   var myObject = {   "ircEvent": "PRIVMSG",   "method": "newURI",   "regex": "^http://.*" };  myObject = Object.keys(myObject).reduce(function(obj, key) {   if (key != "regex") {           //key you want to remove     obj[key] = myObject[key];   }   return obj; }, {});  console.log(myObject);    However, it will mutate the original object. If you want to create a new object without the specified key, just assign the reduce function to a new variable, e.g.:  (ES6)   const myObject = {   ircEvent: 'PRIVMSG',   method: 'newURI',   regex: '^http://.*', };  const myNewObject = Object.keys(myObject).reduce((obj, key) => {   key !== 'regex' ? obj[key] = myObject[key] : null;   return obj; }, {});  console.log(myNewObject);    There are a lot of good answers here but I just want to chime in that when using delete to remove a property in JavaScript, it is often wise to first check if that property exists to prevent errors. E.g Due to the dynamic nature of JavaScript there are often cases where you simply don't know if the property exists or not. Checking if obj exists before the && also makes sure you don't throw an error due to calling the hasOwnProperty() function on an undefined object. Sorry if this didn't add to your specific use case but I believe this to be a good design to adapt when managing objects and their properties. This post is very old and I find it very helpful so I decided to share the unset function I wrote in case someone else see this post and think why it's not so simple as it in PHP unset function. The reason for writing this new unset function, is to keep the index of all other variables in this hash_map. Look at the following example, and see how the index of "test2" did not change after removing a value from the hash_map.   function unset(unsetKey, unsetArr, resort) {   var tempArr = unsetArr;   var unsetArr = {};   delete tempArr[unsetKey];   if (resort) {     j = -1;   }   for (i in tempArr) {     if (typeof(tempArr[i]) !== 'undefined') {       if (resort) {         j++;       } else {         j = i;       }       unsetArr[j] = tempArr[i];     }   }   return unsetArr; }  var unsetArr = ['test', 'deletedString', 'test2'];  console.log(unset('1', unsetArr, true)); // output Object {0: "test", 1: "test2"} console.log(unset('1', unsetArr, false)); // output Object {0: "test", 2: "test2"}    Using ramda#dissoc you will get a new object without the attribute regex: You can also use other functions to achieve the same effect - omit, pick, ... Try the following method. Assign the Object property value to undefined. Then stringify the object and parse.    var myObject = {"ircEvent": "PRIVMSG", "method": "newURI", "regex": "^http://.*"};  myObject.regex = undefined; myObject = JSON.parse(JSON.stringify(myObject));  console.log(myObject);    If you want to delete a property deeply nested in the object then you can use the following recursive function with path to the property as the second argument: Example:  Dan's assertion that 'delete' is very slow and the benchmark he posted were doubted. So I carried out the test myself in Chrome 59. It does seem that 'delete' is about 30 times slower: Note that I purposedly carried out more than one 'delete' operations in one loop cycle to minimize the effect caused by the other operations. There are many different options presented on this page, not because most of the options are wrong—or because the answers are duplicates—but because the appropriate technique depends on the situation you're in and the goals of the tasks you and/or you team are trying to fulfill. To answer you question unequivocally, one needs to know: Once those four queries have been answered, there are essentially four categories of "property removal" in JavaScript to chose from in order to meet your goals. They are: This category is for operating on object literals or object instances when you want to retain/continue to use the original reference and aren't using stateless functional principles in your code. An example piece of syntax in this category: This category is the oldest, most straightforward & most widely supported category of property removal. It supports Symbol & array indexes in addition to strings and works in every version of JavaScript except for the very first release. However, it's mutative which violates some programming principles and has performance implications. It also can result in uncaught exceptions when used on non-configurable properties in strict mode. This category is for operating on plain object or array instances in newer ECMAScript flavors when a non-mutative approach is desired and you don't need to account for Symbol keys: This category is for operating on object literals or object instances when you want to retain/continue to use the original reference while guarding against exceptions being thrown on unconfigurable properties: In addition, while mutating objects in-place isn't stateless, you can use the functional nature of Reflect.deleteProperty to do partial application and other functional techniques that aren't possible with delete statements. This category is for operating on plain object or array instances in newer ECMAScript flavors when a non-mutative approach is desired and you don't need to account for Symbol keys: This category is generally allows for greater functional flexibility, including accounting for Symbols & omitting more than one property in one statement: Using lodash Using Ramda   const obj = {     "Filters":[         {             "FilterType":"between",             "Field":"BasicInformationRow.A0",             "MaxValue":"2017-10-01",             "MinValue":"2017-09-01",             "Value":"Filters value"         }     ] };  let new_obj1 = Object.assign({}, obj.Filters[0]); let new_obj2 = Object.assign({}, obj.Filters[0]);  /*  // old version  let shaped_obj1 = Object.keys(new_obj1).map(     (key, index) => {         switch (key) {             case "MaxValue":                 delete new_obj1["MaxValue"];                 break;             case "MinValue":                 delete new_obj1["MinValue"];                 break;         }         return new_obj1;     } )[0];   let shaped_obj2 = Object.keys(new_obj2).map(     (key, index) => {         if(key === "Value"){             delete new_obj2["Value"];         }         return new_obj2;     } )[0];   */   // new version!  let shaped_obj1 = Object.keys(new_obj1).forEach(     (key, index) => {         switch (key) {             case "MaxValue":                 delete new_obj1["MaxValue"];                 break;             case "MinValue":                 delete new_obj1["MinValue"];                 break;             default:                 break;         }     } );  let shaped_obj2 = Object.keys(new_obj2).forEach(     (key, index) => {         if(key === "Value"){             delete new_obj2["Value"];         }     } );    @johnstock, we can also use JavaScript's prototyping concept to add method to objects to delete any passed key available in calling object. Above answers are appreciated.   var myObject = {   "ircEvent": "PRIVMSG",   "method": "newURI",   "regex": "^http://.*" };  // 1st and direct way  delete myObject.regex; // delete myObject["regex"] console.log(myObject); // { ircEvent: 'PRIVMSG', method: 'newURI' }  // 2 way -  by using the concept of JavaScript's prototyping concept Object.prototype.removeFromObjectByKey = function(key) {   // If key exists, remove it and return true   if (this[key] !== undefined) {     delete this[key]     return true;   }   // Else return false   return false; }  var isRemoved = myObject.removeFromObjectByKey('method') console.log(myObject) // { ircEvent: 'PRIVMSG' }  // More examples var obj = {   a: 45,   b: 56,   c: 67 } console.log(obj) // { a: 45, b: 56, c: 67 }  // Remove key 'a' from obj isRemoved = obj.removeFromObjectByKey('a') console.log(isRemoved); //true console.log(obj); // { b: 56, c: 67 }  // Remove key 'd' from obj which doesn't exist var isRemoved = obj.removeFromObjectByKey('d') console.log(isRemoved); // false console.log(obj); // { b: 56, c: 67 }    You can use filter like below   var myObject = {     "ircEvent": "PRIVMSG",     "method": "newURI",     "regex": "^http://.*" };  // way 1  let filter1 = {}   Object.keys({...myObject}).filter(d => {   if(d !== 'regex'){     filter1[d] = myObject[d];   } })  console.log(filter1)  // way 2  let filter2 = Object.fromEntries(Object.entries({...myObject}).filter(d =>  d[0] !== 'regex' ))  console.log(filter2)    I have used lodash "unset" to make it happen for nested object also.. only this need to write small logic to get path of property key which expected by omit method.     var a = {"bool":{"must":[{"range":{"price_index.final_price":{"gt":"450","lt":"500"}}},{"bool":{"should":[{"term":{"color_value.keyword":"Black"}}]}}]}};  function getPathOfKey(object,key,currentPath, t){      var currentPath = currentPath || [];      for(var i in object){ 		if(i == key){         t = currentPath;       }       else if(typeof object[i] == "object"){         currentPath.push(i)        return getPathOfKey(object[i], key,currentPath)       }     } 	t.push(key);     return t; } document.getElementById("output").innerHTML =JSON.stringify(getPathOfKey(a,"price_index.final_price")) <div id="output">   </div>      var unset = require('lodash.unset'); unset(a,getPathOfKey(a,"price_index.final_price"));      let myObject = {     "ircEvent": "PRIVMSG",     "method": "newURI",     "regex": "^http://.*" };   obj = Object.fromEntries(     Object.entries(myObject).filter(function (m){         return m[0] != "regex"/*or whatever key to delete*/     } ))  console.log(obj)    You can also just treat the object like a2d array using Object.entries, and use splice to remove an element as you would in a normal array, or simply filter through the object, as one would an array, and assign the reconstructed object back to the original variable If you don't want to modify the original object. Remove a property without mutating the object If mutability is a concern, you can create a completely new object by copying all the properties from the old, except the one you want to remove.   let myObject = {   "ircEvent": "PRIVMSG",   "method": "newURI",   "regex": "^http://.*" };  let prop = 'regex'; const updatedObject = Object.keys(myObject).reduce((object, key) => {   if (key !== prop) {     object[key] = myObject[key]   }   return object }, {})  console.log(updatedObject);    Here's an ES6 way to remove the entry easily :   let myObject = {   "ircEvent": "PRIVMSG",   "method": "newURI",   "regex": "^http://.*" };  const removeItem = 'regex';  const { [removeItem]: remove, ...rest } = myObject;  console.log(remove); // "^http://.*" console.log(rest); // Object { ircEvent: "PRIVMSG", method: "newURI" }   
__label__python-class __label__python-datamodel __label__python __label__metaclass __label__oop In Python, what are metaclasses and what do we use them for? A metaclass is the class of a class. A class defines how an instance of the class (i.e. an object) behaves while a metaclass defines how a class behaves. A class is an instance of a metaclass. While in Python you can use arbitrary callables for metaclasses (like Jerub shows), the better approach is to make it an actual class itself. type is the usual metaclass in Python. type is itself a class, and it is its own type. You won't be able to recreate something like type purely in Python, but Python cheats a little. To create your own metaclass in Python you really just want to subclass type. A metaclass is most commonly used as a class-factory. When you create an object by calling the class, Python creates a new class (when it executes the 'class' statement) by calling the metaclass. Combined with the normal __init__ and __new__ methods, metaclasses therefore allow you to do 'extra things' when creating a class, like registering the new class with some registry or replace the class with something else entirely. When the class statement is executed, Python first executes the body of the class statement as a normal block of code. The resulting namespace (a dict) holds the attributes of the class-to-be. The metaclass is determined by looking at the baseclasses of the class-to-be (metaclasses are inherited), at the __metaclass__ attribute of the class-to-be (if any) or the __metaclass__ global variable. The metaclass is then called with the name, bases and attributes of the class to instantiate it. However, metaclasses actually define the type of a class, not just a factory for it, so you can do much more with them. You can, for instance, define normal methods on the metaclass. These metaclass-methods are like classmethods in that they can be called on the class without an instance, but they are also not like classmethods in that they cannot be called on an instance of the class. type.__subclasses__() is an example of a method on the type metaclass. You can also define the normal 'magic' methods, like __add__, __iter__ and __getattr__, to implement or change how the class behaves. Here's an aggregated example of the bits and pieces: Before understanding metaclasses, you need to master classes in Python. And Python has a very peculiar idea of what classes are, borrowed from the Smalltalk language. In most languages, classes are just pieces of code that describe how to produce an object. That's kinda true in Python too: But classes are more than that in Python. Classes are objects too. Yes, objects. As soon as you use the keyword class, Python executes it and creates an OBJECT. The instruction creates in memory an object with the name "ObjectCreator". This object (the class) is itself capable of creating objects (the instances), and this is why it's a class. But still, it's an object, and therefore: e.g.: Since classes are objects, you can create them on the fly, like any object. First, you can create a class in a function using class: But it's not so dynamic, since you still have to write the whole class yourself. Since classes are objects, they must be generated by something. When you use the class keyword, Python creates this object automatically. But as with most things in Python, it gives you a way to do it manually. Remember the function type? The good old function that lets you know what type an object is: Well, type has a completely different ability, it can also create classes on the fly. type can take the description of a class as parameters, and return a class. (I  know, it's silly that the same function can have two completely different uses according to the parameters you pass to it. It's an issue due to backward compatibility in Python) type works this way: Where: e.g.: can be created manually this way: You'll notice that we use "MyShinyClass" as the name of the class and as the variable to hold the class reference. They can be different, but there is no reason to complicate things. type accepts a dictionary to define the attributes of the class. So: Can be translated to: And used as a normal class: And of course, you can inherit from it, so: would be: Eventually, you'll want to add methods to your class. Just define a function with the proper signature and assign it as an attribute. And you can add even more methods after you dynamically create the class, just like adding methods to a normally created class object. You see where we are going: in Python, classes are objects, and you can create a class on the fly, dynamically. This is what Python does when you use the keyword class, and it does so by using a metaclass. Metaclasses are the 'stuff' that creates classes. You define classes in order to create objects, right? But we learned that Python classes are objects. Well, metaclasses are what create these objects. They are the classes' classes, you can picture them this way: You've seen that type lets you do something like this: It's because the function type is in fact a metaclass. type is the metaclass Python uses to create all classes behind the scenes. Now you wonder why the heck is it written in lowercase, and not Type? Well, I guess it's a matter of consistency with str, the class that creates strings objects, and int the class that creates integer objects. type is just the class that creates class objects. You see that by checking the __class__ attribute. Everything, and I mean everything, is an object in Python. That includes ints, strings, functions and classes. All of them are objects. And all of them have been created from a class: Now, what is the __class__ of any __class__ ? So, a metaclass is just the stuff that creates class objects. You can call it a 'class factory' if you wish. type is the built-in metaclass Python uses, but of course, you can create your own metaclass. In Python 2, you can add a __metaclass__ attribute when you write a class (see next section for the Python 3 syntax): If you do so, Python will use the metaclass to create the class Foo. Careful, it's tricky. You write class Foo(object) first, but the class object Foo is not created in memory yet. Python will look for __metaclass__ in the class definition. If it finds it, it will use it to create the object class Foo. If it doesn't, it will use type to create the class. Read that several times. When you do: Python does the following: Is there a __metaclass__ attribute in Foo? If yes, create in-memory a class object (I said a class object, stay with me here), with the name Foo by using what is in __metaclass__. If Python can't find __metaclass__, it will look for a __metaclass__ at the MODULE level, and try to do the same (but only for classes that don't inherit anything, basically old-style classes). Then if it can't find any __metaclass__ at all, it will use the Bar's (the first parent) own metaclass (which might be the default type) to create the class object. Be careful here that the __metaclass__ attribute will not be inherited, the metaclass of the parent (Bar.__class__) will be. If Bar used a __metaclass__ attribute that created Bar with type() (and not type.__new__()), the subclasses will not inherit that behavior. Now the big question is, what can you put in __metaclass__? The answer is something that can create a class. And what can create a class? type, or anything that subclasses or uses it. The syntax to set the metaclass has been changed in Python 3: i.e. the __metaclass__ attribute is no longer used, in favor of a keyword argument in the list of base classes. The behavior of metaclasses however stays largely the same. One thing added to metaclasses in Python 3 is that you can also pass attributes as keyword-arguments into a metaclass, like so: Read the section below for how python handles this. The main purpose of a metaclass is to change the class automatically, when it's created. You usually do this for APIs, where you want to create classes matching the current context. Imagine a stupid example, where you decide that all classes in your module should have their attributes written in uppercase. There are several ways to do this, but one way is to set __metaclass__ at the module level. This way, all classes of this module will be created using this metaclass, and we just have to tell the metaclass to turn all attributes to uppercase. Luckily, __metaclass__ can actually be any callable, it doesn't need to be a formal class (I know, something with 'class' in its name doesn't need to be a class, go figure... but it's helpful). So we will start with a simple example, by using a function. Let's check: Now, let's do exactly the same, but using a real class for a metaclass: Let's rewrite the above, but with shorter and more realistic variable names now that we know what they mean: You may have noticed the extra argument cls. There is nothing special about it: __new__ always receives the class it's defined in, as the first parameter. Just like you have self for ordinary methods which receive the instance as the first parameter, or the defining class for class methods. But this is not proper OOP. We are calling type directly and we aren't overriding or calling the parent's __new__. Let's do that instead: We can make it even cleaner by using super, which will ease inheritance (because yes, you can have metaclasses, inheriting from metaclasses, inheriting from type): Oh, and in python 3 if you do this call with keyword arguments, like this: It translates to this in the metaclass to use it: That's it. There is really nothing more about metaclasses. The reason behind the complexity of the code using metaclasses is not because of metaclasses, it's because you usually use metaclasses to do twisted stuff relying on introspection, manipulating inheritance, vars such as __dict__, etc. Indeed, metaclasses are especially useful to do black magic, and therefore complicated stuff. But by themselves, they are simple: Since __metaclass__ can accept any callable, why would you use a class since it's obviously more complicated? There are several reasons to do so: Now the big question. Why would you use some obscure error-prone feature? Well, usually you don't: Metaclasses are deeper magic that 99% of users should never worry about it. If you wonder whether you need them, you don't (the people who actually need them to know with certainty that they need them and don't need an explanation about why). Python Guru Tim Peters The main use case for a metaclass is creating an API. A typical example of this is the Django ORM. It allows you to define something like this: But if you do this: It won't return an IntegerField object. It will return an int, and can even take it directly from the database. This is possible because models.Model defines __metaclass__ and it uses some magic that will turn the Person you just defined with simple statements into a complex hook to a database field. Django makes something complex look simple by exposing a simple API and using metaclasses, recreating code from this API to do the real job behind the scenes. First, you know that classes are objects that can create instances. Well, in fact, classes are themselves instances. Of metaclasses. Everything is an object in Python, and they are all either instance of classes or instances of metaclasses. Except for type. type is actually its own metaclass. This is not something you could reproduce in pure Python, and is done by cheating a little bit at the implementation level. Secondly, metaclasses are complicated. You may not want to use them for very simple class alterations. You can change classes by using two different techniques: 99% of the time you need class alteration, you are better off using these. But 98% of the time, you don't need class alteration at all. Note, this answer is for Python 2.x as it was written in 2008, metaclasses are slightly different in 3.x. Metaclasses are the secret sauce that make 'class' work. The default metaclass for a new style object is called 'type'. Metaclasses take 3 args. 'name', 'bases' and 'dict' Here is where the secret starts. Look for where name, bases and the dict come from in this example class definition. Lets define a metaclass that will demonstrate how 'class:' calls it. And now, an example that actually means something, this will automatically make the variables in the list "attributes" set on the class, and set to None. Note that the magic behaviour that Initialised gains by having the metaclass init_attributes is not passed onto a subclass of Initialised. Here is an even more concrete example, showing how you can subclass 'type' to make a metaclass that performs an action when the class is created. This is quite tricky: Others have explained how metaclasses work and how they fit into the Python type system. Here's an example of what they can be used for. In a testing framework I wrote, I wanted to keep track of the order in which classes were defined, so that I could later instantiate them in this order. I found it easiest to do this using a metaclass. Anything that's a subclass of MyType then gets a class attribute _order that records the order in which the classes were defined. One use for metaclasses is adding new properties and methods to an instance automatically. For example, if you look at Django models, their definition looks a bit confusing. It looks as if you are only defining class properties: However, at runtime the Person objects are filled with all sorts of useful methods. See the source for some amazing metaclassery. I think the ONLamp introduction to metaclass programming is well written and gives a really good introduction to the topic despite being several years old already. http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html (archived at https://web.archive.org/web/20080206005253/http://www.onlamp.com/pub/a/python/2003/04/17/metaclasses.html) In short: A class is a blueprint for the creation of an instance, a metaclass is a blueprint for the creation of a class. It can be easily seen that in Python classes need to be first-class objects too to enable this behavior. I've never written one myself, but I think one of the nicest uses of metaclasses can be seen in the Django framework. The model classes use a metaclass approach to enable a declarative style of writing new models or form classes. While the metaclass is creating the class, all members get the possibility to customize the class itself. The thing that's left to say is: If you don't know what metaclasses are, the probability that you will not need them is 99%. TLDR: A metaclass instantiates and defines behavior for a class just like a class instantiates and defines behavior for an instance.  Pseudocode: The above should look familiar. Well, where does Class come from? It's an instance of a metaclass (also pseudocode): In real code, we can pass the default metaclass, type, everything we need to instantiate a class and we get a class: A class is to an instance as a metaclass is to a class.  When we instantiate an object, we get an instance: Likewise, when we define a class explicitly with the default metaclass, type, we instantiate it: Put another way, a class is an instance of a metaclass: Put a third way, a metaclass is a class's class. When you write a class definition and Python executes it, it uses a metaclass to instantiate the class object (which will, in turn, be used to instantiate instances of that class). Just as we can use class definitions to change how custom object instances behave, we can use a metaclass class definition to change the way a class object behaves. What can they be used for? From the docs: The potential uses for metaclasses are boundless. Some ideas that have been explored include logging, interface checking, automatic delegation, automatic property creation, proxies, frameworks, and automatic resource locking/synchronization. Nevertheless, it is usually encouraged for users to avoid using metaclasses unless absolutely necessary. When you write a class definition, for example, like this, You instantiate a class object. It is the same as functionally calling type with the appropriate arguments and assigning the result to a variable of that name: Note, some things automatically get added to the __dict__, i.e., the namespace: The metaclass of the object we created, in both cases, is type.  (A side-note on the contents of the class __dict__: __module__ is there because classes must know where they are defined, and  __dict__ and __weakref__ are there because we don't define __slots__ - if we define __slots__ we'll save a bit of space in the instances, as we can disallow __dict__ and __weakref__ by excluding them. For example: ... but I digress.) Here's the default __repr__ of classes: One of the most valuable things we can do by default in writing a Python object is to provide it with a good __repr__. When we call help(repr) we learn that there's a good test for a __repr__ that also requires a test for equality - obj == eval(repr(obj)). The following simple implementation of __repr__ and __eq__ for class instances of our type class provides us with a demonstration that may improve on the default __repr__ of classes: So now when we create an object with this metaclass, the __repr__ echoed on the command line provides a much less ugly sight than the default: With a nice __repr__ defined for the class instance, we have a stronger ability to debug our code. However, much further checking with eval(repr(Class)) is unlikely (as functions would be rather impossible to eval from their default __repr__'s). If, for example, we want to know in what order a class's methods are created in, we could provide an ordered dict as the namespace of the class. We would do this with __prepare__ which returns the namespace dict for the class if it is implemented in Python 3:  And usage: And now we have a record of the order in which these methods (and other class attributes) were created: Note, this example was adapted from the documentation - the new enum in the standard library does this. So what we did was instantiate a metaclass by creating a class. We can also treat the metaclass as we would any other class. It has a method resolution order: And it has approximately the correct repr (which we can no longer eval unless we can find a way to represent our functions.): Python 3 update There are (at this point) two key methods in a metaclass: __prepare__ lets you supply a custom mapping (such as an OrderedDict) to be used as the namespace while the class is being created.  You must return an instance of whatever namespace you choose.  If you don't implement __prepare__ a normal dict is used. __new__ is responsible for the actual creation/modification of the final class. A bare-bones, do-nothing-extra metaclass would like: A simple example: Say you want some simple validation code to run on your attributes -- like it must always be an int or a str.  Without a metaclass, your class would look something like: As you can see, you have to repeat the name of the attribute twice.  This makes typos possible along with irritating bugs. A simple metaclass can address that problem: This is what the metaclass would look like (not using __prepare__ since it is not needed): A sample run of: produces: Note:  This example is simple enough it could have also been accomplished with a class decorator, but presumably an actual metaclass would be doing much more. The 'ValidateType' class for reference: If you've done Python programming for more than a few months you'll eventually stumble upon code that looks like this: The latter is possible when you implement the __call__() magic method on the class. The __call__() method is invoked when an instance of a class is used as a callable. But as we've seen from previous answers a class itself is an instance of a metaclass, so when we use the class as a callable (i.e. when we create an instance of it) we're actually calling its metaclass' __call__() method. At this point most Python programmers are a bit confused because they've been told that when creating an instance like this instance = SomeClass() you're calling its __init__() method. Some who've dug a bit deeper know that before __init__() there's __new__(). Well, today another layer of truth is being revealed, before __new__() there's the metaclass' __call__(). Let's study the method call chain from specifically the perspective of creating an instance of a class. This is a metaclass that logs exactly the moment before an instance is created and the moment it's about to return it. This is a class that uses that metaclass And now let's create an instance of Class_1 Observe that the code above doesn't actually do anything more than logging the tasks. Each method delegates the actual work to its parent's implementation, thus keeping the default behavior. Since type is Meta_1's parent class (type being the default parent metaclass) and considering the ordering sequence of the output above, we now have a clue as to what would be the pseudo implementation of type.__call__(): We can see that the metaclass' __call__() method is the one that's called first. It then delegates creation of the instance to the class's __new__() method and initialization to the instance's __init__(). It's also the one that ultimately returns the instance. From the above it stems that the metaclass' __call__() is also given the opportunity to decide whether or not a call to Class_1.__new__() or Class_1.__init__() will eventually be made. Over the course of its execution it could actually return an object that hasn't been touched by either of these methods. Take for example this approach to the singleton pattern: Let's observe what happens when repeatedly trying to create an object of type Class_2 A metaclass is a class that tells how (some) other class should be created. This is a case where I saw metaclass as a solution to my problem: I had a really complicated problem, that probably could have been solved differently, but I chose to solve it using a metaclass.  Because of the complexity, it is one of the few modules I have written where the comments in the module surpass the amount of code that has been written.  Here it is... The type(obj) function gets you the type of an object.  The type() of a class is its metaclass. To use a metaclass: type is its own metaclass. The class of a class is a metaclass-- the body of a class is the arguments passed to the metaclass that is used to construct the class. Here you can read about how to use metaclasses to customize class construction. type is actually a metaclass -- a class that creates another classes. Most metaclass are the subclasses of type. The metaclass receives the new class as its first argument and provide access to class object with details as mentioned below: Note: Notice that the class was not instantiated at any time; the simple act of creating the class triggered execution of the metaclass. Python classes are themselves objects - as in instance - of their meta-class.  The default metaclass, which is applied when when you determine classes as: meta class are used to apply some rule to an entire set of classes. For example, suppose you're building an ORM to access a database, and you want records from each table to be of a class mapped to that table (based on fields, business rules, etc..,), a possible use of metaclass is for instance, connection pool logic, which is share by all classes of record from all tables. Another use is logic to to support foreign keys, which involves multiple classes of records.  when you define metaclass, you subclass type, and can overrided the following magic methods to insert your logic.  anyhow, those two are the most commonly used hooks. metaclassing is powerful, and above is nowhere near and exhaustive list of uses for metaclassing.  The type() function can return the type of an object or create a new type,  for example, we can create a Hi class with the type() function and do not  need to use this way with class Hi(object): In addition to using type() to create classes dynamically, you can control creation behavior of class and use metaclass. According to the Python object model, the class is the object, so the class must be an instance of another certain class. By default, a Python class is instance of the type class. That is, type is metaclass of most of the built-in classes and metaclass of user-defined classes. Magic will take effect when we passed keyword arguments in metaclass, it indicates the Python interpreter to create the CustomList through ListMetaclass. new (), at this point, we can modify the class definition, for example, and add a new method and then return the revised definition. In addition to the published answers I can say that a metaclass defines the behaviour for a class. So, you can explicitly set your metaclass. Whenever Python gets a keyword class then it starts searching for the metaclass. If it's not found – the default metaclass type is used to create the class's object. Using the __metaclass__ attribute, you can set metaclass of your class: It'll produce the output like this: And, of course, you can create your own metaclass to define the behaviour of any class that are created using your class. For doing that, your default metaclass type class must be inherited as this is the main metaclass: The output will be: In object-oriented programming, a metaclass is a class whose instances are classes. Just as an ordinary class defines the behavior of certain objects, a metaclass defines the behavior of certain class and their instances The term metaclass simply means something used to create classes. In other words, it is the class of a class. The metaclass is used to create the class so like the object being an instance of a class, a class is an instance of a metaclass. In python classes are also considered objects. Here's another example of what it can be used for: The metaclass is powerful, there are many things (such as monkey magic) you can do with it, but be careful this may only be known to you. Note that in python 3.6 a new dunder method __init_subclass__(cls, **kwargs) was introduced to replace a lot of common use cases for metaclasses. Is is called when a subclass of the defining class is created. See python docs. A class, in Python, is an object, and just like any other object, it is an instance of "something". This "something" is what is termed as a Metaclass. This metaclass is a special type of class that creates other class's objects. Hence, metaclass is responsible for making new classes. This allows the programmer to customize the way classes are generated. To create a metaclass, overriding of new() and init() methods is usually done. new() can be overridden to change the way objects are created, while init() can be overridden to change the way of initializing the object. Metaclass can be created by a number of ways. One of the ways is to use type() function. type() function, when called with 3 parameters, creates a metaclass. The parameters are :- Another way of creating a metaclass comprises of 'metaclass' keyword. Define the metaclass as a simple class. In the parameters of inherited class, pass metaclass=metaclass_name Metaclass can be specifically used in the following situations :- Defination: A metaclass is a class whose instances are classes. Like an "ordinary" class defines the behavior of the instances of the class, a metaclass defines the behavior of classes and their instances. Metaclasses are not supported by every object oriented programming language. Those programming language, which support metaclasses, considerably vary in way they implement them. Python is supporting them. Some programmers see metaclasses in Python as "solutions waiting or looking for a problem". There are numerous use cases for metaclasses. Defining Meta class: it will print the content of its arguments in the new method and returns the results of the type.new call: We will use the metaclass "LittleMeta" in the following example: Output: Metaclass is a kind of class which defines how the class will behave like or we can say that A class is itself an instance of a metaclass. 
__label__units-of-measurement __label__android __label__android-layout __label__user-interface __label__dimension What is the difference between Android units of measure? From the Android Developer Documentation: px > Pixels - corresponds to actual pixels on the screen. in > Inches - based on the physical size of the screen. > 1 Inch = 2.54 centimeters mm > Millimeters - based on the physical size of the screen. pt > Points - 1/72 of an inch based on the physical size of the screen. dp or dip > Density-independent Pixels - an abstract unit that is based on the physical density of the screen. These units are relative to a 160 dpi screen, so one dp is one pixel on a 160 dpi screen. The ratio of dp-to-pixel will change with the screen density, but not necessarily in direct proportion. Note: The compiler accepts both "dip" and "dp", though "dp" is more consistent with "sp". sp > Scaleable Pixels OR scale-independent pixels - this is like the dp unit, but it is also scaled by the user's font size preference. It is recommended you use this unit when specifying font sizes, so they will be adjusted for both the screen density and user's preference. Note, the Android documentation is inconsistent on what sp actually stands for, one doc says "scale-independent pixels", the other says "scaleable pixels". From Understanding Density Independence In Android: More info can be also be found in the Google Design Documentation. Pretty much everything about this and how to achieve the best support for multiple screens of different sizes and densities is very well documented here: Screen size   Actual physical size, measured as the screen's diagonal.   For simplicity, Android groups all actual screen sizes into four   generalized sizes: small, normal, large, and extra-large. Screen density    The number of pixels within a physical area of the   screen; usually referred to as dpi (dots per inch). For example, a   "low" density screen has fewer pixels within a given physical area,   compared to a "normal" or "high" density screen. For simplicity,   Android groups all actual screen densities into six generalized   densities: low, medium, high, extra-high, extra-extra-high, and   extra-extra-extra-high. OrientationThe orientation of the screen from the user's point of   view. This is either landscape or portrait, meaning that the screen's   aspect ratio is either wide or tall, respectively. Be aware that not   only do different devices operate in different orientations by   default, but the orientation can change at runtime when the user   rotates the device.  Resolution The total number of physical pixels on   a screen. When adding support for multiple screens, applications do   not work directly with resolution; applications should be concerned   only with screen size and density, as specified by the generalized   size and density groups.  Density-independent pixel (dp) A virtual   pixel unit that you should use when defining UI layout, to express   layout dimensions or position in a density-independent way.    The density-independent pixel is equivalent to one physical pixel on a 160   dpi screen, which is the baseline density assumed by the system for a   "medium" density screen. At runtime, the system transparently handles   any scaling of the dp units, as necessary, based on the actual density   of the screen in use. The conversion of dp units to screen pixels is   simple:    px = dp * (dpi / 160).    For example, on a 240 dpi screen, 1 dp   equals 1.5 physical pixels. You should always use dp units when   defining your application's UI, to ensure proper display of your UI on   screens with different densities. If you are any serious about developing an Android app for more than one type of device, you should have read the screens support development document at least once. In addition to that, it is always a good thing to know the actual number of active devices that have a particular screen configuration. I will elaborate more on how exactly does dp convert to px: The other way around: say, you want to add an image to your application and you need it to fill a 100 * 100 dp control. You'll need to create different size images for supported screen sizes: px - Pixels - point per scale corresponds to actual pixels on the screen. i - Inches - based on the physical size of the screen. mm - Millimeters - based on the physical size of the screen. pt - Points - 1/72 of an inch based on the physical size of the screen. dp - Density-independent Pixels - an abstract unit that is based on the physical density of the screen. These units are relative to a 160 dpi screen, so one dp is one pixel on a 160 dpi screen. The ratio of dp-to-pixel will change with the screen density, but not necessarily in direct proportion. Note: The compiler accepts both dip and dp, though dp is more consistent with sp. sp - scalable pixels - this is like the dp unit, but it is also scaled by the user's font size preference. It is recommended that you use this unit when specifying font sizes, so they will be adjusted for both the screen density and user's preference. Take the example of two screens that are the same size but one has a screen density of 160 dpi (dots per inch, i.e. pixels per inch) and the other is 240 dpi. Moreover you should have clear understanding about the following concepts: Screen size: Actual physical size, measured as the screen's diagonal. For simplicity, Android groups all actual screen sizes into   four generalized sizes: small, normal, large, and extra large. Screen density:  The quantity of pixels within a physical area of the screen; usually referred to as dpi (dots per inch). For example, a   "low" density screen has fewer pixels within a given physical area,   compared to a "normal" or "high" density screen. For simplicity,   Android groups all actual screen densities into four generalized   densities: low, medium, high, and extra high. Orientation:  The orientation of the screen from the user's point of view. This is either landscape or portrait, meaning that the   screen's aspect ratio is either wide or tall, respectively. Be aware   that not only do different devices operate in different orientations   by default, but the orientation can change at runtime when the user   rotates the device. Resolution: The total number of physical pixels on a screen. When adding support for multiple screens, applications do not work directly   with resolution; applications should be concerned only with screen   size and density, as specified by the generalized size and density   groups. Density-independent pixel (dp):  A virtual pixel unit that you should use when defining UI layout, to express layout dimensions or   position in a density-independent way. The density-independent pixel   is equivalent to one physical pixel on a 160 dpi screen, which is the   baseline density assumed by the system for a "medium" density screen.   At runtime, the system transparently handles any scaling of the dp   units, as necessary, based on the actual density of the screen in use.   The conversion of dp units to screen pixels is simple: px = dp * (dpi   / 160). For example, on a 240 dpi screen, 1 dp equals 1.5 physical   pixels. You should always use dp units when defining your   application's UI, to ensure proper display of your UI on screens with   different densities. Reference: Android developers site dp is  dip. Use it for everything (margin, padding, etc.). Use sp for {text-size} only.  See the difference between px, dp and sp on different screen sizes.  Source: Android Programming: The Big Nerd Ranch Guide px or dot is a pixel on the physical screen. dpi are pixels per inch on the physical screen and represent the density of the display. Android gives alias names to several densities dip or dp are density-indenpendant pixels, i.e. they correspond to more or less pixels depending on the physical density.  sp or sip is a scale-independant pixel. They are scaled when the Large Text option is turned on in Settings > Accessibility Use sp for Text size. Use dp for everything else. I have calculated the formula below to make the conversions dpi to dp and sp  Source 1 Source 2 Source 3: (data from source 3 is given below) These are dimension values defined in XML. A dimension is specified   with a number followed by a unit of measure. For example: 10px, 2in,   5sp. The following units of measure are supported by Android: dp Density-independent Pixels - An abstract unit that is based on the   physical density of the screen. These units are relative to a 160 dpi   (dots per inch) screen, on which 1dp is roughly equal to 1px. When   running on a higher density screen, the number of pixels used to draw   1dp is scaled up by a factor appropriate for the screen's dpi.   Likewise, when on a lower density screen, the number of pixels used   for 1dp is scaled down. The ratio of dp-to-pixel will change with the   screen density, but not necessarily in direct proportion. Using dp   units (instead of px units) is a simple solution to making the view   dimensions in your layout resize properly for different screen   densities. In other words, it provides consistency for the real-world   sizes of your UI elements across different devices. sp Scale-independent Pixels - This is like the dp unit, but it is also   scaled by the user's font size preference. It is recommended that you use   this unit when specifying font sizes, so they will be adjusted for   both the screen density and the user's preference. pt Points - 1/72 of an inch based on the physical size of the screen. px Pixels - Corresponds to actual pixels on the screen. This unit of   measure is not recommended because the actual representation can vary   across devices; each devices may have a different number of pixels per   inch and may have more or fewer total pixels available on the screen. mm Millimeters - Based on the physical size of the screen. in Inches - Based on the physical size of the screen. Note: A dimension is a simple resource that is referenced using the value provided in the name attribute (not the name of the XML file). As such, you can combine dimension resources with other simple resources in the one XML file, under one  element. Basically the only time where px applies is one px, and that's if you want exactly one pixel on the screen like in the case of a divider:  On >160 dpi, you may get 2-3 pixels,  On >120 dpi, it rounds to 0. px Pixels - corresponds to actual pixels on the screen. dp or dip Density-independent Pixels - an abstract unit that is based on the physical density of the screen. These units are relative to a 160 dpi screen, so one dp is one pixel on a 160 dpi screen. Use of dp: Density independence - Your application achieves “density independence” when it preserves the physical size (from the user’s point of view) of user interface elements when displayed on screens with different densities. (ie) The image should look the same size (not enlarged or shrinked) in different types of screens. sp Scale-independent Pixels - this is like the dp unit, but it is also scaled by the user's font size preference. http://developer.android.com/guide/topics/resources/more-resources.html#Dimension A virtual pixel unit that you should use when defining UI layout, to express layout dimensions or position in a density-independent way. As described above, the density-independent pixel is equivalent to one physical pixel on a 160 dpi screen, which is the baseline density assumed by the system for a "medium" density screen. At runtime, the system transparently handles any scaling of the dp units, as necessary, based on the actual density of the screen in use. The conversion of dp units to screen pixels is simple: px = dp * (dpi / 160). For example, on a 240 dpi screen, 1 dp equals 1.5 physical pixels. You should always use dp units when defining your application's UI, to ensure proper display of your UI on screens with different densities. Understanding pixel to dp and vice versa is very essential (especially for giving exact dp values to creative team) It is explained above. Try to avoid in layout files. But there are some cases, where px is required. for example, ListView divider line. px is better here for giving a one-pixel line as a divider for all across screen resolutions. Use sp for font sizes. Then only the font inside the application will change while device fonts size changes (that is, Display -> Fonts on Device). If you want to keep a static sized font inside the app, you can give the font dimension in dp. In such a case, it will never change. Developers may get such a requirement for some specific screens, for that, developers can use dp instead of sp. In all other cases, sp is recommended. You can see the difference between px and dp from the below picture, and you can also find that the px and dp could not guarantee the same physical sizes on the different screens.  Anything related with the size of text and appearance must use sp or pt. Whereas, anything related to the size of the controls, the layouts, etc. must be used with dp. You can use both dp and dip at its places. I would only use dp. There is a lot of talk about using "sp" for font sizes, and while I appreciate the point, I don't think that it is the right thing to do from a design point of view. You can end up breaking your design if the user has some wonky font size selection, and the user will end up blaming the app, and not their own life choices. Also, if you take an sp-font app on a 160 dpi tablet, you will find that everything scales up... but your font, which is going to look tiny in comparison. It isn't a good look. While the idea of "sp" fonts has a good heart, it is a poor idea. Stick with dp for everything. sp = scale independent pixel dp = dip = density independent pixels dpi = dots per inch We should avoid to use sp. We should use dp to support multiple screens. Android supports different screen resolutions An 120 dp ldpi device has 120 pixels in 1 inch size. The same for other densities... We as software engineers should use this conversion formula: pixel = dp * (density / 160) So 240 dpi device's 1 dp will have = 1 * (240/160) = 3/2 = 1.5 pixels. And 480 dpi device's 1 dp will have = 1 * (480/160) = 3 pixels. Using this 1.5 and 3 pixels knowledge, a software engineer can design layouts for different densities. To check screen parameters of any device: Difference between dp and sp units mentioned as "user's font size preference" by the answers copied from official documentation can be seen at run time by changing Settings->Accessibility->Large Text option. Large Text option forces text to become 1.3 times bigger. This might be well of course vendor dependent since it lies in packages/apps/Settings. dpi - px - pixel pt - points in - inch  - with respect to physical screen size(1 inch = 2.54 cm). mm- milimeter  - with respect to physical screen size. sp - scale-independent pixel. dip - In standard, dp and sp are used. sp for font size and dp for everything else. Formula for conversion of units: px = dp * ( dpi / 160 ); Please read the answer from community wiki. Below mentioned are some information to be considered in addition to the above answers. Most Android developers miss this while developing apps, so I am adding these points. sp = scale independent pixel dp = density independent pixels dpi = density pixels I have gone through the above answers...not finding them exactly correct. sp for text size, dp for layout bounds - standard. But sp for text size will break the layout if used carelessly in most of the devices.  sp take the textsize of the device, whereas dp take that of device density standard( never change in a device) Say 100sp text can occupies 80% of screen or 100% of screen depending on the font size set in device  You can use sp for layout bounds also, it will work :) No standard app use sp for whole text Use sp and dp for text size considering UX. Some people use huge FONT size in their phone for more readability, giving them small hardcoded sized text will be an UX issue. Put sp for text where necessary, but make sure it won't break the layout when user changes his settings. Similarly if you have a single app supporting all dimensions, adding xxxhdpi assets increases the app size a lot. But now xxxhdpi phones are common so we have to include xxxhdpi assets atleast for icons in side bar, toolbar and bottom bar. Its better to move to vector images to have a uniform and better quality images for all screen sizes. Also note that people use custom font in their phone. So lack of a font can cause problems regarding spacing and all. Say text size 12sp for a custom font may take some pixels extra than default font. Refer google developer site for screendensities and basedensity details for android. https://developer.android.com/training/multiscreen/screendensities Screen Size in Android is grouped into categories small, medium, large, extra large, double-extra and triple-extra. Screen density is the amount of pixels within an area (like inch) of the screen. Generally it is measured in dots-per-inch (dpi). Screen density is grouped as low, medium, high and extra high. Resolution is the total number of pixels in the screen. Formula for Conversion between Units dp to px in device Following example may help understand better. The scaling occurs based on bucket size of 120(ldpi), 160(mdpi), 240(hdpi), 320(xhdpi), 480(xxhdpi) and 640(xxxhdpi). The Google suggested ratio for designing is 3:4:6:8:12 for ldpi:mdpi:hdpi:xhdpi:xxhdpi A 150px X 150px image will occupy, You may use the following DPI calculator to fix your image sizes and other dimensions when you wish to have an uniform UI design in all Android devices. DPI Calculator in Java More Information refer following link. http://javapapers.com/android/difference-between-dp-dip-sp-px-in-mm-pt-in-android/ Here's the formula used by Android: px = dp * (dpi / 160) Where dpi is one of the following screen densities. For a list of all possible densities go here It defines the "DENSITY_*" constants. Taken from here.  This will sort out a lot of the confusion when translating between px and dp, if you know your screen dpi. So, let's say you want an image of 60 dp for an hdpi screen then the physical pixel size of 60 dp is: Normally sp is used for font sizes, while dip is used (also called dp) for others. I've come across a good article about designing Android apps UI for different screen resolutions, and I'd like to leave it here just for somebody searching in this area. Yes, I know that it's somehow described in Google docs (and mentioned in the posts above), I read that but it was not good for me (yeah, I may be too stupid)). It remained unclear for me how to design layouts capable to handle different screen size. I hate DP concept and so on, when I need to implement a "flexible" UI layout for different screens. (Hey iOS developers - yes, you're right it's Storyboard concept). Android has not bad UI concept, but lacks iOS Storyboard features, unfortunately. Designing flexible UI in Android is not easy thing (at the best). Here goes the article that helped me to understand what to do in Android to make layouts for different screen sizes:  JMSTUDIO Blog :- Decide Android App Screen Size  How to Design UI for Android Apps for Different Screen Size  To design an app UI for different screen sizes, our initial design has to   meet a minimum required space for each screen size. Android defines a   minimum size (in dp) for each generalized screen type. Here is an   Android screen size guideline.      When we get the screen size in dp, it is not enough for us to design   the Android app UI. For each screen size, we need to prepare graphics   and bitmap images for each density. Here is an Android screen density   guideline.    For easy calculation, we can follow the 3:4:6:8 scaling ratio between   the four generalized densities. If we create a 36×36 pixel picture for   ldpi device, the rest densities pictures size will be 48×48 for mdpi,   72×72 for hdpi, and 96×96 for xhdpi. How to Design Android Apps UI in Photoshop Many designers have problems for designing Android app UI in photoshop or other pixel   based graphic design tools because of density-independent unit, dp.   Designers don’t know how to map dp to pixel. Google also doesn’t give   a clear Android UI design guide for them, though they give a basic   formula for dp and pixel translation. As Android’s definition, 1pd equal to 1px under 160 dpi device (mdpi).   So we want to design an Android app for xlarge Android device with   mdpi density, we can define our UI size in pixel as 960 pixel in width   and 720px in height; Follow the same mapping rule, we can get   following Android App screen size UI design guideline:  ADDED: If you interested in "flexible" UI too, have a look at this library: An Android SDK that provides a new size unit - sdp (scalable dp). This size unit scales with the screen size (this also mentioned in an answer here, about SDP library) ADDED2 Google has finally understood usefulness of iOS Storeboard UI concept, and here goes ConstraintLayout for Android world: Build a Responsive UI with ConstraintLayout 1) dp: (density independent pixels) The number of pixels represented in one unit of dp will increase as the screen resolution increases (when you have more dots/pixels per inch). Conversely on devices with lower resolution, the number of pixels represented in on unit of dp will decrease. Since this is a relative unit, it needs to have a baseline to be compared with. This baseline is a 160 dpi screen. This is the equation: px = dp * (dpi / 160).  2) sp: (scale independent pixels) This unit scales according to the screen dpi (similar to dp) as well as the user’s font size preference.  3) px: (pixels)  Actual pixels or dots on the screen.  For more details you can visit  Android Developer Guide > Dimension Android Developer Guide > Screens  Screen size in Android is grouped into categories ldpi, mdpi, hdpi, xhdpi, xxhdpi and xxxhdpi. Screen density is the amount of pixels within an area (like inch) of the screen. Generally it is measured in dots-per-inch (dpi). PX(Pixels):  DP/DIP(Density pixels / Density independent pixels): dip == dp. In earlier Android versions dip was used and later changed to dp. This is alternative of px. Generally we never use px because it is absolute value. If you use px to set width or height, and if that application is being downloaded into different screen sized devices, then that view will not stretch as per the screen original size. dp is highly recommended to use in place of px. Use dp if you want to mention width and height to grow & shrink dynamically  based on screen sizes. if we give dp/dip, android will automatically calculate the pixel size on the basis of 160 pixel sized screen. SP(Scale independent pixels):  scaled based on user’s font size preference. Fonts should use sp. when mentioning the font sizes to fit for various screen sizes, use sp. This is similar to dp.Use sp especially for font sizes to grow & shrink dynamically based on screen sizes Android Documentation says: when specifying dimensions, always use either dp or sp units. A dp is   a density-independent pixel that corresponds to the physical size of a   pixel at 160 dpi. An sp is the same base unit, but is scaled by the   user's preferred text size (it’s a scale-independent pixel), so you   should use this measurement unit when defining text size The screen of a mobile phone is made up of thousands of tiny dots known as pixels (px). A pixel is the smallest element which goes to make the picture. The more the number of pixels to make a picture or wording, the sharper it becomes and makes the smartphone screen more easily readable. Screen resolution is measured in terms of number of pixels on the screen. Screen resolution is a commonly-used specification when buying a device, but it's actually not that useful when designing for Android because thinking of screens in terms of pixels ignores the notion of physical size, which for a touch device is really really important. Density independent pixel (dp or dip) allow the designer to create assets that appear in a expected way, no matter the resolution or density of target device. A density independent pixel (dp or dip) is equal to one pixel at the baseline density or 160 dpi (dots per inch). 1 px/1dp = 160 dpi/160 dpi 2 px/1dp = 320 dpi(2x)/160 dpi where, dpi is dots per inch So, at 320 dpi, 1 dp is equal to 2 px. Formula px/dp = dpi/160dpi Dots per inch (dpi) is a measure of the sharpness (that is, the density of illuminated points) on a display screen. The dots per inch for a given picture resolution will differ based on the overall screen size since the same number of pixels are being spread out over a different space. Working with density independent pixels help us to deal with a situation like where you have two devices with same pixel resolution, but differing amount of space. Suppose in a case, a tablet and phone has the same pixel resolution 1280 by 800 pixels (160 dpi) and 800 by 1280 pixels (320 dpi) respectively. Now because a tablet is at baseline density (160 dpi) its physical and density independent pixels sizes are the same, 1280 by 800. The phone on the other hand has a higher pixel density, so it has half as many density independent pixels as physical pixels. So a phone has 400 by 640 density independent pixels. So using a density-independent pixel makes it easier to mentally picture that tablet has much more space than the phone. Similarly, if you have two devices with similar screen size, but different pixel density, say one is 800 by 1280 pixels (320 dpi), and the other is 400 by 640 pixels (160 dpi), we don't need to define totally different layouts for these two devices as we can measure assets in terms of density independent pixel which is same for both devices. 800 by 1280 pixels (320dpi)=400 by 640 density independent pixel (dp) 400 by 640 pixels (160 dpi)=400 by 640 density independent pixel (dp) Scale independent pixels(sp) is the preferred unit for font size. For accessibility purposes, Android allows users to customize their device's font size. Users that have trouble reading text can increase their device's font size. You can normally find this option in the display setting on your phone or tablet under font size. It's often also available through the accessibility settings. With scale independent pixels, 16 sp is exactly the same as 16 dp when the device's font size is normal or 100%. But when device's font size is large, for example 125%, 16 sp will translate to 20 dp or 1.25 times 16. If you use dp as the unit for font size, then that piece of text has a specific physical size no matter if the user has customize device's font size. Using sp units will make a better experience for people with impaired eyesight. Reference: Udacity, Google Screen pixel density and resolution vary depending on the platform. Device-independent pixels and scalable pixels are units that provide a flexible way to accommodate a design across platforms. The number of pixels that fit into an inch is referred to as pixel density. High-density screens have more pixels per inch than low-density ones.... The number of pixels that fit into an inch is referred to as pixel density. High-density screens have more pixels per inch than low-density ones. As a result, UI elements of the same pixel dimensions appear larger on low-density screens, and smaller on high-density screens. To calculate screen density, you can use this equation: Screen density = Screen width (or height) in pixels / Screen width (or height) in inches Screen pixel density and resolution vary depending on the platform. Device-independent pixels and scalable pixels are units that provide a flexible way to accommodate a design across platforms. Calculating pixel density The number of pixels that fit into an inch is referred to as pixel density. High-density screens have more pixels per inch than low-density ones.... Density independence refers to the uniform display of UI elements on screens with different densities. Density-independent pixels, written as dp (pronounced “dips”), are flexible units that scale to have uniform dimensions on any screen. Material UIs use density-independent pixels to display elements consistently on screens with different densities.  Read full text https://material.io/design/layout/pixel-density.html sp: scale independent pixel You should use it with texts because it is automatically scaled according to the font size that is being used by the user in his device. px: pixel or picture element is the single point on the screen Pixels(px) – corresponds to actual pixels on the screen. This is used if you want to give in terms of absolute pixels for width or height. Density-independent Pixels (dp or dip) – an abstract unit that is based on the physical density of the screen. These units are relative to a 160 dpi screen, so one dp is one pixel on a 160 dpi screen. The ratio of dp-to-pixel will change with the screen density, but not necessarily in direct proportion. Note: The compiler accepts both “dip” and “dp”, though “dp” is more consistent with “sp”. Scale-independent Pixels(sp) – this is like the dp unit, but it is also scaled by the user’s font size preference. It is recommend you use this unit when specifying font sizes, so they will be adjusted for both the screen density and user’s preference. Always use dp and sp only. sp for font sizes and dp for everything else. It will make UI compatible for Android devices with different densities.  You can learn more about pixel and dp from   https://www.google.com/design/spec/layout/units-measurements.html#units-measurements-density-independent-pixels-dp- Source url:- http://www.androidtutorialshub.com/what-is-the-difference-between-px-dp-dip-sp-on-android/ I want to provide an easy way to understand dp. In fact, I think dp is the easiest one to understand. dp is just a physical length unit. It's of the same dimension as mm or inch. It's just convenient for us to write 50dp, 60dp rather than 50/160 inch or 60/160 inch, because one dp is just 1/160 inch whatever the screen size or resolution is.  The only problem is that, the android dpi of some screens are not accurate. For example, a screen classified to 160dpi may have 170dpi indeed. So the computation result of dp is fuzzy. It should be approximately the same as 1/160 inch.
__label__git __label__gitignore __label__git-rm There is a file that was being tracked by git, but now the file is on the .gitignore list. However, that file keeps showing up in git status after it's edited. How do you force git to completely forget about it? .gitignore will prevent untracked files from being added (without an add -f) to the set of files tracked by git, however git will continue to track any files that are already being tracked. To stop tracking a file you need to remove it from the index. This can be achieved with this command. If you want to remove a whole folder, you need to remove all files in it recursively. The removal of the file from the head revision will happen on the next commit. WARNING: While this will not remove the physical file from your local, it will remove the files from other developers machines on next git pull. The series of commands below will remove all of the items from the Git Index (not from the working directory or local repo), and then updates the Git Index, while respecting git ignores. PS. Index = Cache First: Then: Or one-liner: git update-index does the job for me: Note: This solution is actually independent on .gitignore as gitignore is only for untracked files. Since this answer was posted, a new option has been created and that should be preferred.  You should use --skip-worktree which is for modified tracked files that the user don't want to commit anymore and keep --assume-unchanged for performance to prevent git to check status of big tracked files. See https://stackoverflow.com/a/13631525/717372 for more details... This takes the list of the ignored files and removes them from the index, then commits the changes. I always use this command to remove those untracked files.  One-line, Unix-style, clean output: It lists all your ignored files, replace every output line with a quoted line instead to handle paths with spaces inside, and pass everything to git rm -r --cached to remove the paths/files/dirs from the index. move it out, commit, then move it back in. This has worked for me in the past. There is probably a 'gittier' way to accomplish this. If you cannot git rm a tracked file because other people might need it (warning, even if you git rm --cached, when someone else gets this change, their files will be deleted in their filesystem).  These are often done due to config file overrides, authentication credentials, etc. Please look at https://gist.github.com/1423106 for ways people have worked around the problem. To summarize: Source link: http://www.codeblocq.com/2016/01/Untrack-files-already-added-to-git-repository-based-on-gitignore/ Let’s say you have already added/committed some files to your git repository and you then add them to your .gitignore; these files will still be present in your repository index. This article we will see how to get rid of them. Before proceeding, make sure all your changes are committed, including your .gitignore file. To clear your repo, use: The rm command can be unforgiving. If you wish to try what it does beforehand, add the -n or --dry-run flag to test things out. Your repository is clean :) Push the changes to your remote to see the changes effective there as well. I accomplished this by using git filter-branch. The exact command I used was taken from the man page: WARNING: this will delete the file from your entire history This command will recreate the entire commit history, executing git rm before each commit and so will get rid of the specified file. Don't forget to back it up before running the command as it will be lost. (Under Linux), I wanted to use the posts here suggesting the ls-files --ignored --exclude-standard | xargs git rm -r --cached approach.  However, (some of) the files to be removed had an embedded newline/LF/\n in their names.  Neither of the solutions: cope with this situation (get errors about files not found). This uses the -z argument to ls-files, and the -0 argument to xargs to cater safely/correctly for "nasty" characters in filenames. In the manual page git-ls-files(1), it states: When -z option is not used, TAB, LF, and backslash characters in pathnames are represented as \t, \n, and \\, respectively. so I think my solution is needed if filenames have any of these characters in them. Update your .gitignore file – for instance, add a folder you don't want to track to .gitignore. git rm -r --cached . – Remove all tracked files, including wanted and unwanted. Your code will be safe as long as you have saved locally. git add . – All files will be added back in, except those in .gitignore. Hat tip to @AkiraYamamoto for pointing us in the right direction. Do the following steps serially,you will be fine. 1.remove the mistakenly added files from the directory/storage. You can use "rm -r"(for linux) command or delete them by browsing the directories. Or move them to another location on your PC.[You maybe need to close the IDE if running for moving/removing] 2.add the files / directories to gitignore file now and save it. 3.now remove them from git cache by using these commands (if there are more than one directory, remove them one by one by repeatedly issuing this command) 4.now do a commit and push, use these commands. This will remove those files from git remote and make git stop tracking those files. I think, that maybe git can't totally forget about file because of its conception (section "Snapshots, Not Differences"). This problem is absent, for example, when using CVS. CVS stores information as a list of file-based changes. Information for CVS is a set of files and the changes made to each file over time. But in Git every time you commit, or save the state of your project, it basically takes a picture of what all your files look like at that moment and stores a reference to that snapshot. So, if you added file once, it will always be present  in that snapshot. These 2 articles were helpful for me: git assume-unchanged vs skip-worktree  and How to ignore changes in tracked files with Git Basing on it I do the following, if file is already tracked: From this moment all local changes in this file will be ignored and will not go to remote. If file is changed on remote, conflict will occure, when git pull. Stash won't work. To resolve it, copy file content to the safe place and follow these steps: File content will be replaced by the remote content. Paste your changes from safe place to file and perform again: If everyone, who works with project, will perform git update-index --skip-worktree <file>, problems with pull should be absent. This solution is OK for configurations files, when every developer has their own project configuration. It is not very convenient to do this every time, when file has been changed on remote, but can protect it from overwriting by remote content. The copy/paste answer is git rm --cached -r .; git add .; git status This command will ignore the files that have already been committed to a Git repository but now we have added them to .gitignore. The answer from Matt Fear was the most effective IMHO. The following is just a PowerShell script for those in windows to only remove files from their git repo that matches their exclusion list. Move or copy the file to a safe location, so you don't lose it. Then git rm the file and commit. The file will still show up if you revert to one of those earlier commits, or another branch where it has not been removed. However, in all future commits, you will not see the file again. If the file is in the git ignore, then you can move it back into the folder, and git won't see it. Using the git rm --cached command does not answer the original question: How do you force git to completely forget about [a file]? In fact, this solution will cause the file to be deleted in every other instance of the repository when executing a git pull! The correct way to force git to forget about a file is documented by GitHub here. I recommend reading the documentation, but basically: just replace full/path/to/file with the full path of the file. Make sure you've added the file to your .gitignore. You'll also need to (temporarily) allow non-fast-forward pushes to your repository, since you're changing your git history. Do the following steps for file/folder: Remove File: For example: I want to delete test.txt file. I accidentally pushed to GitHub want to remove commands will be followed as: 1st add test.txt in .gitignore Remove Folder: For example: I want to delete the .idea folder/dir. I accidentally pushed to GitHub want to remove commands will be followed as: 1st add .idea in .gitignore The BFG is specifically designed for removing unwanted data like big files or passwords from Git repos, so it has a simple flag that will remove any large historical (not-in-your-current-commit) files: '--strip-blobs-bigger-than' If you'd like to specify files by name, you can do that too: The BFG is 10-1000x faster than git filter-branch, and generally much easier to use - check the full usage instructions and examples for more details. Source: https://confluence.atlassian.com/bitbucket/reduce-repository-size-321848262.html If you don't want to use the CLI and are working on Windows, a very simple solution is to use TortoiseGit, it has the "Delete (keep local)" Action in the menu which works fine. I liked JonBrave's answer but I have messy enough working directories that commit -a scares me a bit, so here's what I've done: git config --global alias.exclude-ignored '!git ls-files -z --ignored --exclude-standard | xargs -0 git rm -r --cached &&  git ls-files -z --ignored --exclude-standard | xargs -0 git stage &&  git stage .gitignore && git commit -m "new gitignore and remove ignored files from index"' breaking it down: This is no longer an issue in the latest git (v2.17.1 at the time of writing). The .gitignore finally ignores tracked-but-deleted files. You can test this for yourself by running the following script. The final git status statement should report "nothing to commit". The accepted answer does not "make Git "forget" about a file..." (historically).  It only makes git ignore the file in the present/future. This method makes git completely forget ignored files (past/present/future), but does not delete anything from working directory (even when re-pulled from remote). This method requires usage of /.git/info/exclude (preferred) OR a pre-existing .gitignore in all the commits that have files to be ignored/forgotten. 1 All methods of enforcing git ignore behavior after-the-fact effectively re-write history and thus have significant ramifications for any public/shared/collaborative repos that might be pulled after this process. 2 General advice: start with a clean repo - everything committed, nothing pending in working directory or index, and make a backup! Also, the comments/revision history of this answer (and revision history of this question) may be useful/enlightening. Finally, follow the rest of this GitHub guide (starting at step 6) which includes important warnings/information about the commands below. Other devs that pull from now-modified remote repo should make a backup and then: 1 Because /.git/info/exclude can be applied to all historical commits using the instructions above, perhaps details about getting a .gitignore file into the historical commit(s) that need it is beyond the scope of this answer.  I wanted a proper .gitignore to be in the root commit, as if it was the first thing I did.  Others may not care since /.git/info/exclude can accomplish the same thing regardless where the .gitignore exists in the commit history, and clearly re-writing history is a very touchy subject, even when aware of the ramifications. FWIW, potential methods may include git rebase or a git filter-branch that copies an external .gitignore into each commit, like the answers to this question 2 Enforcing git ignore behavior after-the-fact by committing the results of a standalone git rm --cached command may result in newly-ignored file deletion in future pulls from the force-pushed remote. The --prune-empty flag in the following git filter-branch command avoids this problem by automatically removing the previous "delete all ignored files" index-only commit.  Re-writing git history also changes commit hashes, which will wreak havoc on future pulls from public/shared/collaborative repos.  Please understand the ramifications fully before doing this to such a repo. This GitHub guide specifies the following: Tell your collaborators to rebase, not merge, any branches they created off of your old (tainted) repository history. One merge commit could reintroduce some or all of the tainted history that you just went to the trouble of purging. Alternative solutions that do not affect the remote repo are git update-index --assume-unchanged </path/file> or git update-index --skip-worktree <file>, examples of which can be found here. In case of already committed DS_Store: Ignore them by: Finally, make a commit! Especially for the IDE based files, I use this: For instance the slnx.sqlite, I just got rid off it completely like following: Just keep that in mind that some of those files stores some local user settings and preferences for projects (like what files you had open). So every time you navigate or do some changes in your IDE, that file is changed and therefore it checks it out and show as there are uncommitted changes. If anyone having hard time on Windows and you wanna ignore entire folder, 'cd' to desired the 'folder' and do 'Git Bash Here'. In my case here, I had several .lock files in several directories that I needed to remove. I ran the following and it worked without having to go into each directory to remove them: Doing this went into each folder under the 'root' of where I was at and excluded all files that matched the pattern. Hope this helps others!
__label__file-exists __label__file __label__python How do I check if a file exists or not, without using the try statement? If the reason you're checking is so you can do something like if file_exists: open_it(), it's safer to use a try around the attempt to open it. Checking and then opening risks the file being deleted or moved or something between when you check and when you try to open it. If you're not planning to open the file immediately, you can use os.path.isfile Return True if path is an existing regular file. This follows symbolic links, so both islink() and isfile() can be true for the same path. if you need to be sure it's a file. Starting with Python 3.4, the pathlib module offers an object-oriented approach (backported to pathlib2 in Python 2.7): To check a directory, do: To check whether a Path object exists independently of whether is it a file or directory, use exists(): You can also use resolve(strict=True) in a try block: You have the os.path.exists function: This returns True for both files and directories but you can instead use to test if it's a file specifically. It follows symlinks. Unlike isfile(), exists() will return True for directories. So depending on if you want only plain files or also directories, you'll use isfile() or exists(). Here is some simple REPL output:  Use os.path.isfile() with os.access():  Although almost every possible way has been listed in (at least one of) the existing answers (e.g. Python 3.4 specific stuff was added), I'll try to group everything together. Note: every piece of Python standard library code that I'm going to post, belongs to version 3.5.3. Problem statement: Possible solutions: [Python 3]: os.path.exists(path) (also check other function family members like os.path.isfile, os.path.isdir, os.path.lexists for slightly different behaviors) Return True if path refers to an existing path or an open file descriptor. Returns False for broken symbolic links. On some platforms, this function may return False if permission is not granted to execute os.stat() on the requested file, even if the path physically exists. All good, but if following the import tree: os.path - posixpath.py (ntpath.py) genericpath.py, line ~#20+ it's just a try / except block around [Python 3]: os.stat(path, *, dir_fd=None, follow_symlinks=True). So, your code is try / except free, but lower in the framestack there's (at least) one such block. This also applies to other funcs (including os.path.isfile). 1.1. [Python 3]: Path.is_file() Under the hood, it does exactly the same thing (pathlib.py, line ~#1330): [Python 3]: With Statement Context Managers. Either: Create one: And its usage - I'll replicate the os.path.isfile behavior (note that this is just for demonstrating purposes, do not attempt to write such code for production): Use [Python 3]: contextlib.suppress(*exceptions) - which was specifically designed for selectively suppressing exceptions But, they seem to be wrappers over try / except / else / finally blocks, as [Python 3]: The with statement states: This allows common try...except...finally usage patterns to be encapsulated for convenient reuse. Filesystem traversal functions (and search the results for matching item(s)) [Python 3]: os.listdir(path='.') (or [Python 3]: os.scandir(path='.') on Python v3.5+, backport: [PyPI]: scandir) Under the hood, both use: via [GitHub]: python/cpython - (master) cpython/Modules/posixmodule.c Using scandir() instead of listdir() can significantly increase the performance of code that also needs file type or file attribute information, because os.DirEntry objects expose this information if the operating system provides it when scanning a directory. All os.DirEntry methods may perform a system call, but is_dir() and is_file() usually only require a system call for symbolic links; os.DirEntry.stat() always requires a system call on Unix but only requires one for symbolic links on Windows. Since these iterate over folders, (in most of the cases) they are inefficient for our problem (there are exceptions, like non wildcarded globbing - as @ShadowRanger pointed out), so I'm not going to insist on them. Not to mention that in some cases, filename processing might be required. [Python 3]: os.access(path, mode, *, dir_fd=None, effective_ids=False, follow_symlinks=True) whose behavior is close to os.path.exists (actually it's wider, mainly because of the 2nd argument) ...test if the invoking user has the specified access to path. mode should be F_OK to test the existence of path... os.access("/tmp", os.F_OK) Since I also work in C, I use this method as well because under the hood, it calls native APIs (again, via "${PYTHON_SRC_DIR}/Modules/posixmodule.c"), but it also opens a gate for possible user errors, and it's not as Pythonic as other variants. So, as @AaronHall rightly pointed out, don't use it unless you know what you're doing: Note: calling native APIs is also possible via [Python 3]: ctypes - A foreign function library for Python, but in most cases it's more complicated. (Win specific): Since vcruntime* (msvcr*) .dll exports a [MS.Docs]: _access, _waccess function family as well, here's an example: Notes: The Lnx (Ubtu (16 x64)) counterpart as well: Notes: Instead hardcoding libc's path ("/lib/x86_64-linux-gnu/libc.so.6") which may (and most likely, will) vary across systems, None (or the empty string) can be passed to CDLL constructor (ctypes.CDLL(None).access(b"/tmp", os.F_OK)). According to [man7]: DLOPEN(3): If filename is NULL, then the returned handle is for the main   program.  When given to dlsym(), this handle causes a search for a   symbol in the main program, followed by all shared objects loaded at   program startup, and then all shared objects loaded by dlopen() with   the flag RTLD_GLOBAL. Install some third-party module with filesystem capabilities Most likely, will rely on one of the ways above (maybe with slight customizations). One example would be (again, Win specific) [GitHub]: mhammond/pywin32 - Python for Windows (pywin32) Extensions, which is a Python wrapper over WINAPIs. But, since this is more like a workaround, I'm stopping here. Another (lame) workaround (gainarie) is (as I like to call it,) the sysadmin approach: use Python as a wrapper to execute shell commands Win: Nix (Lnx (Ubtu)): Bottom line: Final note(s): Python 3.4+ has an object-oriented path module: pathlib.  Using this new module, you can check whether a file exists like this: You can (and usually should) still use a try/except block when opening files: The pathlib module has lots of cool stuff in it: convenient globbing, checking file's owner, easier path joining, etc.  It's worth checking out.  If you're on an older Python (version 2.6 or later), you can still install pathlib with pip: Then import it as follows: This is the simplest way to check if a file exists. Just because the file existed when you checked doesn't guarantee that it will be there when you need to open it. Prefer the try statement. It's considered better style and avoids race conditions. Don't take my word for it. There's plenty of support for this theory. Here's a couple: Now available since Python 3.4, import and instantiate a Path object with the file name, and check the is_file method (note that this returns True for symlinks pointing to regular files as well): If you're on Python 2, you can backport the pathlib module from pypi, pathlib2, or otherwise check isfile from the os.path module: Now the above is probably the best pragmatic direct answer here, but there's the possibility of a race condition (depending on what you're trying to accomplish), and the fact that the underlying implementation uses a try, but Python uses try everywhere in its implementation.  Because Python uses try everywhere, there's really no reason to avoid an implementation that uses it. But the rest of this answer attempts to consider these caveats. Available since Python 3.4, use the new Path object in pathlib. Note that .exists is not quite right, because directories are not files (except in the unix sense that everything is a file). So we need to use is_file: Here's the help on is_file: So let's get a file that we know is a file: By default, NamedTemporaryFile deletes the file when closed (and will automatically close when no more references exist to it). If you dig into the implementation, though, you'll see that is_file uses try: We like try because it avoids race conditions. With try, you simply attempt to read your file, expecting it to be there, and if not, you catch the exception and perform whatever fallback behavior makes sense. If you want to check that a file exists before you attempt to read it, and you might be deleting it and then you might be using multiple threads or processes, or another program knows about that file and could delete it - you risk the chance of a race condition if you check it exists, because you are then racing to open it before its condition (its existence) changes.  Race conditions are very hard to debug because there's a very small window in which they can cause your program to fail. But if this is your motivation, you can get the value of a try statement by using the suppress context manager. Python 3.4 gives us the suppress context manager (previously the ignore context manager), which does semantically exactly the same thing in fewer lines, while also (at least superficially) meeting the original ask to avoid a try statement: Usage: For earlier Pythons, you could roll your own suppress, but without a try will be more verbose than with. I do believe this actually is the only answer that doesn't use try at any level in the Python that can be applied to prior to Python 3.4 because it uses a context manager instead: Perhaps easier with a try: isfile from the docs: os.path.isfile(path) Return True if path is an existing regular file. This follows symbolic   links, so both islink() and isfile() can be true for the same path. But if you examine the source of this function, you'll see it actually does use a try statement: All it's doing is using the given path to see if it can get stats on it,  catching OSError and then checking if it's a file if it didn't raise the exception. If you intend to do something with the file, I would suggest directly attempting it with a try-except to avoid a race condition: os.access Available for Unix and Windows is os.access, but to use you must pass flags, and it does not differentiate between files and directories. This is more used to test if the real invoking user has access in an elevated privilege environment: It also suffers from the same race condition problems as isfile. From the docs: Note:   Using access() to check if a user is authorized to e.g. open a file   before actually doing so using open() creates a security hole, because   the user might exploit the short time interval between checking and   opening the file to manipulate it. It’s preferable to use EAFP   techniques. For example: is better written as: Avoid using os.access. It is a low level function that has more opportunities for user error than the higher level objects and functions discussed above. Another answer says this about os.access: Personally, I prefer this one because under the hood, it calls native APIs (via "${PYTHON_SRC_DIR}/Modules/posixmodule.c"), but it also opens a gate for possible user errors, and it's not as Pythonic as other variants: This answer says it prefers a non-Pythonic, error-prone method, with no justification. It seems to encourage users to use low-level APIs without understanding them.  It also creates a context manager which, by unconditionally returning True, allows all Exceptions (including KeyboardInterrupt and SystemExit!) to pass silently, which is a good way to hide bugs. This seems to encourage users to adopt poor practices. Importing os makes it easier to navigate and perform standard actions with your operating system.  For reference also see How to check whether a file exists using Python? If you need high-level operations, use shutil. Testing for files and folders with os.path.isfile(), os.path.isdir() and os.path.exists() Assuming that the "path" is a valid path, this table shows what is returned by each function for files and folders:  You can also test if a file is a certain type of file using os.path.splitext() to get the extension (if you don't already know it) In 2016 the best way is still using os.path.isfile: Or in Python 3 you can use pathlib: It doesn't seem like there's a meaningful functional difference between try/except and isfile(), so you should use which one makes sense. If you want to read a file, if it exists, do But if you just wanted to rename a file if it exists, and therefore don't need to open it, do If you want to write to a file, if it doesn't exist, do If you need file locking, that's a different matter. You could try this (safer): The ouput would be: ([Errno 2] No such file or directory:   'whatever.txt') Then, depending on the result, your program can just keep running from there or you can code to stop it if you want. Date:2017-12-04 Every possible solution has been listed in other answers. An intuitive and arguable way to check if a file exists is the following: I made an exhaustive cheatsheet for your reference: Although I always recommend using try and except statements, here are a few possibilities for you (my personal favourite is using os.access): Try opening the file: Opening the file will always verify the existence of the file. You can make a function just like so: If it's False, it will stop execution with an unhanded IOError or OSError in later versions of Python. To catch the exception, you have to use a try except clause. Of course, you can always use a try except` statement like so (thanks to hsandt for making me think): Use os.path.exists(path): This will check the existence of what you specify. However, it checks for files and directories so beware about how you use it. Use os.access(path, mode): This will check whether you have access to the file. It will check for permissions. Based on the os.py documentation, typing in os.F_OK, it will check the existence of the path. However, using this will create a security hole, as someone can attack your file using the time between checking the permissions and opening the file. You should instead go directly to opening the file instead of checking its permissions. (EAFP vs LBYP). If you're not going to open the file afterwards, and only checking its existence, then you can use this. Anyway, here: I should also mention that there are two ways that you will not be able to verify the existence of a file. Either the issue will be permission denied or no such file or directory. If you catch an IOError, set the IOError as e (like my first option), and then type in print(e.args) so that you can hopefully determine your issue. I hope it helps! :) Additionally, os.access(): Being R_OK, W_OK, and X_OK the flags to test for permissions (doc). If the file is for opening you could use one of the following techniques: UPDATE Just to avoid confusion and based on the answers I got, current answer finds either a file or a directory with the given name. Raising exceptions is considered to be an acceptable, and Pythonic,   approach for flow control in your program. Consider handling missing   files with IOErrors. In this situation, an IOError exception will be   raised if the file exists but the user does not have read permissions. SRC: http://www.pfinn.net/python-check-if-file-exists.html If you imported NumPy already for other purposes then there is no need to import other libraries like pathlib, os, paths, etc. This will return true or false based on its existence. You can write Brian's suggestion without the try:. suppress is part of Python 3.4. In older releases you can quickly write your own suppress: You can follow these three ways: Note1: The os.path.isfile used only for files Note2: The os.path.exists used for both files and directories The pathlib.Path method (included in Python 3+, installable with pip for Python 2) Adding one more slight variation which isn't exactly reflected in the other answers. This will handle the case of the file_path being None or empty string.  Adding a variant based on suggestion from Shahbaz  Adding a variant based on suggestion from Peter Wood  I'm the author of a package that's been around for about 10 years, and it has a function that addresses this question directly. Basically, if you are on a non-Windows system, it uses Popen to access find.  However, if you are on Windows, it replicates find with an efficient filesystem walker. The code itself does not use a try block… except in determining the operating system and thus steering you to the "Unix"-style find or the hand-buillt find. Timing tests showed that the try was faster in determining the OS, so I did use one there (but nowhere else). And the doc… The implementation, if you care to look, is here: https://github.com/uqfoundation/pox/blob/89f90fb308f285ca7a62eabe2c38acb87e89dad9/pox/shutils.py#L190 Here's a 1 line Python command for the Linux command line environment. I find this VERY HANDY since I'm not such a hot Bash guy. I hope this is helpful. You can use the "OS" library of Python: How do I check whether a file exists, without using the try statement? In 2016, this is still arguably the easiest way to check if both a file exists and if it is a file: isfile is actually just a helper method that internally uses os.stat and stat.S_ISREG(mode) underneath. This os.stat is a lower-level method that will provide you with detailed information about files, directories, sockets, buffers, and more. More about os.stat here Note: However, this approach will not lock the file in any way and therefore your code can become vulnerable to "time of check to time of use" (TOCTTOU) bugs. So raising exceptions is considered to be an acceptable, and Pythonic, approach for flow control in your program. And one should consider handling missing files with IOErrors, rather than if statements (just an advice). 
__label__javascript __label__ajax __label__jquery __label__asynchronous I have a function foo which makes an asynchronous request. How can I return the response/result from foo? I tried returning the value from the callback, as well as assigning the result to a local variable inside the function and returning that one, but none of those ways actually return the response (they all return undefined or whatever the initial value of the variable result is). Example using jQuery's ajax function: Example using node.js: Example using the then block of a promise: → For a more general explanation of async behaviour with different examples, please see Why is my variable unaltered after I modify it inside of a function? - Asynchronous code reference → If you already understand the problem, skip to the possible solutions below. The A in Ajax stands for asynchronous . That means sending the request (or rather receiving the response) is taken out of the normal execution flow. In your example, $.ajax returns immediately and the next statement, return result;, is executed before the function you passed as success callback was even called. Here is an analogy which hopefully makes the difference between synchronous and asynchronous flow clearer: Imagine you make a phone call to a friend and ask him to look something up for you. Although it might take a while, you wait on the phone and stare into space, until your friend gives you the answer that you needed. The same is happening when you make a function call containing "normal" code: Even though findItem might take a long time to execute, any code coming after var item = findItem(); has to wait until the function returns the result. You call your friend again for the same reason. But this time you tell him that you are in a hurry and he should call you back on your mobile phone. You hang up, leave the house, and do whatever you planned to do. Once your friend calls you back, you are dealing with the information he gave to you. That's exactly what's happening when you do an Ajax request. Instead of waiting for the response, the execution continues immediately and the statement after the Ajax call is executed. To get the response eventually, you provide a function to be called once the response was received, a callback (notice something? call back ?). Any statement coming after that call is executed before the callback is called. Embrace the asynchronous nature of JavaScript! While certain asynchronous operations provide synchronous counterparts (so does "Ajax"), it's generally discouraged to use them, especially in a browser context. Why is it bad do you ask? JavaScript runs in the UI thread of the browser and any long-running process will lock the UI, making it unresponsive. Additionally, there is an upper limit on the execution time for JavaScript and the browser will ask the user whether to continue the execution or not. All of this is a really bad user experience. The user won't be able to tell whether everything is working fine or not. Furthermore, the effect will be worse for users with a slow connection. In the following we will look at three different solutions that are all building on top of each other: All three are available in current browsers, and node 7+. The ECMAScript version released in 2017 introduced syntax-level support for asynchronous functions. With the help of async and await, you can write asynchronous in a "synchronous style". The code is still asynchronous, but it's easier to read/understand. async/await builds on top of promises: an async function always returns a promise. await "unwraps" a promise and either result in the value the promise was resolved with or throws an error if the promise was rejected. Important: You can only use await inside an async function. Right now, top-level await isn't yet supported, so you might have to make an async IIFE (Immediately Invoked Function Expression) to start an async context. You can read more about async and await on MDN. Here is an example that builds on top of delay above: Current browser and node versions support async/await. You can also support older environments by transforming your code to ES5 with the help of regenerator (or tools that use regenerator, such as Babel). A callback is when function 1 is passed to function 2. Function 2 can call function 1 whenever it is ready. In the context of an asynchronous process, the callback will be called whenever the asynchronous process is done. Usually, the result is passed to the callback. In the example of the question, you can make foo accept a callback and use it as success callback. So this becomes Here we defined the function "inline" but you can pass any function reference: foo itself is defined as follows: callback will refer to the function we pass to foo when we call it and we pass it on to success. I.e. once the Ajax request is successful, $.ajax will call callback and pass the response to the callback (which can be referred to with result, since this is how we defined the callback). You can also process the response before passing it to the callback: It's easier to write code using callbacks than it may seem. After all, JavaScript in the browser is heavily event-driven (DOM events). Receiving the Ajax response is nothing else but an event. Difficulties could arise when you have to work with third-party code, but most problems can be solved by just thinking through the application flow. The Promise API is a new feature of ECMAScript 6 (ES2015), but it has good browser support already. There are also many libraries which implement the standard Promises API and provide additional methods to ease the use and composition of asynchronous functions (e.g. bluebird). Promises are containers for future values. When the promise receives the value (it is resolved) or when it is canceled (rejected), it notifies all of its "listeners" who want to access this value. The advantage over plain callbacks is that they allow you to decouple your code and they are easier to compose. Here is an example of using a promise: Applied to our Ajax call we could use promises like this: Describing all the advantages that promise offer is beyond the scope of this answer, but if you write new code, you should seriously consider them. They provide a great abstraction and separation of your code. More information about promises: HTML5 rocks - JavaScript Promises Deferred objects are jQuery's custom implementation of promises (before the Promise API was standardized). They behave almost like promises but expose a slightly different API. Every Ajax method of jQuery already returns a "deferred object" (actually a promise of a deferred object) which you can just return from your function: Keep in mind that promises and deferred objects are just containers for a future value, they are not the value itself. For example, suppose you had the following: This code misunderstands the above asynchrony issues. Specifically, $.ajax() doesn't freeze the code while it checks the '/password' page on your server - it sends a request to the server and while it waits, it immediately returns a jQuery Ajax Deferred object, not the response from the server. That means the if statement is going to always get this Deferred object, treat it as true, and proceed as though the user is logged in. Not good. But the fix is easy: As I mentioned, some(!) asynchronous operations have synchronous counterparts. I don't advocate their use, but for completeness' sake, here is how you would perform a synchronous call: If you directly use a XMLHttpRequest object, pass false as third argument to .open. If you use jQuery, you can set the async option to false. Note that this option is deprecated since jQuery 1.8. You can then either still use a success callback or access the responseText property of the jqXHR object: If you use any other jQuery Ajax method, such as $.get, $.getJSON, etc., you have to change it to $.ajax (since you can only pass configuration parameters to $.ajax). Heads up! It is not possible to make a synchronous JSONP request. JSONP by its very nature is always asynchronous (one more reason to not even consider this option). Your code should be something along the lines of this: Felix Kling did a fine job writing an answer for people using jQuery for AJAX, I've decided to provide an alternative for people who aren't. (Note, for those using the new fetch API, Angular or promises I've added another answer below) This is a short summary of "Explanation of the problem" from the other answer, if you're not sure after reading this, read that. The A in AJAX stands for asynchronous. That means sending the request (or rather receiving the response) is taken out of the normal execution flow. In your example, .send returns immediately and the next statement, return result;, is executed before the function you passed as success callback was even called. This means when you're returning, the listener you've defined did not execute yet, which means the value you're returning has not been defined. Here is a simple analogy (Fiddle) The value of a returned is undefined since the a=5 part has not executed yet. AJAX acts like this, you're returning the value before the server got the chance to tell your browser what that value is. One possible solution to this problem is to code re-actively , telling your program what to do when the calculation completed. This is called CPS. Basically, we're passing getFive an action to perform when it completes, we're telling our code how to react when an event completes (like our AJAX call, or in this case the timeout). Usage would be: Which should alert "5" to the screen. (Fiddle). There are basically two ways how to solve this: As for synchronous AJAX, don't do it! Felix's answer raises some compelling arguments about why it's a bad idea. To sum it up, it'll freeze the user's browser until the server returns the response and create a very bad user experience. Here is another short summary taken from MDN on why: XMLHttpRequest supports both synchronous and asynchronous communications. In general, however, asynchronous requests should be preferred to synchronous requests for performance reasons. In short, synchronous requests block the execution of code... ...this can cause serious issues... If you have to do it, you can pass a flag: Here is how: Let your function accept a callback. In the example code foo can be made to accept a callback. We'll be telling our code how to react when foo completes. So: Becomes: Here we passed an anonymous function, but we could just as easily pass a reference to an existing function, making it look like: For more details on how this sort of callback design is done, check Felix's answer. Now, let's define foo itself to act accordingly (fiddle) We have now made our foo function accept an action to run when the AJAX completes successfully, we can extend this further by checking if the response status is not 200 and acting accordingly (create a fail handler and such). Effectively solving our issue. If you're still having a hard time understanding this read the AJAX getting started guide at MDN. XMLHttpRequest 2 (first of all read the answers from Benjamin Gruenbaum & Felix Kling) If you don't use jQuery and want a nice short XMLHttpRequest 2 which works on the modern browsers and also on the mobile browsers I suggest to use it this way: As you can see: There are two ways to get the response of this Ajax call (three using the XMLHttpRequest var name): The simplest: Or if for some reason you bind() the callback to a class: Example: Or (the above one is better anonymous functions are always a problem): Nothing easier. Now some people will probably say that it's better to use onreadystatechange or the even the XMLHttpRequest variable name. That's wrong. Check out XMLHttpRequest advanced features It supported all *modern browsers. And I can confirm as I'm using this approach since XMLHttpRequest 2 exists. I never had any type of problem on all browsers I use. onreadystatechange is only useful if you want to get the headers on state 2. Using the XMLHttpRequest variable name is another big error as you need to execute the callback inside the onload/oreadystatechange closures else you lost it. Now if you want something more complex using post and FormData you can easily extend this function: Again ... it's a very short function, but it does get & post. Examples of usage: Or pass a full form element (document.getElementsByTagName('form')[0]): Or set some custom values: As you can see I didn't implement sync... it's a bad thing. Having said that ... why don't do it the easy way? As mentioned in the comment the use of error && synchronous does completely break the point of the answer. Which is a nice short way to use Ajax in the proper way? Error handler In the above script, you have an error handler which is statically defined so it does not compromise the function. The error handler can be used for other functions too. But to really get out an error the only way is to write a wrong URL in which case every browser throws an error. Error handlers are maybe useful if you set custom headers, set the responseType to blob array buffer or whatever... Even if you pass 'POSTAPAPAP' as the method it won't throw an error. Even if you pass 'fdggdgilfdghfldj' as formdata it won't throw an error. In the first case the error is inside the displayAjax() under this.statusText as Method not Allowed. In the second case, it simply works. You have to check at the server side if you passed the right post data. cross-domain not allowed throws error automatically. In the error response, there are no error codes. There is only the this.type which is set to error. Why add an error handler if you totally have no control over errors? Most of the errors are returned inside this in the callback function displayAjax(). So: No need for error checks if you're able to copy and paste the URL properly. ;) PS: As the first test I wrote x('x', displayAjax)..., and it totally got a response...??? So I checked the folder where the HTML is located, and there was a file called 'x.xml'. So even if you forget the extension of your file XMLHttpRequest 2 WILL FIND IT. I LOL'd Read a file synchronous Don't do that. If you want to block the browser for a while load a nice big .txt file synchronous. Now you can do There is no other way to do this in a non-asynchronous way. (Yeah, with setTimeout loop... but seriously?) Another point is... if you work with APIs or just your own list's files or whatever you always use different functions for each request... Only if you have a page where you load always the same XML/JSON or whatever you need only one function. In that case, modify a little the Ajax function and replace b with your special function. The functions above are for basic use. If you want to EXTEND the function... Yes, you can. I'm using a lot of APIs and one of the first functions I integrate into every HTML page is the first Ajax function in this answer, with GET only... But you can do a lot of stuff with XMLHttpRequest 2: I made a download manager (using ranges on both sides with resume, filereader, filesystem), various image resizers converters using canvas, populate web SQL databases with base64images and much more... But in these cases you should create a function only for that purpose... sometimes you need a blob, array buffers, you can set headers, override mimetype and there is a lot more... But the question here is how to return an Ajax response... (I added an easy way.) This means AngularJS, jQuery (with deferred), native XHR's replacement (fetch), EmberJS, BackboneJS's save or any node library that returns promises. Your code should be something along the lines of this: Felix Kling did a fine job writing an answer for people using jQuery with callbacks for AJAX. I have an answer for native XHR. This answer is for generic usage of promises either on the frontend or backend.  The JavaScript concurrency model in the browser and on the server with NodeJS/io.js is asynchronous and reactive. Whenever you call a method that returns a promise, the then handlers are always executed asynchronously - that is, after the code below them that is not in a .then handler. This means when you're returning data the then handler you've defined did not execute yet. This in turn means that the value you're returning has not been set to the correct value in time.  Here is a simple analogy for the issue:       function getFive(){         var data;         setTimeout(function(){ // set a timer for one second in the future            data = 5; // after a second, do this         }, 1000);         return data;     }     document.body.innerHTML = getFive(); // `undefined` here and not 5    The value of data is undefined since the data = 5 part has not executed yet. It will likely execute in a second but by that time it is irrelevant to the returned value. Since the operation did not happen yet (AJAX, server call, IO, timer) you're returning the value before the request got the chance to tell your code what that value is. One possible solution to this problem is to code re-actively , telling your program what to do when the calculation completed. Promises actively enable this by being temporal (time-sensitive) in nature. A Promise is a value over time. Promises have state, they start as pending with no value and can settle to: A promise can only change states once after which it will always stay at the same state forever. You can attach then handlers to promises to extract their value and handle errors. then handlers allow chaining of calls. Promises are created by using APIs that return them. For example, the more modern AJAX replacement fetch or jQuery's $.get return promises. When we call .then on a promise and return something from it - we get a promise for the processed value. If we return another promise we'll get amazing things, but let's hold our horses. Let's see how we can solve the above issue with promises. First, let's demonstrate our understanding of promise states from above by using the Promise constructor for creating a delay function: Now, after we converted setTimeout to use promises, we can use then to make it count:   function delay(ms){ // takes amount of milliseconds   // returns a new promise   return new Promise(function(resolve, reject){     setTimeout(function(){ // when the time is up       resolve(); // change the promise to the fulfilled state     }, ms);   }); }  function getFive(){   // we're RETURNING the promise, remember, a promise is a wrapper over our value   return delay(100).then(function(){ // when the promise is ready       return 5; // return the value 5, promises are all about return values   }) } // we _have_ to wrap it like this in the call site, we can't access the plain value getFive().then(function(five){     document.body.innerHTML = five; });    Basically, instead of returning a value which we can't do because of the concurrency model - we're returning a wrapper for a value that we can unwrap with then. It's like a box you can open with then. This stands the same for your original API call, you can: So this works just as well. We've learned we can't return values from already asynchronous calls but we can use promises and chain them to perform processing. We now know how to return the response from an asynchronous call. ES6 introduces generators which are functions that can return in the middle and then resume the point they were at. This is typically useful for sequences, for example: Is a function that returns an iterator over the sequence 1,2,3,3,3,3,.... which can be iterated. While this is interesting on its own and opens room for a lot of possibility there is one particular interesting case. If the sequence we're producing is a sequence of actions rather than numbers - we can pause the function whenever an action is yielded and wait for it before we resume the function. So instead of a sequence of numbers, we need a sequence of future values - that is: promises. This somewhat tricky but very powerful trick lets us write asynchronous code in a synchronous manner. There are several "runners" that do this for you, writing one is a short few lines of code but is beyond the scope of this answer. I'll be using Bluebird's Promise.coroutine here, but there are other wrappers like co or Q.async.  This method returns a promise itself, which we can consume from other coroutines. For example: In ES7, this is further standardized, there are several proposals right now but in all of them you can await promise. This is just "sugar" (nicer syntax) for the ES6 proposal above by adding the async and await keywords. Making the above example: It still returns a promise just the same :) You are using Ajax incorrectly. The idea is not to have it return anything, but instead hand off the data to something called a callback function, which handles the data. That is: Returning anything in the submit handler will not do anything. You must instead either hand off the data, or do what you want with it directly inside the success function. The simplest solution is create a JavaScript function and call it for the Ajax success callback. I will answer with a horrible-looking, hand-drawn comic. The second image is the reason why result is undefined in your code example.  For people who are using AngularJS, can handle this situation using Promises. Here it says, Promises can be used to unnest asynchronous functions and allows one to chain multiple functions together. You can find a nice explanation here also. Example found in docs mentioned below. In Angular2 with look at the following example, but its recommended to use Observables with Angular2. } You can consume that in this way, See the original post here. But Typescript does not support native es6 Promises, if you want to use it, you might need plugin for that. Additionally here is the promises spec define here. Most of the answers here give useful suggestions for when you have a single async operation, but sometimes, this comes up when you need to do an asynchronous operation for each entry in an array or other list-like structure. The temptation is to do this: Example:   // WRONG var theArray = [1, 2, 3]; var results = []; theArray.forEach(function(entry) {     doSomethingAsync(entry, function(result) {         results.push(result);     }); }); console.log("Results:", results); // E.g., using them, returning them, etc.  function doSomethingAsync(value, callback) {     console.log("Starting async operation for " + value);     setTimeout(function() {         console.log("Completing async operation for " + value);         callback(value * 2);     }, Math.floor(Math.random() * 200)); } .as-console-wrapper {   max-height: 100% !important; }    The reason that doesn't work is that the callbacks from doSomethingAsync haven't run yet by the time you're trying to use the results. So, if you have an array (or list of some kind) and want to do async operations for each entry, you have two options: Do the operations in parallel (overlapping), or in series (one after another in sequence). You can start all of them and keep track of how many callbacks you're expecting, and then use the results when you've gotten that many callbacks: Example:   var theArray = [1, 2, 3]; var results = []; var expecting = theArray.length; theArray.forEach(function(entry, index) {     doSomethingAsync(entry, function(result) {         results[index] = result;         if (--expecting === 0) {             // Done!             console.log("Results:", results); // E.g., using the results         }     }); });  function doSomethingAsync(value, callback) {     console.log("Starting async operation for " + value);     setTimeout(function() {         console.log("Completing async operation for " + value);         callback(value * 2);     }, Math.floor(Math.random() * 200)); } .as-console-wrapper {   max-height: 100% !important; }    (We could do away with expecting and just use results.length === theArray.length, but that leaves us open to the possibility that theArray is changed while the calls are outstanding...) Notice how we use the index from forEach to save the result in results in the same position as the entry it relates to, even if the results arrive out of order (since async calls don't necessarily complete in the order in which they were started). But what if you need to return those results from a function? As the other answers have pointed out, you can't; you have to have your function accept and call a callback (or return a Promise). Here's a callback version: Example:   function doSomethingWith(theArray, callback) {     var results = [];     var expecting = theArray.length;     theArray.forEach(function(entry, index) {         doSomethingAsync(entry, function(result) {             results[index] = result;             if (--expecting === 0) {                 // Done!                 callback(results);             }         });     }); } doSomethingWith([1, 2, 3], function(results) {     console.log("Results:", results); });  function doSomethingAsync(value, callback) {     console.log("Starting async operation for " + value);     setTimeout(function() {         console.log("Completing async operation for " + value);         callback(value * 2);     }, Math.floor(Math.random() * 200)); } .as-console-wrapper {   max-height: 100% !important; }    Or here's a version returning a Promise instead: Of course, if doSomethingAsync passed us errors, we'd use reject to reject the promise when we got an error.) Example:   function doSomethingWith(theArray) {     return new Promise(function(resolve) {         var results = [];         var expecting = theArray.length;         theArray.forEach(function(entry, index) {             doSomethingAsync(entry, function(result) {                 results[index] = result;                 if (--expecting === 0) {                     // Done!                     resolve(results);                 }             });         });     }); } doSomethingWith([1, 2, 3]).then(function(results) {     console.log("Results:", results); });  function doSomethingAsync(value, callback) {     console.log("Starting async operation for " + value);     setTimeout(function() {         console.log("Completing async operation for " + value);         callback(value * 2);     }, Math.floor(Math.random() * 200)); } .as-console-wrapper {   max-height: 100% !important; }    (Or alternately, you could make a wrapper for doSomethingAsync that returns a promise, and then do the below...) If doSomethingAsync gives you a Promise, you can use Promise.all: If you know that doSomethingAsync will ignore a second and third argument, you can just pass it directly to map (map calls its callback with three arguments, but most people only use the first most of the time): Example:   function doSomethingWith(theArray) {     return Promise.all(theArray.map(doSomethingAsync)); } doSomethingWith([1, 2, 3]).then(function(results) {     console.log("Results:", results); });  function doSomethingAsync(value) {     console.log("Starting async operation for " + value);     return new Promise(function(resolve) {         setTimeout(function() {             console.log("Completing async operation for " + value);             resolve(value * 2);         }, Math.floor(Math.random() * 200));     }); } .as-console-wrapper {   max-height: 100% !important; }    Note that Promise.all resolves its promise with an array of the results of all of the promises you give it when they are all resolved, or rejects its promise when the first of the promises you give it rejects. Suppose you don't want the operations to be in parallel? If you want to run them one after another, you need to wait for each operation to complete before you start the next. Here's an example of a function that does that and calls a callback with the result: (Since we're doing the work in series, we can just use results.push(result) since we know we won't get results out of order. In the above we could have used results[index] = result;, but in some of the following examples we don't have an index to use.) Example:   function doSomethingWith(theArray, callback) {     var results = [];     doOne(0);     function doOne(index) {         if (index < theArray.length) {             doSomethingAsync(theArray[index], function(result) {                 results.push(result);                 doOne(index + 1);             });         } else {             // Done!             callback(results);         }     } } doSomethingWith([1, 2, 3], function(results) {     console.log("Results:", results); });  function doSomethingAsync(value, callback) {     console.log("Starting async operation for " + value);     setTimeout(function() {         console.log("Completing async operation for " + value);         callback(value * 2);     }, Math.floor(Math.random() * 200)); } .as-console-wrapper {   max-height: 100% !important; }    (Or, again, build a wrapper for doSomethingAsync that gives you a promise and do the below...) If doSomethingAsync gives you a Promise, if you can use ES2017+ syntax (perhaps with a transpiler like Babel), you can use an async function with for-of and await: Example:   async function doSomethingWith(theArray) {     const results = [];     for (const entry of theArray) {         results.push(await doSomethingAsync(entry));     }     return results; } doSomethingWith([1, 2, 3]).then(function(results) {     console.log("Results:", results); });  function doSomethingAsync(value) {     console.log("Starting async operation for " + value);     return new Promise(function(resolve) {         setTimeout(function() {             console.log("Completing async operation for " + value);             resolve(value * 2);         }, Math.floor(Math.random() * 200));     }); } .as-console-wrapper {   max-height: 100% !important; }    If you can't use ES2017+ syntax (yet), you can use a variation on the "Promise reduce" pattern (this is more complex than the usual Promise reduce because we're not passing the result from one into the next, but instead gathering up their results in an array): Example:   function doSomethingWith(theArray) {     return theArray.reduce(function(p, entry) {         return p.then(function(results) {             return doSomethingAsync(entry).then(function(result) {                 results.push(result);                 return results;             });         });     }, Promise.resolve([])); } doSomethingWith([1, 2, 3]).then(function(results) {     console.log("Results:", results); });  function doSomethingAsync(value) {     console.log("Starting async operation for " + value);     return new Promise(function(resolve) {         setTimeout(function() {             console.log("Completing async operation for " + value);             resolve(value * 2);         }, Math.floor(Math.random() * 200));     }); } .as-console-wrapper {   max-height: 100% !important; }    ...which is less cumbersome with ES2015+ arrow functions: Example:   function doSomethingWith(theArray) {     return theArray.reduce((p, entry) => p.then(results => doSomethingAsync(entry).then(result => {         results.push(result);         return results;     })), Promise.resolve([])); } doSomethingWith([1, 2, 3]).then(function(results) {     console.log("Results:", results); });  function doSomethingAsync(value) {     console.log("Starting async operation for " + value);     return new Promise(function(resolve) {         setTimeout(function() {             console.log("Completing async operation for " + value);             resolve(value * 2);         }, Math.floor(Math.random() * 200));     }); } .as-console-wrapper {   max-height: 100% !important; }    Have a look at this example: As you can see getJoke is returning a resolved promise (it is resolved when returning res.data.value). So you wait until the $http.get request is completed and then console.log(res.joke) is executed (as a normal asynchronous flow). This is the plnkr: http://embed.plnkr.co/XlNR7HpCaIhJxskMJfSg/ ES6 way (async - await) This is one of the places which two ways data binding or store concept that's used in many new JavaScript frameworks will work great for you... So if you are using Angular, React or any other frameworks which do two ways data binding or store concept this issue is simply fixed for you, so in easy word, your result is undefined at the first stage, so you have got result = undefined before you receive the data, then as soon as you get the result, it will be updated and get assigned to the new value which response of your Ajax call... But how you can do it in pure javascript or jQuery for example as you asked in this question? You can use a callback, promise and recently observable to handle it for you, for example in promises we have some function like success() or then() which will be executed when your data is ready for you, same with callback or subscribe function on observable. For example in your case which you are using jQuery, you can do something like this: For more information study about promises and observables which are newer ways to do this async stuffs. It's a very common issue we face while struggling with the 'mysteries' of JavaScript. Let me try demystifying this mystery today. Let's start with a simple JavaScript function: That's a simple synchronous function call (where each line of code is 'finished with its job' before the next one in sequence), and the result is same as expected. Now let's add a bit of twist, by introducing little delay in our function, so that all lines of code are not 'finished' in sequence. Thus, it will emulate the asynchronous behavior of function : So there you go, that delay just broke the functionality we expected! But what exactly happened ? Well, it's actually pretty logical if you look at the code. the function foo(), upon execution, returns nothing (thus returned value is undefined), but it does start a timer, which executes a function after 1s to return 'wohoo'. But as you can see, the value that's assigned to bar is the immediately returned stuff from foo(), which is nothing i.e. just undefined. So, how do we tackle this issue? Let's ask our function for a PROMISE. Promise is really about what it means : it means that the function guarantees you to provide with any output it gets in future. so let's see it in action for our little problem above :  Thus, the summary is - to tackle the asynchronous functions like ajax based calls etc., you can use a promise to resolve the value (which you intend to return). Thus, in short you resolve value instead of returning, in asynchronous functions. Apart from using then/catch to work with promises, there exists one more approach. The idea is to recognize an asynchronous function and then wait for the promises to resolve, before moving to the next line of code. It's still just the promises under the hood, but with a different syntactical approach. To make things clearer, you can find a comparison below: Another approach to return a value from an asynchronous function, is to pass in an object that will store the result from the asynchronous function. Here is an example of the same: I am using the result object to store the value during the asynchronous operation. This allows the result be available even after the asynchronous job. I use this approach a lot. I would be interested to know how well this approach works where wiring the result back through consecutive modules is involved. While promises and callbacks work fine in many situations, it is a pain in the rear to express something like: You'd end up going through async1; check if name is undefined or not and call the callback accordingly. While it is okay in small examples it gets annoying when you have a lot of similar cases and error handling involved. Fibers helps in solving the issue. You can checkout the project here. The following example I have written shows how to This working example is self-contained. It will define a simple request object that uses the window XMLHttpRequest object to make calls. It will define a simple function to wait for a bunch of promises to be completed. Context. The example is querying the Spotify Web API endpoint in order to search for playlist objects for a given set of query strings: For each item, a new Promise will fire a block - ExecutionBlock, parse the result, schedule a new set of promises based on the result array, that is a list of Spotify user objects and execute the new HTTP call within the ExecutionProfileBlock asynchronously. You can then see a nested Promise structure, that lets you spawn multiple and completely asynchronous nested HTTP calls, and join the results from each subset of calls through Promise.all. NOTE Recent Spotify search APIs will require an access token to be specified in the request headers: So, you to run the following example you need to put your access token in the request headers:   var spotifyAccessToken = "YourSpotifyAccessToken"; var console = {     log: function(s) {         document.getElementById("console").innerHTML += s + "<br/>"     } }  // Simple XMLHttpRequest // based on https://davidwalsh.name/xmlhttprequest SimpleRequest = {     call: function(what, response) {         var request;         if (window.XMLHttpRequest) { // Mozilla, Safari, ...             request = new XMLHttpRequest();         } else if (window.ActiveXObject) { // Internet Explorer             try {                 request = new ActiveXObject('Msxml2.XMLHTTP');             }             catch (e) {                 try {                   request = new ActiveXObject('Microsoft.XMLHTTP');                 } catch (e) {}             }         }          // State changes         request.onreadystatechange = function() {             if (request.readyState === 4) { // Done                 if (request.status === 200) { // Complete                     response(request.responseText)                 }                 else                     response();             }         }         request.open('GET', what, true);         request.setRequestHeader("Authorization", "Bearer " + spotifyAccessToken);         request.send(null);     } }  //PromiseAll var promiseAll = function(items, block, done, fail) {     var self = this;     var promises = [],                    index = 0;     items.forEach(function(item) {         promises.push(function(item, i) {             return new Promise(function(resolve, reject) {                 if (block) {                     block.apply(this, [item, index, resolve, reject]);                 }             });         }(item, ++index))     });     Promise.all(promises).then(function AcceptHandler(results) {         if (done) done(results);     }, function ErrorHandler(error) {         if (fail) fail(error);     }); }; //promiseAll  // LP: deferred execution block var ExecutionBlock = function(item, index, resolve, reject) {     var url = "https://api.spotify.com/v1/"     url += item;     console.log( url )     SimpleRequest.call(url, function(result) {         if (result) {              var profileUrls = JSON.parse(result).playlists.items.map(function(item, index) {                 return item.owner.href;             })             resolve(profileUrls);         }         else {             reject(new Error("call error"));         }     }) }  arr = [     "search?type=playlist&q=%22doom%20metal%22",     "search?type=playlist&q=Adele" ]  promiseAll(arr, function(item, index, resolve, reject) {     console.log("Making request [" + index + "]")     ExecutionBlock(item, index, resolve, reject); }, function(results) { // Aggregated results      console.log("All profiles received " + results.length);     //console.log(JSON.stringify(results[0], null, 2));      ///// promiseall again      var ExecutionProfileBlock = function(item, index, resolve, reject) {         SimpleRequest.call(item, function(result) {             if (result) {                 var obj = JSON.parse(result);                 resolve({                     name: obj.display_name,                     followers: obj.followers.total,                     url: obj.href                 });             } //result         })     } //ExecutionProfileBlock      promiseAll(results[0], function(item, index, resolve, reject) {         //console.log("Making request [" + index + "] " + item)         ExecutionProfileBlock(item, index, resolve, reject);     }, function(results) { // aggregated results         console.log("All response received " + results.length);         console.log(JSON.stringify(results, null, 2));     }      , function(error) { // Error         console.log(error);     })      /////    },   function(error) { // Error       console.log(error);   }); <div id="console" />    I have extensively discussed this solution here. Short answer is, you have to implement a callback like this: This is quite simple: Here's a working version of your code: await is supported in all current browsers and node 8 Js is a single threaded. Browser can be divided into three parts: 1)Event Loop 2)Web API 3)Event Queue Event Loop runs for forever i.e kind of infinite loop.Event Queue is where all your function are pushed on some event(example:click) this is one by one carried out of queue and put into Event loop which execute this function and prepares it self for next one after first one is executed.This means Execution of one function doesn't starts till the function before it in queue is executed in event loop. Now let us think we pushed two functions in a queue one is for getting a data from server and another utilises that data.We pushed the serverRequest() function in queue first then utiliseData() function. serverRequest function goes in event loop and makes a call to server as we never know how much time it will take to get data from server so this process is expected to take time and so we busy our event loop thus hanging our page, that's where Web API come into role it take this function from event loop and deals with server making event loop free so that we can execute next function from queue.The next function in queue is utiliseData() which goes in loop but because of no data available it goes waste and execution of next function continues till end of the queue.(This is called Async calling i.e we can do something else till we get data) Let suppose our serverRequest() function had a return statement in a code, when we get back data from server Web API will push it in queue at the end of queue. As it get pushed at end in queue we cannot utilise its data as there is no function left in our queue to utilise this data.Thus it is not possible to return something from Async Call. Thus Solution to this is callback or promise. A Image from one of the answers here, Correctly explains callback use... We give our function(function utilising data returned from server) to function calling server.  In my Code it is called as Javscript.info callback You can use this custom library (written using Promise) to make a remote call. Simple usage example: Another solution is to execute code via sequential executor nsynjs. nsynjs will evaluate all promises sequentially, and put promise result into data property:   function synchronousCode() {      var getURL = function(url) {         return window.fetch(url).data.text().data;     };          var url = 'https://ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min.js';     console.log('received bytes:',getURL(url).length);      };  nsynjs.run(synchronousCode,{},function(){     console.log('synchronousCode done'); }); <script src="https://rawgit.com/amaksr/nsynjs/master/nsynjs.js"></script>    Step 1. Wrap function with callback into nsynjs-aware wrapper (if it has promisified version, you can skip this step): Step 2. Put synchronous logic into function: Step 3. Run function in synchronous manner via nsynjs: Nsynjs will evaluate all operators and expressions step-by-step, pausing execution in case if result of some slow function is not ready. More examples here: https://github.com/amaksr/nsynjs/tree/master/examples ECMAScript 6 has 'generators' which allow you to easily program in an asynchronous style. To run the above code you do this: If you need to target browsers that don't support ES6 you can run the code through Babel or closure-compiler to generate ECMAScript 5. The callback ...args are wrapped in an array and destructured when you read them so that the pattern can cope with callbacks that have multiple arguments. For example with node fs:   var App = App || {};  App = {     getDataFromServer: function(){        var self = this,                  deferred = $.Deferred(),                  requests = [];        requests.push($.getJSON('request/ajax/url/1'));       requests.push($.getJSON('request/ajax/url/2'));        $.when.apply(jQuery, requests).done(function(xhrResponse) {         return deferred.resolve(xhrResponse.result);       });       return deferred;     },      init: function(){          this.getDataFromServer().done(_.bind(function(resp1, resp2) {             // Do the operations which you wanted to do when you            // get a response from Ajax, for example, log response.         }, this));     } }; App.init();    We find ourselves in a universe which appears to progress along a dimension we call "time". We don't really understand what time is, but we have developed abstractions and vocabulary that let us reason and talk about it: "past", "present", "future", "before", "after". The computer systems we build--more and more--have time as an important dimension. Certain things are set up to happen in the future. Then other things need to happen after those first things eventually occur. This is the basic notion called "asynchronicity". In our increasingly networked world, the most common case of asynchronicity is waiting for some remote system to respond to some request. Consider an example. You call the milkman and order some milk. When it comes, you want to put it in your coffee. You can't put the milk in your coffee right now, because it is not here yet. You have to wait for it to come before putting it in your coffee. In other words, the following won't work: Because JS has no way to know that it needs to wait for order_milk to finish before it executes put_in_coffee. In other words, it does not know that order_milk is asynchronous--is something that is not going to result in milk until some future time. JS, and other declarative languages execute one statement after another without waiting. The classic JS approach to this problem, taking advantage of the fact that JS supports functions as first-class objects which can be passed around, is to pass a function as a parameter to the asynchronous request, which it will then invoke when it has completed its task sometime in the future. That is the "callback" approach. It looks like this: order_milk kicks off, orders the milk, then, when and only when it arrives, it invokes put_in_coffee. The problem with this callback approach is that it pollutes the normal semantics of a function reporting its result with return; instead, functions must not reports their results by calling a callback given as a parameter. Also, this approach can rapidly become unwieldy when dealing with longer sequences of events. For example, let's say that I want to wait for the milk to be put in the coffee, and then and only then perform a third step, namely drinking the coffee. I end up needing to write something like this: where I am passing to put_in_coffee both the milk to put in it, and also the action (drink_coffee) to execute once the milk has been put in. Such code becomes hard to write, and read, and debug. In this case, we could rewrite the code in the question as: This was the motivation for the notion of a "promise", which is a particular type of value which represents a future or asynchronous outcome of some sort. It can represent something that already happened, or that is going to happen in the future, or might never happen at all. Promises have a single method, named then, to which you pass an action to be executed when the outcome the promise represents has been realized. In the case of our milk and coffee, we design order_milk to return a promise for the milk arriving, then specify put_in_coffee as a then action, as follows: One advantage of this is that we can string these together to create sequences of future occurrences ("chaining"): Let's apply promises to your particular problem. We will wrap our request logic inside a function, which returns a promise: Actually, all we've done is added a return to the call to $.ajax. This works because jQuery's $.ajax already returns a kind of promise-like thing. (In practice, without getting into details, we would prefer to wrap this call so as for return a real promise, or use some alternative to $.ajax that does so.) Now, if we want to load the file and wait for it to finish and then do something, we can simply say  for instance, When using promises, we end up passing lots of functions into then, so it's often helpful to use the more compact ES6-style arrow functions: But there's still something vaguely dissatisfying about having to write code one way if synchronous and a quite different way if asynchronous. For synchronous, we write but if a is asynchronous, with promises we have to write Above, we said, "JS has no way to know that it needs to wait for the first call to finish before it executes the second". Wouldn't it be nice if there was some way to tell JS that? It turns out that there is--the await keyword, used inside a special type of function called an "async" function. This feature is part of the upcoming version of ES but is already available in transpilers such as Babel given the right presets. This allows us to simply write In your case, you would be able to write something like  Short answer: Your foo() method returns immediately, while the $ajax() call executes asynchronously after the function returns. The problem is then how or where to store the results retrieved by the async call once it returns. Several solutions have been given in this thread. Perhaps the easiest way is to pass an object to the foo() method, and to store the results in a member of that object after the async call completes. Note that the call to foo() will still return nothing useful. However, the result of the async call will now be stored in result.response. Use a callback() function inside the foo() success. Try in this way. It is simple and easy to understand.   The most perfect answer to this question is using Promise. There is a problem with using promises! I was using this solution for a while until I figured out there is an error in old browsers: So i decided to implement my own Promise class for ES3 to below js compilers if its not defined. Just add this code before your main code and then safely use Promise! The question was:  How do I return the response from an asynchronous call? which CAN be interpreted as:  How to make asynchronous code look synchronous? The solution will be to avoid callbacks, and use a combination of Promises and async/await. I would like to give an example for a Ajax request.  (Although it can be written in Javascript, I prefer to write it in Python, and compile it to Javascript using Transcrypt. It will be clear enough.) Lets first enable JQuery usage, to have $ available as S: Define a function which returns a Promise, in this case an Ajax call: Use the asynchronous code as if it were synchronous: Of course there are many approaches like synchronous request, promise, but from my experience I think you should use the callback approach. It's natural to asynchronous behavior of Javascript. So, your code snippet can be rewrite a little different: After reading all the responses here and with my experiences, I would like to resume the detail of callback, promise and async/await for the asynchronous programming in JavaScript. 1) Callback : The fundamental reason for a callback is to run code in response of an event (see the example below). We use callback in JavaScript every time. But if you must use many nested callbacks in the example below, it will be fairy terrible for the code refactoring. 2) Promise : a syntax ES6 - Promise resolves the callback hell issue ! myFirstPromise is a Promise instance that represents the process of async codes. The resolve function signals that the Promise instance has finished. Afterwards, we can call .then() (a chain of .then as you want) and .catch() on the promise instance: 3) Async/Await : a new syntax ES6 - Await is basically sugar syntax of Promise ! Async function provide us with a clean and concise syntax that enables us to write less code to accomplish the same outcome we would get with promises. Async/Await looks similar to synchronous code, and synchronous code is much easier to read and write. To catch errors with Async/Await, we can use the block try...catch. In here, you don't need to write a chain of .then() of Promise syntax. Conclusion : These are totally the three syntaxs for asynchronous programming in JavaScript that you shoud well understand. So if possible, I recommend that you should use "promise" or "async/await" for refactoring your asynchronous codes (mostly for XHR requests) ! Rather than throwing code at you, there are 2 concepts that are key to understanding how JS handles callbacks and asynchronicity. (is that even a word?) There are three things you need to be aware of; The queue; the event loop and the stack In broad, simplistic terms, the event loop is like the project manager, it is constantly listening for any functions that want to run and communicates between the queue and the stack. Once it receives a message to run something it adds it to the queue. The queue is the list of things that are waiting to execute (like your AJAX request). imagine it like this: When one of these messages is going to execute it pops the message from the queue and creates a stack, the stack is everything JS needs to execute to perform the instruction in the message. So in our example it's being told to call foobarFunc So anything that foobarFunc needs to execute (in our case anotherFunction) will get pushed onto the stack. executed, and then forgotten about - the event loop will then move onto the next thing in the queue (or listen for messages) The key thing here is the order of execution. That is When you make a call using AJAX to an external party or run any asynchronous code (a setTimeout for example), Javascript is dependant upon a response before it can proceed. The big question is when will it get the response? The answer is we don't know - so the event loop is waiting for that message to say "hey run me". If JS just waited around for that message synchronously your app would freeze and it will suck. So JS carries on executing the next item in the queue whilst waiting for the message to get added back to the queue. That's why with asynchronous functionality we use things called callbacks. - A function or handler that, when passed into another function, will be executed at a later date. A promise uses callbacks (functions passed to .then() for eg.) as a way to reason about this asynchronous behaviour in a more linear way. The promise is a way of saying "I promise to return something at some point" and the callback is how we handle that value that is eventually returned. jQuery uses specific callbacks called deffered.done deffered.fail and deffered.always (amongst others). You can see them all here So what you need to do is pass a function that is promised to execute at some point with data that is passed to it. Because a callback is not executed immediately but at a later time it's important to pass the reference to the function not it executed. so so most of the time (but not always) you'll pass foo not foo() Hopefully that will make some sense. When you encounter things like this that seem confusing - i highly recommend reading the documentation fully to at least get an understanding of it. It will make you a much better developer.
__label__text __label__grep __label__directory __label__linux __label__find I'm trying to find a way to scan my entire Linux system for all files containing a specific string of text. Just to clarify, I'm looking for text within the file, not in the file name. When I was looking up how to do this, I came across this solution twice: However, it doesn't work. It seems to display every single file in the system. Is this close to the proper way to do it? If not, how should I? This ability to find text strings in files would be extraordinarily useful for some programming projects I'm doing. Do the following: Along with these, --exclude, --include, --exclude-dir flags could be used for efficient searching: This works very well for me, to achieve almost the same purpose like yours. For more options check man grep. You can use grep -ilR: You can use ack. It is like grep for source code. You can scan your entire file system with it. Just do: In your root directory. You can also use regular expressions, specify the filetype, etc. UPDATE I just discovered The Silver Searcher, which is like ack but 3-5x faster than it and even ignores patterns from a .gitignore file. You can use: The r stands for recursive and so will search in the path specified and also its sub-directories. This will tell you the file name as well as print out the line in the file where the string appears. Or a command similar to the one you are trying (example: ) for searching in all javascript files (*.js): This will print the lines in the files where the text appears, but it does not print the file name. In addition to this command, we can write this too: grep -rn "String to search" /path/to/directory/or/file -r: recursive search n: line number will be shown for matches You can use this: First of all, I believe you have used -H instead of -l. Also you can try adding the text inside quotes followed by {} \. Let's say you are searching for files containing specific text "Apache License" inside your directory. It will display results somewhat similar to below (output will be different based on your directory content). Even if you are not use about the case like "text" vs "TEXT", you can use the -i switch to ignore case. You can read further details here.  Hope this helps you. You can use grep tool to search recursively the current folder, like: Note: -r - Recursively search subdirectories. You can also use globbing syntax to search within specific files such as: Note: By using globbing option (**), it scans all the files recursively with specific extension or pattern. To enable this syntax, run: shopt -s globstar. You may also use **/*.* for all files (excluding hidden and without extension) or any other pattern. If you've the error that your argument is too long, consider narrowing down your search, or use find syntax instead such as: Alternatively, use ripgrep. If you're working on larger projects or big files, you should use ripgrep instead, like: Checkout the docs, installation steps or source code on the GitHub project page. It's much quicker than any other tool like GNU/BSD grep, ucg, ag, sift, ack, pt or similar, since it is built on top of Rust's regex engine which uses finite automata, SIMD and aggressive literal optimizations to make searching very fast. It supports ignore patterns specified in .gitignore files, so a single file path can be matched against multiple glob patterns simultaneously. You can use common parameters such as: If your grep doesn't support recursive search, you can combine find with xargs: I find this easier to remember than the format for find -exec. This will output the filename and the content of the matched line, e.g. Optional flags you may want to add to grep:  There's a new utility called The Silversearcher It works closely with Git and other VCS. So you won't get anything in a .git or another directory. You can simply use And it will do the task for you! How do I find all files containing specific text on Linux?   (...) I came across this solution twice: find / -type f -exec grep -H 'text-to-find-here' {} \; If using find like in your example, better add -s (--no-messages) to grep, and 2>/dev/null at the end of the command to avoid lots of Permission denied messages issued by grep and find: find is the standard tool for searching files - combined with grep when looking for specific text - on Unix-like platforms. The find command is often combined with xargs, by the way. Faster and easier tools exist for the same purpose - see below. Better try them, provided they're available on your platform, of course: RipGrep - fastest search tool around: The Silver Searcher: ack: Note: You can add 2>/dev/null to these commands as well, to hide many error messages. Warning: unless you really can't avoid it, don't search from '/' (the root directory) to avoid a long and inefficient search!  So in the examples above, you'd better replace '/' by a sub-directory name, e.g. "/home" depending where you actually want to search... Try: Use pwd to search from any directory you are in, recursing downward Update Depending on the version of grep you are using, you can omit pwd. On newer versions . seems to be the default case for grep if no directory is given thus: grep -rnw -e "pattern"  or  grep -rnw "pattern"  will do the same thing as above! grep can be used even if we're not looking for a string. Simply running, will print out the path to all text files, i.e. those containing only printable characters. Silver Searcher is a terrific tool, but ripgrep may be even better.   It works on Linux, Mac and Windows, and was written up on Hacker News a couple of months ago (this has a link to Andrew Gallant's Blog which has a GitHub link): Ripgrep – A new command line search tool Here are the several list of commands that can be used to search file. Explanation from comments find is a command that lets you find files and other objects like directories and links in subdirectories of a given path. If you don't specify a mask that filesnames should meet, it enumerates all directory objects. Hope this is of assistance... Expanding the grep a bit to give more information in the output, for example, to get the line number in the file where the text is can be done as follows: And if you have an idea what the file type is you can narrow your search down by specifying file type extensions to search for, in this case .pas OR .dfm files: Short explanation of the options: Try: which will search all file systems, because / is the root folder. For home folder use: For current folder use: A Simple find can work handy. alias it in your ~/.bashrc file: Start a new terminal and issue: If you strictly want to use find then use find + grep: find /path/to/somewhere/ -type f -exec grep -nw 'textPattern' {} \; Steps: This gives you the power of find to find files. find /path/to/somewhere/ -type f -name \*.cpp -exec grep -nw 'textPattern' {} \; You can use different options of find to improve your file search. I wrote a Python script which does something similar. This is how one should use this script. The first argument, path, is the directory in which we will search recursively. The second argument, pattern_to_search, is a regular expression which we want to search in a file. We use the regular expression format defined in the Python re library. In this script, the . also matches newline. The third argument, file_pattern, is optional. This is another regular expression which works on a filename. Only those files which matches this regular expression will be considered.  For example, if I want to search Python files with the extension py containing Pool( followed by word Adaptor, I do the following, And voila, it generates the path of matched files and line number at which the match was found. If more than one match was found, then each line number will be appended to the filename. I am fascinated by how simple grep makes it with 'rl': Use '-r' without 'l' to see the file names followed by text in which the pattern is found! It works just perfect... Use: This will report how many copies of your pattern are there in each of the files in the current directory. There is an ack tool that would do exactly what you are looking for. http://linux.die.net/man/1/ack You may ignore -i for case sensitive search grep is your good friend to achieve this. If you don't care about the case of the text to find, then use: To search for the string and output just that line with the search string:  e.g.: To display filename containing the search string: e.g.: All previous answers suggest grep and find. But there is another way: Use Midnight Commander It is a free utility (30 years old, proven by time) which is visual without being GUI. It has tons of functions, and finding files is just one of them.  The below command will work fine for this approach: Avoid the hassle and install ack-grep. It eliminates a lot of permission and quotation issues. Then go to the directory you want to search and run the command below
__label__equality __label__operators __label__javascript __label__equality-operator __label__identity-operator I'm using JSLint to go through JavaScript, and it's returning many suggestions to replace == (two equals signs) with === (three equals signs) when doing things like comparing idSele_UNVEHtype.value.length == 0 inside of an if statement. Is there a performance benefit to replacing == with ===?  Any performance improvement would be welcomed as many comparison operators exist. If no type conversion takes place, would there be a performance gain over ==? The strict equality operator (===) behaves identically to the abstract equality operator (==) except no type conversion is done, and the types must be the same to be considered equal. Reference: Javascript Tutorial: Comparison Operators The == operator will compare for equality after doing any necessary type conversions.  The === operator will not do the conversion, so if two values are not the same type === will simply return false. Both are equally quick. To quote Douglas Crockford's excellent JavaScript: The Good Parts, JavaScript has two sets of equality operators: === and !==, and their evil twins == and !=.  The good ones work the way you would expect.  If the two operands are of the same type and have the same value, then === produces true and !== produces false.  The evil twins do the right thing when the operands are of the same type, but if they are of different types, they attempt to coerce the values.  the rules by which they do that are complicated and unmemorable.  These are some of the interesting cases:  The lack of transitivity is alarming.  My advice is to never use the evil twins.  Instead, always use === and !==.  All of the comparisons just shown produce false with the === operator. A good point was brought up by @Casebash in the comments and in @Phillipe Laybaert's answer concerning objects.  For objects, == and === act consistently with one another (except in a special case). The special case is when you compare a primitive with an object that evaluates to the same primitive, due to its toString or valueOf method. For example, consider the comparison of a string primitive with a string object created using the String constructor. Here the == operator is checking the values of the two objects and returning true, but the === is seeing that they're not the same type and returning false.  Which one is correct?  That really depends on what you're trying to compare.  My advice is to bypass the question entirely and just don't use the String constructor to create string objects from string literals. Reference http://www.ecma-international.org/ecma-262/5.1/#sec-11.9.3 Using the == operator (Equality) Using the === operator (Identity) This is because the equality operator == does type coercion, meaning that the interpreter implicitly tries to convert the values before comparing. On the other hand, the identity operator === does not do type coercion, and thus does not convert the values when comparing, and is therefore faster (as according to This JS benchmark test) as it skips one step. An interesting pictorial representation of the equality comparison between == and ===.   Source: http://dorey.github.io/JavaScript-Equality-Table/ When using === for JavaScript equality testing, everything is as is. Nothing gets converted before being evaluated.  When using == for JavaScript equality testing, some   funky conversions take place.  Moral of the story:  Use === unless you fully understand the   conversions that take place with ==. In the answers here, I didn't read anything about what equal means. Some will say that === means equal and of the same type, but that's not really true. It actually means that both operands reference the same object, or in case of value types, have the same value. So, let's take the following code: The same here: Or even: This behavior is not always obvious. There's more to the story than being equal and being of the same type. The rule is: For value types (numbers): a === b returns true if a and b have the same value and are of the same type For reference types: a === b returns true if a and b reference the exact same object For strings: a === b returns true if a and b are both strings and contain the exact same characters Strings are not value types, but in Javascript they behave like value types, so they will be "equal" when the characters in the string are the same and when they are of the same length (as explained in the third rule) Now it becomes interesting: But how about this?: I thought strings behave like value types? Well, it depends who you ask... In this case a and b are not the same type. a is of type Object, while b is of type string. Just remember that creating a string object using the String constructor creates something of type Object that behaves as a string most of the time. Let me add this counsel: If in doubt, read the specification! ECMA-262 is the specification for a scripting language of which JavaScript is a dialect. Of course in practice it matters more how the most important browsers behave than an esoteric definition of how something is supposed to be handled. But it is helpful to understand why new String("a") !== "a". Please let me explain how to read the specification to clarify this question. I see that in this very old topic nobody had an answer for the very strange effect. So, if you can read a specification, this will help you in your profession tremendously. It is an acquired skill. So, let's continue. Searching the PDF file for === brings me to page 56 of the specification: 11.9.4. The Strict Equals Operator ( === ), and after wading through the specificationalese I find: 11.9.6 The Strict Equality Comparison Algorithm The comparison x === y, where x and y are values, produces true or false. Such a comparison is performed as follows:   1. If Type(x) is different from Type(y), return false.   2. If Type(x) is Undefined, return true.   3. If Type(x) is Null, return true.   4. If Type(x) is not Number, go to step 11.   5. If x is NaN, return false.   6. If y is NaN, return false.   7. If x is the same number value as y, return true.   8. If x is +0 and y is −0, return true.   9. If x is −0 and y is +0, return true.   10. Return false.   11. If Type(x) is String, then return true if x and y are exactly the same sequence of characters (same length and same characters in corresponding positions); otherwise, return false.   12. If Type(x) is Boolean, return true if x and y are both true or both false; otherwise, return false.   13. Return true if x and y refer to the same object or if they refer to objects joined to each other (see 13.1.2). Otherwise, return false. Interesting is step 11. Yes, strings are treated as value types. But this does not explain why new String("a") !== "a". Do we have a browser not conforming to ECMA-262? Not so fast! Let's check the types of the operands. Try it out for yourself by wrapping them in typeof(). I find that new String("a") is an object, and step 1 is used: return false if the types are different. If you wonder why new String("a") does not return a string, how about some exercise reading a specification? Have fun! Aidiakapi wrote this in a comment below: From the specification 11.2.2 The new Operator: If Type(constructor) is not Object, throw a TypeError exception. With other words, if String wouldn't be of type Object it couldn't be used with the new operator. new always returns an Object, even for String constructors, too. And alas! The value semantics for strings (see step 11) is lost. And this finally means: new String("a") !== "a". I tested this in Firefox with Firebug using code like this:   console.time("testEquality"); var n = 0; while (true) {   n++;   if (n == 100000)     break; } console.timeEnd("testEquality");    and   console.time("testTypeEquality"); var n = 0; while (true) {   n++;   if (n === 100000)     break; } console.timeEnd("testTypeEquality");    My results (tested five times each and averaged): So I'd say that the miniscule difference (this is over 100000 iterations, remember) is negligible. Performance isn't a reason to do ===. Type safety (well, as safe as you're going to get in JavaScript), and code quality is. In PHP and JavaScript, it is a strict equality operator. Which means, it will compare both type and values. In JavaScript it means of the same value and type. For example, but The === operator is called a strict comparison operator, it does differ from the == operator. Lets take 2 vars a and b. For "a == b" to evaluate to true a and b need to be the same value. In the case of "a === b" a and b must be the same value and also the same type for it to evaluate to true.   Take the following example In summary; using the == operator might evaluate to true in situations where you do not want it to so using the === operator would be safer.   In the 90% usage scenario it won't matter which one you use, but it is handy to know the difference when you get some unexpected behaviour one day. Why == is so unpredictable? What do you get when you compare an empty string "" with the number zero 0? true Yep, that's right according to == an empty string and the number zero are the same time. And it doesn't end there, here's another one: Things get really weird with arrays. Then weirder with strings It get's worse: When is equal not equal? Let me say that again: And this is just the crazy stuff you get with primitives. It's a whole new level of crazy when you use == with objects. At this point your probably wondering... Why does this happen? Well it's because unlike "triple equals" (===) which just checks if two values are the same. == does a whole bunch of other stuff. It has special handling for functions, special handling for nulls, undefined, strings, you name it. It get's pretty wacky. In fact, if you tried to write a function that does what == does it would look something like this: So what does this mean? It means == is complicated. Because it's complicated it's hard to know what's going to happen when you use it. Which means you could end up with bugs. So the moral of the story is... Make your life less complicated. Use === instead of ==. The End. Many times an untyped check would be handy because you do not care if the value is either undefined, null, 0  or "" Javascript execution flow diagram for strict equality / Comparison '==='  Javascript execution flow diagram for non strict equality / comparison '=='  JavaScript === vs == . It means equality without type coercion type coercion means JavaScript do not automatically convert any other data types to string data types  In a typical script there will be no performance difference. More important may be the fact that thousand "===" is 1 KB heavier than thousand "==" :) JavaScript profilers can tell you if there is a performance difference in your case. But personally I would do what JSLint suggests. This recommendation is there not because of performance issues, but because type coercion means ('\t\r\n' == 0) is true. The equal comparison operator == is confusing and should be avoided.  If you HAVE TO live with it, then remember the following 3 things:  EQUAL OPERATOR TRUTH TABLE IN JAVASCRIPT ** STRANGE: note that any two values on the first column are not equal in that sense.** There is unlikely to be any performance difference between the two operations in your usage. There is no type-conversion to be done because both parameters are already the same type. Both operations will have a type comparison followed by a value comparison. Yes! It does matter. === operator in javascript checks value as well as type where as == operator just checks the value (does type conversion if required).  You can easily test it. Paste following code in an HTML file and open it in browser You will get 'false' in alert. Now modify the onPageLoad() method to alert(x == 5); you will get true. === operator  checks the values as well as the types of the variables for equality. == operator just checks the value of the variables for equality. It's a strict check test. It's a good thing especially if you're checking between 0 and false and null.  For example, if you have: Then: All returns true and you may not want this. Let's suppose you have a function that can return the 0th index of an array or false on failure. If you check with "==" false, you can get a confusing result. So with the same thing as above, but a strict test: JSLint sometimes gives you unrealistic reasons to modify stuff. === has exactly the same performance as == if the types are already the same.  It is faster only when the types are not the same, in which case it does not try to convert types but directly returns a false. So, IMHO, JSLint maybe used to write new code, but useless over-optimizing should be avoided at all costs.  Meaning, there is no reason to change == to === in a check like if (a == 'test') when you know it for a fact that a can only be a String.  Modifying a lot of code that way wastes developers' and reviewers' time and achieves nothing. Simply  == means comparison between operands with type conversion & === means comparison between operands without type conversion Type conversion in javaScript means javaScript automatically convert any other data types to string data types. For example: A simple example is  As a rule of thumb, I would generally use === instead of == (and !== instead of !=). Reasons are explained in in the answers above and also Douglas Crockford is pretty clear about it (JavaScript: The Good Parts). However there is one single exception: == null is an efficient way to check for 'is null or undefined': For example jQuery 1.9.1 uses this pattern 43 times, and  the JSHint syntax checker even provides the eqnull relaxing option for this reason. From the jQuery style guide: Strict equality checks (===) should be used in favor of ==. The only   exception is when checking for undefined and null by way of null. The top 2 answers both mentioned == means equality and === means identity. Unfortunately, this statement is incorrect.  If both operands of == are objects, then they are compared to see if they are the same object. If both operands point to the same object, then the equal operator returns true. Otherwise, the two are not equal.  In the code above, both == and === get false because a and b are not the same objects. That's to say: if both operands of == are objects, == behaves same as ===, which also means identity. The essential difference of this two operators is about type conversion. == has conversion before it checks equality, but === does not. The problem is that you might easily get into trouble since JavaScript have a lot of implicit conversions meaning... Which pretty soon becomes a problem. The best sample of why implicit conversion is "evil" can be taken from this code in MFC / C++ which actually will compile due to an implicit conversion from CString to HANDLE which is a pointer typedef type... Which obviously during runtime does very undefined things... Google for implicit conversions in C++ and STL to get some of the arguments against it... From the core javascript reference === Returns true if the operands are strictly equal (see above)   with no type conversion. Equality comparison:  Operator == Returns true, when both operands are equal. The operands are converted to the same type before being compared. Equality and type comparison:  Operator === Returns true if both operands are equal and of the same type. It's generally  better and safer if you compare this way, because there's no behind-the-scenes type conversions. Here is a handy comparison table that shows the conversions that happen and the differences between == and ===. As the conclusion states: "Use three equals unless you fully understand the conversions that take   place for two-equals." http://dorey.github.io/JavaScript-Equality-Table/ null and undefined are nothingness, that is, Here a and b do not have values. Whereas, 0, false and '' are all values. One thing common beween all these are that they are all falsy values, which means they all satisfy falsy conditions. So, the 0, false and '' together form a sub-group. And on other hand, null & undefined form the second sub-group. Check the comparisons in the below image. null and undefined would equal. The other three would equal to each other. But, they all are treated as falsy conditions in JavaScript.  This is same as any object (like {}, arrays, etc.), non-empty string & Boolean true are all truthy conditions. But, they are all not equal.
__label__rest __label__put __label__http __label__post According to the HTTP/1.1 Spec:  The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line In other words, POST is used to create. The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI." That is, PUT is used to create or replace. So, which one should be used to create a resource? Or one needs to support both? Overall: Both PUT and POST can be used for creating. You have to ask, "what are you performing the action upon?", to distinguish what you should be using. Let's assume you're designing an API for asking questions.  If you want to use POST, then you would do that to a list of questions. If you want to use PUT, then you would do that to a particular question. Great, both can be used, so which one should I use in my RESTful design: You do not need to support both PUT and POST. Which you use is up to you.  But just remember to use the right one depending on what object you are referencing in the request. Some considerations: An example: I wrote the following as part of another answer on SO regarding this: POST: Used to modify and update a resource Note that the following is an error: If the URL is not yet created, you should not be using POST to create it while specifying the name.  This should result in a 'resource not found' error because <new_question> does not exist yet.  You should PUT the <new_question> resource on the server first. You could though do something like this to create a resources using POST: Note that in this case the resource name is not specified, the new objects URL path would be returned to you. PUT: Used to create a resource, or overwrite it.  While you specify the resources new URL. For a new resource: To overwrite an existing resource: Additionally, and a bit more concisely, RFC 7231 Section 4.3.4 PUT states (emphasis added), 4.3.4.  PUT The PUT method requests that the state of the target resource be created or replaced with the state defined by the representation enclosed in the request message payload. You can find assertions on the web that say Neither is quite right.  Better is to choose between PUT and POST based on idempotence of the action.  PUT implies putting a resource - completely replacing whatever is available at the given URL with a different thing.  By definition, a PUT is idempotent.  Do it as many times as you like, and the result is the same. x=5 is idempotent.  You can PUT a resource whether it previously exists, or not (eg, to Create, or to Update)!  POST updates a resource, adds a subsidiary resource, or causes a change.  A POST is not idempotent, in the way that x++ is not idempotent.  By this argument, PUT is for creating when you know the URL of the thing you will create. POST can be used to create when you know the URL of the "factory" or manager for the category of things you want to create.  so:  or: The relevant specification for PUT and POST is RFC 2616 §9.5ff. POST creates a child resource, so POST to /items creates a resources that lives under the /items resource.  Eg. /items/1. Sending the same post packet twice will create two resources. PUT is for creating or replacing a resource at a URL known by the client.  Therefore: PUT is only a candidate for CREATE where the client already knows the url before the resource is created. Eg. /blogs/nigel/entry/when_to_use_post_vs_put as the title is used as the resource key PUT replaces the resource at the known url if it already exists, so sending the same request twice has no effect. In other words, calls to PUT are idempotent. The RFC reads like this: The fundamental difference between the POST and PUT requests is reflected in the different meaning of the Request-URI. The URI in a POST request identifies the resource that will handle the enclosed entity. That resource might be a data-accepting process, a gateway to some other protocol, or a separate entity that accepts annotations. In contrast, the URI in a PUT request identifies the entity enclosed with the request -- the user agent knows what URI is intended and the server MUST NOT attempt to apply the request to some other resource. If the server desires that the request be applied to a different URI, Note: PUT has mostly been used to update resources (by replacing them in their entireties), but recently there is movement towards using PATCH for updating existing resources, as PUT specifies that it replaces the whole resource. RFC 5789. Update 2018: There is a case that can be made to avoid PUT. See "REST without PUT"  With “REST without PUT” technique, the idea is that consumers are   forced to post new 'nounified' request resources. As discussed   earlier, changing a customer’s mailing address is a POST to a new   “ChangeOfAddress” resource, not a PUT of a “Customer” resource with a   different mailing address field value. taken from REST API Design - Resource Modeling by Prakash Subramaniam of Thoughtworks This forces the API to avoid state transition problems with multiple clients updating a single resource, and matches more nicely with event sourcing and CQRS. When the work is done asynchronously, POSTing the transformation and waiting for it to be applied seems appropriate. Can be performed with both PUT or POST in the following way: Creates THE new resource with newResourceId as the identifier, under the /resources URI, or collection. Creates A new resource under the /resources URI, or collection. Usually the identifier is returned by the server. Can only be performed with PUT in the following way: Updates the resource with existingResourceId as the identifier, under the /resources URI, or collection. When dealing with REST and URI as general, you have generic on the left and specific on the right. The generics are usually called collections and the more specific items can be called resource. Note that a resource can contain a collection. <-- generic -- specific --> When you use POST you are always refering to a collection, so whenever you say: you are posting a new user to the users collection. If you go on and try something like this: it will work, but semantically you are saying that you want to add a resource to the john collection under the users collection. Once you are using PUT you are refering to a resource or single item, possibly inside a collection. So when you say: you are telling to the server update, or create if it doesn't exist, the john resource under the users collection. Let me highlight some important parts of the spec: The POST method is used to request that the origin server accept the entity enclosed in the request as a new subordinate of the resource identified by the Request-URI in the Request-Line Hence, creates a new resource on a collection. The PUT method requests that the enclosed entity be stored under the supplied Request-URI. If the Request-URI refers to an already existing resource, the enclosed entity SHOULD be considered as a modified version of the one residing on the origin server. If the Request-URI does not point to an existing resource, and that URI is capable of being defined as a new resource by the requesting user agent, the origin server can create the resource with that URI." Hence, create or update based on existence of the resource. POST means "create new" as in "Here is the input for creating a user, create it for me". PUT means "insert, replace if already exists" as in "Here is the data for user 5". You POST to example.com/users since you don't know the URL of the user yet, you want the server to create it.  You PUT to example.com/users/id since you want to replace/create a specific user. POSTing twice with the same data means create two identical users with different ids. PUTing twice with the same data creates the user the first and updates him to the same state the second time (no changes). Since you end up with the same state after a PUT no matter how many times you perform it, it is said to be "equally potent" every time - idempotent. This is useful for automatically retrying requests. No more 'are you sure you want to resend' when you push the back button on the browser. A general advice is to use POST when you need the server to be in control of URL generation of your resources. Use PUT otherwise.  Prefer PUT  over POST.   I'd like to add my "pragmatic" advice.  Use PUT when you know the "id" by which the object you are saving can be retrieved.  Using PUT won't work too well if you need, say, a database generated id to be returned for you to do future lookups or updates. So: To save an existing user, or one where the client generates the id and it's been verified that the id is unique: Otherwise, use POST to initially create the object, and PUT to update the object: Use POST to create, and PUT to update. That's how Ruby on Rails is doing it, anyway. Both are used for data transmission between client to server, but there are subtle differences between them, which are:    Analogy:     Social Media/Network Analogy: REST is a very high-level concept. In fact, it doesn't even mention HTTP at all! If you have any doubts about how to implement REST in HTTP, you can always take a look at the Atom Publication Protocol (AtomPub) specification. AtomPub is a standard for writing RESTful webservices with HTTP that was developed by many HTTP and REST luminaries, with some input from Roy Fielding, the inventor of REST and (co-)inventor of HTTP himself. In fact, you might even be able to use AtomPub directly. While it came out of the blogging community, it is in no way restricted to blogging: it is a generic protocol for RESTfully interacting with arbitrary (nested) collections of arbitrary resources via HTTP. If you can represent your application as a nested collection of resources, then you can just use AtomPub and not worry about whether to use PUT or POST, what HTTP Status Codes to return and all those details. This is what AtomPub has to say about resource creation (section 9.2): To add members to a Collection, clients send POST requests to the URI of the Collection. The decision of whether to use PUT or POST to create a resource on a server with an HTTP + REST API is based on who owns the URL structure. Having the client know, or participate in defining, the URL struct is an unnecessary coupling akin to the undesirable couplings that arose from SOA. Escaping types of couplings is the reason REST is so popular. Therefore, the proper method to use is POST. There are exceptions to this rule and they occur when the client wishes to retain control over the location structure of the resources it deploys. This is rare and likely means something else is wrong. At this point some people will argue that if RESTful-URL's are used, the client does knows the URL of the resource and therefore a PUT is acceptable. After all, this is why canonical, normalized, Ruby on Rails, Django URLs are important, look at the Twitter API … blah blah blah. Those people need to understand there is no such thing as a Restful-URL and that Roy Fielding himself states that: A REST API must not define fixed resource names or hierarchies (an   obvious coupling of client and server). Servers must have the freedom   to control their own namespace. Instead, allow servers to instruct   clients on how to construct appropriate URIs, such as is done in HTML   forms and URI templates, by defining those instructions within media   types and link relations. [Failure here implies that clients are   assuming a resource structure due to out-of band information, such as   a domain-specific standard, which is the data-oriented equivalent to   RPC's functional coupling]. http://roy.gbiv.com/untangled/2008/rest-apis-must-be-hypertext-driven The idea of a RESTful-URL is actually a violation of REST as the server is in charge of the URL structure and should be free to decide how to use it to avoid coupling. If this confuses you read about the significance of self discovery on API design. Using POST to create resources comes with a design consideration because POST is not idempotent. This means that repeating a POST several times does not guarantee the same behavior each time. This scares people into using PUT to create resources when they should not. They know it's wrong (POST is for CREATE) but they do it anyway because they don't know how to solve this problem.  This concern is demonstrated in the following situation: Step 6 is where people commonly get confused about what to do. However, there is no reason to create a kludge to solve this issue. Instead, HTTP can be used as specified in RFC 2616 and the server replies: 10.4.10 409 Conflict The request could not be completed due to a conflict with the current   state of the resource. This code is only allowed in situations where   it is expected that the user might be able to resolve the conflict and   resubmit the request. The response body SHOULD include enough information for the user to recognize the source of the conflict.   Ideally, the response entity would include enough information for the   user or user agent to fix the problem; however, that might not be   possible and is not required. Conflicts are most likely to occur in response to a PUT request. For   example, if versioning were being used and the entity being PUT   included changes to a resource which conflict with those made by an   earlier (third-party) request, the server might use the 409 response   to indicate that it can’t complete the request. In this case, the   response entity would likely contain a list of the differences between   the two versions in a format defined by the response Content-Type. Replying with a status code of 409 Conflict is the correct recourse because: Update based on release of RFC 7231 to Replace 2616 RFC 7231 is designed to replace 2616 and in Section 4.3.3 describes the follow possible response for a POST If the result of processing a POST would be equivalent to a   representation of an existing resource, an origin server MAY redirect   the user agent to that resource by sending a 303 (See Other) response   with the existing resource's identifier in the Location field.  This   has the benefits of providing the user agent a resource identifier   and transferring the representation via a method more amenable to   shared caching, though at the cost of an extra request if the user   agent does not already have the representation cached. It now may be tempting to simply return a 303 in the event that a POST is repeated. However, the opposite is true. Returning a 303 would only make sense if multiple create requests (creating different resources) return the same content. An example would be a "thank you for submitting your request message" that the client need not re-download each time. RFC 7231 still maintains in section 4.2.2 that POST is not to be idempotent and continues to maintain that POST should be used for create. For more information about this, read this article. I like this advice, from RFC 2616's definition of PUT: The fundamental difference between the POST and PUT requests is reflected in the different meaning of the Request-URI. The URI in a POST request identifies the resource that will handle the enclosed entity. That resource might be a data-accepting process, a gateway to some other protocol, or a separate entity that accepts annotations. In contrast, the URI in a PUT request identifies the entity enclosed with the request -- the user agent knows what URI is intended and the server MUST NOT attempt to apply the request to some other resource. This jibes with the other advice here, that PUT is best applied to resources that already have a name, and POST is good for creating a new object under an existing resource (and letting the server name it). I interpret this, and the idempotency requirements on PUT, to mean that: In short: PUT is idempotent, where the resource state will be the same if the same operation is executed one time or multiple times. POST is non-idempotent, where the resource state may become different if the operation is executed multiple times as compared to executing a single time. PUT You can think of similar to "UPDATE STUDENT SET address = "abc" where id="123"; POST You can think of something like "INSERT INTO STUDENT(name, address) VALUES ("abc", "xyzzz"); Student Id is auto generated. With PUT, if the same query is executed multiple times or one time, the STUDENT table state remains the same. In case of POST, if the same query is executed multiple times then multiple Student records get created in the database and the database state changes on each execution of an "INSERT" query. NOTE: PUT needs a resource location (already-resource) on which update needs to happen, whereas POST doesn't require that. Therefore intuitively POST is meant for creation of a new resource, whereas PUT is needed for updating the already existing resource. Some may come up with that updates can be performed with POST. There is no hard rule which one to use for updates or which one to use for create. Again these are conventions, and intuitively I'm inclined with the above mentioned reasoning and follow it. POST is like posting a letter to a mailbox or posting an email to an email queue. PUT is like when you put an object in a cubby hole or a place on a shelf (it has a known address). With POST, you're posting to the address of the QUEUE or COLLECTION. With PUT, you're putting to the address of the ITEM. PUT is idempotent. You can send the request 100 times and it will not matter. POST is not idempotent. If you send the request 100 times, you'll get 100 emails or 100 letters in your postal box. A general rule: if you know the id or name of the item, use PUT. If you want the id or name of the item to be assigned by the receiving party, use POST.   New answer (now that I understand REST better): PUT is merely a statement of what content the service should, from now on, use to render representations of the resource identified by the client; POST is a statement of what content the service should, from now on, contain (possibly duplicated) but it's up to the server how to identify that content. PUT x (if x identifies a resource): "Replace the content of the resource identified by x with my content." PUT x (if x does not identify a resource): "Create a new resource containing my content and use x to identify it." POST x: "Store my content and give me an identifier that I can use to identify a resource (old or new) containing said content (possibly mixed with other content). Said resource should be identical or subordinate to that which x identifies." "y's resource is subordinate to x's resource" is typically but not necessarily implemented by making y a subpath of x (e.g. x = /foo and y = /foo/bar) and modifying the representation(s) of x's resource to reflect the existence of a new resource, e.g. with a hyperlink to y's resource and some metadata. Only the latter is really essential to good design, as URLs are opaque in REST -- you're supposed to use hypermedia instead of client-side URL construction to traverse the service anyways. In REST, there's no such thing as a resource containing "content". I refer as "content" to data that the service uses to render representations consistently. It typically consists of some related rows in a database or a file (e.g. an image file). It's up to the service to convert the user's content into something the service can use, e.g. converting a JSON payload into SQL statements. Original answer (might be easier to read): PUT /something (if /something already exists): "Take whatever you have at /something and replace it with what I give you." PUT /something (if /something does not already exist): "Take what I give you and put it at /something." POST /something: "Take what I give you and put it anywhere you want under /something as long as you give me its URL when you're done." Short Answer: Simple rule of thumb: Use POST to create, use PUT to update. Long Answer: POST: PUT: Longer Answer: To understand it we need to question why PUT was required, what were the problems PUT was trying to solve that POST couldn't. From a REST architecture's point of view there is none that matters. We could have lived without PUT as well. But from a client developer's point of view it made his/her life a lot simpler. Prior to PUT, clients couldn't directly know the URL that the server generated or if all it had generated any or whether the data to be sent to the server is already updated or not. PUT relieved the developer of all these headaches. PUT is idempotent, PUT handles race conditions, and PUT lets the client choose the URL. Ruby on Rails 4.0 will use the 'PATCH' method instead of PUT to do partial updates. RFC 5789 says about PATCH (since 1995): A new method is necessary to improve interoperability and prevent      errors.  The PUT method is already defined to overwrite a resource      with a complete new body, and cannot be reused to do partial changes.      Otherwise, proxies and caches, and even clients and servers, may get      confused as to the result of the operation.  POST is already used but      without broad interoperability (for one, there is no standard way to      discover patch format support).  PATCH was mentioned in earlier HTTP      specifications, but not completely defined. "Edge Rails: PATCH is the new primary HTTP method for updates" explains it. At the risk of restating what has already been said, it seems important to remember that PUT implies that the client controls what the URL is going to end up being, when creating a resource. So part of the choice between PUT and POST is going to be about how much you can trust the client to provide correct, normalized URL that are coherent with whatever your URL scheme is.  When you can't fully trust the client to do the right thing, it would be  more appropriate to use POST to create a new item and then send the URL back to the client in the response. In a very simple way I'm taking the example of the Facebook timeline. Case 1: When you post something on your timeline, it's a fresh new entry. So in this case they use the POST method because the POST method is non-idempotent. Case 2: If your friend comment on your post the first time, that also will create a new entry in the database so the POST method used. Case 3: If your friend edits his comment, in this case, they had a comment id, so they will update an existing comment instead of creating a new entry in the database. Therefore for this type of operation use the PUT method because it is idempotent.* In a single line, use POST to add a new entry in the database and PUT to update something in the database. The most important consideration is reliability. If a POST message gets lost the state of the system is undefined. Automatic recovery is impossible. For PUT messages, the state is undefined only until the first successful retry. For instance, it may not be a good idea to create credit card transactions with POST.  If you happen to have auto generated  URI's on your resource you can still use PUT by passing a generated URI (pointing to an empty resource) to the client. Some other considerations: In addition to differences suggested by others, I want to add one more. In POST method you can send body params in form-data In PUT method you have to send body params in x-www-form-urlencoded Header Content-Type:application/x-www-form-urlencoded According to this, you cannot send files or multipart data in the PUT method EDIT The content type "application/x-www-form-urlencoded" is inefficient   for sending large quantities of binary data or text containing   non-ASCII characters. The content type "multipart/form-data" should be   used for submitting forms that contain files, non-ASCII data, and   binary data. Which means if you have to submit  files, non-ASCII data, and binary data you should use POST method Readers new to this topic will be struck by the endless discussion about what you should do, and the relative absence of lessons from experience. The fact that REST is "preferred" over SOAP is, I suppose, a high-level learning from experience, but goodness we must have progressed from there? It's 2016. Roy's dissertation was in 2000. What have we developed? Was it fun? Was it easy to integrate with? To support? Will it handle the rise of smartphones and flaky mobile connections? According to ME, real-life networks are unreliable. Requests timeout. Connections are reset. Networks go down for hours or days at a time. Trains go into tunnels with mobile users aboard. For any given request (as occasionally acknowledged in all this discussion) the request can fall in the water on its way, or the response can fall in the water on its way back. In these conditions, issuing PUT, POST and DELETE requests directly against substantive resources has always struck me as a little brutal and naive. HTTP does nothing to ensure reliable completion of the request-response, and that's just fine because this is properly the job of network-aware applications. Developing such an application, you can jump through hoops to use PUT instead of POST, then more hoops to give a certain kind of error on the server if you detect duplicate requests. Back at the client, you then have to jump through hoops to interpret these errors, refetch, revalidate and repost. Or you can do this: consider your unsafe requests as ephemeral single-user resources (let's call them actions). Clients request a new "action" on a substantive resource with an empty POST to the resource. POST will be used only for this. Once safely in possession of the URI of the freshly minted action, the client PUTs the unsafe request to the action URI, not the target resource. Resolving the action and updating the "real" resource is properly the job of your API, and is here decoupled from the unreliable network. The server does the business, returns the response and stores it against the agreed action URI. If anything goes wrong, the client repeats the request (natural behaviour!), and if the server has already seen it, it repeats the stored response and does nothing else. You will quickly spot the similarity with promises: we create and return the placeholder for the result before doing anything. Also like a promise, an action can succeed or fail one time, but its result can be fetched repeatedly. Best of all, we give sending and receiving applications a chance to link the uniquely identified action to uniqueness in their respective environments. And we can start to demand, and enforce!, responsible behaviour from clients: repeat your requests as much as you like, but don't go generating a new action until you're in possession of a definitive result from the existing one. As such, numerous thorny problems go away. Repeated insert requests won't create duplicates, and we don't create the real resource until we're in possession of the data. (database columns can stay not-nullable). Repeated update requests won't hit incompatible states and won't overwrite subsequent changes. Clients can (re)fetch and seamlessy process the original confirmation for whatever reason (client crashed, response went missing, etc.). Successive delete requests can see and process the original confirmation, without hitting a 404 error. If things take longer than expected, we can respond provisionally, and we have a place where the client can check back for the definitive result. The nicest part of this pattern is its Kung-Fu (Panda) property. We take a weakness, the propensity for clients to repeat a request any time they don't understand the response, and turn it into a strength :-) Before telling me this is not RESTful, please consider the numerous ways in which REST principles are respected. Clients don't construct URLs. The API stays discoverable, albeit with a little change in semantics. HTTP verbs are used appropriately. If you think this is a huge change to implement, I can tell you from experience that it's not. If you think you'll have huge amounts of data to store, let's talk volumes: a typical update confirmation is a fraction of a kilobyte. HTTP currently gives you a minute or two to respond definitively. Even if you only store actions for a week, clients have ample chance to catch up. If you have very high volumes, you may want a dedicated acid-compliant key value store, or an in-memory solution.  There seems to always be some confusion as to when to use the HTTP POST versus the HTTP PUT method for REST services. Most developers will try to associate CRUD operations directly to HTTP methods. I will argue that this is not correct and one can not simply associate the CRUD concepts to the HTTP methods. That is: It is true that the R(etrieve) and D(elete) of the CRUD operations can be mapped directly to the HTTP methods GET and DELETE respectively. However, the confusion lies in the C(reate) and U(update) operations. In some cases, one can use the PUT for a create while in other cases a POST will be required. The ambiguity lies in the definition of an HTTP PUT method versus an HTTP POST method. According to the HTTP 1.1 specifications the GET, HEAD, DELETE, and PUT methods must be idempotent, and the POST method is not idempotent. That is to say that an operation is idempotent if it can be performed on a resource once or many times and always return the same state of that resource. Whereas a non idempotent operation can return a modified state of the resource from one request to another. Hence, in a non idempotent operation, there is no guarantee that one will receive the same state of a resource. Based on the above idempotent definition, my take on using the HTTP PUT method versus using the HTTP POST method for REST services is: Use the HTTP PUT method when: In both cases, these operations can be performed multiple times with the same results. That is the resource will not be changed by requesting the operation more than once. Hence, a true idempotent operation. Use the HTTP POST method when: Conclusion Do not directly correlate and map CRUD operations to HTTP methods for REST services. The use of an HTTP PUT method versus an HTTP POST method should be based on the idempotent aspect of that operation. That is, if the operation is idempotent, then use the HTTP PUT method. If the operation is non idempotent, then use the HTTP POST method. the origin server can create the resource with that URI So you use POST and probably, but not necessary PUT for resource creation. You don't have to support both. For me POST is perfectly enough. So it is a design decision. As your quote mentioned, you use PUT for creation of there is no resource assigned to an IRI, and you want to create a resource anyway. For example, PUT /users/123/password usually replaces the old password with a new one, but you can use it to create a password if it does not exist already (for example, by freshly registered users or by restoring banned users). I'm going to land with the following: PUT refers to a resource, identified by the URI. In this case, you are updating it. It is the part of the three verbs referring to resources -- delete and get being the other two. POST is basically a free form message, with its meaning being defined 'out of band'. If the message can be interpreted as adding a resource to a directory, that would be OK, but basically you need to understand the message you are sending (posting) to know what will happen with the resource. Because PUT and GET and DELETE refer to a resource, they are also by definition idempotent. POST can perform the other three functions, but then the semantics of the request will be lost on the intermediaries such as caches and proxies. This also applies to providing security on the resource, since a post's URI doesn't necessarily indicate the resource it is applying to (it can though). A PUT doesn't need to be a create; the service could error if the resource isn't already created, but otherwise update it. Or vice versa -- it may create the resource, but not allow updates. The only thing required about PUT is that it points to a specific resource, and its payload is the representation of that resource. A successful PUT means (barring interference) that a GET would retrieve the same resource. Edit: One more thing -- a PUT can create, but if it does then the ID has to be a natural ID -- AKA an email address. That way when you PUT twice, the second put is an update of the first. This makes it idempotent. If the ID is generated (a new employee ID, for example), then the second PUT with the same URL would create a new record, which violates the idempotent rule. In this case the verb would be POST, and the message (not resource) would be to create a resource using the values defined in this message. The semantics are supposed be different, in that "PUT", like "GET" is supposed to be idempotent -- meaning, you can the same exact PUT request multiple times and the result will be as if you executed it only once.  I will describe the conventions which I think are most widely used and are most useful: When you PUT a resource at a particular URL what happens is that it should get saved at that URL, or something along those lines. When you POST to a resource at a particular URL, often you are posting a related piece of information to that URL. This implies that the resource at the URL already exists. For example, when you want to create a new stream, you can PUT it to some URL. But when you want to POST a message to an existing stream, you POST to its URL. As for modifying the properties of the stream, you can do that with either PUT or POST. Basically, only use "PUT" when the operation is idempotent - otherwise use POST. Note, however, that not all modern browsers support HTTP verbs other than GET or POST. Most of the time, you will use them like this: For example: In both cases, the request body contains the data for the resource to be created or updated. It should be obvious from the route names that POST is not idempotent (if you call it 3 times it will create 3 objects), but PUT is idempotent (if you call it 3 times the result is the same). PUT is often used for "upsert" operation (create or update), but you can always return a 404 error if you only want to use it to modify. Note that POST "creates" a new element in the collection, and PUT "replaces" an element at a given URL, but it is a very common practice to use PUT for partial modifications, that is, use it only to update existing resources and only modify the included fields in the body (ignoring the other fields). This is technically incorrect, if you want to be REST-purist, PUT should replace the whole resource and you should use PATCH for the partial update. I personally don't care much as far as the behavior is clear and consistent across all your API endpoints. Remember, REST is a set of conventions and guidelines to keep your API simple. If you end up with a complicated work-around just to check the "RESTfull" box then you are defeating the purpose ;) Here's a simple rule: PUT to a URL should be used to update or create the resource that can be located at that URL. POST to a URL should be used to update or create a resource which is located at some other ("subordinate") URL, or is not locatable via HTTP. While there is probably an agnostic way to describe these, it does seem to be conflicting with various statements from answers to websites. Let's be very clear and direct here. If you are a .NET developer working with Web API, the facts are (from the Microsoft API documentation), http://www.asp.net/web-api/overview/creating-web-apis/creating-a-web-api-that-supports-crud-operations: Sure you "can" use "POST" to update, but just follow the conventions laid out for you with your given framework. In my case it is .NET / Web API, so PUT is for UPDATE there is no debate. I hope this helps any Microsoft developers that read all comments with Amazon and Sun/Java website links. If you are familiar with database operations, there are I use PUT for Merge and update like operations and use POST for Insertions. In practice, POST works well for creating resources. The URL of the newly created resource should be returned in the Location response header. PUT should be used for updating a resource completely. Please understand that these are the best practices when designing a RESTful API. HTTP specification as such does not restrict using PUT/POST with a few restrictions for creating/updating resources. Take a look at http://techoctave.com/c7/posts/71-twitter-rest-api-dissected that summarizes the best practices.
__label__javascript __label__include __label__file __label__import Is there something in JavaScript similar to @import in CSS that allows you to include a JavaScript file inside another JavaScript file? The old versions of JavaScript had no import, include, or require, so many different approaches to this problem have been developed. But since 2015 (ES6), JavaScript has had the ES6 modules standard to import modules in Node.js, which is also supported by most modern browsers. For compatibility with older browsers, build tools like Webpack and Rollup and/or transpilation tools like Babel can be used. ECMAScript (ES6) modules have been supported in Node.js since v8.5, with the --experimental-modules flag, and since at least Node.js v13.8.0 without the flag. To enable "ESM" (vs. Node.js's previous CommonJS-style module system ["CJS"]) you either use "type": "module" in package.json or give the files the extension .mjs. (Similarly, modules written with Node.js's previous CJS module can be named .cjs if your default is ESM.) Using package.json: Then module.js: Then main.js: Using .mjs, you'd have module.mjs: Then main.mjs: Browsers have had support for loading ECMAScript modules directly (no tools like Webpack required) since Safari 10.1, Chrome 61, Firefox 60, and Edge 16. Check the current support at caniuse. There is no need to use Node.js' .mjs extension; browsers completely ignore file extensions on modules/scripts. Read more at https://jakearchibald.com/2017/es-modules-in-browsers/ Dynamic imports let the script load other scripts as needed: Read more at https://developers.google.com/web/updates/2017/11/dynamic-import The older CJS module style, still widely used in Node.js, is the module.exports/require system. There are other ways for JavaScript to include external JavaScript contents in browsers that do not require preprocessing. You could load an additional script with an AJAX call and then use eval to run it. This is the most straightforward way, but it is limited to your domain because of the JavaScript sandbox security model. Using eval also opens the door to bugs, hacks and security issues. Like Dynamic Imports you can load one or many scripts with a fetch call using promises to control order of execution for script dependencies using the Fetch Inject library: The jQuery library provides loading functionality in one line: You could add a script tag with the script URL into the HTML. To avoid the overhead of jQuery, this is an ideal solution. The script can even reside on a different server. Furthermore, the browser evaluates the code. The <script> tag can be injected into either the web page <head>, or inserted just before the closing </body> tag. Here is an example of how this could work: This function will add a new <script> tag to the end of the head section of the page, where the src attribute is set to the URL which is given to the function as the first parameter. Both of these solutions are discussed and illustrated in JavaScript Madness: Dynamic Script Loading. Now, there is a big issue you must know about. Doing that implies that you remotely load the code. Modern web browsers will load the file and keep executing your current script because they load everything asynchronously to improve performance. (This applies to both the jQuery method and the manual dynamic script loading method.) It means that if you use these tricks directly, you won't be able to use your newly loaded code the next line after you asked it to be loaded, because it will be still loading. For example: my_lovely_script.js contains MySuperObject: Then you reload the page hitting F5. And it works! Confusing... So what to do about it ? Well, you can use the hack the author suggests in the link I gave you. In summary, for people in a hurry, he uses an event to run a callback function when the script is loaded. So you can put all the code using the remote library in the callback function. For example: Then you write the code you want to use AFTER the script is loaded in a lambda function: Then you run all that: Note that the script may execute after the DOM has loaded, or before, depending on the browser and whether you included the line script.async = false;. There's a great article on Javascript loading in general which discusses this. As mentioned at the top of this answer, many developers use build/transpilation tool(s) like Parcel, Webpack, or Babel in their projects, allowing them to use upcoming JavaScript syntax, provide backward compatibility for older browsers, combine files, minify, perform code splitting etc. If anyone is looking for something more advanced, try out RequireJS. You'll get added benefits such as dependency management, better concurrency, and avoid duplication (that is, retrieving a script more than once). You can write your JavaScript files in "modules" and then reference them as dependencies in other scripts. Or you can use RequireJS as a simple "go get this script" solution. Example: Define dependencies as modules: some-dependency.js implementation.js is your "main" JavaScript file that depends on some-dependency.js Excerpt from the GitHub README: RequireJS loads plain JavaScript files as well as more defined   modules. It is optimized for in-browser use, including in a Web   Worker, but it can be used in other JavaScript environments, like   Rhino and Node. It implements the Asynchronous Module API. RequireJS uses plain script tags to load modules/files, so it should   allow for easy debugging. It can be used simply to load existing   JavaScript files, so you can add it to your existing project without   having to re-write your JavaScript files. ...  There actually is a way to load a JavaScript file not asynchronously, so you could use the functions included in your newly loaded file right after loading it, and I think it works in all browsers. You need to use jQuery.append() on the <head> element of your page, that is: However, this method also has a problem: if an error happens in the imported JavaScript file, Firebug (and also Firefox Error Console and Chrome Developer Tools as well) will report its place incorrectly, which is a big problem if you use Firebug to track JavaScript errors down a lot (I do). Firebug simply doesn't know about the newly loaded file for some reason, so if an error occurs in that file, it reports that it occurred in your main HTML file, and you will have trouble finding out the real reason for the error. But if that is not a problem for you, then this method should work. I have actually written a jQuery plugin called $.import_js() which uses this method: So all you would need to do to import JavaScript is: I also made a simple test for this at Example. It includes a main.js file in the main HTML and then the script in main.js uses $.import_js() to import an additional file called included.js, which defines this function: And right after including included.js, the hello() function is called, and you get the alert. (This answer is in response to e-satis' comment). Another way, that in my opinion is much cleaner, is to make a synchronous Ajax request instead of using a <script> tag. Which is also how Node.js handles includes. Here's an example using jQuery: You can then use it in your code as you'd usually use an include: And be able to call a function from the required script in the next line: There is a good news for you. Very soon you will be able to load JavaScript code easily. It will become a standard way of importing modules of JavaScript code and will be part of core JavaScript itself.  You simply have to write import cond from 'cond.js'; to load a macro named cond from a file cond.js. So you don't have to rely upon any JavaScript framework nor do you have to explicitly make Ajax calls. Refer to: Static module resolution Module loaders It is possible to dynamically generate a JavaScript tag and append it to HTML document from inside other JavaScript code. This will load targeted JavaScript file. Statement import is in ECMAScript 6. Syntax Maybe you can use this function that I found on this page How do I include a JavaScript file in a JavaScript file?: Here is a synchronous version without jQuery: Note that to get this working cross-domain, the server will need to set allow-origin header in its response. I just wrote this JavaScript code (using Prototype for DOM manipulation): Usage: Gist: http://gist.github.com/284442. Here's the generalized version of how Facebook does it for their ubiquitous Like button:   <script>   var firstScript = document.getElementsByTagName('script')[0],       js = document.createElement('script');   js.src = 'https://cdnjs.cloudflare.com/ajax/libs/Snowstorm/20131208/snowstorm-min.js';   js.onload = function () {     // do stuff with your dynamically loaded script     snowStorm.snowColor = '#99ccff';   };   firstScript.parentNode.insertBefore(js, firstScript); </script>    If it works for Facebook, it will work for you. The reason why we look for the first script element instead of head or body is because some browsers don't create one if missing, but we're guaranteed to have a script element - this one. Read more at http://www.jspatterns.com/the-ridiculous-case-of-adding-a-script-element/. If you want it in pure JavaScript, you can use document.write. If you use the jQuery library, you can use the $.getScript method. You can also assemble your scripts using PHP: File main.js.php: Most of solutions shown here imply dynamical loading. I was searching instead for a compiler which assemble all the depended files into a single output file. The same as Less/Sass preprocessors deal with the CSS @import at-rule. Since I didn't find anything decent of this sort, I wrote a simple tool solving the issue. So here is the compiler, https://github.com/dsheiko/jsic, which replaces $import("file-path") with the requested file content securely. Here is the corresponding Grunt plugin: https://github.com/dsheiko/grunt-jsic. On the jQuery master branch, they simply concatenate atomic source files into a single one starting with intro.js and ending with outtro.js. That doesn't suits me as it provides no flexibility on the source code design. Check out how it works with jsic: src/main.js src/Form/Input/Tel.js Now we can run the compiler: And get the combined file build/main.js If your intention to load the JavaScript file is using the functions from the imported/included file, you can also define a global object and set the functions as object items. For instance: You just need to be careful when you are including scripts in an HTML file. The order should be as in below: Or rather than including at run time, use a script to concatenate prior to upload. I use Sprockets (I don't know if there are others). You build your JavaScript code in separate files and include comments that are processed by the Sprockets engine as includes. For development you can include files sequentially, then for production to merge them... See also: This should do: I had a simple issue, but I was baffled by responses to this question. I had to use a variable (myVar1) defined in one JavaScript file (myvariables.js) in another JavaScript file (main.js). For this I did as below: Loaded the JavaScript code in the HTML file, in the correct order, myvariables.js first, then main.js: File: myvariables.js File: main.js As you saw, I had use a variable in one JavaScript file in another JavaScript file, but I didn't need to include one in another. I just needed to ensure that the first JavaScript file loaded before the second JavaScript file, and, the first JavaScript file's variables are accessible in the second JavaScript file, automatically. This saved my day. I hope this helps. In a modern language with the check if script has already been loaded, it would be: Usage (async/await): or Usage (Promise): In case you are using Web Workers and want to include additional scripts in the scope of the worker, the other answers provided about adding scripts to the head tag, etc. will not work for you. Fortunately, Web Workers have their own importScripts function which is a global function in the scope of the Web Worker, native to the browser itself as it is part of the specification. Alternatively, as the second highest voted answer to your question highlights, RequireJS can also handle including scripts inside a Web Worker (likely calling importScripts itself, but with a few other useful features). The @import syntax for achieving CSS-like JavaScript importing is possible using a tool such as Mixture via their special .mix file type (see here). I assume the application does this via one of above-mentioned methods. From the Mixture documentation on .mix files: Mix files are simply .js or .css files with .mix. in the file name. A mix file simply     extends the functionality of a normal style or script file and allows you to import and combine. Here's an example .mix file that combines multiple .js files into one: Mixture outputs this as scripts-global.js and also as a minified version (scripts-global.min.js). Note: I'm not in any way affiliated with Mixture, other than using it as a front-end development tool. I came across this question upon seeing a .mix JavaScript file in action (in one of the Mixture boilerplates) and being a bit confused by it ("you can do this?" I thought to myself). Then I realized that it was an application-specific file type (somewhat disappointing, agreed). Nevertheless, figured the knowledge might be helpful for others. Note: Mixture was discontinued on 2016/07/26 (after being open sourced on 2015/04/12). Although these answers are great, there is a simple "solution" that has been around since script loading existed, and it will cover 99.999% of most people's use cases. Just include the script you need before the script that requires it. For most projects it does not take long to determine which scripts are needed and in what order. If script2 requires script1, this really is the absolute easiest way to do something like this. I'm very surprised no-one has brought this up, as it's the most obvious and simplest answer that will apply in nearly every single case.  My usual method is: It works great and uses no page-reloads for me. I've tried the AJAX method (one of the other answers) but it doesn't seem to work as nicely for me. Here's an explanation of how the code works for those that are curious: essentially, it creates a new script tag (after the first one) of the URL. It sets it to asynchronous mode so it doesn't block the rest of the code, but calls a callback when the readyState (the state of the content to be loaded) changes to 'loaded'. I wrote a simple module that automates the job of importing/including module scripts in JavaScript. For detailed explanation of the code, refer to the blog post JavaScript require / import / include modules. Yes, use type="module" in a script tag (support): And in a script.js file include another file like this: In 'module.js' you must export the function/class that you will import: A working example is here. There is also Head.js. It is very easy to deal with: As you see, it's easier than Require.js and as convenient as jQuery's $.getScript method. It also has some advanced features, like conditional loading, feature detection and much more. There are a lot of potential answers for this question. My answer is obviously based on a number of them. This is what I ended up with after reading through all the answers. The problem with $.getScript and really any other solution that requires a callback when loading is complete is that if you have multiple files that use it and depend on each other you no longer have a way to know when all scripts have been loaded (once they are nested in multiple files). file3.js file2.js: file1.js: You are right when you say that you could specify Ajax to run synchronously or use XMLHttpRequest, but the current trend appears to be to deprecate synchronous requests, so you may not get full browser support now or in the future. You could try to use $.when to check an array of deferred objects, but now you are doing this in every file and file2 will be considered loaded as soon as the $.when is executed not when the callback is executed, so file1 still continues execution before file3 is loaded. This really still has the same problem. I decided to go backwards instead of forwards. Thank you document.writeln. I know it's taboo, but as long as it is used correctly this works well. You end up with code that can be debugged easily, shows in the DOM correctly and can ensure the order the dependencies are loaded correctly. You can of course use $ ("body").append(), but then you can no longer debug correctly any more. NOTE: You must use this only while the page is loading, otherwise you get a blank screen. In other words, always place this before / outside of document.ready. I have not tested using this after the page is loaded in a click event or anything like that, but I am pretty sure it'll fail. I liked the idea of extending jQuery, but obviously you don't need to. Before calling document.writeln, it checks to make sure the script has not already been loading by evaluating all the script elements. I assume that a script is not fully executed until its document.ready event has been executed. (I know using document.ready is not required, but many people use it, and handling this is a safeguard.) When the additional files are loaded the document.ready callbacks will get executed in the wrong order. To address this when a script is actually loaded, the script that imported it is re-imported itself and execution halted. This causes the originating file to now have its document.ready callback executed after any from any scripts that it imports. Instead of this approach you could attempt to modify the jQuery readyList, but this seemed like a worse solution. Solution: Usage: File3: File2: File1: I came to this question because I was looking for a simple way to maintain a collection of useful JavaScript plugins. After seeing some of the solutions here, I came up with this: Set up a file called "plugins.js" (or extensions.js or whatever you want). Keep your plugin files together with that one master file. plugins.js will have an array called pluginNames[] that we will iterate over each(), then append a <script> tag to the head for each plugin BUT: Even though all of the plugins get dropped into the head tag the way they ought to, they don't always get run by the browser when you click into the page or refresh. I've found it's more reliable to just write the script tags in a PHP include. You only have to write it once and that's just as much work as calling the plugin using JavaScript. There are several ways to implement modules in JavaScript. Here are the two most popular ones: Browsers do not support this moduling system yet, so in order for you to use this syntax you must use a bundler like Webpack. Using a bundler is better anyway because this can combine all of your different files into a single (or a couple of related) files. This will serve the files from the server to the client faster because each HTTP request has some associated overhead accompanied with it. Thus by reducing the overall HTTP request we improve the performance. Here is an example of ES6 modules: This moduling system is used in Node.js. You basically add your exports to an object which is called module.exports. You then can access this object via a require('modulePath'). Important here is to realize that these modules are being cached, so if you require() a certain module twice it will return the already created module.
__label__highlight __label__css __label__textselection __label__cross-browser For anchors that act like buttons (for example Questions, Tags, Users, etc. which are located on the top of the Stack Overflow page) or tabs, is there a CSS standard way to disable the highlighting effect if the user accidentally selects the text? I realize that this could be done with JavaScript and a little googling yielded the Mozilla-only -moz-user-select option. Is there a standard-compliant way to accomplish this with CSS, and if not, what is the "best practice" approach? UPDATE January, 2017: According to Can I use, the user-select is currently supported in all browsers except Internet Explorer 9 and its earlier versions (but sadly still needs a vendor prefix). These are all of the available correct CSS variations:   .noselect {   -webkit-touch-callout: none; /* iOS Safari */     -webkit-user-select: none; /* Safari */      -khtml-user-select: none; /* Konqueror HTML */        -moz-user-select: none; /* Old versions of Firefox */         -ms-user-select: none; /* Internet Explorer/Edge */             user-select: none; /* Non-prefixed version, currently                                   supported by Chrome, Edge, Opera and Firefox */ } <p>   Selectable text. </p> <p class="noselect">   Unselectable text. </p>    Note that user-select is in standardization process (currently in a W3C working draft). It is not guaranteed to work everywhere and there might be differences in implementation among browsers. Also browsers can drop support for it in the future. More information can be found in Mozilla Developer Network documentation. In most browsers, this can be achieved using proprietary variations on the CSS user-select property, originally proposed and then abandoned in CSS 3 and now proposed in CSS UI Level 4: For Internet Explorer < 10 and Opera < 15, you will need to use the unselectable attribute of the element you wish to be unselectable. You can set this using an attribute in HTML: Sadly this property isn't inherited, meaning you have to put an attribute in the start tag of every element inside the <div>. If this is a problem, you could instead use JavaScript to do this recursively for an element's descendants: Update 30 April 2014: This tree traversal needs to be rerun whenever a new element is added to the tree, but it seems from a comment by @Han that it is possible to avoid this by adding a mousedown event handler that sets unselectable on the target of the event. See http://jsbin.com/yagekiji/1 for details. This still doesn't cover all possibilities. While it is impossible to initiate selections in unselectable elements, in some browsers (Internet Explorer and Firefox, for example) it's still impossible to prevent selections that start before and end after the unselectable element without making the whole document unselectable. Until CSS 3's user-select property becomes available, Gecko-based browsers support the -moz-user-select property you already found. WebKit and Blink-based browsers support the -webkit-user-select property. This of course is not supported in browsers that do not use the Gecko rendering engine. There is no "standards" compliant quick-and-easy way to do it; using JavaScript is an option. The real question is, why do you want users to not be able to highlight and presumably copy and paste certain elements? I have not come across a single time that I wanted to not let users highlight a certain portion of my website. Several of my friends, after spending many hours reading and writing code will use the highlight feature as a way to remember where on the page they were, or providing a marker so that their eyes know where to look next. The only place I could see this being useful is if you have buttons for forms that should not be copy and pasted if a user copy and pasted the website. A JavaScript solution for Internet Explorer is: If you want to disable text selection on everything except on <p> elements, you can do this in CSS (watch out for the -moz-none which allows override in sub-elements, which is allowed in other browsers with none): In the solutions in previous answers selection is stopped, but the user still thinks you can select text because the cursor still changes. To keep it static, you'll have to set your CSS cursor:   .noselect {     cursor: default;     -webkit-touch-callout: none;     -webkit-user-select: none;     -khtml-user-select: none;     -moz-user-select: none;     -ms-user-select: none;     user-select: none; } <p>   Selectable text. </p> <p class="noselect">   Unselectable text. </p>    This will make your text totally flat, like it would be in a desktop application. You can do so in Firefox and Safari (Chrome also?) Workaround for WebKit: I found it in a CardFlip example. I like the hybrid CSS + jQuery solution. To make all elements inside <div class="draggable"></div> unselectable, use this CSS: And then, if you're using jQuery, add this inside a $(document).ready() block: I figure you still want any input elements to be interactable, hence the :not() pseudo-selector. You could use '*' instead if you don't care. Caveat: Internet Explorer 9 may not need this extra jQuery piece, so you may want to add a version check in there.   .hidden:after {     content: attr(data-txt); } <p class="hidden" data-txt="Some text you don't want to be selected"></p>    It's not the best way, though. You can use CSS or JavaScript for that. The JavaScript way is supported in older browsers, like old versions of Internet Explorer as well, but if it's not your case, use the CSS way then: HTML/JavaScript:   <html onselectstart='return false;'>   <body>     <h1>This is the Heading!</h1>     <p>And I'm the text, I won't be selected if you select me.</p>   </body> </html>    HTML/CSS:   .not-selectable {   -webkit-touch-callout: none;   -webkit-user-select: none;   -khtml-user-select: none;   -moz-user-select: none;   -ms-user-select: none;   user-select: none; } <body class="not-selectable">   <h1>This is the Heading!</h1>   <p>And I'm the text, I won't be selected if you select me.</p> </body>    For Internet Explorer in addition, you need to add pseudo class focus (.ClassName:focus) and outline-style: none. Try to insert these rows into the CSS and call the "disHighlight" at class property: A Quick Hack Update If you use the value none for all the CSS user-select properties (including browser prefixes of it), there is a problem which can be still occurred by this. As CSS-Tricks says, the problem is: WebKit still allows the text to be copied, if you select elements around it. You can also use the below one to enforce that an entire element gets selected which means if you click on an element, all the text wrapped in that element will get selected. For this all you have to do is changing the value none to all. For those who have trouble achieving the same in the Android browser with the touch event, use: You can do this with a mixin: In an HTML tag: Try it in this CodePen. If you are using an autoprefixer you can remove other prefixes. Browser compatibility here. If you are using Less and Bootstrap you could write:  Working CSS: This should work, but it won't work for the old browsers. There is a browser compatibility issue. Aside from the Mozilla-only property, no, there is no way to disable text selection with just standard CSS (as of now). If you notice, Stack Overflow doesn't disable text selection for their navigation buttons, and I would recommend against doing so in most cases, since it modifies normal selection behavior and makes it conflict with a user's expectations. This works in some browsers: Simply add your desired elements/ids in front of the selectors separated by commas without spaces, like so: The other answers are better; this should probably be seen as a last resort/catchall. Suppose there are two divs like this:   .second {   cursor: default;   user-select: none;   -webkit-user-select: none;   /* Chrome/Safari/Opera */   -moz-user-select: none;   /* Firefox */   -ms-user-select: none;   /* Internet Explorer/Edge */   -webkit-touch-callout: none;   /* iOS Safari */ } <div class="first">   This is my first div </div>  <div class="second">   This is my second div </div>    Set cursor to default so that it will give a unselectable feel to the user. Prefix need to be used to support it in all browsers. Without a prefix this may not work in all the answers. This will be useful if color selection is also not needed: ...all other browser fixes. It will work in Internet Explorer 9 or later. Add this to the first div in which you want to disable the selection for text: NOTE: The correct answer is correct in that it prevents you from being able to select the text. However, it does not prevent you from being able to copy the text, as I'll show with the next couple of screenshots (as of 7th Nov 2014).    As you can see, we were unable to select the numbers, but we were able to copy them. Tested on: Ubuntu, Google Chrome 38.0.2125.111. To get the result I needed, I found I had to use both ::selection and user-select It is easily done with: Alternatively: Let's say you have a <h1 id="example">Hello, World!</h1>. You will have to remove the innerHTML of that h1, in this case Hello, World. Then you will have to go to CSS and do this: Now it simply thinks it is a block-element, and not text. This is not CSS, but it is worth a mention: jQuery UI Disable Selection: Check my solution without JavaScript: jsFiddle   li:hover {     background-color: silver; } #id1:before {     content: "File"; } #id2:before {     content: "Edit"; } #id3:before {     content: "View"; } <ul>     <li><a id="id1" href="www.w1.com"></a>     <li><a id="id2" href="www.w2.com"></a>     <li><a id="id3" href="www.w3.com"></a> </ul>    Popup menu with my technique applied: http://jsfiddle.net/y4Lac/2/ Though this pseudo-element was in drafts of CSS Selectors Level 3, it was removed during the Candidate Recommendation phase, as it appeared that its behavior was under-specified, especially with nested elements, and interoperability wasn't achieved. It's being discussed in How ::selection works on nested elements. Despite it is being implemented in browsers, you can make an illusion of text not being selected by using the same color and background color on selection as of the tab design (in your case). Disallowing users to select the text will raise usability issues.
__label__language-agnostic __label__http __label__security __label__authentication __label__article  Want to improve this question? Update the question so it focuses on one problem only by editing this post.                  Closed 4 years ago. We believe that Stack Overflow should not just be a resource for very specific technical questions, but also for general guidelines on how to solve variations on common problems. "Form based authentication for websites" should be a fine topic for such an experiment. We'll assume you already know how to build a login+password HTML form which POSTs the values to a script on the server side for authentication. The sections below will deal with patterns for sound practical auth, and how to avoid the most common security pitfalls. To HTTPS or not to HTTPS? Unless the connection is already secure (that is, tunneled through HTTPS using SSL/TLS), your login form values will be sent in cleartext, which allows anyone eavesdropping on the line between browser and web server will be able to read logins as they pass through. This type of wiretapping is done routinely by governments, but in general, we won't address 'owned' wires other than to say this: Just use HTTPS. In essence, the only practical way to protect against wiretapping/packet sniffing during login is by using HTTPS or another certificate-based encryption scheme (for example, TLS) or a proven & tested challenge-response scheme (for example, the Diffie-Hellman-based SRP). Any other method can be easily circumvented by an eavesdropping attacker. Of course, if you are willing to get a little bit impractical, you could also employ some form of two-factor authentication scheme (e.g. the Google Authenticator app, a physical 'cold war style' codebook, or an RSA key generator dongle). If applied correctly, this could work even with an unsecured connection, but it's hard to imagine that a dev would be willing to implement two-factor auth but not SSL. (Do not) Roll-your-own JavaScript encryption/hashing Given the perceived (though now avoidable) cost and technical difficulty of setting up an SSL certificate on your website, some developers are tempted to roll their own in-browser hashing or encryption schemes in order to avoid passing cleartext logins over an unsecured wire. While this is a noble thought, it is essentially useless (and can be a security flaw) unless it is combined with one of the above - that is, either securing the line with strong encryption or using a tried-and-tested challenge-response mechanism (if you don't know what that is, just know that it is one of the most difficult to prove, most difficult to design, and most difficult to implement concepts in digital security). While it is true that hashing the password can be effective against password disclosure, it is vulnerable to replay attacks, Man-In-The-Middle attacks / hijackings (if an attacker can inject a few bytes into your unsecured HTML page before it reaches your browser, they can simply comment out the hashing in the JavaScript), or brute-force attacks (since you are handing the attacker both username, salt and hashed password). CAPTCHAS against humanity CAPTCHA is meant to thwart one specific category of attack: automated dictionary/brute force trial-and-error with no human operator. There is no doubt that this is a real threat, however, there are ways of dealing with it seamlessly that don't require a CAPTCHA, specifically properly designed server-side login throttling schemes - we'll discuss those later. Know that CAPTCHA implementations are not created alike; they often aren't human-solvable, most of them are actually ineffective against bots, all of them are ineffective against cheap third-world labor (according to OWASP, the current sweatshop rate is $12 per 500 tests), and some implementations may be technically illegal in some countries (see OWASP Authentication Cheat Sheet). If you must use a CAPTCHA, use Google's reCAPTCHA, since it is OCR-hard by definition (since it uses already OCR-misclassified book scans) and tries very hard to be user-friendly. Personally, I tend to find CAPTCHAS annoying, and use them only as a last resort when a user has failed to log in a number of times and throttling delays are maxed out. This will happen rarely enough to be acceptable, and it strengthens the system as a whole. Storing Passwords / Verifying logins This may finally be common knowledge after all the highly-publicized hacks and user data leaks we've seen in recent years, but it has to be said: Do not store passwords in cleartext in your database. User databases are routinely hacked, leaked or gleaned through SQL injection, and if you are storing raw, plaintext passwords, that is instant game over for your login security. So if you can't store the password, how do you check that the login+password combination POSTed from the login form is correct? The answer is hashing using a key derivation function. Whenever a new user is created or a password is changed, you take the password and run it through a KDF, such as Argon2, bcrypt, scrypt or PBKDF2, turning the cleartext password ("correcthorsebatterystaple") into a long, random-looking string, which is a lot safer to store in your database. To verify a login, you run the same hash function on the entered password, this time passing in the salt and compare the resulting hash string to the value stored in your database. Argon2, bcrypt and scrypt store the salt with the hash already. Check out this article on sec.stackexchange for more detailed information. The reason a salt is used is that hashing in itself is not sufficient -- you'll want to add a so-called 'salt' to protect the hash against rainbow tables. A salt effectively prevents two passwords that exactly match from being stored as the same hash value, preventing the whole database being scanned in one run if an attacker is executing a password guessing attack. A cryptographic hash should not be used for password storage because user-selected passwords are not strong enough (i.e. do not usually contain enough entropy) and a password guessing attack could be completed in a relatively short time by an attacker with access to the hashes. This is why KDFs are used - these effectively "stretch the key", which means that every password guess an attacker makes causes multiple repetitions of the hash algorithm, for example 10,000 times, which causes the attacker to guess the password 10,000 times slower. Session data - "You are logged in as Spiderman69" Once the server has verified the login and password against your user database and found a match, the system needs a way to remember that the browser has been authenticated. This fact should only ever be stored server side in the session data. If you are unfamiliar with session data, here's how it works: A single randomly-generated string is stored in an expiring cookie and used to reference a collection of data - the session data - which is stored on the server. If you are using an MVC framework, this is undoubtedly handled already. If at all possible, make sure the session cookie has the secure and HTTP Only flags set when sent to the browser. The HttpOnly flag provides some protection against the cookie being read through XSS attack. The secure flag ensures that the cookie is only sent back via HTTPS, and therefore protects against network sniffing attacks. The value of the cookie should not be predictable. Where a cookie referencing a non-existent session is presented, its value should be replaced immediately to prevent session fixation. Session state can also be maintained on the client side. This is achieved by using techniques like JWT (JSON Web Token). Persistent Login Cookies ("remember me" functionality) are a danger zone; on the one hand, they are entirely as safe as conventional logins when users understand how to handle them; and on the other hand, they are an enormous security risk in the hands of careless users, who may use them on public computers and forget to log out, and who may not know what browser cookies are or how to delete them. Personally, I like persistent logins for the websites I visit on a regular basis, but I know how to handle them safely. If you are positive that your users know the same, you can use persistent logins with a clean conscience. If not - well, then you may subscribe to the philosophy that users who are careless with their login credentials brought it upon themselves if they get hacked. It's not like we go to our user's houses and tear off all those facepalm-inducing Post-It notes with passwords they have lined up on the edge of their monitors, either. Of course, some systems can't afford to have any accounts hacked; for such systems, there is no way you can justify having persistent logins. If you DO decide to implement persistent login cookies, this is how you do it: First, take some time to read Paragon Initiative's article on the subject. You'll need to get a bunch of elements right, and the article does a great job of explaining each. And just to reiterate one of the most common pitfalls, DO NOT STORE THE PERSISTENT LOGIN COOKIE (TOKEN) IN YOUR DATABASE, ONLY A HASH OF IT! The login token is Password Equivalent, so if an attacker got their hands on your database, they could use the tokens to log in to any account, just as if they were cleartext login-password combinations. Therefore, use hashing (according to https://security.stackexchange.com/a/63438/5002 a weak hash will do just fine for this purpose) when storing persistent login tokens. Don't implement 'secret questions'. The 'secret questions' feature is a security anti-pattern. Read the paper from link number 4 from the MUST-READ list. You can ask Sarah Palin about that one, after her Yahoo! email account got hacked during a previous presidential campaign because the answer to her security question was... "Wasilla High School"! Even with user-specified questions, it is highly likely that most users will choose either: A 'standard' secret question like mother's maiden name or favorite pet A simple piece of trivia that anyone could lift from their blog, LinkedIn profile, or similar Any question that is easier to answer than guessing their password. Which, for any decent password, is every question you can imagine In conclusion, security questions are inherently insecure in virtually all their forms and variations, and should not be employed in an authentication scheme for any reason. The true reason why security questions even exist in the wild is that they conveniently save the cost of a few support calls from users who can't access their email to get to a reactivation code. This at the expense of security and Sarah Palin's reputation. Worth it? Probably not. I already mentioned why you should never use security questions for handling forgotten/lost user passwords; it also goes without saying that you should never e-mail users their actual passwords. There are at least two more all-too-common pitfalls to avoid in this field: Don't reset a forgotten password to an autogenerated strong password - such passwords are notoriously hard to remember, which means the user must either change it or write it down - say, on a bright yellow Post-It on the edge of their monitor. Instead of setting a new password, just let users pick a new one right away - which is what they want to do anyway. (An exception to this might be if the users are universally using a password manager to store/manage passwords that would normally be impossible to remember without writing it down). Always hash the lost password code/token in the database. AGAIN, this code is another example of a Password Equivalent, so it MUST be hashed in case an attacker got their hands on your database. When a lost password code is requested, send the plaintext code to the user's email address, then hash it, save the hash in your database -- and throw away the original. Just like a password or a persistent login token. A final note: always make sure your interface for entering the 'lost password code' is at least as secure as your login form itself, or an attacker will simply use this to gain access instead. Making sure you generate very long 'lost password codes' (for example, 16 case-sensitive alphanumeric characters) is a good start, but consider adding the same throttling scheme that you do for the login form itself. First, you'll want to read this small article for a reality check: The 500 most common passwords Okay, so maybe the list isn't the canonical list of most common passwords on any system anywhere ever, but it's a good indication of how poorly people will choose their passwords when there is no enforced policy in place. Plus, the list looks frighteningly close to home when you compare it to publicly available analyses of recently stolen passwords. So: With no minimum password strength requirements, 2% of users use one of the top 20 most common passwords. Meaning: if an attacker gets just 20 attempts, 1 in 50 accounts on your website will be crackable. Thwarting this requires calculating the entropy of a password and then applying a threshold.  The National Institute of Standards and Technology (NIST) Special Publication 800-63 has a set of very good suggestions.  That, when combined with a dictionary and keyboard layout analysis (for example, 'qwertyuiop' is a bad password), can reject 99% of all poorly selected passwords at a level of 18 bits of entropy.  Simply calculating password strength and showing a visual strength meter to a user is good, but insufficient.  Unless it is enforced, a lot of users will most likely ignore it. And for a refreshing take on user-friendliness of high-entropy passwords, Randall Munroe's Password Strength xkcd is highly recommended. Utilize Troy Hunt's Have I Been Pwned API to check users passwords against passwords compromised in public data breaches. First, have a look at the numbers: Password Recovery Speeds - How long will your password stand up If you don't have the time to look through the tables in that link, here's the list of them: It takes virtually no time to crack a weak password, even if you're cracking it with an abacus It takes virtually no time to crack an alphanumeric 9-character password if it is case insensitive It takes virtually no time to crack an intricate, symbols-and-letters-and-numbers, upper-and-lowercase password if it is less than 8 characters long (a desktop PC can search the entire keyspace up to 7 characters in a matter of days or even hours) It would, however, take an inordinate amount of time to crack even a 6-character password, if you were limited to one attempt per second! So what can we learn from these numbers? Well, lots, but we can focus on the most important part: the fact that preventing large numbers of rapid-fire successive login attempts (ie. the brute force attack) really isn't that difficult. But preventing it right isn't as easy as it seems. Generally speaking, you have three choices that are all effective against brute-force attacks (and dictionary attacks, but since you are already employing a strong passwords policy, they shouldn't be an issue): Present a CAPTCHA after N failed attempts (annoying as hell and often ineffective -- but I'm repeating myself here) Locking accounts and requiring email verification after N failed attempts (this is a DoS attack waiting to happen) And finally, login throttling: that is, setting a time delay between attempts after N failed attempts (yes, DoS attacks are still possible, but at least they are far less likely and a lot more complicated to pull off). Best practice #1: A short time delay that increases with the number of failed attempts, like: DoS attacking this scheme would be very impractical, since the resulting lockout time is slightly larger than the sum of the previous lockout times. To clarify: The delay is not a delay before returning the response to the browser. It is more like a timeout or refractory period during which login attempts to a specific account or from a specific IP address will not be accepted or evaluated at all. That is, correct credentials will not return in a successful login, and incorrect credentials will not trigger a delay increase. Best practice #2: A medium length time delay that goes into effect after N failed attempts, like: DoS attacking this scheme would be quite impractical, but certainly doable. Also, it might be relevant to note that such a long delay can be very annoying for a legitimate user. Forgetful users will dislike you. Best practice #3: Combining the two approaches - either a fixed, short time delay that goes into effect after N failed attempts, like: Or, an increasing delay with a fixed upper bound, like: This final scheme was taken from the OWASP best-practices suggestions (link 1 from the MUST-READ list) and should be considered best practice, even if it is admittedly on the restrictive side. As a rule of thumb, however, I would say: the stronger your password policy is, the less you have to bug users with delays. If you require strong (case-sensitive alphanumerics + required numbers and symbols) 9+ character passwords, you could give the users 2-4 non-delayed password attempts before activating the throttling. DoS attacking this final login throttling scheme would be very impractical. And as a final touch, always allow persistent (cookie) logins (and/or a CAPTCHA-verified login form) to pass through, so legitimate users won't even be delayed while the attack is in progress. That way, the very impractical DoS attack becomes an extremely impractical attack. Additionally, it makes sense to do more aggressive throttling on admin accounts, since those are the most attractive entry points Just as an aside, more advanced attackers will try to circumvent login throttling by 'spreading their activities': Distributing the attempts on a botnet to prevent IP address flagging Rather than picking one user and trying the 50.000 most common passwords (which they can't, because of our throttling), they will pick THE most common password and try it against 50.000 users instead. That way, not only do they get around maximum-attempts measures like CAPTCHAs and login throttling, their chance of success increases as well, since the number 1 most common password is far more likely than number 49.995 Spacing the login requests for each user account, say, 30 seconds apart, to sneak under the radar Here, the best practice would be logging the number of failed logins, system-wide, and using a running average of your site's bad-login frequency as the basis for an upper limit that you then impose on all users. Too abstract? Let me rephrase: Say your site has had an average of 120 bad logins per day over the past 3 months. Using that (running average), your system might set the global limit to 3 times that -- ie. 360 failed attempts over a 24 hour period. Then, if the total number of failed attempts across all accounts exceeds that number within one day (or even better, monitor the rate of acceleration and trigger on a calculated threshold), it activates system-wide login throttling - meaning short delays for ALL users (still, with the exception of cookie logins and/or backup CAPTCHA logins). I also posted a question with more details and a really good discussion of how to avoid tricky pitfals in fending off distributed brute force attacks Credentials can be compromised, whether by exploits, passwords being written down and lost, laptops with keys being stolen, or users entering logins into phishing sites.  Logins can be further protected with two-factor authentication, which uses out-of-band factors such as single-use codes received from a phone call, SMS message, app, or dongle. Several providers offer two-factor authentication services. Authentication can be completely delegated to a single-sign-on service, where another provider handles collecting credentials. This pushes the problem to a trusted third party. Google and Twitter both provide standards-based SSO services, while Facebook provides a similar proprietary solution. The only practical way to send credentials 100% securely is by using SSL. Using JavaScript to hash the password is not safe. Common pitfalls for client-side password hashing: There's another secure method called SRP, but it's patented (although it is freely licensed) and there are few good implementations available. Don't ever store passwords as plaintext in the database. Not even if you don't care about the security of your own site. Assume that some of your users will reuse the password of their online bank account. So, store the hashed password, and throw away the original. And make sure the password doesn't show up in access logs or application logs. OWASP recommends the use of Argon2 as your first choice for new applications. If this is not available, PBKDF2 or scrypt should be used instead. And finally if none of the above are available, use bcrypt. Hashes by themselves are also insecure. For instance, identical passwords mean identical hashes--this makes hash lookup tables an effective way of cracking lots of passwords at once. Instead, store the salted hash. A salt is a string appended to the password prior to hashing - use a different (random) salt per user. The salt is a public value, so you can store them with the hash in the database. See here for more on this. This means that you can't send the user their forgotten passwords (because you only have the hash). Don't reset the user's password unless you have authenticated the user (users must prove that they are able to read emails sent to the stored (and validated) email address.) Security questions are insecure - avoid using them. Why? Anything a security question does, a password does better. Read PART III: Using Secret Questions in @Jens Roland answer here in this wiki. After the user logs in, the server sends the user a session cookie. The server can retrieve the username or id from the cookie, but nobody else can generate such a cookie (TODO explain mechanisms). Cookies can be hijacked: they are only as secure as the rest of the client's machine and other communications. They can be read from disk, sniffed in network traffic, lifted by a cross-site scripting attack, phished from a poisoned DNS so the client sends their cookies to the wrong servers. Don't send persistent cookies. Cookies should expire at the end of the client session (browser close or leaving your domain). If you want to autologin your users, you can set a persistent cookie, but it should be distinct from a full-session cookie. You can set an additional flag that the user has auto-logged in, and needs to log in for real for sensitive operations. This is popular with shopping sites that want to provide you with a seamless, personalized shopping experience but still protect your financial details. For example, when you return to visit Amazon, they show you a page that looks like you're logged in, but when you go to place an order (or change your shipping address, credit card etc.), they ask you to confirm your password. Financial websites such as banks and credit cards, on the other hand, only have sensitive data and should not allow auto-login or a low-security mode. First, a strong caveat that this answer is not the best fit for this exact question. It should definitely not be the top answer! I will go ahead and mention Mozilla’s proposed BrowserID (or perhaps more precisely, the Verified Email Protocol) in the spirit of finding an upgrade path to better approaches to authentication in the future. I’ll summarize it this way: This is not strictly “form-based authentication for websites”. But it is an effort to transition from the current norm of form-based authentication to something more secure: browser-supported authentication. I just thought I'd share this solution that I found to be working just fine. I call it the Dummy Field (though I haven't invented this so don't credit me). In short: you just have to insert this into your <form> and check for it to be empty at when validating: The trick is to fool a bot into thinking it has to insert data into a required field, that's why I named the input "email". If you already have a field called email that you're using you should try naming the dummy field something else like "company", "phone" or "emailaddress". Just pick something you know you don't need and what sounds like something people would normally find logical to fill in into a web form. Now hide the input field using CSS or JavaScript/jQuery - whatever fits you best - just don't set the input type to hidden or else the bot won't fall for it. When you are validating the form (either client or server side) check if your dummy field has been filled to determine if it was sent by a human or a bot. Example: In case of a human: The user will not see the dummy field (in my case named "email") and will not attempt to fill it. So the value of the dummy field should still be empty when the form has been sent. In case of a bot: The bot will see a field whose type is text and a name email (or whatever it is you called it) and will logically attempt to fill it with appropriate data. It doesn't care if you styled the input form with some fancy CSS, web-developers do it all the time. Whatever the value in the dummy field is, we don't care as long as it's larger than 0 characters. I used this method on a guestbook in combination with CAPTCHA, and I haven't seen a single spam post since. I had used a CAPTCHA-only solution before, but eventually, it resulted in about five spam posts every hour. Adding the dummy field in the form has stopped (at least until now) all the spam from appearing.  I believe this can also be used just fine with a login/authentication form. Warning: Of course this method is not 100% foolproof. Bots can be programmed to ignore input fields with the style display:none applied to it. You also have to think about people who use some form of auto-completion (like most browsers have built-in!) to auto-fill all form fields for them. They might just as well pick up a dummy field. You can also vary this up a little by leaving the dummy field visible but outside the boundaries of the screen, but this is totally up to you.  Be creative! I do not think the above answer is "wrong" but there are large areas of authentication that are not touched upon (or rather the emphasis is on "how to implement cookie sessions", not on "what options are available and what are the trade-offs". My suggested edits/answers are Do NOT try to implement your own login form or database storage of passwords, unless  the data being stored is valueless at account creation and self-generated (that is, web 2.0 style like Facebook, Flickr, etc.) This avoids any need to have "sessions" or cookies as the browser itself will re-encrypt the communication each time. It is the most "lightweight" development approach. However, I do not recommend this, except for public, low-value services. This is an issue with some of the other answers above - do not try an re-implement server-side authentication mechanisms - this problem has been solved and is supported by most major browsers. Do not use cookies. Do not store anything in your own hand-rolled database. Just ask, per request, if the request is authenticated. Everything else should be supported by configuration and third-party trusted software. So ... First, we are confusing the initial creation of an account (with a password) with the re-checking of the password subsequently. If I am Flickr and creating your site for the first time, the new user has access to zero value (blank web space). I truly do not care if the person creating the account is lying about their name. If I am creating an account of the hospital intranet/extranet, the value lies in all the medical records, and so I do care about the identity (*) of the account creator. This is the very very hard part. The only decent solution is a web of trust. For example, you join the hospital as a doctor. You create a web page hosted somewhere with your photo, your passport number, and a public key, and hash them all with the private key. You then visit the hospital and the system administrator looks at your passport, sees if the photo matches you, and then hashes the web page/photo hash with the hospital private key. From now on we can securely exchange keys and tokens. As can anyone who trusts the hospital (there is the secret sauce BTW). The system administrator can also give you an RSA dongle or other two-factor authentication. But this is a lot of a hassle, and not very web 2.0. However, it is the only secure way to create new accounts that have access to valuable information that is not self-created. Kerberos and SPNEGO - single sign-on mechanisms with a trusted third party - basically the user verifies against a trusted third party. (NB this is not in any way the not to be trusted OAuth) SRP - sort of clever password authentication without a trusted third party. But here we are getting into the realms of "it's safer to use two-factor authentication, even if that's costlier" SSL client side - give the clients a public key certificate (support in all major browsers - but raises questions over client machine security). In the end, it's a tradeoff - what is the cost of a security breach vs the cost of implementing more secure approaches. One day, we may see a proper PKI widely accepted and so no more own rolled authentication forms and databases. One day... When hashing, don't use fast hash algorithms such as MD5 (many hardware implementations exist).  Use something like SHA-512.  For passwords, slower hashes are better. The faster you can create hashes, the faster any brute force checker can work. Slower hashes will therefore slow down brute forcing. A slow hash algorithm will make brute forcing impractical for longer passwords (8 digits +) A good article about realistic password strength estimation is: Dropbox Tech Blog » Blog Archive » zxcvbn: realistic password strength estimation My favourite rule in regards to authentication systems: use passphrases, not passwords. Easy to remember, hard to crack. More info: Coding Horror: Passwords vs. Pass Phrases I'd like to add one suggestion I've used, based on defense in depth. You don't need to have the same auth&auth system for admins as regular users. You can have a separate login form on a separate url executing separate code for requests that will grant high privileges. This one can make choices that would be a total pain to regular users. One such that I've used is to actually scramble the login URL for admin access and email the admin the new URL. Stops any brute force attack right away as your new URL can be arbitrarily difficult (very long random string) but your admin user's only inconvenience is following a link in their email. The attacker no longer knows where to even POST to.  I dont't know whether it was best to answer this as an answer or as a comment. I opted for the first option. Regarding the poing PART IV: Forgotten Password Functionality in the first answer, I would make a point about Timing Attacks.  In the Remember your password forms, an attacker could potentially check a full list of emails and detect which are registered to the system (see link below).  Regarding the Forgotten Password Form, I would add that it is a good idea to equal times between successful and unsucessful queries with some delay function.  https://crypto.stanford.edu/~dabo/papers/webtiming.pdf I would like to add one very important comment: - Many corporations deploy "internal use only" websites which are, effectively, "corporate applications" that happen to have been implemented through URLs. These URLs can (supposedly ...) only be resolved within "the company's internal network." (Which network magically includes all VPN-connected 'road warriors.')  When a user is dutifully-connected to the aforesaid network, their identity ("authentication") is [already ...] "conclusively known," as is their permission ("authorization") to do certain things ... such as ... "to access this website." This "authentication + authorization" service can be provided by several different technologies, such as LDAP (Microsoft OpenDirectory), or Kerberos. From your point-of-view, you simply know this: that anyone who legitimately winds-up at your website must be accompanied by [an environment-variable magically containing ...] a "token." (i.e. The absence of such a token must be immediate grounds for 404 Not Found.) The token's value makes no sense to you, but, should the need arise, "appropriate means exist" by which your website can "[authoritatively] ask someone who knows (LDAP... etc.)" about any and every(!) question that you may have. In other words, you do not avail yourself of any "home-grown logic." Instead, you inquire of The Authority and implicitly trust its verdict. Uh huh ... it's quite a mental-switch from the "wild-and-wooly Internet." Use OpenID Connect or User-Managed Access. As nothing is more efficient than not doing it at all.
__label__dictionary __label__merge __label__python I have two Python dictionaries, and I want to write a single expression that returns these two dictionaries, merged (i.e. taking the union).  The update() method would be what I need, if it returned its result instead of modifying a dictionary in-place. How can I get that final merged dictionary in z, not x? (To be extra-clear, the last-one-wins conflict-handling of dict.update() is what I'm looking for as well.) For dictionaries x and y, z becomes a shallowly merged dictionary with values from y replacing those from x. In Python 3.9.0 or greater (released 17 October 2020): PEP-584, discussed here, was implemented and provides the simplest method: In Python 3.5 or greater: In Python 2, (or 3.4 or lower) write a function: and now: Say you have two dictionaries and you want to merge them into a new dict without altering the original dictionaries: The desired result is to get a new dictionary (z) with the values merged, and the second dictionary's values overwriting those from the first. A new syntax for this, proposed in PEP 448 and available as of Python 3.5, is And it is indeed a single expression. Note that we can merge in with literal notation as well: and now: It is now showing as implemented in the release schedule for 3.5, PEP 478, and it has now made its way into What's New in Python 3.5 document. However, since many organizations are still on Python 2, you may wish to do this in a backward-compatible way. The classically Pythonic way, available in Python 2 and Python 3.0-3.4, is to do this as a two-step process: In both approaches, y will come second and its values will replace x's values, thus 'b' will point to 3 in our final result. If you are not yet on Python 3.5 or need to write backward-compatible code, and you want this in a single expression, the most performant while the correct approach is to put it in a function: and then you have a single expression: You can also make a function to merge an undefined number of dictionaries, from zero to a very large number: This function will work in Python 2 and 3 for all dictionaries. e.g. given dictionaries a to g: and key-value pairs in g will take precedence over dictionaries a to f, and so on. Don't use what you see in the formerly accepted answer: In Python 2, you create two lists in memory for each dict, create a third list in memory with length equal to the length of the first two put together, and then discard all three lists to create the dict. In Python 3, this will fail because you're adding two dict_items objects together, not two lists - and you would have to explicitly create them as lists, e.g. z = dict(list(x.items()) + list(y.items())). This is a waste of resources and computation power. Similarly, taking the union of items() in Python 3 (viewitems() in Python 2.7) will also fail when values are unhashable objects (like lists, for example). Even if your values are hashable, since sets are semantically unordered, the behavior is undefined in regards to precedence. So don't do this: This example demonstrates what happens when values are unhashable: Here's an example where y should have precedence, but instead the value from x is retained due to the arbitrary order of sets: Another hack you should not use: This uses the dict constructor and is very fast and memory-efficient (even slightly more-so than our two-step process) but unless you know precisely what is happening here (that is, the second dict is being passed as keyword arguments to the dict constructor), it's difficult to read, it's not the intended usage, and so it is not Pythonic. Here's an example of the usage being remediated in django. Dictionaries are intended to take hashable keys (e.g. frozensets or tuples), but this method fails in Python 3 when keys are not strings. From the mailing list, Guido van Rossum, the creator of the language, wrote: I am fine with declaring dict({}, **{1:3}) illegal, since after all it is abuse of the ** mechanism. and Apparently dict(x, **y) is going around as "cool hack" for "call x.update(y) and return x". Personally, I find it more despicable than cool. It is my understanding (as well as the understanding of the creator of the language) that the intended usage for dict(**y) is for creating dictionaries for readability purposes, e.g.: instead of Despite what Guido says, dict(x, **y) is in line with the dict specification, which btw. works for both Python 2 and 3. The fact that this only works for string keys is a direct consequence of how keyword parameters work and not a short-coming of dict. Nor is using the ** operator in this place an abuse of the mechanism, in fact, ** was designed precisely to pass dictionaries as keywords. Again, it doesn't work for 3 when keys are non-strings. The implicit calling contract is that namespaces take ordinary dictionaries, while users must only pass keyword arguments that are strings. All other callables enforced it. dict broke this consistency in Python 2: This inconsistency was bad given other implementations of Python (Pypy, Jython, IronPython). Thus it was fixed in Python 3, as this usage could be a breaking change. I submit to you that it is malicious incompetence to intentionally write code that only works in one version of a language or that only works given certain arbitrary constraints. More comments: dict(x.items() + y.items()) is still the most readable solution for Python 2. Readability counts. My response: merge_two_dicts(x, y) actually seems much clearer to me, if we're actually concerned about readability. And it is not forward compatible, as Python 2 is increasingly deprecated. {**x, **y} does not seem to handle nested dictionaries. the contents of nested keys are simply overwritten, not merged [...] I ended up being burnt by these answers that do not merge recursively and I was surprised no one mentioned it. In my interpretation of the word "merging" these answers describe "updating one dict with another", and not merging. Yes. I must refer you back to the question, which is asking for a shallow merge of two dictionaries, with the first's values being overwritten by the second's - in a single expression. Assuming two dictionaries of dictionaries, one might recursively merge them in a single function, but you should be careful not to modify the dictionaries from either source, and the surest way to avoid that is to make a copy when assigning values. As keys must be hashable and are usually therefore immutable, it is pointless to copy them: Usage: Coming up with contingencies for other value types is far beyond the scope of this question, so I will point you at my answer to the canonical question on a "Dictionaries of dictionaries merge". These approaches are less performant, but they will provide correct behavior. They will be much less performant than copy and update or the new unpacking because they iterate through each key-value pair at a higher level of abstraction, but they do respect the order of precedence (latter dictionaries have precedence) You can also chain the dictionaries manually inside a dict comprehension: or in python 2.6 (and perhaps as early as 2.4 when generator expressions were introduced): itertools.chain will chain the iterators over the key-value pairs in the correct order: I'm only going to do the performance analysis of the usages known to behave correctly. (Self-contained so you can copy and paste yourself.) In Python 3.8.1, NixOS: In your case, what you can do is: This will, as you want it, put the final dict in z, and make the value for key b be properly overridden by the second (y) dict's value: If you use Python 2, you can even remove the list() calls. To create z: If you use Python version 3.9.0a4 or greater, then you can directly use: An alternative: Another, more concise, option: Note: this has become a popular answer, but it is important to point out that if y has any non-string keys, the fact that this works at all is an abuse of a CPython implementation detail, and it does not work in Python 3, or in PyPy, IronPython, or Jython. Also, Guido is not a fan. So I can't recommend this technique for forward-compatible or cross-implementation portable code, which really means it should be avoided entirely. This probably won't be a popular answer, but you almost certainly do not want to do this.  If you want a copy that's a merge, then use copy (or deepcopy, depending on what you want) and then update.  The two lines of code are much more readable - more Pythonic - than the single line creation with .items() + .items().  Explicit is better than implicit. In addition, when you use .items() (pre Python 3.0), you're creating a new list that contains the items from the dict.  If your dictionaries are large, then that is quite a lot of overhead (two large lists that will be thrown away as soon as the merged dict is created).  update() can work more efficiently, because it can run through the second dict item-by-item. In terms of time: IMO the tiny slowdown between the first two is worth it for the readability.  In addition, keyword arguments for dictionary creation was only added in Python 2.3, whereas copy() and update() will work in older versions. In a follow-up answer, you asked about the relative performance of these two alternatives: On my machine, at least (a fairly ordinary x86_64 running Python 2.5.2), alternative z2 is not only shorter and simpler but also significantly faster.  You can verify this for yourself using the timeit module that comes with Python. Example 1: identical dictionaries mapping 20 consecutive integers to themselves: z2 wins by a factor of 3.5 or so.  Different dictionaries seem to yield quite different results, but z2 always seems to come out ahead.  (If you get inconsistent results for the same test, try passing in -r with a number larger than the default 3.) Example 2: non-overlapping dictionaries mapping 252 short strings to integers and vice versa: z2 wins by about a factor of 10.  That's a pretty big win in my book! After comparing those two, I wondered if z1's poor performance could be attributed to the overhead of constructing the two item lists, which in turn led me to wonder if this variation might work better: A few quick tests, e.g. lead me to conclude that z3 is somewhat faster than z1, but not nearly as fast as z2.  Definitely not worth all the extra typing. This discussion is still missing something important, which is a performance comparison of these alternatives with the "obvious" way of merging two lists: using the update method.  To try to keep things on an equal footing with the expressions, none of which modify x or y, I'm going to make a copy of x instead of modifying it in-place, as follows: A typical result: In other words, z0 and z2 seem to have essentially identical performance.  Do you think this might be a coincidence?  I don't.... In fact, I'd go so far as to claim that it's impossible for pure Python code to do any better than this.  And if you can do significantly better in a C extension module, I imagine the Python folks might well be interested in incorporating your code (or a variation on your approach) into the Python core.  Python uses dict in lots of places; optimizing its operations is a big deal. You could also write this as as Tony does, but (not surprisingly) the difference in notation turns out not to have any measurable effect on performance.  Use whichever looks right to you.  Of course, he's absolutely correct to point out that the two-statement version is much easier to understand. In Python 3.0 and later, you can use collections.ChainMap which groups multiple dicts or other mappings together to create a single, updateable view: Update for Python 3.5 and later: You can use PEP 448 extended dictionary packing and unpacking.  This is fast and easy: Update for Python 3.9 and later:  You can use the PEP 584 union operator: I wanted something similar, but with the ability to specify how the values on duplicate keys were merged, so I hacked this out (but did not heavily test it).  Obviously this is not a single expression, but it is a single function call. Demonstration: Outputs: Thanks rednaw for edits. The best version I could think while not using copy would be: It's faster than dict(x.items() + y.items()) but not as fast as n = copy(a); n.update(b), at least on CPython. This version also works in Python 3 if you change iteritems() to items(), which is automatically done by the 2to3 tool. Personally I like this version best because it describes fairly good what I want in a single  functional syntax. The only minor problem is that it doesn't make completely obvious that values from y takes precedence over values from x, but I don't believe it's difficult to figure that out. Python 3.5 (PEP 448) allows a nicer syntax option: Or even  In Python 3.9 you also use | and |= with the below example from PEP 584 For items with keys in both dictionaries ('b'), you can control which one ends up in the output by putting that one last. While the question has already been answered several times, this simple solution to the problem has not been listed yet. It is as fast as z0 and the evil z2 mentioned above, but easy to understand and change. Among such shady and dubious answers, this shining example is the one and only good way to merge dicts in Python, endorsed by dictator for life Guido van Rossum himself!  Someone else suggested half of this, but did not put it in a function. gives: If you think lambdas are evil then read no further. As requested, you can write the fast and memory-efficient solution with one expression: As suggested above, using two lines or writing a function is probably a better way to go. Be pythonic. Use a comprehension: In python3, the items method no longer returns a list, but rather a view, which acts like a set. In this case you'll need to take the set union since concatenating with + won't work: For python3-like behavior in version 2.7, the viewitems method should work in place of items: I prefer this notation anyways since it seems more natural to think of it as a set union operation rather than concatenation (as the title shows). Edit: A couple more points for python 3. First, note that the dict(x, **y) trick won't work in python 3 unless the keys in y are strings. Also, Raymond Hettinger's Chainmap answer is pretty elegant, since it can take an arbitrary number of dicts as arguments, but from the docs it looks like it sequentially looks through a list of all the dicts for each lookup: Lookups search the underlying mappings successively until a key is found. This can slow you down if you have a lot of lookups in your application: So about an order of magnitude slower for lookups. I'm a fan of Chainmap, but looks less practical where there may be many lookups. Two dictionaries n dictionaries sum has bad performance. See https://mathieularose.com/how-not-to-flatten-a-list-of-lists-in-python/ Simple solution using itertools that preserves order (latter dicts have precedence) And it's usage: Abuse leading to a one-expression solution for Matthew's answer: You said you wanted one expression, so I abused lambda to bind a name, and tuples to override lambda's one-expression limit. Feel free to cringe. You could also do this of course if you don't care about copying it: Even though the answers were good for this shallow dictionary, none of the methods defined here actually do a deep dictionary merge. Examples follow: One would expect a result of something like this: Instead, we get this: The 'one' entry should have had 'depth_2' and 'extra' as items inside its dictionary if it truly was a merge. Using chain also, does not work: Results in: The deep merge that rcwesick gave also creates the same result. Yes, it will work to merge the sample dictionaries, but none of them are a generic mechanism to merge.  I'll update this later once I write a method that does a true merge. If you don't mind mutating x, Simple, readable, performant. You know update() always returns None, which is a false value. So the above expression will always evaluate to x, after updating it. Most mutating methods in the standard library (like .update()) return None by convention, so this kind of pattern will work on those too. However, if you're using a dict subclass or some other method that doesn't follow this convention, then or may return its left operand, which may not be what you want. Instead, you can use a tuple display and index, which works regardless of what the first element evaluates to (although it's not quite as pretty): If you don't have x in a variable yet, you can use lambda to make a local without using an assignment statement. This amounts to using lambda as a let expression, which is a common technique in functional languages, but maybe unpythonic. Although it's not that different from the following use of the new walrus operator (Python 3.8+ only): If you do want a copy, PEP 584 style x | y is the most Pythonic on 3.9+. If you must support older versions, PEP 448 style {**x, **y} is easiest for 3.5+. But if that's not available in your (even older) Python version, the let pattern works here too. (That is, of course, nearly equivalent to (z := x.copy()).update(y) or z, but if your Python version is new enough for that, then the PEP 448 style will be available.) (For Python2.7* only; there are simpler solutions for Python3*.) If you're not averse to importing a standard library module, you can do (The or a bit in the lambda is necessary because dict.update always returns None on success.) Drawing on ideas here and elsewhere I've comprehended a function: Usage (tested in python 3): You could use a lambda instead. The problem I have with solutions listed to date is that, in the merged dictionary, the value for key "b" is 10 but, to my way of thinking, it should be 12. In that light, I present the following: It's so silly that .update returns nothing. I just use a simple helper function to solve the problem: Examples: This should solve your problem. There will be a new option when Python 3.8 releases (scheduled for 20 October, 2019), thanks to PEP 572: Assignment Expressions. The new assignment expression operator := allows you to assign the result of the copy and still use it to call update, leaving the combined code a single expression, rather than two statements, changing: to: while behaving identically in every way. If you must also return the resulting dict (you asked for an expression returning the dict; the above creates and assigns to newdict, but doesn't return it, so you couldn't use it to pass an argument to a function as is, a la myfunc((newdict := dict1.copy()).update(dict2))), then just add or newdict to the end (since update returns None, which is falsy, it will then evaluate and return newdict as the result of the expression): Important caveat: In general, I'd discourage this approach in favor of: The unpacking approach is clearer (to anyone who knows about generalized unpacking in the first place, which you should), doesn't require a name for the result at all (so it's much more concise when constructing a temporary that is immediately passed to a function or included in a list/tuple literal or the like), and is almost certainly faster as well, being (on CPython) roughly equivalent to: but done at the C layer, using the concrete dict API, so no dynamic method lookup/binding or function call dispatch overhead is involved (where (newdict := dict1.copy()).update(dict2) is unavoidably identical to the original two-liner in behavior, performing the work in discrete steps, with dynamic lookup/binding/invocation of methods. It's also more extensible, as merging three dicts is obvious: where using assignment expressions won't scale like that; the closest you could get would be: or without the temporary tuple of Nones, but with truthiness testing of each None result: either of which is obviously much uglier, and includes further inefficiencies (either a wasted temporary tuple of Nones for comma separation, or pointless truthiness testing of each update's None return for or separation). The only real advantage to the assignment expression approach occurs if: New in Python 3.9: Use the union operator (|) to merge dicts similar to sets: For matching keys, the right dict takes precedence. This also works for |= to modify a dict in-place: This can be done with a single dict comprehension: In my view the best answer for the 'single expression' part as no extra functions are needed, and it is short.
__label__git __label__git-branch __label__branching-and-merging I'd like to move the last several commits I've committed to master to a new branch and take master back to before those commits were made. Unfortunately, my Git-fu is not strong enough yet, any help? I.e. How can I go from this to this? If you want to move your commits to an existing branch, it will look like this: WARNING: This method works because you are creating a new branch with the first command: git branch newbranch. If you want to move commits to an existing branch you need to merge your changes into the existing branch before executing git reset --hard HEAD~3 (see Moving to an existing branch above). If you don't merge your changes first, they will be lost. Unless there are other circumstances involved, this can be easily done by branching and rolling back. But do make sure how many commits to go back. Alternatively, you can instead of HEAD~3, simply provide the hash of the commit (or the reference like origin/master) you want to "revert back to" on the master (/current) branch, e.g: *1 You will only be "losing" commits from the master branch, but don't worry, you'll have those commits in newbranch! WARNING: With Git version 2.0 and later, if you later git rebase the new branch upon the original (master) branch, you may need an explicit --no-fork-point option during the rebase to avoid losing the carried-over commits.  Having branch.autosetuprebase always set makes this more likely.  See John Mellor's answer for details. For those wondering why it works (as I was at first): You want to go back to C, and move D and E to the new branch.  Here's what it looks like at first: After git branch newBranch: After git reset --hard HEAD~2: Since a branch is just a pointer, master pointed to the last commit.  When you made newBranch, you simply made a new pointer to the last commit.  Then using git reset you moved the master pointer back two commits.  But since you didn't move newBranch, it still points to the commit it originally did. The method exposed by sykora is the best option in this case. But sometimes is not the easiest and it's not a general method. For a general method use git cherry-pick: To achieve what OP wants, its a 2-step process: Execute Note the hashes of (say 3) commits you want on newbranch. Here I shall use: C commit: 9aa1233 D commit: 453ac3d E commit: 612ecb3  Note: You can use the first seven characters or   the whole commit hash git cherry-pick applies those three commits to newbranch. Do NOT do this: As the next time you run git rebase (or git pull --rebase) those 3 commits would be silently discarded from newbranch! (see explanation below) Instead do this: Warning: the reflog is enabled by default, but if you've manually disabled it (e.g. by using a "bare" git repository), you won't be able to get the 3 commits back after running git reset --keep HEAD~3. An alternative that doesn't rely on the reflog is: (if you prefer you can write @{-1} - the previously checked out branch - instead of oldbranch). Why would git rebase discard the 3 commits after the first example? It's because git rebase with no arguments enables the --fork-point option by default, which uses the local reflog to try to be robust against the upstream branch being force-pushed. Suppose you branched off origin/master when it contained commits M1, M2, M3, then made three commits yourself: but then someone rewrites history by force-pushing origin/master to remove M2: Using your local reflog, git rebase can see that you forked from an earlier incarnation of the origin/master branch, and hence that the M2 and M3 commits are not really part of your topic branch. Hence it reasonably assumes that since M2 was removed from the upstream branch, you no longer want it in your topic branch either once the topic branch is rebased: This behavior makes sense, and is generally the right thing to do when rebasing. So the reason that the following commands fail: is because they leave the reflog in the wrong state. Git sees newbranch as having forked off the upstream branch at a revision that includes the 3 commits, then the reset --hard rewrites the upstream's history to remove the commits, and so next time you run git rebase it discards them like any other commit that has been removed from the upstream. But in this particular case we want those 3 commits to be considered as part of the topic branch. To achieve that, we need to fork off the upstream at the earlier revision that doesn't include the 3 commits. That's what my suggested solutions do, hence they both leave the reflog in the correct state. For more details, see the definition of --fork-point in the git rebase and git merge-base docs. Yet another way to do this, using just 2 commands. Also keeps your current working tree intact. Old version - before I learned about git branch -f Being able to push to . is a nice trick to know. Here's a far simpler solution for commits to the wrong branch. Starting on branch master that has three mistaken commits: You can now use git add and git commit as you normally would. All new commits will be added to newbranch.  The OP stated the goal was to "take master back to before those commits were made" without losing changes and this solution does that. I do this at least once a week when I accidentally make new commits to master instead of develop. Usually I have only one commit to rollback in which case using git reset HEAD^ on line 1 is a simpler way to rollback just one commit.  Don't do this if you pushed master's changes upstream Someone else may have pulled those changes. If you are only rewriting your local master there's no impact when it's pushed upstream, but pushing a rewritten history to collaborators can cause headaches.  This doesn't "move" them in the technical sense but it has the same effect: To do this without rewriting history (i.e. if you've already pushed the commits): Both branches can then be pushed without force! Had just this situation: I performed: I expected that commit I would be the HEAD, but commit L is it now... To be sure to land on the right spot in the history its easier to work with the hash of the commit How can I go from this to this? With two commands giving and  giving 1. Rename master branch to your newbranch (assuming you are on master branch): 2. Create master branch from the commit that you wish: e.g. git checkout -b master a34bc22 If you just need to move all your unpushed commits to a new branch, then you just need to, create a new branch from the current one :git branch new-branch-name push your new branch: git push origin new-branch-name revert your old(current) branch to the last pushed/stable state: git reset --hard origin/old-branch-name Some people also have other upstreams rather than origin,  they should use appropriate upstream You can do this is just 3  simple step that i used.   1) make new branch where you want to commit you recent update.   git branch <branch name>  2)  Find  Recent Commit Id for commit on new branch.  git log 3)  Copy that commit id  note that Most Recent commit list take place on top. so you can find your commit. you also find this via message.  git cherry-pick d34bcef232f6c...  you can also provide some rang of commit id. git cherry-pick d34bcef...86d2aec Now your job done. If you picked correct id and correct branch then you will success. So before do this be careful. else another problem can occur.   Now you can push your code  git push 1) Create a new branch, which moves all your changes to new_branch. 2) Then go back to old branch. 3) Do git rebase  4) Then the opened editor contains last 3 commit information. 5) Change pick to drop in all those 3 commits. Then save and close the editor.  6) Now last 3 commits are removed from current branch (master). Now push the branch forcefully, with + sign before branch name. Most of the solutions here count the amount of commits you'd like to go back. I think this is an error prone methodology. Counting would require recounting. You can simply pass the commit hash of the commit you want to be at HEAD or in other words, the commit you'd like to be the last commit via: (Notice see commit hash) To avoid this:
__label__directory __label__bash How do I get the path of the directory in which a Bash script is located, inside that script? I want to use a Bash script as a launcher for another application. I want to change the working directory to the one where the Bash script is located, so I can operate on the files in that directory, like so: is a useful one-liner which will give you the full directory name of the script no matter where it is being called from. It will work as long as the last component of the path used to find the script is not a symlink (directory links are OK).  If you also want to resolve any links to the script itself, you need a multi-line solution: This last one will work with any combination of aliases, source, bash -c, symlinks, etc. Beware: if you cd to a different directory before running this snippet, the result may be incorrect! Also, watch out for $CDPATH gotchas, and stderr output side effects if the user has smartly overridden cd to redirect output to stderr instead (including escape sequences, such as when calling update_terminal_cwd >&2 on Mac). Adding >/dev/null 2>&1 at the end of your cd command will take care of both possibilities. To understand how it works, try running this more verbose form: And it will print something like: Use dirname "$0": Using pwd alone will not work if you are not running the script from the directory it is contained in. The dirname command is the most basic, simply parsing the path up to the filename off of the $0 (script name) variable: But, as matt b pointed out, the path returned is different depending on how the script is called. pwd doesn't do the job because that only tells you what the current directory is, not what directory the script resides in. Additionally, if a symbolic link to a script is executed, you're going to get a (probably relative) path to where the link resides, not the actual script. Some others have mentioned the readlink command, but at its simplest, you can use: readlink will resolve the script path to an absolute path from the root of the filesystem. So, any paths containing single or double dots, tildes and/or symbolic links will be resolved to a full path. Here's a script demonstrating each of these, whatdir.sh: Running this script in my home dir, using a relative path: Again, but using the full path to the script: Now changing directories: And finally using a symbolic link to execute the script: It works for all versions, including Alternatively, if the Bash script itself is a relative symlink you want to follow it and return the full path of the linked-to script: SCRIPT_PATH is given in full path, no matter how it is called. Just make sure you locate this at start of the script. You can use $BASH_SOURCE: Note that you need to use #!/bin/bash and not #!/bin/sh since it's a Bash extension. Short answer: or (preferably): This should do it: This works with symlinks and spaces in path. See the man pages for dirname and readlink. From the comment track it seems not to work with Mac OS. I have no idea why that is. Any suggestions? Here is an easy-to-remember script: pwd can be used to find the current working directory, and dirname to find the directory of a particular file (command that was run, is $0, so dirname $0 should give you the directory of the current script). However, dirname gives precisely the directory portion of the filename, which more likely than not is going to be relative to the current working directory. If your script needs to change directory for some reason, then the output from dirname becomes meaningless. I suggest the following: This way, you get an absolute, rather than a relative directory. Since the script will be run in a separate Bash instance, there isn't any need to restore the working directory afterwards, but if you do want to change back in your script for some reason, you can easily assign the value of pwd to a variable before you change directory, for future use. Although just solves the specific scenario in the question, I find having the absolute path to more more useful generally. I don't think this is as easy as others have made it out to be.  pwd doesn't work, as the current directory is not necessarily the directory with the script.  $0 doesn't always have the information either.  Consider the following three ways to invoke a script: In the first and third ways $0 doesn't have the full path information.  In the second and third, pwd does not work.  The only way to get the directory in the third way would be to run through the path and find the file with the correct match.  Basically the code would have to redo what the OS does. One way to do what you are asking would be to just hardcode the data in the /usr/share directory, and reference it by its full path.  Data shoudn't be in the /usr/bin directory anyway, so this is probably the thing to do.   This gets the current working directory on Mac OS X v10.6.6 (Snow Leopard): This is Linux specific, but you could use: Here is a POSIX compliant one-liner: The shortest and most elegant way to do this is: This would work on all platforms and is super clean. More details can be found in "Which directory is that bash script in?".  I tried all of these and none worked. One was very close, but it had a tiny bug that broke it badly; they forgot to wrap the path in quotation marks. Also a lot of people assume you're running the script from a shell, so they forget when you open a new script it defaults to your home. Try this directory on for size: This gets it right regardless how or where you run it: So to make it actually useful, here's how to change to the directory of the running script: Here is the simple, correct way: Explanation: ${BASH_SOURCE[0]} - the full path to the script. The value of this will be correct even when the script is being sourced, e.g. source <(echo 'echo $0') prints bash, while replacing it with ${BASH_SOURCE[0]} will print the full path of the script. (Of course, this assumes you're OK taking a dependency on Bash.) readlink -f - Recursively resolves any symlinks in the specified path. This is a GNU extension, and not available on (for example) BSD systems. If you're running a Mac, you can use Homebrew to install GNU coreutils and supplant this with greadlink -f. And of course dirname gets the parent directory of the path.  This is a slight revision to the solution e-satis and 3bcdnlklvc04a pointed out in their answer: This should still work in all the cases they listed. This will prevent popd after a failed pushd. Thanks to konsolebox. I would use something like this: For systems having GNU coreutils readlink (for example, Linux): There's no need to use BASH_SOURCE when $0 contains the script filename. $_ is worth mentioning as an alternative to $0.  If you're running a script from Bash, the accepted answer can be shortened to: Note that this has to be the first statement in your script. These are short ways to get script information: Folders and files: Using these commands: And I got this output: Also see: https://pastebin.com/J8KjxrPF This works in Bash 3.2: If you have a ~/bin directory in your $PATH, you have  A inside this directory. It sources the script ~/bin/lib/B. You know where the included script is relative to the original one, in the lib subdirectory, but not where it is relative to the user's current directory. This is solved by the following (inside A): It doesn't matter where the user is or how he/she calls the script. This will always work. I've compared many of the answers given, and came up with some more compact solutions. These seem to handle all of the crazy edge cases that arise from your favorite combination of: If you're running from Linux, it seems that using the proc handle is the best solution to locate the fully resolved source of the currently running script (in an interactive session, the link points to the respective /dev/pts/X): This has a small bit of ugliness to it, but the fix is compact and easy to understand. We aren't using bash primitives only, but I'm okay with that because readlink simplifies the task considerably. The echo X adds an X to the end of the variable string so that any trailing whitespace in the filename doesn't get eaten, and the parameter substitution ${VAR%X} at the end of the line gets rid of the X. Because readlink adds a newline of its own (which would normally be eaten in the command substitution if not for our previous trickery), we have to get rid of that, too. This is most easily accomplished using the $'' quoting scheme, which lets us use escape sequences such as \n to represent newlines (this is also how you can easily make deviously named directories and files). The above should cover your needs for locating the currently running script on Linux, but if you don't have the proc filesystem at your disposal, or if you're trying to locate the fully resolved path of some other file, then maybe you'll find the below code helpful. It's only a slight modification from the above one-liner. If you're playing around with strange directory/filenames, checking the output with both ls and readlink is informative, as ls will output "simplified" paths, substituting ? for things like newlines. Try using: I believe I've got this one. I'm late to the party, but I think some will appreciate it being here if they come across this thread. The comments should explain: For many cases, all you need to acquire is the full path to the script you just called. This can be easily accomplished using realpath. Note that realpath is part of GNU coreutils. If you don't have it already installed (it comes default on Ubuntu), you can install it with sudo apt update && sudo apt install coreutils. get_script_path.sh: Example output: Note that realpath also successfully walks down symbolic links to determine and point to their targets rather than pointing to the symbolic link. The code above is now part of my eRCaGuy_hello_world repo in this file here: bash/get_script_path.sh. Try the following cross-compatible solution: As the commands such as realpath or readlink could be not available (depending on the operating system). Note: In Bash, it's recommended to use ${BASH_SOURCE[0]} instead of $0, otherwise path can break when sourcing the file (source/.). Alternatively you can try the following function in Bash: This function takes one argument. If argument has already absolute path, print it as it is, otherwise print $PWD variable + filename argument (without ./ prefix). Related:
__label__javascript __label__object __label__clone What is the most efficient way to clone a JavaScript object? I've seen obj = eval(uneval(o)); being used, but that's non-standard and only supported by Firefox. I've done things like obj = JSON.parse(JSON.stringify(o)); but question the efficiency.  I've also seen recursive copying functions with various flaws.  I'm surprised no canonical solution exists. It's called "structured cloning", works experimentally in Node 11 and later, and hopefully will land in browsers. See this answer for more details. If you do not use Dates, functions, undefined, Infinity, RegExps, Maps, Sets, Blobs, FileLists, ImageDatas, sparse Arrays, Typed Arrays or other complex types within your object, a very simple one liner to deep clone an object is: JSON.parse(JSON.stringify(object))   const a = {   string: 'string',   number: 123,   bool: false,   nul: null,   date: new Date(),  // stringified   undef: undefined,  // lost   inf: Infinity,  // forced to 'null'   re: /.*/,  // lost } console.log(a); console.log(typeof a.date);  // Date object const clone = JSON.parse(JSON.stringify(a)); console.log(clone); console.log(typeof clone.date);  // result of .toISOString()    See Corban's answer for benchmarks. Since cloning objects is not trivial (complex types, circular references, function etc.), most major libraries provide function to clone objects. Don't reinvent the wheel - if you're already using a library, check if it has an object cloning function. For example, For completeness, note that ES6 offers two shallow copy mechanisms: Object.assign() and the spread syntax. which copies values of all enumerable own properties from one object to another. For example: Checkout this benchmark: http://jsben.ch/#/bWfk9 In my previous tests where speed was a main concern I found  to be the slowest way to deep clone an object (it is slower than jQuery.extend with deep flag set true by 10-20%). jQuery.extend is pretty fast when the deep flag is set to false (shallow clone). It is a good option, because it includes some extra logic for type validation and doesn't copy over undefined properties, etc., but this will also slow you down a little. If you know the structure of the objects you are trying to clone or can avoid deep nested arrays you can write a simple for (var i in obj) loop to clone your object while checking hasOwnProperty and it will be much much faster than jQuery. Lastly if you are attempting to clone a known object structure in a hot loop you can get MUCH MUCH MORE PERFORMANCE by simply in-lining the clone procedure and manually constructing the object. JavaScript trace engines suck at optimizing for..in loops and checking hasOwnProperty will slow you down as well. Manual clone when speed is an absolute must. Beware using the JSON.parse(JSON.stringify(obj)) method on Date objects - JSON.stringify(new Date()) returns a string representation of the date in ISO format, which JSON.parse() doesn't convert back to a Date object. See this answer for more details. Additionally, please note that, in Chrome 65 at least, native cloning is not the way to go. According to JSPerf, performing native cloning by creating a new function is nearly 800x slower than using JSON.stringify which is incredibly fast all the way across the board. Update for ES6 If you are using Javascript ES6 try this native method for cloning or shallow copy. Assuming that you have only variables and not any functions in your object, you can  just use: The HTML standard includes an internal structured cloning/serialization algorithm that can create deep clones of objects. It is still limited to certain built-in types, but in addition to the few types supported by JSON it also supports Dates, RegExps, Maps, Sets, Blobs, FileLists, ImageDatas, sparse Arrays, Typed Arrays, and probably more in the future. It also preserves references within the cloned data, allowing it to support cyclical and recursive structures that would cause errors for JSON. The v8 module in Node.js currently (as of Node 11) exposes the structured serialization API directly, but this functionality is still marked as "experimental", and subject to change or removal in future versions. If you're using a compatible version, cloning an object is as simple as: Browsers do not currently provide a direct interface for the structured cloning algorithm, but a global structuredClone() function has been discussed in whatwg/html#793 on GitHub. As currently proposed, using it for most purposes would be as simple as: Unless this is shipped, browsers' structured clone implementations are only exposed indirectly. The lower-overhead way to create a structured clone with existing APIs is to post the data through one port of a MessageChannels. The other port will emit a message event with a structured clone of the attached .data. Unfortunately, listening for these events is necessarily asynchronous, and the synchronous alternatives are less practical. There are no good options for creating structured clones synchronously. Here are a couple of impractical hacks instead. history.pushState() and history.replaceState() both create a structured clone of their first argument, and assign that value to history.state. You can use this to create a structured clone of any object like this:   'use strict';  const main = () => {   const original = { date: new Date(), number: Math.random() };   original.self = original;    const clone = structuredClone(original);      // They're different objects:   console.assert(original !== clone);   console.assert(original.date !== clone.date);    // They're cyclical:   console.assert(original.self === original);   console.assert(clone.self === clone);    // They contain equivalent values:   console.assert(original.number === clone.number);   console.assert(Number(original.date) === Number(clone.date));      console.log("Assertions complete."); };  const structuredClone = obj => {   const oldState = history.state;   history.replaceState(obj, null);   const clonedObj = history.state;   history.replaceState(oldState, null);   return clonedObj; };  main();    Though synchronous, this can be extremely slow. It incurs all of the overhead associated with manipulating the browser history. Calling this method repeatedly can cause Chrome to become temporarily unresponsive. The Notification constructor creates a structured clone of its associated data. It also attempts to display a browser notification to the user, but this will silently fail unless you have requested notification permission. In case you have the permission for other purposes, we'll immediately close the notification we've created.   'use strict';  const main = () => {   const original = { date: new Date(), number: Math.random() };   original.self = original;    const clone = structuredClone(original);      // They're different objects:   console.assert(original !== clone);   console.assert(original.date !== clone.date);    // They're cyclical:   console.assert(original.self === original);   console.assert(clone.self === clone);    // They contain equivalent values:   console.assert(original.number === clone.number);   console.assert(Number(original.date) === Number(clone.date));      console.log("Assertions complete."); };  const structuredClone = obj => {   const n = new Notification('', {data: obj, silent: true});   n.close();   return n.data; };  main();    If there wasn't any builtin one, you could try: An Object.assign method is part of the ECMAScript 2015 (ES6) standard and does exactly what you need. The Object.assign() method is used to copy the values of all enumerable own properties from one or more source objects to a target object. Read more... The polyfill to support older browsers: Code: Test: This is what I'm using: Deep copy by performance: Ranked from best to worst Deep copy an array of strings or numbers (one level - no reference pointers): When an array contains numbers and strings - functions like .slice(), .concat(), .splice(), the assignment operator "=", and Underscore.js's clone function; will make a deep copy of the array's elements. Where reassignment has the fastest performance: And .slice() has better performance than .concat(), http://jsperf.com/duplicate-array-slice-vs-concat/3 Deep copy an array of objects (two or more levels - reference pointers): Write a custom function (has faster performance than $.extend() or JSON.parse): Use third-party utility functions: Where jQuery's $.extend has better performance: Deep copying objects in JavaScript (I think the best and the simplest) 1. Using JSON.parse(JSON.stringify(object)); 2.Using created method 3. Using Lo-Dash's _.cloneDeep link lodash 4. Using Object.assign() method BUT WRONG WHEN 5.Using Underscore.js _.clone link Underscore.js BUT WRONG WHEN JSBEN.CH Performance Benchmarking Playground 1~3 http://jsben.ch/KVQLd   There’s a library (called “clone”), that does this quite well. It provides the most complete recursive cloning/copying of arbitrary objects that I know of. It also supports circular references, which is not covered by the other answers, yet. You can find it on npm, too. It can be used for the browser as well as Node.js. Here is an example on how to use it: Install it with or package it with Ender. You can also download the source code manually. Then you can use it in your source code. (Disclaimer: I’m the author of the library.) Cloning an Object was always a concern in JS, but it was all about before ES6, I list different ways of copying an object in JavaScript below, imagine you have the Object below and would like to have a deep copy of that: There are few ways to copy this object, without changing the origin: 1) ES5+, Using a simple function to do the copy for you: 2) ES5+, using JSON.parse and JSON.stringify. 3) AngularJs:  4) jQuery:  5) UnderscoreJs & Loadash:  Hope these help...  I know this is an old post, but I thought this may be of some help to the next person who stumbles along. As long as you don't assign an object to anything it maintains no reference in memory.  So to make an object that you want to share among other objects, you'll have to create a factory like so: If you're using it, the Underscore.js library has a clone method. Here's a version of ConroyP's answer above that works even if the constructor has required parameters: This function is also available in my simpleoo library. Edit: Here's a more robust version (thanks to Justin McCandless this now supports cyclic references as well): The following creates two instances of the same object. I found it and am using it currently. It's simple and easy to use. Crockford suggests (and I prefer) using this function: It's terse, works as expected and you don't need a library. EDIT: This is a polyfill for Object.create, so you also can use this. NOTE:  If you use some of this, you may have problems with some iteration who use hasOwnProperty. Because, create create new empty object who inherits oldObject. But it is still useful and practical for cloning objects. For exemple if  oldObject.a = 5; but: Lodash has a nice _.cloneDeep(value) method:  Shallow copy one-liner (ECMAScript 5th edition): And shallow copy one-liner (ECMAScript 6th edition, 2015): There seems to be no ideal deep clone operator yet for array-like objects.  As the code below illustrates, John Resig's jQuery cloner turns arrays with non-numeric properties into objects that are not arrays, and RegDwight's JSON cloner drops the non-numeric properties. The following tests illustrate these points on multiple browsers: Just because I didn't see AngularJS mentioned and thought that people might want to know... angular.copy also provides a method of deep copying objects and arrays. I have two good answers depending on whether your objective is to clone a "plain old JavaScript object" or not. Let's also assume that your intention is to create a complete clone with no prototype references back to the source object. If you're not interested in a complete clone, then you can use many of the Object.clone() routines provided in some of the other answers (Crockford's pattern). For plain old JavaScript objects, a tried and true good way to clone an object in modern runtimes is quite simply: Note that the source object must be a pure JSON object. This is to say, all of its nested properties must be scalars (like boolean, string, array, object, etc). Any functions or special objects like RegExp or Date will not be cloned. Is it efficient? Heck yes. We've tried all kinds of cloning methods and this works best. I'm sure some ninja could conjure up a faster method. But I suspect we're talking about marginal gains. This approach is just simple and easy to implement. Wrap it into a convenience function and if you really need to squeeze out some gain, go for at a later time. Now, for non-plain JavaScript objects, there isn't a really simple answer.  In fact, there can't be because of the dynamic nature of JavaScript functions and inner object state. Deep cloning a JSON structure with functions inside requires you recreate those functions and their inner context. And JavaScript simply doesn't have a standardized way of doing that. The correct way to do this, once again, is via a convenience method that you declare and reuse within your code. The convenience method can be endowed with some understanding of your own objects so you can make sure to properly recreate the graph within the new object. We're written our own, but the best general approach I've seen is covered here: http://davidwalsh.name/javascript-clone This is the right idea. The author (David Walsh) has commented out the cloning of generalized functions. This is something you might choose to do, depending on your use case. The main idea is that you need to special handle the instantiation of your functions (or prototypal classes, so to speak) on a per-type basis. Here, he's provided a few examples for RegExp and Date. Not only is this code brief, but it's also very readable. It's pretty easy to extend. Is this efficient? Heck yes. Given that the goal is to produce a true deep-copy clone, then you're going to have to walk the members of the source object graph. With this approach, you can tweak exactly which child members to treat and how to manually handle custom types. So there you go. Two approaches. Both are efficient in my view. This isn't generally the most efficient solution, but it does what I need. Simple test cases below... Cyclic array test... Function test... I disagree with the answer with the greatest votes here. A Recursive Deep Clone is much faster than the JSON.parse(JSON.stringify(obj)) approach mentioned.  And here's the function for quick reference:  Here is a comprehensive clone() method that can clone any JavaScript object. It handles almost all the cases: Only when you can use ECMAScript 6 or transpilers. Features: Code: Well if you're using angular you could do this too
__label__subprocess __label__command __label__terminal __label__shell __label__python How do you call an external command (as if I'd typed it at the Unix shell or Windows command prompt) from within a Python script? Look at the subprocess module in the standard library: The advantage of subprocess vs. system is that it is more flexible (you can get the stdout, stderr, the "real" status code, better error handling, etc...). The official documentation recommends the subprocess module over the alternative os.system(): The subprocess module provides more powerful facilities for spawning new processes and retrieving their results; using that module is preferable to using this function [os.system()]. The Replacing Older Functions with the subprocess Module section in the subprocess documentation may have some helpful recipes. For versions of Python before 3.5, use call: Here's a summary of the ways to call external programs and the advantages and disadvantages of each: os.system("some_command with args") passes the command and arguments to your system's shell.  This is nice because you can actually run multiple commands at once in this manner and set up pipes and input/output redirection.  For example:   However, while this is convenient, you have to manually handle the escaping of shell characters such as spaces, etc.  On the other hand, this also lets you run commands which are simply shell commands and not actually external programs.  See the documentation. stream = os.popen("some_command with args") will do the same thing as os.system except that it gives you a file-like object that you can use to access standard input/output for that process.  There are 3 other variants of popen that all handle the i/o slightly differently.  If you pass everything as a string, then your command is passed to the shell; if you pass them as a list then you don't need to worry about escaping anything.  See the documentation. The Popen class of the subprocess module.  This is intended as a replacement for os.popen but has the downside of being slightly more complicated by virtue of being so comprehensive.  For example, you'd say: instead of:  but it is nice to have all of the options there in one unified class instead of 4 different popen functions.  See the documentation. The call function from the subprocess module.  This is basically just like the Popen class and takes all of the same arguments, but it simply waits until the command completes and gives you the return code.  For example: See the documentation. If you're on Python 3.5 or later, you can use the new subprocess.run function, which is a lot like the above but even more flexible and returns a CompletedProcess object when the command finishes executing. The os module also has all of the fork/exec/spawn functions that you'd have in a C program, but I don't recommend using them directly. The subprocess module should probably be what you use. Finally please be aware that for all methods where you pass the final command to be executed by the shell as a string and you are responsible for escaping it. There are serious security implications if any part of the string that you pass can not be fully trusted. For example, if a user is entering some/any part of the string. If you are unsure, only use these methods with constants. To give you a hint of the implications consider this code: and imagine that the user enters something "my mama didnt love me && rm -rf /" which could erase the whole filesystem. Typical implementation: You are free to do what you want with the stdout data in the pipe.  In fact, you can simply omit those parameters (stdout= and stderr=) and it'll behave like os.system(). Some hints on detaching the child process from the calling one (starting the child process in background). Suppose you want to start a long task from a CGI script. That is, the child process should live longer than the CGI script execution process. The classical example from the subprocess module documentation is: The idea here is that you do not want to wait in the line 'call subprocess' until the longtask.py is finished. But it is not clear what happens after the line 'some more code here' from the example. My target platform was FreeBSD, but the development was on Windows, so I faced the problem on Windows first. On Windows (Windows XP), the parent process will not finish until the longtask.py has finished its work. It is not what you want in a CGI script. The problem is not specific to Python; in the PHP community the problems are the same. The solution is to pass DETACHED_PROCESS Process Creation Flag to the underlying CreateProcess function in Windows API. If you happen to have installed pywin32, you can import the flag from the win32process module, otherwise you should define it yourself: /* UPD 2015.10.27 @eryksun in a comment below notes, that the semantically correct flag is CREATE_NEW_CONSOLE (0x00000010) */ On FreeBSD we have another problem: when the parent process is finished, it finishes the child processes as well. And that is not what you want in a CGI script either. Some experiments showed that the problem seemed to be in sharing sys.stdout. And the working solution was the following: I have not checked the code on other platforms and do not know the reasons of the behaviour on FreeBSD. If anyone knows, please share your ideas. Googling on starting background processes in Python does not shed any light yet. Note that this is dangerous, since the command isn't cleaned. I leave it up to you to google for the relevant documentation on the 'os' and 'sys' modules. There are a bunch of functions (exec* and spawn*) that will do similar things. I'd recommend using the subprocess module instead of os.system because it does shell escaping for you and is therefore much safer. If you want to return the results of the command, you can use os.popen. However, this is deprecated since version 2.6 in favor of the subprocess module, which other answers have covered well. There are lots of different libraries which allow you to call external commands with Python. For each library I've given a description and shown an example of calling an external command. The command I used as the example is ls -l (list all files). If you want to find out more about any of the libraries I've listed and linked the documentation for each of them. Sources: These are all the libraries: Hopefully this will help you make a decision on which library to use :) subprocess Subprocess allows you to call external commands and connect them to their input/output/error pipes (stdin, stdout, and stderr). Subprocess is the default choice for running commands, but sometimes other modules are better. os os is used for "operating system dependent functionality". It can also be used to call external commands with os.system and os.popen (Note: There is also a subprocess.popen). os will always run the shell and is a simple alternative for people who don't need to, or don't know how to use subprocess.run. sh sh is a subprocess interface which lets you call programs as if they were functions. This is useful if you want to run a command multiple times. plumbum plumbum is a library for "script-like" Python programs. You can call programs like functions as in sh. Plumbum is useful if you want to run a pipeline without the shell. pexpect pexpect lets you spawn child applications, control them and find patterns in their output. This is a better alternative to subprocess for commands that expect a tty on Unix. fabric fabric is a Python 2.5 and 2.7 library. It allows you to execute local and remote shell commands. Fabric is simple alternative for running commands in a secure shell (SSH) envoy envoy is known as "subprocess for humans". It is used as a convenience wrapper around the subprocess module. commands commands contains wrapper functions for os.popen, but it has been removed from Python 3 since subprocess is a better alternative. The edit was based on J.F. Sebastian's comment. I always use fabric for this things like: But this seem to be a good tool: sh (Python subprocess interface). Look at an example: Use the subprocess module (Python 3): It is the recommended standard way. However, more complicated tasks (pipes, output, input, etc.) can be tedious to construct and write. Note on Python version: If you are still using Python 2, subprocess.call works in a similar way. ProTip: shlex.split can help you to parse the command for run, call, and other subprocess functions in case you don't want (or you can't!) provide them in form of lists: If you do not mind external dependencies, use plumbum: It is the best subprocess wrapper. It's cross-platform, i.e. it works on both Windows and Unix-like systems. Install by pip install plumbum. Another popular library is sh: However, sh dropped Windows support, so it's not as awesome as it used to be. Install by pip install sh. Check the "pexpect" Python library, too. It allows for interactive controlling of external programs/commands, even ssh, ftp, telnet, etc. You can just type something like: If you need the output from the command you are calling, then you can use subprocess.check_output (Python 2.7+). Also note the shell parameter. If shell is True, the specified command will be executed through the shell. This can be useful if you are using Python primarily for the enhanced control flow it offers over most system shells and still want convenient access to other shell features such as shell pipes, filename wildcards, environment variable expansion, and expansion of ~ to a user’s home directory. However, note that Python itself offers implementations of many shell-like features (in particular, glob, fnmatch, os.walk(), os.path.expandvars(), os.path.expanduser(), and shutil). This is how I run my commands. This code has everything you need pretty much subprocess.run is the recommended approach as of Python 3.5 if your code does not need to maintain compatibility with earlier Python versions. It's more consistent and offers similar ease-of-use as Envoy. (Piping isn't as straightforward though. See this question for how.) Here's some examples from the documentation. Run a process: Raise on failed run: Capture output: I recommend trying Envoy. It's a wrapper for subprocess, which in turn aims to replace the older modules and functions. Envoy is subprocess for humans. Example usage from the README: Pipe stuff around too: Use subprocess. ...or for a very simple command: Simple, use subprocess.run, which returns a CompletedProcess object: As of Python 3.5, the documentation recommends subprocess.run: The recommended approach to invoking subprocesses is to use the run() function for all use cases it can handle. For more advanced use cases, the underlying Popen interface can be used directly. Here's an example of the simplest possible usage - and it does exactly as asked: run waits for the command to successfully finish, then returns a CompletedProcess object. It may instead raise TimeoutExpired (if you give it a timeout= argument) or CalledProcessError (if it fails and you pass check=True). As you might infer from the above example, stdout and stderr both get piped to your own stdout and stderr by default. We can inspect the returned object and see the command that was given and the returncode: If you want to capture the output, you can pass subprocess.PIPE to the appropriate stderr or stdout: (I find it interesting and slightly counterintuitive that the version info gets put to stderr instead of stdout.) One might easily move from manually providing a command string (like the question suggests) to providing a string built programmatically. Don't build strings programmatically. This is a potential security issue. It's better to assume you don't trust the input.  Note, only args should be passed positionally. Here's the actual signature in the source and as shown by help(run): The popenargs and kwargs are given to the Popen constructor. input can be a string of bytes (or unicode, if specify encoding or universal_newlines=True) that will be piped to the subprocess's stdin. The documentation describes timeout= and check=True better than I could: The timeout argument is passed to Popen.communicate(). If the timeout   expires, the child process will be killed and waited for. The   TimeoutExpired exception will be re-raised after the child process has   terminated. If check is true, and the process exits with a non-zero exit code, a   CalledProcessError exception will be raised. Attributes of that   exception hold the arguments, the exit code, and stdout and stderr if   they were captured. and this example for check=True is better than one I could come up with: Here's an expanded signature, as given in the documentation: Note that this indicates that only the args list should be passed positionally. So pass the remaining arguments as keyword arguments. When use Popen instead? I would struggle to find use-case based on the arguments alone. Direct usage of Popen would, however, give you access to its methods, including poll, 'send_signal', 'terminate', and 'wait'. Here's the Popen signature as given in the source. I think this is the most precise encapsulation of the information (as opposed to help(Popen)): But more informative is the Popen documentation: Execute a child program in a new process. On POSIX, the class uses   os.execvp()-like behavior to execute the child program. On Windows,   the class uses the Windows CreateProcess() function. The arguments to   Popen are as follows. Understanding the remaining documentation on Popen will be left as an exercise for the reader. os.system is OK, but kind of dated.  It's also not very secure.  Instead, try subprocess.  subprocess does not call sh directly and is therefore more secure than os.system. Get more information here. There is also Plumbum It can be this simple: Use: os - This module provides a portable way of using operating system-dependent functionality. For the more os functions, here is the documentation. I quite like shell_command for its simplicity.  It's built on top of the subprocess module. Here's an example from the documentation: There is another difference here which is not mentioned previously. subprocess.Popen executes the <command> as a subprocess. In my case, I need to execute file <a> which needs to communicate with another program, <b>.  I tried subprocess, and execution was successful. However <b> could not communicate with <a>. Everything is normal when I run both from the terminal. One more:  (NOTE: kwrite behaves different from other applications. If you try the below with Firefox, the results will not be the same.) If you try os.system("kwrite"), program flow freezes until the user closes kwrite. To overcome that I tried instead os.system(konsole -e kwrite). This time program continued to flow, but kwrite became the subprocess of the console. Anyone runs the kwrite not being a subprocess (i.e. in the system monitor it must appear at the leftmost edge of the tree). os.system does not allow you to store results, so if you want to store results in some list or something, a subprocess.call works. subprocess.check_call is convenient if you don't want to test return values. It throws an exception on any error. I tend to use subprocess together with shlex (to handle escaping of quoted strings): Shameless plug, I wrote a library for this :P https://github.com/houqp/shell.py It's basically a wrapper for popen and shlex for now. It also supports piping commands so you can chain commands easier in Python. So you can do things like: In Windows you can just import the subprocess module and run external commands by calling subprocess.Popen(), subprocess.Popen().communicate() and subprocess.Popen().wait() as below: Output: You can use Popen, and then you can check the procedure's status: Check out subprocess.Popen. To fetch the network id from the OpenStack Neutron: Output of nova net-list Output of print(networkId) Under Linux, in case you would like to call an external command that will execute independently (will keep running after the python script terminates), you can use a simple queue as task spooler or the at command An example with task spooler: Notes about task spooler (ts):  You could set the number of concurrent processes to be run ("slots") with: ts -S <number-of-slots> Installing ts doesn't requires admin privileges. You can download and compile it from source with a simple make, add it to your path and you're done.
__label__git __label__version-control How do I discard changes in my working copy that are not in the index? Another quicker way is: You don't need to include --include-untracked if you don't want to be thorough about it. After that, you can drop that stash with a git stash drop command if you like. For all unstaged files in current working directory use: For a specific file use: -- here to remove argument disambiguation. For Git 2.23 onwards, one may want to use the more specific resp. that together with git switch replaces the overloaded git checkout (see here), and thus removes the argument disambiguation. It seems like the complete solution is: git clean removes all untracked files (warning: while it won't delete ignored files mentioned directly in .gitignore, it may delete ignored files residing in folders) and git checkout clears all unstaged changes. This checks out the current index for the current directory, throwing away all changes in files from the current directory downwards. or this which checks out all files from the index, overwriting working tree files. Cleans the working tree by recursively removing files that are not under version control, starting from the current directory. -d: Remove untracked directories in addition to untracked files -f: Force (might be not necessary depending on  clean.requireForce setting) Run git help clean to see the manual My favorite is That lets you selectively revert chunks. See also: Since no answer suggests the exact option combination that I use, here it is: This is the online help text for the used git clean options: -d Remove untracked directories in addition to untracked files. If an untracked directory is managed by a different Git repository, it is not removed by default. Use -f option twice if you really want to remove such a directory. -x Don’t use the standard ignore rules read from .gitignore (per directory) and $GIT_DIR/info/exclude, but do still use the ignore rules given with -e options. This allows removing all untracked files, including build products. This can be used (possibly in conjunction with git reset) to create a pristine working directory to test a clean build. -n Don’t actually remove anything, just show what would be done. -f If the Git configuration variable clean.requireForce is not set to false, Git clean will refuse to delete files or directories unless given -f, -n, or -i. Git will refuse to delete directories within the .git subdirectory or file, unless a second -f is given. You can now discard unstaged changes in one tracked file with: and in all tracked files in the current directory (recursively) with: If you run the latter from the root of the repo, it will discard unstaged changes in all tracked files. If you merely wish to remove changes to existing files, use checkout (documented here).  If you want to remove files added since your last commit, use clean (documented here):  If you wish to move changes to a holding space for later access, use stash (documented here):  I really found this article helpful for explaining when to use what command: http://www.szakmeister.net/blog/2011/oct/12/reverting-changes-git/ There are a couple different cases: If you haven't staged the file, then you use git checkout.  Checkout "updates files in the working tree to match the version in the index".  If the files have not been staged (aka added to the index)... this command will essentially revert the files to what your last commit was.   git checkout -- foo.txt If you have staged the file, then use git reset.  Reset changes the index to match a commit. git reset -- foo.txt  I suspect that using git stash is a popular choice since it's a little less dangerous.  You can always go back to it if you accidently blow too much away when using git reset.  Reset is recursive by default. Take a look at the article above for further advice. The easiest way to do this is by using this command: This command is used to discard changes in working directory - https://git-scm.com/docs/git-checkout  In git command, stashing of untracked files is achieved by using: http://git-scm.com/docs/git-stash If you aren't interested in keeping the unstaged changes (especially if the staged changes are new files), I found this handy: git checkout -f man git-checkout: -f, --force When switching branches, proceed even if the index or the working tree differs from HEAD. This is used to throw away local changes. When checking out paths from the index, do not fail upon unmerged entries; instead, unmerged entries are ignored. As you type git status,  (use "git checkout -- ..." to discard changes in working directory) is shown. e.g. git checkout -- . You can use git stash - if something goes wrong, you can still revert from the stash. Similar to some other answer here, but this one also removes all unstaged files and also all unstaged deletes: if you check that everything is OK, throw the stash away: The answer from Bilal Maqsood with git clean also worked for me, but with the stash I have more control - if I do sth accidentally, I can still get my changes back UPDATE I think there is 1 more change (don't know why this worked for me before): git add . -A instead of git add . without the -A the removed files will not be staged Instead of discarding changes, I reset my remote to the origin. Note - this method is to completely restore your folder to that of the repo.  So I do this to make sure they don't sit there when I git reset (later - excludes gitignores on the Origin/branchname) NOTE: If you want to keep files not yet tracked, but not in GITIGNORE you may wish to skip this step, as it will Wipe these untracked files not found on your remote repository (thanks @XtrmJosh). Then I  Then I reset to origin That will put it back to square one. Just like RE-Cloning the branch, WHILE keeping all my gitignored files locally and in place. Updated per user comment below: Variation to reset the to whatever current branch the user is on.  Tried all the solutions above but still couldn't get rid of new, unstaged files. Use git clean -f to remove those new files - with caution though! Note the force option. To do a permanent discard: git reset --hard To save changes for later: git stash Just use: Done. Easy. If you really care about your stash stack then you can follow with git stash drop. But at that point you're better off using (from Mariusz Nowak): Nonetheless, I like git stash -u the best because it "discards" all tracked and untracked changes in just one command. Yet git checkout -- . only discards tracked changes, and git clean -df only discards untracked changes... and typing both commands is far too much work :) simply say It will remove all your local changes. You also can use later by saying or      git stash pop you have a very simple git command git checkout . This works even in directories that are; outside of normal git permissions. Happened to me recently  In my opinion, should do the trick. As per Git documentation on git clean git-clean - Remove untracked files from the working tree Description  Cleans the working tree by recursively removing files that   are not under version control, starting from the current directory. Normally, only files unknown to Git are removed, but if the -x option   is specified, ignored files are also removed. This can, for example,   be useful to remove all build products. If any optional ... arguments are given, only those paths are   affected. Options -d Remove untracked directories in addition to untracked files. If an untracked directory is managed by a different Git repository, it is   not removed by default. Use -f option twice if you really want to   remove such a directory. -f   --force If the Git configuration variable clean.requireForce is not set to false, git clean will refuse to run unless given -f, -n or -i. No matter what state your repo is in you can always reset to any previous commit: This will discard all changes which were made after that commit. Another way to get rid of new files that is more specific than git clean -df (it will allow you to get rid of some files not necessarily all), is to add the new files to the index first, then stash, then drop the stash. This technique is useful when, for some reason, you can't easily delete all of the untracked files by some ordinary mechanism (like rm). What follows is really only a solution if you are working with a fork of a repository where you regularly synchronize (e.g. pull request) with another repo. Short answer: delete fork and refork, but read the warnings on github. I had a similar problem, perhaps not identical, and I'm sad to say my solution is not ideal, but it is ultimately effective. I would often have git status messages like this (involving at least 2/4 files): A keen eye will note that these files have dopplegangers that are a single letter in case off. Somehow, and I have no idea what led me down this path to start with (as I was not working with these files myself from the upstream repo), I had switched these files. Try the many solutions listed on this page (and other pages) did not seem to help.  I was able to fix the problem by deleting my forked repository and all local repositories, and reforking. This alone was not enough; upstream had to rename the files in question to new filenames. As long as you don't have any uncommited work, no wikis, and no issues that diverge from the upstream repository, you should be just fine. Upstream may not be very happy with you, to say the least. As for my problem, it is undoubtedly a user error as I'm not that proficient with git, but the fact that it is far from easy to fix points to an issue with git as well.  I had a weird situation where a file is always unstaged, this helps me to resolve. git rm .gitattributes   git add -A   git reset --hard  When you want to transfer a stash to someone else: [edit] as commented, it ís possible to name stashes. Well, use this if you want to share your stash ;) You could create your own alias which describes how to do it in a descriptive way. I use the next alias to discard changes. Then you can use it as next to discard all changes: Or just a file: Otherwise, if you want to discard all changes and also the untracked files, I use a mix of checkout and clean: So the use is simple as next: Now is available in the next Github repo which contains a lot of aliases:
__label__browser __label__url __label__http What is the maximum length of a URL in different browsers? Does it differ among browsers? Is a maximum URL length part of the HTTP specification? If you keep URLs under 2000 characters, they'll work in virtually any combination of client and server software. If you are targeting particular browsers, see below for more details on specific limits. RFC 2616 (Hypertext Transfer Protocol HTTP/1.1) section 3.2.1 says The HTTP protocol does not place any a priori limit on the length of a URI. Servers MUST be able to handle the URI of any resource they    serve, and SHOULD be able to handle URIs of unbounded length if they    provide GET-based forms that could generate such URIs. A server    SHOULD return 414 (Request-URI Too Long) status if a URI is longer    than the server can handle (see section 10.4.15). That RFC has been obsoleted by RFC7230 which is a refresh of the HTTP/1.1 specification. It contains similar language, but also goes on to suggest this: Various ad hoc limitations on request-line length are found in practice. It is RECOMMENDED that all HTTP senders and recipients support, at a minimum, request-line lengths of 8000 octets. That's what the standards say. For the reality, there was an article on boutell.com (link goes to Internet Archive backup) that discussed what individual browser and server implementations will support. The executive summary is: Extremely long URLs are usually a mistake. URLs over 2,000 characters will not work in the most popular web browsers. Don't use them if you intend your site to work for the majority of Internet users. (Note: this is a quote from an article written in 2006, but in 2015 IE's declining usage means that longer URLs do work for the majority. However, IE still has the limitation...) IE8's maximum URL length is 2083 chars, and it seems IE9 has a similar limit. I've tested IE10 and the address bar will only accept 2083 chars. You can click a URL which is longer than this, but the address bar will still only show 2083 characters of this link. There's a nice writeup on the IE Internals blog which goes into some of the background to this. There are mixed reports IE11 supports longer URLs - see comments below. Given some people report issues, the general advice still stands. Be aware that the sitemaps protocol, which allows a site to inform search engines about available pages, has a limit of 2048 characters in a URL. If you intend to use sitemaps, a limit has been decided for you! (see Calin-Andrei Burloiu's answer below) There's also some research from 2010 into the maximum URL length that search engines will crawl and index. They found the limit was 2047 chars, which appears allied to the sitemap protocol spec. However, they also found the Google SERP tool wouldn't cope with URLs longer than 1855 chars. CDNs also impose limits on URI length, and will return a 414 Too long request when these limits are reached, for example: (credit to timrs2998 for providing that info in the comments) I tested the following against an Apache 2.4 server configured with a very large LimitRequestLine and LimitRequestFieldSize. See also this answer from Matas Vaitkevicius below. This is a popular question, and as the original research is ~14 years old I'll try to keep it up to date: As of Sep 2020, the advice still stands. Even though IE11 may possibly accept longer URLs, the ubiquity of older IE installations plus the search engine limitations mean staying under 2000 chars is the best general policy. The longest URLs I came across are data URLs Example image URL from Google image results (11747 characters) I wrote this test that keeps on adding 'a' to parameter until the browser fails C# part: View: PART 1 On Chrome I got: It then blew up with: HTTP Error 404.15 - Not Found The request filtering module is   configured to deny a request where the query string is too long. Same on Internet Explorer 8 and Firefox PART 2 I went easy mode and added additional limits to IISExpress applicationhost.config and web.config setting maxQueryStringLength="32768". after 7744 characters. PART 3 Added which didn't help at all. I finally decided to use fiddler to remove the referrer from header. Which did nicely. Chrome: got to 15613 characters. (I guess it's a 16K limit for IIS) And it failed again with: Firefox: Internet Explorer 8 failed with iexplore.exe crashing.  After 2505 Android Emulator Internet Explorer 11 Internet Explorer 10 Internet Explorer 9 WWW FAQs: What is the maximum length of a URL? has its own answer based on empirical testing and research. The short answer is that going over 2048 characters makes Internet Explorer unhappy and thus this is the limit you should use. See the page for a long answer. There is really no universal maximum URL length. The max length is determined only by what the client browser chooses to support, which varies widely. The 2,083 limit is only present in Internet Explorer (all versions up to 7.0). The max length in Firefox and Safari seems to be unlimited, although instability occurs with URLs reaching around 65,000 characters. Opera seems to have no max URL length whatsoever, and doesn't suffer instability at extremely long lengths. On Apple platforms (iOS/macOS/tvOS/watchOS), the limit may be a 2 GB long URL scheme, as seen by this comment in the source code of Swift: On iOS, I've tested and confirmed that even a 300+ MB long URL is accepted. You can try such a long URL like this in Objective-C: And catch if it succeed with: The URI RFC (of which URLs are a subset) doesn't define a maximum length, however, it does recommend that the hostname part of the URI (if applicable) not exceed 255 characters in length: URI producers should use names that   conform to the DNS syntax, even when   use of DNS is not immediately   apparent, and should limit these names   to no more than 255 characters in   length. As noted in other posts though, some browsers have a practical limitation on the length of a URL. The HTTP 1.1 specification says: URIs in HTTP can be represented in   absolute form or relative to some   known base URI [11], depending upon   the context of their use. The two   forms are differentiated by the fact   that absolute URIs always begin   with a scheme name followed by a   colon. For definitive information on   URL syntax and semantics, see "Uniform   Resource Identifiers (URI):    Generic   Syntax and Semantics," RFC 2396 [42]   (which replaces RFCs    1738 [4] and   RFC 1808 [11]). This specification   adopts the    definitions of   "URI-reference", "absoluteURI",   "relativeURI", "port",   "host","abs_path", "rel_path", and   "authority" from that   specification. The HTTP protocol does not place   any a priori limit on the length of   a URI. Servers MUST be able to handle   the URI of any resource they    serve,   and SHOULD be able to handle URIs of   unbounded length if they    provide   GET-based forms that could generate   such URIs.* A server    SHOULD return   414 (Request-URI Too Long) status if a   URI is longer    than the server can   handle (see section 10.4.15). Note: Servers ought to be cautious about depending on URI   lengths         above 255 bytes, because some older client or proxy         implementations might not properly support these lengths. As mentioned by @Brian, the HTTP clients (e.g. browsers) may have their own limits, and HTTP servers will have different limits. Microsoft Support says "Maximum URL length is 2,083 characters in Internet Explorer". IE has problems with URLs longer than that. Firefox seems to work fine with >4k chars. In URL as UI Jakob Nielsen recommends: the social interface to the Web relies on email when users want to recommend Web pages to each other, and email is the second-most common way users get to new sites (search engines being the most common): make sure that all URLs on your site are less than 78 characters long so that they will not wrap across a line feed. This is not the maximum but I'd consider this a practical maximum if you want your URL to be shared. Sitemaps protocol, which is a way for webmasters to inform search engines about pages on their sites (also used by Google in Webmaster Tools), supports URLs with less than 2048 characters. So if you are planning to use this feature for Search Engine Optimization, take this into account. ASP.NET 2 and SQL Server reporting services 2005 have a limit of 2028. I found this out the hard way, where my dynamic URL generator would not pass over some parameters to a report beyond that point. This was under Internet Explorer 8. Why is the Internet Explorer limit only 2K while IIS has a limit of 16K? I don't think it makes sense. So I want to start an experiment about Ajax request URL size limits. I have set my Tomcat HTTP connector's maxHttpHeaderSize="1048576". And prepared a very long URL. Then I send a request with the long URL like the following: jQuery reports done. Tomcat reports the URL requested is 1048015 bytes. It was tested with Chrome 50 and Internet Explorer 11. So web browsers won't truncate or limit your URL intentionally when sending Ajax requests. Limit request line directive sets the maximum length of a URL. By default, it is set to 8190, which gives you a lot of room. However other servers and some browses, limit the length more. Because all parameters are passed on the URL line, items that were in password of hidden fields will also be displayed in the URL of course. Neither mobile should be used for real security measures and should be considered cosmetic security at best. It seems that Chrome at least has raised this limit. I pasted 20,000 characters into the bookmarklet and it took it. I have experience with SharePoint 2007, 2010 and there is a limit of the length URL you can create from the server side in this case SharePoint, so it depends mostly on, 1) the client (browser, version, and OS) and 2) the server technology, IIS, Apache, etc. According to the HTTP spec, there is no limit to a URL's length. Keep your URLs under 2048 characters; this will ensure the URLs work in all clients & server configurations. Also, search engines like URLs to remain under approximately 2000 characters. public void ConfigureServices(IServiceCollection services) { /Identity/ services.AddDbContext(options => options.UseSqlServer(Configuration["ConnectionStrings:IdentityConnection"])); services.AddIdentity<AppUser, IdentityRole>().AddEntityFrameworkStores().AddDefaultTokenProviders(); /End/
__label__big-o __label__computer-science __label__algorithm __label__complexity-theory __label__time-complexity I'd prefer as little formal definition as possible and simple mathematics. Quick note, this is almost certainly confusing Big O notation (which is an upper bound) with Theta notation "Θ" (which is a two-side bound). In my experience, this is actually typical of discussions in non-academic settings. Apologies for any confusion caused. Big O complexity can be visualized with this graph:  The simplest definition I can give for Big-O notation is this: Big-O notation is a relative representation of the complexity of an algorithm. There are some important and deliberately chosen words in that sentence: Come back and reread the above when you've read the rest. The best example of Big-O I can think of is doing arithmetic.  Take two numbers (123456 and 789012).  The basic arithmetic operations we learned in school were: Each of these is an operation or a problem.  A method of solving these is called an algorithm. The addition is the simplest.  You line the numbers up (to the right) and add the digits in a column writing the last number of that addition in the result.  The 'tens' part of that number is carried over to the next column. Let's assume that the addition of these numbers is the most expensive operation in this algorithm. It stands to reason that to add these two numbers together we have to add together 6 digits (and possibly carry a 7th). If we add two 100 digit numbers together we have to do 100 additions.  If we add two 10,000 digit numbers we have to do 10,000 additions. See the pattern?  The complexity (being the number of operations) is directly proportional to the number of digits n in the larger number.  We call this O(n) or linear complexity. Subtraction is similar (except you may need to borrow instead of carry). Multiplication is different. You line the numbers up, take the first digit in the bottom number and multiply it in turn against each digit in the top number and so on through each digit. So to multiply our two 6 digit numbers we must do 36 multiplications. We may need to do as many as 10 or 11 column adds to get the end result too. If we have two 100-digit numbers we need to do 10,000 multiplications and 200 adds.  For two one million digit numbers we need to do one trillion (1012) multiplications and two million adds. As the algorithm scales with n-squared, this is O(n2) or quadratic complexity. This is a good time to introduce another important concept: We only care about the most significant portion of complexity. The astute may have realized that we could express the number of operations as: n2 + 2n.  But as you saw from our example with two numbers of a million digits apiece, the second term (2n) becomes insignificant (accounting for 0.0002% of the total operations by that stage). One can notice that we've assumed the worst case scenario here. While multiplying 6 digit numbers, if one of them has 4 digits and the other one has 6 digits, then we only have 24 multiplications. Still, we calculate the worst case scenario for that 'n', i.e when both are 6 digit numbers. Hence Big-O notation is about the Worst-case scenario of an algorithm. The next best example I can think of is the telephone book, normally called the White Pages or similar but it varies from country to country.  But I'm talking about the one that lists people by surname and then initials or first name, possibly address and then telephone numbers. Now if you were instructing a computer to look up the phone number for "John Smith" in a telephone book that contains 1,000,000 names, what would you do?  Ignoring the fact that you could guess how far in the S's started (let's assume you can't), what would you do? A typical implementation might be to open up to the middle, take the 500,000th and compare it to "Smith". If it happens to be "Smith, John", we just got really lucky.  Far more likely is that "John Smith" will be before or after that name.  If it's after we then divide the last half of the phone book in half and repeat.  If it's before then we divide the first half of the phone book in half and repeat.  And so on. This is called a binary search and is used every day in programming whether you realize it or not. So if you want to find a name in a phone book of a million names you can actually find any name by doing this at most 20 times.  In comparing search algorithms we decide that this comparison is our 'n'. That is staggeringly good, isn't it? In Big-O terms this is O(log n) or logarithmic complexity.  Now the logarithm in question could be ln (base e), log10, log2 or some other base.  It doesn't matter it's still O(log n) just like O(2n2) and O(100n2) are still both O(n2). It's worthwhile at this point to explain that Big O can be used to determine three cases with an algorithm: Normally we don't care about the best case.  We're interested in the expected and worst case.  Sometimes one or the other of these will be more important. Back to the telephone book. What if you have a phone number and want to find a name?  The police have a reverse phone book but such look-ups are denied to the general public.  Or are they?  Technically you can reverse look-up a number in an ordinary phone book.  How? You start at the first name and compare the number.  If it's a match, great, if not, you move on to the next.  You have to do it this way because the phone book is unordered (by phone number anyway). So to find a name given the phone number (reverse lookup): This is quite a famous problem in computer science and deserves a mention.  In this problem, you have N towns. Each of those towns is linked to 1 or more other towns by a road of a certain distance. The Traveling Salesman problem is to find the shortest tour that visits every town. Sounds simple?  Think again. If you have 3 towns A, B, and C with roads between all pairs then you could go: Well, actually there's less than that because some of these are equivalent (A → B → C and C → B → A are equivalent, for example, because they use the same roads, just in reverse). In actuality, there are 3 possibilities. This is a function of a mathematical operation called a factorial.  Basically: So the Big-O of the Traveling Salesman problem is O(n!) or factorial or combinatorial complexity. By the time you get to 200 towns there isn't enough time left in the universe to solve the problem with traditional computers. Something to think about. Another point I wanted to make a quick mention of is that any algorithm that has a complexity of O(na) is said to have polynomial complexity or is solvable in polynomial time. O(n), O(n2) etc. are all polynomial time. Some problems cannot be solved in polynomial time. Certain things are used in the world because of this. Public Key Cryptography is a prime example. It is computationally hard to find two prime factors of a very large number. If it wasn't, we couldn't use the public key systems we use. Anyway, that's it for my (hopefully plain English) explanation of Big O (revised). It shows how an algorithm scales based on input size. O(n2):  known as Quadratic complexity Notice that the number of items increases by a factor of 10, but the time increases by a factor of 102. Basically, n=10 and so O(n2) gives us the scaling factor n2 which is 102. O(n):  known as Linear complexity This time the number of items increases by a factor of 10, and so does the time. n=10 and so O(n)'s scaling factor is 10. O(1):  known as Constant complexity The number of items is still increasing by a factor of 10, but the scaling factor of O(1) is always 1. O(log n):  known as Logarithmic complexity The number of computations is only increased by a log of the input value.  So in this case, assuming each computation takes 1 second, the log of the input n is the time required, hence log n. That's the gist of it. They reduce the maths down so it might not be exactly n2 or whatever they say it is, but that'll be the dominating factor in the scaling. Big-O notation (also called "asymptotic growth" notation) is what functions "look like" when you ignore constant factors and stuff near the origin. We use it to talk about how thing scale. Basics for "sufficiently" large inputs... big-O notation doesn't care about constant factors: the function 9x² is said to "grow exactly like" 10x². Neither does big-O asymptotic notation care about non-asymptotic stuff ("stuff near the origin" or "what happens when the problem size is small"): the function 10x² is said to "grow exactly like" 10x² - x + 2. Why would you want to ignore the smaller parts of the equation? Because they become completely dwarfed by the big parts of the equation as you consider larger and larger scales; their contribution becomes dwarfed and irrelevant. (See example section.) Put another way, it's all about the ratio as you go to infinity. If you divide the actual time it takes by the O(...), you will get a constant factor in the limit of large inputs. Intuitively this makes sense: functions "scale like" one another if you can multiply one to get the other. That is when we say... ... this means that for "large enough" problem sizes N (if we ignore stuff near the origin), there exists some constant (e.g. 2.5, completely made up) such that: There are many choices of constant; often the "best" choice is known as the "constant factor" of the algorithm... but we often ignore it like we ignore non-largest terms (see Constant Factors section for why they don't usually matter). You can also think of the above equation as a bound, saying "In the worst-case scenario, the time it takes will never be worse than roughly N*log(N), within a factor of 2.5 (a constant factor we don't care much about)". In general, O(...) is the most useful one because we often care about worst-case behavior. If f(x) represents something "bad" like the processor or memory usage, then "f(x) ∈ O(upperbound)" means "upperbound is the worst-case scenario of processor/memory usage". Applications As a purely mathematical construct, big-O notation is not limited to talking about processing time and memory. You can use it to discuss the asymptotics of anything where scaling is meaningful, such as: Example For the handshake example above, everyone in a room shakes everyone else's hand. In that example, #handshakes ∈ Ɵ(N²). Why? Back up a bit: the number of handshakes is exactly n-choose-2 or N*(N-1)/2 (each of N people shakes the hands of N-1 other people, but this double-counts handshakes so divide by 2):   However, for very large numbers of people, the linear term N is dwarfed and effectively contributes 0 to the ratio (in the chart: the fraction of empty boxes on the diagonal over total boxes gets smaller as the number of participants becomes larger). Therefore the scaling behavior is order N², or the number of handshakes "grows like N²". It's as if the empty boxes on the diagonal of the chart (N*(N-1)/2 checkmarks) weren't even there (N2 checkmarks asymptotically). (temporary digression from "plain English":) If you wanted to prove this to yourself, you could perform some simple algebra on the ratio to split it up into multiple terms (lim means "considered in the limit of", just ignore it if you haven't seen it, it's just notation for "and N is really really big"): tl;dr: The number of handshakes 'looks like' x² so much for large values, that if we were to write down the ratio #handshakes/x², the fact that we don't need exactly x² handshakes wouldn't even show up in the decimal for an arbitrarily large while. e.g. for x=1million, ratio #handshakes/x²: 0.499999... Building Intuition This lets us make statements like... "For large enough inputsize=N, no matter what the constant factor is, if I double the input size...  N → (2N) = 2(N)  N² → (2N)² = 4(N²)  cN³ → c(2N)³ = 8(cN³)  c log(N) → c log(2N) = (c log(2))+(c log(N)) = (fixed amount)+(c log(N))  c*1 → c*1   it's less than O(N1.000001), which you might be willing to call basically linear  2N → 22N = (4N)............put another way...... 2N → 2N+1 = 2N21 = 2 2N [for the mathematically inclined, you can mouse over the spoilers for minor sidenotes] (with credit to https://stackoverflow.com/a/487292/711085 ) (technically the constant factor could maybe matter in some more esoteric examples, but I've phrased things above (e.g. in log(N)) such that it doesn't) These are the bread-and-butter orders of growth that programmers and applied computer scientists use as reference points. They see these all the time. (So while you could technically think "Doubling the input makes an O(√N) algorithm 1.414 times slower," it's better to think of it as "this is worse than logarithmic but better than linear".) Constant factors Usually, we don't care what the specific constant factors are, because they don't affect the way the function grows. For example, two algorithms may both take O(N) time to complete, but one may be twice as slow as the other. We usually don't care too much unless the factor is very large since optimizing is tricky business ( When is optimisation premature? ); also the mere act of picking an algorithm with a better big-O will often improve performance by orders of magnitude. Some asymptotically superior algorithms (e.g. a non-comparison O(N log(log(N))) sort) can have so large a constant factor (e.g. 100000*N log(log(N))), or overhead that is relatively large like O(N log(log(N))) with a hidden + 100*N, that they are rarely worth using even on "big data". Why O(N) is sometimes the best you can do, i.e. why we need datastructures O(N) algorithms are in some sense the "best" algorithms if you need to read all your data. The very act of reading a bunch of data is an O(N) operation. Loading it into memory is usually O(N) (or faster if you have hardware support, or no time at all if you've already read the data). However, if you touch or even look at every piece of data (or even every other piece of data), your algorithm will take O(N) time to perform this looking. No matter how long your actual algorithm takes, it will be at least O(N) because it spent that time looking at all the data. The same can be said for the very act of writing. All algorithms which print out N things will take N time because the output is at least that long (e.g. printing out all permutations (ways to rearrange) a set of N playing cards is factorial: O(N!)). This motivates the use of data structures: a data structure requires reading the data only once (usually O(N) time), plus some arbitrary amount of preprocessing (e.g. O(N) or O(N log(N)) or O(N²)) which we try to keep small. Thereafter, modifying the data structure (insertions/deletions/ etc.) and making queries on the data take very little time, such as O(1) or O(log(N)). You then proceed to make a large number of queries! In general, the more work you're willing to do ahead of time, the less work you'll have to do later on. For example, say you had the latitude and longitude coordinates of millions of road segments and wanted to find all street intersections. The moral of the story: a data structure lets us speed up operations. Even more, advanced data structures can let you combine, delay, or even ignore operations in incredibly clever ways. Different problems would have different analogies, but they'd all involve organizing the data in a way that exploits some structure we care about, or which we've artificially imposed on it for bookkeeping. We do work ahead of time (basically planning and organizing), and now repeated tasks are much much easier! Practical example: visualizing orders of growth while coding Asymptotic notation is, at its core, quite separate from programming. Asymptotic notation is a mathematical framework for thinking about how things scale and can be used in many different fields. That said... this is how you apply asymptotic notation to coding. The basics: Whenever we interact with every element in a collection of size A (such as an array, a set, all keys of a map, etc.), or perform A iterations of a loop, that is a multiplicative factor of size A. Why do I say "a multiplicative factor"?--because loops and functions (almost by definition) have multiplicative running time: the number of iterations, times work done in the loop (or for functions: the number of times you call the function, times work done in the function). (This holds if we don't do anything fancy, like skip loops or exit the loop early, or change control flow in the function based on arguments, which is very common.) Here are some examples of visualization techniques, with accompanying pseudocode. (here, the xs represent constant-time units of work, processor instructions, interpreter opcodes, whatever) Example 2: Example 3: If we do something slightly complicated, you might still be able to imagine visually what's going on: Here, the smallest recognizable outline you can draw is what matters; a triangle is a two dimensional shape (0.5 A^2), just like a square is a two-dimensional shape (A^2); the constant factor of two here remains in the asymptotic ratio between the two, however, we ignore it like all factors... (There are some unfortunate nuances to this technique I don't go into here; it can mislead you.) Of course this does not mean that loops and functions are bad; on the contrary, they are the building blocks of modern programming languages, and we love them. However, we can see that the way we weave loops and functions and conditionals together with our data (control flow, etc.) mimics the time and space usage of our program! If time and space usage becomes an issue, that is when we resort to cleverness and find an easy algorithm or data structure we hadn't considered, to reduce the order of growth somehow. Nevertheless, these visualization techniques (though they don't always work) can give you a naive guess at a worst-case running time. Here is another thing we can recognize visually: We can just rearrange this and see it's O(N): Or maybe you do log(N) passes of the data, for O(N*log(N)) total time: Unrelatedly but worth mentioning again: If we perform a hash (e.g. a dictionary/hashtable lookup), that is a factor of O(1). That's pretty fast. If we do something very complicated, such as with a recursive function or divide-and-conquer algorithm, you can use the Master Theorem (usually works), or in ridiculous cases the Akra-Bazzi Theorem (almost always works) you look up the running time of your algorithm on Wikipedia. But, programmers don't think like this because eventually, algorithm intuition just becomes second nature. You will start to code something inefficient and immediately think "am I doing something grossly inefficient?". If the answer is "yes" AND you foresee it actually mattering, then you can take a step back and think of various tricks to make things run faster (the answer is almost always "use a hashtable", rarely "use a tree", and very rarely something a bit more complicated). Amortized and average-case complexity There is also the concept of "amortized" and/or "average case" (note that these are different). Average Case: This is no more than using big-O notation for the expected value of a function, rather than the function itself. In the usual case where you consider all inputs to be equally likely, the average case is just the average of the running time. For example with quicksort, even though the worst-case is O(N^2) for some really bad inputs, the average case is the usual O(N log(N)) (the really bad inputs are very small in number, so few that we don't notice them in the average case). Amortized Worst-Case: Some data structures may have a worst-case complexity that is large, but guarantee that if you do many of these operations, the average amount of work you do will be better than worst-case. For example, you may have a data structure that normally takes constant O(1) time. However, occasionally it will 'hiccup' and take O(N) time for one random operation, because maybe it needs to do some bookkeeping or garbage collection or something... but it promises you that if it does hiccup, it won't hiccup again for N more operations. The worst-case cost is still O(N) per operation, but the amortized cost over many runs is O(N)/N = O(1) per operation. Because the big operations are sufficiently rare, the massive amount of occasional work can be considered to blend in with the rest of the work as a constant factor. We say the work is "amortized" over a sufficiently large number of calls that it disappears asymptotically. The analogy for amortized analysis: You drive a car. Occasionally, you need to spend 10 minutes going to   the gas station and then spend 1 minute refilling the tank with gas.   If you did this every time you went anywhere with your car (spend 10   minutes driving to the gas station, spend a few seconds filling up a   fraction of a gallon), it would be very inefficient. But if you fill   up the tank once every few days, the 11 minutes spent driving to the   gas station is "amortized" over a sufficiently large number of trips,   that you can ignore it and pretend all your trips were maybe 5% longer. Comparison between average-case and amortized worst-case: Though, if you're reasonably worried about an attacker, there are many other algorithmic attack vectors to worry about besides amortization and average-case.) Both average-case and amortization are incredibly useful tools for thinking about and designing with scaling in mind. (See Difference between average case and amortized analysis if interested in this subtopic.) Multidimensional big-O Most of the time, people don't realize that there's more than one variable at work. For example, in a string-search algorithm, your algorithm may take time O([length of text] + [length of query]), i.e. it is linear in two variables like O(N+M). Other more naive algorithms may be O([length of text]*[length of query]) or O(N*M). Ignoring multiple variables is one of the most common oversights I see in algorithm analysis, and can handicap you when designing an algorithm. The whole story Keep in mind that big-O is not the whole story. You can drastically speed up some algorithms by using caching, making them cache-oblivious, avoiding bottlenecks by working with RAM instead of disk, using parallelization, or doing work ahead of time -- these techniques are often independent of the order-of-growth "big-O" notation, though you will often see the number of cores in the big-O notation of parallel algorithms. Also keep in mind that due to hidden constraints of your program, you might not really care about asymptotic behavior. You may be working with a bounded number of values, for example: In practice, even among algorithms which have the same or similar asymptotic performance, their relative merit may actually be driven by other things, such as: other performance factors (quicksort and mergesort are both O(N log(N)), but quicksort takes advantage of CPU caches); non-performance considerations, like ease of implementation; whether a library is available, and how reputable and maintained the library is. Programs will also run slower on a 500MHz computer vs 2GHz computer. We don't really consider this as part of the resource bounds, because we think of the scaling in terms of machine resources (e.g. per clock cycle), not per real second. However, there are similar things which can 'secretly' affect performance, such as whether you are running under emulation, or whether the compiler optimized code or not. These might make some basic operations take longer (even relative to each other), or even speed up or slow down some operations asymptotically (even relative to each other). The effect may be small or large between different implementation and/or environment. Do you switch languages or machines to eke out that little extra work? That depends on a hundred other reasons (necessity, skills, coworkers, programmer productivity, the monetary value of your time, familiarity, workarounds, why not assembly or GPU, etc...), which may be more important than performance. The above issues, like the effect of the choice of which programming language is used, are almost never considered as part of the constant factor (nor should they be); yet one should be aware of them because sometimes (though rarely) they may affect things. For example in cpython, the native priority queue implementation is asymptotically non-optimal (O(log(N)) rather than O(1) for your choice of insertion or find-min); do you use another implementation? Probably not, since the C implementation is probably faster, and there are probably other similar issues elsewhere. There are tradeoffs; sometimes they matter and sometimes they don't. (edit: The "plain English" explanation ends here.) Math addenda For completeness, the precise definition of big-O notation is as follows: f(x) ∈ O(g(x)) means that "f is asymptotically upper-bounded by const*g": ignoring everything below some finite value of x, there exists a constant such that |f(x)| ≤ const * |g(x)|. (The other symbols are as follows: just like O means ≤, Ω means ≥. There are lowercase variants: o means <, and ω means >.) f(x) ∈ Ɵ(g(x)) means both f(x) ∈ O(g(x)) and f(x) ∈ Ω(g(x)) (upper- and lower-bounded by g): there exists some constants such that f will always lie in the "band" between const1*g(x) and const2*g(x). It is the strongest asymptotic statement you can make and roughly equivalent to ==. (Sorry, I elected to delay the mention of the absolute-value symbols until now, for clarity's sake; especially because I have never seen negative values come up in a computer science context.) People will often use = O(...), which is perhaps the more correct 'comp-sci' notation, and entirely legitimate to use; "f = O(...)" is read "f is order ... / f is xxx-bounded by ..." and is thought of as "f is some expression whose asymptotics are ...". I was taught to use the more rigorous ∈ O(...). ∈ means "is an element of" (still read as before). In this particular case, O(N²) contains elements like {2 N², 3 N², 1/2 N², 2 N² + log(N), - N² + N^1.9, ...} and is infinitely large, but it's still a set. O and Ω are not symmetric (n = O(n²), but n² is not O(n)), but Ɵ is symmetric, and (since these relations are all transitive and reflexive) Ɵ, therefore, is symmetric and transitive and reflexive, and therefore partitions the set of all functions into equivalence classes. An equivalence class is a set of things that we consider to be the same. That is to say, given any function you can think of, you can find a canonical/unique 'asymptotic representative' of the class (by generally taking the limit... I think); just like you can group all integers into odds or evens, you can group all functions with Ɵ into x-ish, log(x)^2-ish, etc... by basically ignoring smaller terms (but sometimes you might be stuck with more complicated functions which are separate classes unto themselves). The = notation might be the more common one and is even used in papers by world-renowned computer scientists. Additionally, it is often the case that in a casual setting, people will say O(...) when they mean Ɵ(...); this is technically true since the set of things Ɵ(exactlyThis) is a subset of O(noGreaterThanThis)... and it's easier to type. ;-) EDIT: Quick note, this is almost certainly confusing Big O notation (which is an upper bound) with Theta notation (which is both an upper and lower bound). In my experience this is actually typical of discussions in non-academic settings. Apologies for any confusion caused. In one sentence: As the size of your job goes up, how much longer does it take to complete it? Obviously that's only using "size" as the input and "time taken" as the output — the same idea applies if you want to talk about memory usage etc. Here's an example where we have N T-shirts which we want to dry. We'll assume it's incredibly quick to get them in the drying position (i.e. the human interaction is negligible). That's not the case in real life, of course... Using a washing line outside: assuming you have an infinitely large back yard, washing dries in O(1) time. However much you have of it, it'll get the same sun and fresh air, so the size doesn't affect the drying time. Using a tumble dryer: you put 10 shirts in each load, and then they're done an hour later. (Ignore the actual numbers here — they're irrelevant.) So drying 50 shirts takes about 5 times as long as drying 10 shirts. Putting everything in an airing cupboard: If we put everything in one big pile and just let general warmth do it, it will take a long time for the middle shirts to get dry. I wouldn't like to guess at the detail, but I suspect this is at least O(N^2) — as you increase the wash load, the drying time increases faster. One important aspect of "big O" notation is that it doesn't say which algorithm will be faster for a given size. Take a hashtable (string key, integer value)  vs an array of pairs (string, integer). Is it faster to find a key in the hashtable or an element in the array, based on a string? (i.e. for the array, "find the first element where the string part matches the given key.") Hashtables are generally amortised (~= "on average") O(1) — once they're set up, it should take about the same time to find an entry in a 100 entry table as in a 1,000,000 entry table. Finding an element in an array (based on content rather than index) is linear, i.e. O(N) — on average, you're going to have to look at half the entries. Does this make a hashtable faster than an array for lookups? Not necessarily. If you've got a very small collection of entries, an array may well be faster — you may be able to check all the strings in the time that it takes to just calculate the hashcode of the one you're looking at. As the data set grows larger, however, the hashtable will eventually beat the array. Big O describes an upper limit on the growth behaviour of a function, for example the runtime of a program, when inputs become large. Examples: O(n): If I double the input size the runtime doubles O(n2): If the input size doubles the runtime quadruples O(log n): If the input size doubles the runtime increases by one O(2n): If the input size increases by one, the runtime doubles The input size is usually the space in bits needed to represent the input. Big O notation is most commonly used by programmers as an approximate measure of how long a computation (algorithm) will take to complete expressed as a function of the size of the input set.  Big O is useful to compare how well two algorithms will scale up as the number of inputs is increased.  More precisely Big O notation is used to express the asymptotic behavior of a function. That means how the function behaves as it approaches infinity.  In many cases the "O" of an algorithm will fall into one of the following cases: Big O ignores factors that do not contribute in a meaningful way to the growth curve of a function as the input size increases towards infinity. This means that constants that are added to or multiplied by the function are simply ignored.  Big O is just a way to "Express" yourself in a common way, "How much time / space does it take to run my code?". You may often see O(n), O(n2), O(nlogn) and so forth, all these are just ways to show; How does an algorithm change? O(n) means Big O is n, and now you might think, "What is n!?" Well "n" is the amount of elements. Imaging you want to search for an Item in an Array. You would have to look on Each element and as "Are you the correct element/item?" in the worst case, the item is at the last index, which means that it took as much time as there are items in the list, so to be generic, we say "oh hey, n is a fair given amount of values!". So then you might understand what "n2" means, but to be even more specific, play with the thought you have a simple, the simpliest of the sorting algorithms; bubblesort. This algorithm needs to look through the whole list, for each item. My list The flow here would be: This is O n2 because, you need to look at all items in the list there are "n" items. For each item, you look at all items once more, for comparing, this is also "n", so for every item, you look "n" times meaning n*n = n2 I hope this is as simple as you want it. But remember, Big O is just a way to experss yourself in the manner of time and space. Big O describes the fundamental scaling nature of an algorithm. There is a lot of information that Big O does not tell you about a given algorithm. It cuts to the bone and gives only information about the scaling nature of an algorithm, specifically how the resource use (think time or memory) of an algorithm scales in response to the "input size". Consider the difference between a steam engine and a rocket. They are not merely different varieties of the same thing (as, say, a Prius engine vs. a Lamborghini engine) but they are dramatically different kinds of propulsion systems, at their core. A steam engine may be faster than a toy rocket, but no steam piston engine will be able to achieve the speeds of an orbital launch vehicle. This is because these systems have different scaling characteristics with regards to the relation of fuel required ("resource usage") to reach a given speed ("input size"). Why is this so important? Because software deals with problems that may differ in size by factors up to a trillion. Consider that for a moment. The ratio between the speed necessary to travel to the Moon and human walking speed is less than 10,000:1, and that is absolutely tiny compared to the range in input sizes software may face. And because software may face an astronomical range in input sizes there is the potential for the Big O complexity of an algorithm, it's fundamental scaling nature, to trump any implementation details. Consider the canonical sorting example. Bubble-sort is O(n2) while merge-sort is O(n log n). Let's say you have two sorting applications, application A which uses bubble-sort and application B which uses merge-sort, and let's say that for input sizes of around 30 elements application A is 1,000x faster than application B at sorting. If you never have to sort much more than 30 elements then it's obvious that you should prefer application A, as it is much faster at these input sizes. However, if you find that you may have to sort ten million items then what you'd expect is that application B actually ends up being thousands of times faster than application A in this case, entirely due to the way each algorithm scales. Here is the plain English bestiary I tend to use when explaining the common varieties of Big-O In all cases, prefer algorithms higher up on the list to those lower on the list.  However, the cost of moving to a more expensive complexity class varies significantly. O(1): No growth.  Regardless of how big as the problem is, you can solve it in the same amount of time.  This is somewhat analogous to broadcasting where it takes the same amount of energy to broadcast over a given distance, regardless of the number of people that lie within the broadcast range. O(log n): This complexity is the same as O(1) except that it's just a little bit worse.  For all practical purposes, you can consider this as a very large constant scaling.  The difference in work between processing 1 thousand and 1 billion items is only a factor six. O(n): The cost of solving the problem is proportional to the size of the problem.  If your problem doubles in size, then the cost of the solution doubles.  Since most problems have to be scanned into the computer in some way, as data entry, disk reads, or network traffic, this is generally an affordable scaling factor. O(n log n): This complexity is very similar to O(n).  For all practical purposes, the two are equivalent.  This level of complexity would generally still be considered scalable.  By tweaking assumptions some O(n log n) algorithms can be transformed into O(n) algorithms.  For example, bounding the size of keys reduces sorting from O(n log n) to O(n). O(n2): Grows as a square, where n is the length of the side of a square.  This is the same growth rate as the "network effect", where everyone in a network might know everyone else in the network.  Growth is expensive.  Most scalable solutions cannot use algorithms with this level of complexity without doing significant gymnastics.  This generally applies to all other polynomial complexities - O(nk) - as well. O(2n): Does not scale.  You have no hope of solving any non-trivially sized problem.  Useful for knowing what to avoid, and for experts to find approximate algorithms which are in O(nk). Big O is a measure of how much time/space an algorithm uses relative to the size of its input.   If an algorithm is O(n) then the time/space will increase at the same rate as its input. If an algorithm is O(n2) then the time/space increase at the rate of its input squared. and so on. It is very difficult to measure the speed of software programs, and when we try, the answers can be very complex and filled with exceptions and special cases. This is a big problem, because all those exceptions and special cases are distracting and unhelpful when we want to compare two different programs with one another to find out which is "fastest". As a result of all this unhelpful complexity, people try to describe the speed of software programs using the smallest and least complex (mathematical) expressions possible. These expressions are very very crude approximations: Although, with a bit of luck, they will capture the "essence" of whether a piece of software is fast or slow. Because they are approximations, we use the letter "O" (Big Oh) in the expression, as a convention to signal to the reader that we are making a gross oversimplification. (And to make sure that nobody mistakenly thinks that the expression is in any way accurate). If you read the "Oh" as meaning "on the order of" or "approximately" you will not go too far wrong. (I think the choice of the Big-Oh might have been an attempt at humour). The only thing that these "Big-Oh" expressions try to do is to describe how much the software slows down as we increase the amount of data that the software has to process. If we double the amount of data that needs to be processed, does the software need twice as long to finish it's work? Ten times as long? In practice, there are a very limited number of big-Oh expressions that you will encounter and need to worry about: The good: The bad: ... and the ugly: What is a plain English explanation of Big O? With as little formal definition as possible and simple mathematics. A Plain English Explanation of the Need for Big-O Notation: When we program, we are trying to solve a problem. What we code is called an algorithm. Big O notation allows us to compare the worse case performance of our algorithms in a standardized way. Hardware specs vary over time and improvements in hardware can reduce the time it takes an algorithms to run. But replacing the hardware does not mean our algorithm is any better or improved over time, as our algorithm is still the same. So in order to allow us to compare different algorithms, to determine if one is better or not, we use Big O notation. A Plain English Explanation of What Big O Notation is: Not all algorithms run in the same amount of time, and can vary based on the number of items in the input, which we'll call n. Based on this, we consider the worse case analysis, or an upper-bound of the run-time as n get larger and larger. We must be aware of what n is, because many of the Big O notations reference it. Ok, my 2cents. Big-O, is rate of increase of resource consumed by program, w.r.t. problem-instance-size Resource : Could be total-CPU time, could be maximum RAM space. By default refers to CPU time. Say the problem is "Find the sum",  problem-instance= {5,10,15}  ==> problem-instance-size = 3, iterations-in-loop= 3 problem-instance= {5,10,15,20,25}  ==> problem-instance-size = 5 iterations-in-loop = 5 For input of size "n" the program is growing at speed of "n" iterations in array. Hence Big-O is N expressed as  O(n) Say the problem is "Find the Combination",  problem-instance= {5,10,15}  ==> problem-instance-size = 3, total-iterations = 3*3 = 9 problem-instance= {5,10,15,20,25}  ==> problem-instance-size = 5, total-iterations= 5*5 =25 For input of size "n" the program is growing at speed of "n*n" iterations in array. Hence Big-O is N2 expressed as  O(n2) A simple straightforward answer can be: Big O represents the worst possible time/space for that algorithm. The algorithm will never take more space/time above that limit. Big O represents time/space complexity in the extreme case. Big O notation is a way of describing the upper bound of an algorithm in terms of space or running time.  The n is the number of elements in the the problem (i.e size of an array, number of nodes in a tree, etc.)  We are interested in describing the running time as n gets  big. When we say some algorithm is O(f(n)) we are saying that the running time (or space required) by that algorithm is always lower than some constant times f(n). To say that binary search has a running time of O(logn) is to say that there exists some constant c which you can multiply log(n) by that will always be larger than the running time of binary search.  In this case you will always have some constant factor of log(n) comparisons. In other words where g(n) is the running time of your algorithm, we say that g(n) = O(f(n)) when g(n) <= c*f(n) when n > k, where c and k are some constants. "What is a plain English explanation of Big O? With as little formal   definition as possible and simple mathematics." Such a beautifully simple and short question seems at least to deserve an equally short answer, like a student might receive during tutoring. Big O notation simply tells how much time* an algorithm can run within,   in terms of only the amount of input data**. ( *in a wonderful, unit-free sense of time!) (**which is what matters, because people will always want more, whether they live today or tomorrow) Well, what's so wonderful about Big O notation if that's what it does? Practically speaking, Big O analysis is so useful and important because Big O puts the focus squarely on the algorithm's own complexity and completely ignores anything that is merely a proportionality constant—like a JavaScript engine, the speed of a CPU, your Internet connection, and all those things which become quickly become as laughably outdated as a Model T. Big O focuses on performance only in the way that matters equally as much to people living in the present or in the future. Big O notation also shines a spotlight directly on the most important principle of computer programming/engineering, the fact which inspires all good programmers to keep thinking and dreaming: the only way to achieve results beyond the slow forward march of technology is to invent a better algorithm. Algorithm example (Java): Algorithm description: This algorithm searches a list, item by item, looking for a key, Iterating on each item in the list, if it's the key then return True, If the loop has finished without finding the key, return False. Big-O notation represents the upper-bound on the Complexity (Time, Space, ..) To find The Big-O on Time Complexity: Calculate how much time (regarding input size) the worst case takes: Worst-Case: the key doesn't exist in the list. Time(Worst-Case) = 4n+1 Time: O(4n+1) = O(n) | in Big-O, constants are neglected O(n) ~ Linear There's also Big-Omega, which represent the complexity of the Best-Case: Best-Case: the key is the first item. Time(Best-Case) = 4 Time: Ω(4) = O(1) ~ Instant\Constant Big O f(x) = O(g(x)) when x goes to a (for example, a = +∞) means that there is a function k such that: f(x) = k(x)g(x) k is bounded in some neighborhood of a (if a = +∞, this means that there are numbers N and M such that for every x > N, |k(x)| < M). In other words, in plain English: f(x) = O(g(x)), x → a, means that in a neighborhood of a, f decomposes into the product of g and some bounded function. Small o By the way, here is for comparison the definition of small o. f(x) = o(g(x)) when x goes to a means that there is a function k such that: f(x) = k(x)g(x) k(x) goes to 0 when x goes to a. Examples sin x = O(x) when x → 0. sin x = O(1) when x → +∞, x2 + x = O(x) when x → 0, x2 + x = O(x2) when x → +∞, ln(x) = o(x) = O(x) when x → +∞. Attention! The notation with the equal sign "=" uses a "fake equality": it is true that o(g(x)) = O(g(x)), but false that O(g(x)) = o(g(x)).  Similarly, it is ok to write "ln(x) = o(x) when x → +∞", but the formula "o(x) = ln(x)" would make no sense. More examples O(1) = O(n) = O(n2) when n → +∞ (but not the other way around, the equality is "fake"), O(n) + O(n2) = O(n2) when n → +∞ O(O(n2)) = O(n2) when n → +∞ O(n2)O(n3) = O(n5) when n → +∞ Here is the Wikipedia article: https://en.wikipedia.org/wiki/Big_O_notation Big O notation is a way of describing how quickly an algorithm will run given an arbitrary number of input parameters, which we'll call "n". It is useful in computer science because different machines operate at different speeds, and simply saying that an algorithm takes 5 seconds doesn't tell you much because while you may be running a system with a 4.5 Ghz octo-core processor, I may be running a 15 year old, 800 Mhz system, which could take longer regardless of the algorithm. So instead of specifying how fast an algorithm runs in terms of time, we say how fast it runs in terms of number of input parameters, or "n". By describing algorithms in this way, we are able to compare the speeds of algorithms without having to take into account the speed of the computer itself. Not sure I'm further contributing to the subject but still thought I'd share: I once found this blog post to have some quite helpful (though very basic) explanations & examples on Big O:  Via examples, this helped get the bare basics into my tortoiseshell-like skull, so I think it's a pretty descent 10-minute read to get you headed in the right direction.  You want to know all there is to know of big O?  So do I. So to talk of big O, I will use words that have just one beat in them.  One sound per word.  Small words are quick.  You know these words, and so do I.  We will use words with one sound.  They are small.  I am sure you will know all of the words we will use! Now, let’s you and me talk of work. Most of the time, I do not like work.  Do you like work?  It may be the case that you do, but I am sure I do not. I do not like to go to work.  I do not like to spend time at work.  If I had my way, I would like just to play, and do fun things.  Do you feel the same as I do? Now at times, I do have to go to work.  It is sad, but true.  So, when I am at work, I have a rule: I try to do less work.  As near to no work as I can.  Then I go play! So here is the big news: the big O can help me not to do work!  I can play more of the time, if I know big O.  Less work, more play!  That is what big O helps me do. Now I have some work.  I have this list: one, two, three, four, five, six.  I must add all things in this list.   Wow, I hate work.  But oh well, I have to do this.  So here I go. One plus two is three… plus three is six... and four is... I don’t know.  I got lost.  It is too hard for me to do in my head.  I don’t much care for this kind of work. So let's not do the work.  Let's you and me just think how hard it is.  How much work would I have to do, to add six numbers? Well, let’s see.  I must add one and two, and then add that to three, and then add that to four… All in all, I count six adds.  I have to do six adds to solve this. Here comes big O, to tell us just how hard this math is. Big O says: we must do six adds to solve this.  One add, for each thing from one to six.  Six small bits of work... each bit of work is one add. Well, I will not do the work to add them now.  But I know how hard it would be.  It would be six adds. Oh no, now I have more work.  Sheesh.  Who makes this kind of stuff?! Now they ask me to add from one to ten!  Why would I do that?  I did not want to add one to six.  To add from one to ten… well… that would be even more hard! How much more hard would it be?  How much more work would I have to do?  Do I need more or less steps? Well, I guess I would have to do ten adds… one for each thing from one to ten.  Ten is more than six.  I would have to work that much more to add from one to ten, than one to six! I do not want to add right now.  I just want to think on how hard it might be to add that much.  And, I hope, to play as soon as I can. To add from one to six, that is some work.  But do you see, to add from one to ten, that is more work? Big O is your friend and mine.  Big O helps us think on how much work we have to do, so we can plan.  And, if we are friends with big O, he can help us choose work that is not so hard! Now we must do new work.  Oh, no.  I don’t like this work thing at all. The new work is: add all things from one to n. Wait!  What is n?  Did I miss that?  How can I add from one to n if you don’t tell me what n is? Well, I don’t know what n is.  I was not told.  Were you?  No?   Oh well.  So we can’t do the work.  Whew. But though we will not do the work now, we can guess how hard it would be, if we knew n.  We would have to add up n things, right?  Of course! Now here comes big O, and he will tell us how hard this work is.  He says: to add all things from one to N, one by one, is O(n).  To add all these things, [I know I must add n times.][1]  That is big O!  He tells us how hard it is to do some type of work. To me, I think of big O like a big, slow, boss man.  He thinks on work, but he does not do it.  He might say, "That work is quick."  Or, he might say, "That work is so slow and hard!"  But he does not do the work.  He just looks at the work, and then he tells us how much time it might take. I care lots for big O.  Why?  I do not like to work!  No one likes to work.  That is why we all love big O!  He tells us how fast we can work.  He helps us think of how hard work is. Uh oh, more work.  Now, let’s not do the work.  But, let’s make a plan to do it, step by step. They gave us a deck of ten cards.  They are all mixed up: seven, four, two, six… not straight at all.  And now... our job is to sort them. Ergh.  That sounds like a lot of work! How can we sort this deck?  I have a plan. I will look at each pair of cards, pair by pair, through the deck, from first to last.  If the first card in one pair is big and the next card in that pair is small, I swap them.  Else, I go to the next pair, and so on and so on... and soon, the deck is done. When the deck is done, I ask: did I swap cards in that pass?  If so, I must do it all once more, from the top. At some point, at some time, there will be no swaps, and our sort of the deck would be done.  So much work! Well, how much work would that be, to sort the cards with those rules? I have ten cards.  And, most of the time -- that is, if I don’t have lots of luck -- I must go through the whole deck up to ten times, with up to ten card swaps each time through the deck. Big O, help me! Big O comes in and says: for a deck of n cards, to sort it this way will be done in O(N squared) time. Why does he say n squared? Well, you know n squared is n times n.  Now, I get it: n cards checked, up to what might be n times through the deck.  That is two loops, each with n steps.  That is n squared much work to be done.  A lot of work, for sure! Now when big O says it will take O(n squared) work, he does not mean n squared adds, on the nose.  It might be some small bit less, for some case.  But in the worst case, it will be near n squared steps of work to sort the deck. Now here is where big O is our friend. Big O points out this: as n gets big, when we sort cards, the job gets MUCH MUCH MORE HARD than the old just-add-these-things job.  How do we know this? Well, if n gets real big, we do not care what we might add to n or n squared.   For big n, n squared is more large than n. Big O tells us that to sort things is more hard than to add things.  O(n squared) is more than O(n) for big n.  That means: if n gets real big, to sort a mixed deck of n things MUST take more time, than to just add n mixed things. Big O does not solve the work for us.  Big O tells us how hard the work is. I have a deck of cards.  I did sort them.  You helped.  Thanks. Is there a more fast way to sort the cards?  Can big O help us? Yes, there is a more fast way!  It takes some time to learn, but it works... and it works quite fast.  You can try it too, but take your time with each step and do not lose your place. In this new way to sort a deck, we do not check pairs of cards the way we did a while ago.  Here are your new rules to sort this deck: One: I choose one card in the part of the deck we work on now.  You can choose one for me if you like.  (The first time we do this, “the part of the deck we work on now” is the whole deck, of course.) Two: I splay the deck on that card you chose.  What is this splay; how do I splay?  Well, I go from the start card down, one by one, and I look for a card that is more high than the splay card. Three: I go from the end card up, and I look for a card that is more low than the splay card.   Once I have found these two cards, I swap them, and go on to look for more cards to swap.  That is, I go back to step Two, and splay on the card you chose some more. At some point, this loop (from Two to Three) will end.  It ends when both halves of this search meet at the splay card.  Then, we have just splayed the deck with the card you chose in step One.  Now, all the cards near the start are more low than the splay card; and the cards near the end are more high than the splay card.  Cool trick! Four (and this is the fun part): I have two small decks now, one more low than the splay card, and one more high.  Now I go to step one, on each small deck!  That is to say, I start from step One on the first small deck, and when that work is done, I start from step One on the next small deck.   I break up the deck in parts, and sort each part, more small and more small, and at some time I have no more work to do.  Now this may seem slow, with all the rules.  But trust me, it is not slow at all.  It is much less work than the first way to sort things! What is this sort called?  It is called Quick Sort!  That sort was made by a man called C. A. R. Hoare and he called it Quick Sort.  Now, Quick Sort gets used all the time! Quick Sort breaks up big decks in small ones.  That is to say, it breaks up big tasks in small ones. Hmmm.  There may be a rule in there, I think.  To make big tasks small, break them up. This sort is quite quick.  How quick?  Big O tells us: this sort needs O(n log n) work to be done, in the mean case.   Is it more or less fast than the first sort?  Big O, please help! The first sort was O(n squared).  But Quick Sort is O(n log n).  You know that n log n is less than n squared, for big n, right?  Well, that is how we know that Quick Sort is fast! If you have to sort a deck, what is the best way?  Well, you can do what you want, but I would choose Quick Sort. Why do I choose Quick Sort?  I do not like to work, of course!  I want work done as soon as I can get it done. How do I know Quick Sort is less work?  I know that O(n log n) is less than O(n squared).  The O's are more small, so Quick Sort is less work! Now you know my friend, Big O.  He helps us do less work.  And if you know big O, you can do less work too! You learned all that with me!  You are so smart!  Thank you so much! Now that work is done, let’s go play! [1]: There is a way to cheat and add all the things from one to n, all at one time.  Some kid named Gauss found this out when he was eight.  I am not that smart though, so don't ask me how he did it. I've more simpler way to understand the time complexity he most common metric for calculating time complexity is Big O notation. This removes all constant factors so that the running time can be estimated in relation to N as N approaches infinity. In general you can think of it like this: Is constant. The running time of the statement will not change in relation to N Is linear. The running time of the loop is directly proportional to N. When N doubles, so does the running time. Is quadratic. The running time of the two loops is proportional to the square of N. When N doubles, the running time increases by N * N. Is logarithmic. The running time of the algorithm is proportional to the number of times N can be divided by 2. This is because the algorithm divides the working area in half with each iteration. Is N * log ( N ). The running time consists of N loops (iterative or recursive) that are logarithmic, thus the algorithm is a combination of linear and logarithmic. In general, doing something with every item in one dimension is linear, doing something with every item in two dimensions is quadratic, and dividing the working area in half is logarithmic. There are other Big O measures such as cubic, exponential, and square root, but they're not nearly as common. Big O notation is described as O (  ) where  is the measure. The quicksort algorithm would be described as O ( N * log ( N ) ). Note: None of this has taken into account best, average, and worst case measures. Each would have its own Big O notation. Also note that this is a VERY simplistic explanation. Big O is the most common, but it's also more complex that I've shown. There are also other notations such as big omega, little o, and big theta. You probably won't encounter them outside of an algorithm analysis course.  Say you order Harry Potter: Complete 8-Film Collection [Blu-ray] from Amazon and download the same film collection online at the same time. You want to test which method is faster. The delivery takes almost a day to arrive and the download completed about 30 minutes earlier. Great! So it’s a tight race. What if I order several Blu-ray movies like The Lord of the Rings, Twilight, The Dark Knight Trilogy, etc. and download all the movies online at the same time? This time, the delivery still take a day to complete, but the online download takes 3 days to finish. For online shopping, the number of purchased item (input) doesn’t affect the delivery time. The output is constant. We call this O(1). For online downloading, the download time is directly proportional to the movie file sizes (input). We call this O(n). From the experiments, we know that online shopping scales better than online downloading. It is very important to understand big O notation because it helps you to analyze the scalability and efficiency of algorithms. Note: Big O notation represents the worst-case scenario of an algorithm. Let’s assume that O(1) and O(n) are the worst-case scenarios of the example above. Reference : http://carlcheo.com/compsci Assume we're talking about an algorithm A, which should do something with a dataset of size n.  Then O( <some expression X involving n> ) means, in simple English: If you're unlucky when executing A, it might take as much as X(n) operations to   complete. As it happens, there are certain functions (think of them as implementations of X(n)) that tend to occur quite often. These are well known and easily compared (Examples: 1, Log N, N, N^2, N!, etc..) By comparing these when talking about A and other algorithms, it is easy to rank the algorithms according to the number of operations they may (worst-case) require to complete.  In general, our goal will be to find or structure an algorithm A in such a way that it will have a function X(n) that returns as low a number as possible.  If you have a suitable notion of infinity in your head, then there is a very brief description: Big O notation tells you the cost of solving an infinitely large problem. And furthermore Constant factors are negligible If you upgrade to a computer that can run your algorithm twice as fast, big O notation won't notice that. Constant factor improvements are too small to even be noticed in the scale that big O notation works with. Note that this is an intentional part of the design of big O notation. Although anything "larger" than a constant factor can be detected, however. When interested in doing computations whose size is "large" enough to be considered as approximately infinity, then big O notation is approximately the cost of solving your problem. If the above doesn't make sense, then you don't have a compatible intuitive notion of infinity in your head, and you should probably disregard all of the above; the only way I know to make these ideas rigorous, or to explain them if they aren't already intuitively useful, is to first teach you big O notation or something similar. (although, once you well understand big O notation in the future, it may be worthwhile to revisit these ideas) What is a plain English explanation of “Big O” notation? Very Quick Note: The O in "Big O" refers to as "Order"(or precisely "order of") so you could get its idea literally that it's used to order something to  compare them.   "Big O" does two things: There are seven most used notations  Suppose you get notation O(N^2), not only you are clear the method takes N*N steps to accomplish a task, also you see that it's not good as O(NlogN) from its ranking. Please note the order at line end, just for your better understanding.There's more than 7 notations if all possibilities considered. In CS, the set of steps to accomplish a task is called algorithms. In Terminology, Big O notation is used to describe the performance or complexity of an algorithm. In addition, Big O establishes the worst-case or measure the Upper-Bound steps. You could refer to Big-Ω (Big-Omega) for best case. Big-Ω (Big-Omega) notation (article) | Khan Academy Summary "Big O" describes the algorithm's performance and evaluates it. or address it formally, "Big O" classifies the algorithms and standardize the comparison process.  Simplest way to look at it (in plain English) We are trying to see how the number of input parameters, affects the running time of an algorithm. If the running time of your application is proportional to the number of input parameters, then it is said to be in Big O of n. The above statement is a good start but not completely true. A more accurate explanation (mathematical) Suppose n=number of input parameters T(n)= The actual function that expresses the running time of the algorithm as a function of n c= a constant f(n)= An approximate function that expresses the running time of the algorithm as a function of n Then as far as Big O is concerned, the approximation f(n) is considered good enough as long as the below condition is true. The equation is read as As n approaches infinity, T of n, is less than or equal to c times f of n. In big O notation this is written as This is read as T of n is in big O of n. Back to English Based on the mathematical definition above, if you say your algorithm is a Big O of n, it means it is a function of n (number of input parameters) or faster. If your algorithm is Big O of n, then it is also automatically the Big O of n square. Big O of n means my algorithm runs at least as fast as this. You cannot look at Big O notation of your algorithm and say its slow. You can only say its fast. Check this out for a video tutorial on Big O from UC Berkley. It is actually a simple concept. If you hear professor Shewchuck (aka God level teacher) explaining it, you will say "Oh that's all it is!". Definition :- Big O notation is a notation which says how a algorithm performance will perform if the data input increases.  When we talk about algorithms there are 3 important pillars Input , Output and Processing of algorithm. Big O is symbolic notation which says if the data input is increased in what rate will the performance vary of the algorithm processing. I would encourage you to see this youtube video which explains Big O Notation in depth with code examples.  So for example assume that a algorithm takes 5 records and the time required for processing the same is  27 seconds. Now if we increase the records to 10 the algorithm takes 105 seconds. In simple words the time taken is square of the number of records. We can denote this by O(n ^ 2). This symbolic representation is termed as Big O notation. Now please note the units can be anything in inputs it can be bytes , bits number of records , the performance can be measured in any unit like second , minutes , days and so on. So its not the exact unit but rather the relationship.  For example look at the below function "Function1" which takes a collection and does processing on the first record. Now for this function the performance will be same irrespective you put 1000 , 10000 or 100000 records. So we can denote it by O(1). Now see the below function "Function2()". In this case the processing time will increase with number of records. We can denote this algorithm performance using O(n). When we see a Big O notation for any algorithm we can classify them in to three categories of performance :- So  by looking at Big O notation we categorize good and bad zones for algorithms.  I would recommend you to watch this 10 minutes video which discusses Big O with sample code https://www.youtube.com/watch?v=k6kxtzICG_g I found a really great explanation about big O notation especially for a someone who's not much into mathematics. https://rob-bell.net/2009/06/a-beginners-guide-to-big-o-notation/ Big O notation is used in Computer Science to describe the performance   or complexity of an algorithm. Big O specifically describes the   worst-case scenario, and can be used to describe the execution time   required or the space used (e.g. in memory or on disk) by an   algorithm. Anyone who's read Programming Pearls or any other Computer Science   books and doesn’t have a grounding in Mathematics will have hit a wall   when they reached chapters that mention O(N log N) or other seemingly   crazy syntax. Hopefully this article will help you gain an   understanding of the basics of Big O and Logarithms. As a programmer first and a mathematician second (or maybe third or   fourth) I found the best way to understand Big O thoroughly was to   produce some examples in code. So, below are some common orders of   growth along with descriptions and examples where possible. O(1) describes an algorithm that will always execute in the same time   (or space) regardless of the size of the input data set. O(N) describes an algorithm whose performance will grow linearly and   in direct proportion to the size of the input data set. The example   below also demonstrates how Big O favours the worst-case performance   scenario; a matching string could be found during any iteration of the   for loop and the function would return early, but Big O notation will   always assume the upper limit where the algorithm will perform the   maximum number of iterations. O(N2) represents an algorithm whose performance is directly   proportional to the square of the size of the input data set. This is   common with algorithms that involve nested iterations over the data   set. Deeper nested iterations will result in O(N3), O(N4) etc. O(2N) denotes an algorithm whose growth doubles with each additon to   the input data set. The growth curve of an O(2N) function is   exponential - starting off very shallow, then rising meteorically. An   example of an O(2N) function is the recursive calculation of Fibonacci   numbers: Logarithms are slightly trickier to explain so I'll use a common   example: Binary search is a technique used to search sorted data sets. It works   by selecting the middle element of the data set, essentially the   median, and compares it against a target value. If the values match it   will return success. If the target value is higher than the value of   the probe element it will take the upper half of the data set and   perform the same operation against it. Likewise, if the target value   is lower than the value of the probe element it will perform the   operation against the lower half. It will continue to halve the data   set with each iteration until the value has been found or until it can   no longer split the data set. This type of algorithm is described as O(log N). The iterative halving   of data sets described in the binary search example produces a growth   curve that peaks at the beginning and slowly flattens out as the size   of the data sets increase e.g. an input data set containing 10 items   takes one second to complete, a data set containing 100 items takes   two seconds, and a data set containing 1000 items will take three   seconds. Doubling the size of the input data set has little effect on   its growth as after a single iteration of the algorithm the data set   will be halved and therefore on a par with an input data set half the   size. This makes algorithms like binary search extremely efficient   when dealing with large data sets. This is a very simplified explanation, but I hope it covers most important details. Let's say your algorithm dealing with the problem depends on some 'factors', for example let's make it N and X. Depending on N and X, your algorithm will require some operations, for example in the WORST case it's 3(N^2) + log(X) operations. Since Big-O doesn't care too much about constant factor (aka 3), the Big-O of your algorithm is O(N^2 + log(X)). It basically translates 'the amount of operations your algorithm needs for the worst case scales with this'.
__label__git __label__merge-conflict-resolution __label__git-merge __label__git-merge-conflict I want to resolve merge conflicts in my Git repository. How to do that ? Try: git mergetool It opens a GUI that steps you through each conflict, and you get to choose how to merge.  Sometimes it requires a bit of hand editing afterwards, but usually it's enough by itself.  It is much better than doing the whole thing by hand certainly. As per @JoshGlover comment: The command doesn't necessarily open a GUI unless you install one. Running git mergetool for me resulted in vimdiff being used. You can install one of the following tools to use it instead: meld, opendiff, kdiff3, tkdiff, xxdiff, tortoisemerge, gvimdiff, diffuse, ecmerge, p4merge, araxis, vimdiff, emerge. Below is the sample procedure to use vimdiff for resolve merge conflicts. Based on this link Step 1: Run following commands in your terminal This will set vimdiff as the default merge tool. Step 2: Run following command in terminal Step 3: You will see a vimdiff display in following format These 4 views are LOCAL – this is file from the current branch BASE – common ancestor, how file looked before both changes REMOTE – file you are merging into your branch MERGED – merge result, this is what gets saved in the repo You can navigate among these views using ctrl+w. You can directly reach MERGED view using ctrl+w followed by j. More info about vimdiff navigation here and here Step 4. You could edit the MERGED view the following way If you want to get changes from REMOTE If you want to get changes from BASE If you want to get changes from LOCAL Step 5. Save, Exit, Commit and Clean up :wqa save and exit from vi git commit -m "message" git clean Remove extra files (e.g. *.orig) created by diff tool. Here's a probable use-case, from the top: You're going to pull some changes, but oops, you're not up to date: So you get up-to-date and try again, but have a conflict: So you decide to take a look at the changes: Oh my, oh my, upstream changed some things, but just to use my changes...no...their changes... And then we try a final time Ta-da! I find merge tools rarely help me understand the conflict or the resolution. I'm usually more successful looking at the conflict markers in a text editor and using git log as a supplement. Here are a few tips: The best thing I have found is to use the "diff3" merge conflict style: git config merge.conflictstyle diff3 This produces conflict markers like this: The middle section is what the common ancestor looked like. This is useful because you can compare it to the top and bottom versions to get a better sense of what was changed on each branch, which gives you a better idea for what the purpose of each change was. If the conflict is only a few lines, this generally makes the conflict very obvious. (Knowing how to fix a conflict is very different; you need to be aware of what other people are working on. If you're confused, it's probably best to just call that person into your room so they can see what you're looking at.) If the conflict is longer, then I will cut and paste each of the three sections into three separate files, such as "mine", "common" and "theirs". Then I can run the following commands to see the two diff hunks that caused the conflict: This is not the same as using a merge tool, since a merge tool will include all of the non-conflicting diff hunks too. I find that to be distracting. Somebody already mentioned this, but understanding the intention behind each diff hunk is generally very helpful for understanding where a conflict came from and how to handle it. This shows all of the commits that touched that file in between the common ancestor and the two heads you are merging. (So it doesn't include commits that already exist in both branches before merging.) This helps you ignore diff hunks that clearly are not a factor in your current conflict. Verify your changes with automated tools. If you have automated tests, run those. If you have a lint, run that. If it's a buildable project, then build it before you commit, etc. In all cases, you need to do a bit of testing to make sure your changes didn't break anything. (Heck, even a merge without conflicts can break working code.) Plan ahead; communicate with co-workers. Planning ahead and being aware of what others are working on can help prevent merge conflicts and/or help resolve them earlier -- while the details are still fresh in mind.  For example, if you know that you and another person are both working on different refactoring that will both affect the same set of files, you should talk to each other ahead of time and get a better sense for what types of changes each of you is making. You might save considerable time and effort if you conduct your planned changes serially rather than in parallel.  For major refactorings that cut across a large swath of code, you should strongly consider working serially: everybody stops working on that area of the code while one person performs the complete refactoring. If you can't work serially (due to time pressure, maybe), then communicating about expected merge conflicts at least helps you solve the problems sooner while the details are still fresh in mind. For example, if a co-worker is making a disruptive series of commits over the course of a one-week period, you may choose to merge/rebase on that co-workers branch once or twice each day during that week. That way, if you do find merge/rebase conflicts, you can solve them more quickly than if you wait a few weeks to merge everything together in one big lump. If you're unsure of a merge, don't force it. Merging can feel overwhelming, especially when there are a lot of conflicting files and the conflict markers cover hundreds of lines. Often times when estimating software projects we don't include enough time for overhead items like handling a gnarly merge, so it feels like a real drag to spend several hours dissecting each conflict. In the long run, planning ahead and being aware of what others are working on are the best tools for anticipating merge conflicts and prepare yourself to resolve them correctly in less time. Identify which files are in conflict (Git should tell you this). Open each file and examine the diffs; Git demarcates them.  Hopefully it will be obvious which version of each block to keep.  You may need to discuss it with fellow developers who committed the code. Once you've resolved the conflict in a file git add the_file. Once you've resolved all conflicts, do git rebase --continue or whatever command  Git said to do when you completed. Merge conflicts happens when changes are made to a file at the same time. Here is how to solve it. Here are simple steps what to do when you get into conflicted state: Solve the conflicts separately for each file by one of the following approaches: Use GUI to solve the conflicts: git mergetool (the easiest way). To accept remote/other version, use: git checkout --theirs path/file. This will reject any local changes you did for that file. To accept local/our version, use: git checkout --ours path/file However you've to be careful, as remote changes that conflicts were done for some reason. Related: What is the precise meaning of "ours" and "theirs" in git? Edit the conflicted files manually and look for the code block between <<<<</>>>>> then choose the version either from above or below =====. See: How conflicts are presented. Path and filename conflicts can be solved by git add/git rm. Finally, review the files ready for commit using: git status. If you still have any files under Unmerged paths, and you did solve the conflict manually, then let Git know that you solved it by: git add path/file. If all conflicts were solved successfully, commit the changes by: git commit -a and push to remote as usual. See also: Resolving a merge conflict from the command line at GitHub For practical tutorial, check: Scenario 5 - Fixing Merge Conflicts by Katacoda. I've successfully used DiffMerge which can visually compare and merge files on Windows, macOS and Linux/Unix. It graphically can show the changes between 3 files and it allows automatic merging (when safe to do so) and full control over editing the resulting file.  Image source: DiffMerge (Linux screenshot) Simply download it and run in repo as: On macOS you can install via: And probably (if not provided) you need the following extra simple wrapper placed in your PATH (e.g. /usr/bin): Then you can use the following keyboard shortcuts: Alternatively you can use opendiff (part of Xcode Tools) which lets you merge two files or directories together to create a third file or directory. Check out the answers in Stack Overflow question Aborting a merge in Git, especially Charles Bailey's answer which shows how to view the different versions of the file with problems, for example,  If you're making frequent small commits, then start by looking at the commit comments with git log --merge. Then git diff will show you the conflicts. For conflicts that involve more than a few lines, it's easier to see what's going on in an external GUI tool. I like opendiff -- Git also supports vimdiff, gvimdiff, kdiff3, tkdiff, meld, xxdiff, emerge out of the box and you can install others: git config merge.tool "your.tool" will set your chosen tool and then git mergetool after a failed merge will  show you the diffs in context. Each time you edit a file to resolve a conflict, git add filename will update the index and your diff will no longer show it. When all the conflicts are handled and their files have been git add-ed, git commit will complete your merge. See How Conflicts Are Presented or, in Git, the git merge documentation to understand what merge conflict markers are. Also, the How to Resolve Conflicts section explains how to resolve the conflicts: After seeing a conflict, you can do two things: Decide not to merge. The only clean-ups you need are to reset the index file to the HEAD commit to reverse 2. and to clean up working tree changes made by 2. and 3.; git merge --abort can be used for this. Resolve the conflicts. Git will mark the conflicts in the working tree. Edit the files into shape and git add them to the index. Use git commit to seal the deal. You can work through the conflict with a number of tools: Use a mergetool. git mergetool to launch a graphical mergetool which will work you through the merge. Look at the diffs. git diff will show a three-way diff, highlighting changes from both the HEAD and MERGE_HEAD versions. Look at the diffs from each branch. git log --merge -p <path> will show diffs first for the HEAD version and then the MERGE_HEAD version. Look at the originals. git show :1:filename shows the common ancestor, git show :2:filename shows the HEAD version, and git show :3:filename shows the MERGE_HEAD version. You can also read about merge conflict markers and how to resolve them in the Pro Git book section Basic Merge Conflicts. I either want my or their version in full, or want to review individual changes and decide for each of them. Fully accept my or theirs version: Accept my version (local, ours): Accept their version (remote, theirs): If you want to do for all conflict files run: or Review all changes and accept them individually Default mergetool works in command line. How to use a command line mergetool should be a separate question. You can also install visual tool for this, e.g. meld and run It will open local version (ours), "base" or "merged" version (the current result of the merge) and remote version (theirs). Save the merged version when you are finished, run git mergetool -t meld again until you get "No files need merging", then go to Steps 3. and 4. For Emacs users which want to resolve merge conflicts semi-manually: shows all files which require conflict resolution. Open each of those files one by one, or all at once by: When visiting a buffer requiring edits in Emacs, type This will open three buffers (mine, theirs, and the output buffer). Navigate by pressing 'n' (next region), 'p' (prevision region). Press 'a' and 'b' to copy mine or theirs region to the output buffer, respectively. And/or edit the output buffer directly. When finished: Press 'q'. Emacs asks you if you want to save this buffer: yes. After finishing a buffer mark it as resolved by running from the teriminal: When finished with all buffers type to finish the merge. In speaking of pull/fetch/merge in the above answers, I would like to share an interesting and productive trick, This above command is the most useful command in my git life which saved a lots of time. Before pushing your newly committed change to remote server, try git pull --rebase rather git pull and manual merge and it will automatically sync latest remote server changes (with a fetch + merge) and will put your local latest commit at the top in git log. No need to worry about manual pull/merge. In case of conflict, just use Find details at: http://gitolite.com/git-pull--rebase Simply, if you know well that changes in one of the repositories is not important, and want to resolve all changes in favor of the other one, use: to resolve changes in the favor of your repository, or to resolve changes in favor of the other or the main repository. Or else you will have to use a GUI merge tool to step through files one by one, say the merge tool is p4merge, or write any one's name you've already installed and after finishing a file, you will have to save and close, so the next one will open. Please follow the following steps to fix merge conflicts in Git: Check the Git status: git status Get the patchset: git fetch (checkout the right patch from your Git commit) Checkout a local branch (temp1 in my example here): git checkout -b temp1 Pull the recent contents from master: git pull --rebase origin master Start the mergetool and check the conflicts and fix them...and check the changes in the remote branch with your current branch: git mergetool Check the status again:  git status Delete the unwanted files locally created by mergetool, usually mergetool creates extra file with *.orig extension. Please delete that file as that is just the duplicate and fix changes locally and add the correct version of your files. git add #your_changed_correct_files Check the status again: git status Commit the changes to the same commit id (this avoids a new separate patch set): git commit --amend Push to the master branch: git push (to your Git repository) There are 3 steps: Find which files cause conflicts by command Check the files, in which you would find the conflicts marked like Change it to the way you want it, then commit with commands CoolAJ86's answer sums up pretty much everything. In case you have changes in both branches in the same piece of code you will have to do a manual merge. Open the file in conflict in any text editor and you should see following structure. Choose one of the alternatives or a combination of both in a way that you want new code to be, while removing equal signs and angle brackets.  You could fix merge conflicts in a number of ways as other have detailed. I think the real key is knowing how changes flow with local and remote repositories.  The key to this is understanding tracking branches.  I have found that I think of the tracking branch as the 'missing piece in the middle' between me my local, actual files directory and the remote defined as origin.   I've personally got into the habit of 2 things to help avoid this. Instead of: Which has two drawbacks -  a) All new/changed files get added and that might include some unwanted changes. b) You don't get to review the file list first. So instead I do: This way you are more deliberate about which files get added and you also get to review the list and think a bit more while using the editor for the message.  I find it also improves my commit messages when I use a full screen editor rather than the -m option. [Update - as time has passed I've switched more to: ] Also (and more relevant to your situation), I try to avoid: or because pull implies a merge and if you have changes locally that you didn't want merged you can easily end up with merged code and/or merge conflicts for code that shouldn't have been merged. Instead I try to do You may also find this helpful: git branch, fork, fetch, merge, rebase and clone, what are the differences? If you want to merge from branch test to master, you can follow these steps: Step 1: Go to the branch Step 2: Step 3: If there are some conflicts, go to these files to modify it. Step 4: Add these changes Step 5: Step 6: If there is still conflict, go back to step 3 again. If there is no conflict, do following: Step 7: And then there is no conflict between test and master. You can use merge directly. Does not seem to always work for me and usually ends up displaying every commit that was different between the two branches, this happens even when using -- to separate the path from the command. What I do to work around this issue is open up two command lines and in one run and in the other Replacing $MERGED_IN_BRANCH with the branch I merged in and [path] with the file that is conflicting. This command will log all the commits, in patch form, between (..) two commits. If you leave one side empty like in the commands above git will automatically use HEAD (the branch you are merging into in this case). This will allow you to see what commits went into the file in the two branches after they diverged. It usually makes it much easier to solve conflicts. For a big merge conflict, using patience provided good results for me. It will try to match blocks rather than individual lines. If you change the indentation of your program for instance, the default Git merge strategy sometimes matches single braces { which belongs to different functions. This is avoided with patience: From the documentation: If you have a merge conflict and want to see what others had in mind when modifying their branch, it's sometimes easier to compare their branch directly with the common ancestor (instead of our branch). For that you can use merge-base: Usually, you only want to see the changes for a particular file: As of December 12th 2016, you can merge branches and resolve conflicts on github.com Thus, if you don't want to use the command-line or any 3rd party tools that are offered here from older answers, go with GitHub's native tool. This blog post explains in detail, but the basics are that upon 'merging' two branches via the UI, you will now see a 'resolve conflicts' option that will take you to an editor allowing you to deal with these merge conflicts.  I always follow the below steps to avoid conflicts. Now you can do the same and maintain as many local branches you want and work simultaneous my just doing a git checkout to your branch when ever necessary. Merge conflicts could occur in different situations: You need to install a merge tool which is compatible with Git to resolve the conflicts. I personally use KDiff3, and I've found it nice and handy. You can download its Windows version here: https://sourceforge.net/projects/kdiff3/files/ BTW if you install Git Extensions there is an option in its setup wizard to install Kdiff3. Then setup git configs to use Kdiff as its mergetool: (Remember to replace the path with the actual path of Kdiff exe file.) Then every time you come across a merge conflict you just need to run this command: Then it opens the Kdiff3, and first tries to resolve the merge conflicts automatically. Most of the conflicts would be resolved spontaneously and you need to fix the rest manually. Here's what Kdiff3 looks like:  Then once you're done, save the file and it goes to the next file with conflict and you do the same thing again until all the conflicts are resolved. To check if everything is merged successfully, just run the mergetool command again, you should get this result: This answers is to add an alternative for those VIM users like I that prefers to do everything within the editor.  Tpope came up with this great plugin for VIM called fugitive. Once installed you can run :Gstatus to check the files that have conflict and :Gdiff to open Git in a 3 ways merge.  Once in the 3-ways merge, fugitive will let you get the changes of any of the branches you are merging in the following fashion: Once you are finished merging the file, type :Gwrite in the merged buffer.  Vimcasts released a great video explaining in detail this steps. You can try Gitlense for VS Code, They key features are: I already like this feature:     And there are many features you can check them here. In this step you will try to fix the conflict using your preferred IDE. You can follow this link to check how to fix the conflict in the file Now everything is fine and you will find your commit in gerrit I hope that this will help everyone concerning this issue. A safer way to resolve conflicts is to use git-mediate (the common solutions suggested here are quite error prone imho). See this post for a quick intro on how to use it. For those who are using Visual Studio (2015 in my case) Close your project in VS. Especially in big projects VS tends to freak out when merging using the UI. Do the merge in command prompt.  git checkout target_branch git merge source_branch Then open the project in VS and go to Team Explorer -> Branch.  Now there is a message that says Merge is pending and conflicting files are listed right below the message. Click the conflicting file and you will have the option to Merge, Compare, Take Source, Take Target.  The merge tool in VS is very easy to use. Try Visual Studio Code for editing if you aren't already. What it does is after you try merging(and land up in merge conflicts).VS code automatically detects the merge conflicts. It can help you very well by showing what are the changes made to the original one and should you accept  incoming or  current change(meaning original one before merging)'?. It helped for me and it can work for you too !  PS: It will work only if you've configured git with with your code and Visual Studio Code.  I am using Microsoft's Visual Code for resolving conflicts. Its very simple to use. I keep my project open in the workspace. It detects and highlights conflicts, moreover give GUI options to select whatever change I want to keep from HEAD or incoming.    If you are using intelliJ as IDE Try to merge parent to your branch by It will show all conflicts like this A_MBPro:test anu$ git merge origin/ Auto-merging   src/test/java/com/.../TestClass.java CONFLICT   (content): Merge conflict in   src/test/java/com/.../TestClass.java Now note that the file TestClass.java is shown in red in intelliJ Also git status will show  Open the file in intelliJ, it will have sections with  where HEAD is changes on your local branch and origin/ is changes from the remote branch. Here keep the stuff that you need and remove the stuff you don't need.After that the normal steps should do. That is
__label__ecmascript-6 __label__let __label__scope __label__javascript __label__var I've heard that it's described as a local variable, but I'm still not quite sure how it behaves differently than the var keyword. What are the differences?. When should let be used instead of var? Main difference is scoping rules. Variables declared by var keyword are scoped to the immediate function body (hence the function scope) while let variables are scoped to the immediate enclosing block denoted by { } (hence the block scope). The reason why let keyword was introduced to the language was function scope is confusing and was one of the main sources of bugs in JavaScript. Take a look at this example from another stackoverflow question: My value: 3 was output to console each time funcs[j](); was invoked since anonymous functions were bound to the same variable. People had to create immediately invoked functions to capture correct value from the loops but that was also hairy. While variables declared with var keyword are hoisted (initialized with undefined before the code is run) which means they are accessible in their enclosing scope even before they are declared: let variables are not initialized until their definition is evaluated. Accessing them before the initialization results in a ReferenceError. Variable said to be in "temporal dead zone" from the start of the block until the initialization is processed. At the top level, let, unlike var, does not create a property on the global object: In strict mode, var will let you re-declare the same variable in the same scope while let raises a SyntaxError. let can also be used to avoid problems with closures. It binds fresh value rather than keeping an old reference as shown in examples below.   for(var i=1; i<6; i++) {   $("#div" + i).click(function () { console.log(i); }); } <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script> <p>Clicking on each number will log to console:</p>  <div id="div1">1</div> <div id="div2">2</div> <div id="div3">3</div> <div id="div4">4</div> <div id="div5">5</div>    Code above demonstrates a classic JavaScript closure problem. Reference to the i variable is being stored in the click handler closure, rather than the actual value of i. Every single click handler will refer to the same object because there’s only one counter object which holds 6 so you get six on each click. A general workaround is to wrap this in an anonymous function and pass i as an argument. Such issues can also be avoided now by using let instead var as shown in the code below. (Tested in Chrome and Firefox 50)   for(let i=1; i<6; i++) {   $("#div" + i).click(function () { console.log(i); }); } <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script> <p>Clicking on each number will log to console:</p>  <div id="div1">1</div> <div id="div2">2</div> <div id="div3">3</div> <div id="div4">4</div> <div id="div5">5</div>    To understand the difference, consider the following code: Here, we can see that our variable j is only known in the first for loop, but not before and after. Yet, our variable i is known in the entire function. Also, consider that block scoped variables are not known before they are declared because they are not hoisted. You're also not allowed to redeclare the same block scoped variable within the same block. This makes block scoped variables less error prone than globally or functionally scoped variables, which are hoisted and which do not produce any errors in case of multiple declarations. Some people would argue that in the future we'll ONLY use let statements and that var statements will become obsolete. JavaScript guru Kyle Simpson wrote a very elaborate article on why he believes that won't be the case. Today, however, that is definitely not the case. In fact, we need actually to ask ourselves whether it's safe to use the let statement. The answer to that question depends on your environment: If you're writing server-side JavaScript code (Node.js), you can safely use the let statement. If you're writing client-side JavaScript code and use a browser based transpiler (like Traceur or babel-standalone), you can safely use the let statement, however your code is likely to be anything but optimal with respect to performance. If you're writing client-side JavaScript code and use a Node based transpiler (like the traceur shell script or Babel), you can safely use the let statement. And because your browser will only know about the transpiled code, performance drawbacks should be limited. If you're writing client-side JavaScript code and don't use a transpiler, you need to consider browser support. There are still some browsers that don't support let at all :  For an up-to-date overview of which browsers support the let statement at the time of your reading this answer, see this Can I Use page. (*) Globally and functionally scoped variables can be initialized and used before they are declared because JavaScript variables are hoisted. This means that declarations are always much to the top of the scope. (**) Block scoped variables are not hoisted Here's an explanation of the let keyword with some examples. let works very much like var. The main difference is that the scope of a var variable is the entire enclosing function This table on Wikipedia shows which browsers support Javascript 1.7. Note that only Mozilla and Chrome browsers support it. IE, Safari, and potentially others don't. The accepted answer is missing a point: Variables declared using the let keyword are block-scoped, which means that they are available only in the block in which they were declared. At the top level, variables declared using let don't create properties on the global object. Inside a function (but outside of a block), let has the same scope as var. Variables declared using let inside a block can't be accessed outside that block. Variables declared with let in loops can be referenced only inside that loop. If you use let instead of var in a loop, with each iteration you get a new variable. That means that you can safely use a closure inside a loop. Because of the temporal dead zone, variables declared using let can't be accessed before they are declared. Attempting to do so throws an error. You can't declare the same variable multiple times using let. You also can't declare a variable using let with the same identifier as another variable which was declared using var. const is quite similar to let—it's block-scoped and has TDZ. There are, however, two things which are different. Variable declared using const can't be re-assigned. Note that it doesn't mean that the value is immutable. Its properties still can be changed. If you want to have an immutable object, you should use Object.freeze(). You always must specify a value when declaring a variable using const. The main difference is the scope difference, while let can be only available inside the scope it's declared, like in for loop, var can be accessed outside the loop for example. From the documentation in MDN (examples also from MDN): let allows you to declare variables that are limited in scope to the block, statement, or expression on which it is used. This is unlike the var keyword, which defines a variable globally, or locally to an entire function regardless of block scope. Variables declared by let have as their scope the block in which they are defined, as well as in any contained sub-blocks. In this way, let works very much like var. The main difference is that the scope of a var variable is the entire enclosing function: At the top level of programs and functions, let, unlike var, does not create a property on the global object. For example: When used inside a block, let limits the variable's scope to that block. Note the difference between var whose scope is inside the function where it is declared. Also don't forget it's ECMA6 feature, so it's not fully supported yet, so it's better always transpiles it to ECMA5 using Babel etc... for more info about visit babel website Here is an example for the difference between the two (support just started for chrome):  As you can see the var j variable is still having a value outside of the for loop scope (Block Scope), but the let i variable is undefined outside of the for loop scope.   "use strict"; console.log("var:"); for (var j = 0; j < 2; j++) {   console.log(j); }  console.log(j);  console.log("let:"); for (let i = 0; i < 2; i++) {   console.log(i); }  console.log(i);    There are some subtle differences — let scoping behaves more like variable scoping does in more or less any other languages.  e.g. It scopes to the enclosing block, They don't exist before they're declared, etc. However it's worth noting that let is only a part of newer Javascript implementations and has varying degrees of browser support. ⚡️ Sandbox to play around ↓  Variable Not Hoisting let will not hoist to the entire scope of the block they appear in. By contrast, var could hoist as below. Actually, Per @Bergi, Both var and let are hoisted. Garbage Collection Block scope of let is useful relates to closures and garbage collection to reclaim memory. Consider, The click handler callback does not need the hugeData variable at all. Theoretically, after process(..) runs, the huge data structure hugeData could be garbage collected. However, it's possible that some JS engine will still have to keep this huge structure, since the click function has a closure over the entire scope. However, the block scope can make this huge data structure to garbage collected. let loops let in the loop can re-binds it to each iteration of the loop, making sure to re-assign it the value from the end of the previous loop iteration. Consider, However, replace var with let Because let create a new lexical environment with those names for a) the initialiser expression b) each iteration (previosly to evaluating the increment expression), more details are here. The difference is in the scope of the variables declared with each. In practice, there are a number of useful consequences of the difference in scope: The restrictions imposed by let reduce the visibility of the variables and increase the likelihood that unexpected name collisions will be found early.  This makes it easier to track and reason about variables, including their reachability(helping with reclaiming unused memory). Consequently, let variables are less likely to cause problems when used in large programs or when independently-developed frameworks are combined in new and unexpected ways. var may still be useful if you are sure you want the single-binding effect when using a closure in a loop (#5) or for declaring externally-visible global variables in your code (#4).  Use of var for exports may be supplanted if export migrates out of transpiler space and into the core language. 1. No use outside nearest enclosing block: This block of code will throw a reference error because the second use of x occurs outside of the block where it is declared with let:  In contrast, the same example with var works. 2. No use before declaration: This block of code will throw a ReferenceError before the code can be run because x is used before it is declared:  In contrast, the same example with var parses and runs without throwing any exceptions. 3. No redeclaration: The following code demonstrates that a variable declared with let may not be redeclared later:  4. Globals not attached to window:  5. Easy use with closures: Variables declared with var do not work well with closures inside loops.  Here is a simple loop that outputs the sequence of values that the variable i has at different points in time:  Specifically, this outputs: In JavaScript we often use variables at a significantly later time than when they are created.  When we demonstrate this by delaying the output with a closure passed to setTimeout:  ... the output remains unchanged as long as we stick with let.  In contrast, if we had used var i instead:  ... the loop unexpectedly outputs "i is 5" five times: Here's an example to add on to what others have already written. Suppose you want to make an array of functions, adderFunctions, where each function takes a single Number argument and returns the sum of the argument and the function's index in the array. Trying to generate adderFunctions with a loop using the var keyword won't work the way someone might naïvely expect: The process above doesn't generate the desired array of functions because i's scope extends beyond the iteration of the for block in which each function was created. Instead, at the end of the loop, the i in each function's closure refers to i's value at the end of the loop (1000) for every anonymous function in adderFunctions. This isn't what we wanted at all: we now have an array of 1000 different functions in memory with exactly the same behavior. And if we subsequently update the value of i, the mutation will affect all the adderFunctions. However, we can try again using the let keyword: This time, i is rebound on each iteration of the for loop. Each function now keeps the value of i at the time of the function's creation, and adderFunctions behaves as expected. Now, image mixing the two behaviors and you'll probably see why it's not recommended to mix the newer let and const with the older var in the same script. Doing so can result is some spectacularly confusing code. Don't let this happen to you. Use a linter. NOTE: This is a teaching example intended to demonstrate the var/let behavior in loops and with function closures that would also be easy to understand. This would be a terrible way to add numbers. But the general technique of capturing data in anonymous function closures might be encountered in the real world in other contexts. YMMV. May the following two functions show the difference: let is interesting, because it allows us to do something like this: Which results in counting [0, 7]. Whereas Only counts [0, 1]. The main difference between var and let is that variables declared with var are function scoped. Whereas functions declared with let are block scoped. For example: variables with var: When the first function testVar gets called the variable foo, declared with var, is still accessible outside the if statement. This variable foo would be available everywhere within the scope of the testVar function. variables with let: When the second function testLet gets called the variable bar, declared with let, is only accessible inside the if statement. Because variables declared with let are block scoped (where a block is the code between curly brackets e.g if{} , for{}, function{}).  Another difference between var and let is variables with declared with let don't get hoisted. An example is the best way to illustrate this behavior: variables with let don't get hoisted: variables with var do get hoisted: A variable declared with let in the global scope (which is code that is not in a function) doesn't get added as a property on the global window object. For example (this code is in global scope):  When should let be used over var? Use let over var whenever you can because it is simply scoped more specific. This reduces potential naming conflicts which can occur when dealing with a large number of variables. var can be used when you want a global variable explicitly to be on the window object (always consider carefully if this is really necessary).  It also appears that, at least in Visual Studio 2015, TypeScript 1.5, "var" allows multiple declarations of the same variable name in a block, and "let" doesn't. This won't generate a compile error: This will: var is global scope (hoist-able) variable. let and const is block scope. test.js   {     let l = 'let';     const c = 'const';     var v = 'var';     v2 = 'var 2'; }  console.log(v, this.v); console.log(v2, this.v2); console.log(l); // ReferenceError: l is not defined console.log(c); // ReferenceError: c is not defined    When Using let The let keyword attaches the variable declaration to the scope of whatever block (commonly a { .. } pair) it's contained in. In other words,let implicitly hijacks any block's scope for its variable declaration. let variables cannot be accessed in the window object because they cannot be globally accessed. When Using var var and variables in ES5 has scopes in functions meaning the variables are valid within the function and not outside the function itself. var variables can be accessed in the window object because they cannot be globally accessed. If you want to know more continue reading below one of the most famous interview questions on scope also can suffice the exact use of let and var as below; When using let This is because when using let, for every loop iteration the variable is scoped and has its own copy. When using var This is because when using var, for every loop iteration the variable is scoped and has shared copy. If I read the specs right then let thankfully can also be leveraged to avoid self invoking functions used to simulate private only members - a popular design pattern that decreases code readability, complicates debugging, that adds no real code protection or other benefit - except maybe satisfying someone's desire for semantics, so stop using it. /rant See 'Emulating private interfaces' Some hacks with let: 1. 2. 3. ES6 introduced two new keyword(let and const) alternate to var. When you need a block level deceleration you can go with let and const   instead of var. The below table summarize the difference between var, let and const  The below shows how 'let' and 'var' are different in the scope: The gfoo, defined by let initially is in the global scope, and when we declare gfoo again inside the if clause its scope changed and when a new value is assigned to the variable inside that scope it does not affect the global scope. Whereas hfoo, defined by var is initially in the global scope, but again when we declare it inside the if clause, it considers the global scope hfoo, although var has been used again to declare it. And when we re-assign its value we see that the global scope hfoo is also affected. This is the primary difference. let is a part of es6. These functions will explain the difference in easy way. let vs var. It's all about scope.  var variables are global and can be accessed basically everywhere, while let variables are not global and only exist until a closing parenthesis kills them.  See my example below, and note how the lion (let) variable acts differently in the two console.logs; it becomes out of scope in the 2nd console.log. As mentioned above: The difference is scoping. var is scoped to the nearest function   block and let is scoped to the nearest enclosing block, which   can be smaller than a function block. Both are global if outside any   block.Lets see an example: Example1: In my both examples I have a function myfunc. myfunc contains a variable myvar equals to 10.  In my first example  I check   if myvar equals to 10 (myvar==10) . If yes, I agian declare  a variable  myvar (now I have two myvar variables)using var keyword and assign it a new value (20). In next line I  print its value on my console.  After the conditional block I again print the value of myvar on my console. If you look at the output of myfunc,   myvar has value equals to 20.   Example2: In my second example  instead of using var keyword in my conditional block I declare myvar using let keyword . Now when I call myfunc  I get two different outputs: myvar=20 and myvar=10. So the difference is very simple i.e its scope.  Now I think there is better scoping of variables to a block of statements using let: I think people will start using let here after so that they will have similar scoping in JavaScript like other languages, Java, C#, etc. People with not a clear understanding about scoping in JavaScript used to make the mistake earlier. Hoisting is not supported using let. With this approach errors present in JavaScript are getting removed.  Refer to ES6 In Depth: let and const to understand it better. This article clearly defines the difference between var, let and const const is a signal that the identifier won’t be reassigned. let, is a signal that the variable may be reassigned, such as a   counter in a loop, or a value swap in an algorithm. It also signals   that the variable will be used only in the block it’s defined in,   which is not always the entire containing function. var is now the weakest signal available when you define a variable   in JavaScript. The variable may or may not be reassigned, and the   variable may or may not be used for an entire function, or just for   the purpose of a block or loop. https://medium.com/javascript-scene/javascript-es6-var-let-or-const-ba58b8dcde75#.esmkpbg9b I want to link these keywords to the Execution Context, because the Execution Context is important in all of this. The Execution Context has two phases: a Creation Phase and Execution Phase. In addition, each Execution Context has a Variable Environment and Outer Environment (its Lexical Environment).  During the Creation Phase of an Execution Context, var, let and const will still store its variable in memory with an undefined value in the Variable Environment of the given Execution Context. The difference is in the Execution Phase. If you use reference a variable defined with var before it is assigned a value, it will just be undefined. No exception will be raised.  However, you cannot reference the variable declared with let or const until it is declared. If you try to use it before it is declared, then an exception will be raised during the Execution Phase of the Execution Context. Now the variable will still be in memory, courtesy of the Creation Phase of the Execution Context, but the Engine will not allow you to use it: With a variable defined with var, if the Engine cannot find the variable in the current Execution Context's Variable Environment, then it will go up the scope chain (the Outer Environment) and check the Outer Environment's Variable Environment for the variable. If it cannot find it there, it will continue searching the Scope Chain. This is not the case with let and const.  The second feature of let is it introduces block scope. Blocks are defined by curly braces. Examples include function blocks, if blocks, for blocks, etc. When you declare a variable with let inside of a block, the variable is only available inside of the block. In fact, each time the block is run, such as within a for loop, it will create a new variable in memory. ES6 also introduces the const keyword for declaring variables. const is also block scoped. The difference between let and const is that const variables need to be declared using an initializer, or it will generate an error. And, finally, when it comes to the Execution Context, variables defined with var will be attached to the 'this' object. In the global Execution Context, that will be the window object in browsers. This is not the case for let or const. I think the terms and most of the examples are a bit overwhelming, The main issue i had personally with the difference is understanding what a "Block" is. At some point i realized, a block would be any curly brackets except for IF statement. an opening bracket { of a function or loop will define a new block, anything defined with let within it, will not be available after the closing bracket } of the same thing (function or loop); With that in mind, it was easier to understand:   let msg = "Hello World";  function doWork() { // msg will be available since it was defined above this opening bracket!   let friends = 0;   console.log(msg);    // with VAR though:   for (var iCount2 = 0; iCount2 < 5; iCount2++) {} // iCount2 will be available after this closing bracket!   console.log(iCount2);        for (let iCount1 = 0; iCount1 < 5; iCount1++) {} // iCount1 will not be available behind this closing bracket, it will return undefined   console.log(iCount1);    } // friends will no be available after this closing bracket! doWork(); console.log(friends);   
__label__foreach __label__arrays __label__javascript __label__iteration __label__loops How can I loop through all the entries in an array using JavaScript? I thought it was something like this: Where theArray is my array, but this seems to be incorrect. TL;DR Your best bets are usually But there's lots more to explore, read on... JavaScript has powerful semantics for looping through arrays and array-like objects. I've split the answer into two parts: Options for genuine arrays, and options for things that are just array-like, such as the arguments object, other iterable objects (ES2015+), DOM collections, and so on. I'll quickly note that you can use the ES2015 options now, even on ES5 engines, by transpiling ES2015 to ES5. Search for "ES2015 transpiling" / "ES6 transpiling" for more... Okay, let's look at our options: You have three options in ECMAScript 5 ("ES5"), the version most broadly supported at the moment, and two more added in ECMAScript 2015 ("ES2015", "ES6"): Details: In any vaguely-modern environment (so, not IE8) where you have access to the Array features added by ES5 (directly or using polyfills), you can use forEach (spec | MDN):   var a = ["a", "b", "c"]; a.forEach(function(entry) {     console.log(entry); });    forEach accepts a callback function and, optionally, a value to use as this when calling that callback (not used above). The callback is called for each entry in the array, in order, skipping non-existent entries in sparse arrays. Although I only used one argument above, the callback is called with three: The value of each entry, the index of that entry, and a reference to the array you're iterating over (in case your function doesn't already have it handy). Unless you're supporting obsolete browsers like IE8 (which NetApps shows at just over 4% market share as of this writing in September 2016), you can happily use forEach in a general-purpose web page without a shim. If you do need to support obsolete browsers, shimming/polyfilling forEach is easily done (search for "es5 shim" for several options). forEach has the benefit that you don't have to declare indexing and value variables in the containing scope, as they're supplied as arguments to the iteration function, and so nicely scoped to just that iteration. If you're worried about the runtime cost of making a function call for each array entry, don't be; details. Additionally, forEach is the "loop through them all" function, but ES5 defined several other useful "work your way through the array and do things" functions, including: Sometimes the old ways are the best:   var index; var a = ["a", "b", "c"]; for (index = 0; index < a.length; ++index) {     console.log(a[index]); }    If the length of the array won't change during the loop, and it's in performance-sensitive code (unlikely), a slightly more complicated version grabbing the length up front might be a tiny bit faster:   var index, len; var a = ["a", "b", "c"]; for (index = 0, len = a.length; index < len; ++index) {     console.log(a[index]); }    And/or counting backward:   var index; var a = ["a", "b", "c"]; for (index = a.length - 1; index >= 0; --index) {     console.log(a[index]); }    But with modern JavaScript engines, it's rare you need to eke out that last bit of juice. In ES2015 and higher, you can make your index and value variables local to the for loop:   let a = ["a", "b", "c"]; for (let index = 0; index < a.length; ++index) {     let value = a[index];     console.log(index, value); } try {     console.log(index); } catch (e) {     console.error(e);   // "ReferenceError: index is not defined" } try {     console.log(value); } catch (e) {     console.error(e);   // "ReferenceError: value is not defined" }    And when you do that, not just value but also index is recreated for each loop iteration, meaning closures created in the loop body keep a reference to the index (and value) created for that specific iteration:   let divs = document.querySelectorAll("div"); for (let index = 0; index < divs.length; ++index) {     divs[index].addEventListener('click', e => {         console.log("Index is: " + index);     }); } <div>zero</div> <div>one</div> <div>two</div> <div>three</div> <div>four</div>    If you had five divs, you'd get "Index is: 0" if you clicked the first and "Index is: 4" if you clicked the last. This does not work if you use var instead of let. You'll get people telling you to use for-in, but that's not what for-in is for. for-in loops through the enumerable properties of an object, not the indexes of an array. The order is not guaranteed, not even in ES2015 (ES6). ES2015+ does define an order to object properties (via [[OwnPropertyKeys]], [[Enumerate]], and things that use them like Object.getOwnPropertyKeys), but it didn't define that for-in would follow that order; ES2020 did, though. (Details in this other answer.) The only real use cases for for-in on an array are: Looking only at that first example: You can use for-in to visit those sparse array elements if you use appropriate safeguards:   // `a` is a sparse array var key; var a = []; a[0] = "a"; a[10] = "b"; a[10000] = "c"; for (key in a) {     if (a.hasOwnProperty(key)  &&        // These checks are         /^0$|^[1-9]\d*$/.test(key) &&    // explained         key <= 4294967294                // below         ) {         console.log(a[key]);     } }    Note the three checks: That the object has its own property by that name (not one it inherits from its prototype), and That the key is all decimal digits (e.g., normal string form, not scientific notation), and That the key's value when coerced to a number is <= 2^32 - 2 (which is 4,294,967,294). Where does that number come from? It's part of the definition of an array index in the specification. Other numbers (non-integers, negative numbers, numbers greater than 2^32 - 2) are not array indexes. The reason it's 2^32 - 2 is that that makes the greatest index value one lower than 2^32 - 1, which is the maximum value an array's length can have. (E.g., an array's length fits in a 32-bit unsigned integer.) (Props to RobG for pointing out in a comment on my blog post that my previous test wasn't quite right.) You wouldn't do that in inline code, of course. You'd write a utility function. Perhaps:   // Utility function for antiquated environments without `forEach` var hasOwn = Object.prototype.hasOwnProperty; var rexNum = /^0$|^[1-9]\d*$/; function sparseEach(array, callback, thisArg) {     var index;     for (var key in array) {         index = +key;         if (hasOwn.call(a, key) &&             rexNum.test(key) &&             index <= 4294967294             ) {             callback.call(thisArg, array[key], index, array);         }     } }  var a = []; a[5] = "five"; a[10] = "ten"; a[100000] = "one hundred thousand"; a.b = "bee";  sparseEach(a, function(value, index) {     console.log("Value at " + index + " is " + value); });    ES2015 added iterators to JavaScript. The easiest way to use iterators is the new for-of statement. It looks like this:   const a = ["a", "b", "c"]; for (const val of a) {     console.log(val); }    Under the covers, that gets an iterator from the array and loops through it, getting the values from it. This doesn't have the issue that using for-in has, because it uses an iterator defined by the object (the array), and arrays define that their iterators iterate through their entries (not their properties). Unlike for-in in ES5, the order in which the entries are visited is the numeric order of their indexes. Sometimes, you might want to use an iterator explicitly. You can do that, too, although it's a lot clunkier than for-of. It looks like this:   const a = ["a", "b", "c"]; const it = a.values(); let entry; while (!(entry = it.next()).done) {     console.log(entry.value); }    The iterator is an object matching the Iterator definition in the specification. Its next method returns a new result object each time you call it. The result object has a property, done, telling us whether it's done, and a property value with the value for that iteration. (done is optional if it would be false, value is optional if it would be undefined.) The meaning of value varies depending on the iterator; arrays support (at least) three functions that return iterators: Aside from true arrays, there are also array-like objects that have a length property and properties with numeric names: NodeList instances, the arguments object, etc. How do we loop through their contents? At least some, and possibly most or even all, of the array approaches above frequently apply equally well to array-like objects: Use forEach and related (ES5+) The various functions on Array.prototype are "intentionally generic" and can usually be used on array-like objects via Function#call or Function#apply. (See the Caveat for host-provided objects at the end of this answer, but it's a rare issue.) Suppose you wanted to use forEach on a Node's childNodes property. You'd do this: If you're going to do that a lot, you might want to grab a copy of the function reference into a variable for reuse, e.g.: Use a simple for loop Obviously, a simple for loop applies to array-like objects. Use for-in correctly for-in with the same safeguards as with an array should work with array-like objects as well; the caveat for host-provided objects on #1 above may apply. Use for-of (use an iterator implicitly) (ES2015+) for-of uses the iterator provided by the object (if any). That includes host-provided objects. For instance, the specification for the NodeList from querySelectorAll was updated to support iteration. The spec for the HTMLCollection from getElementsByTagName was not. Use an iterator explicitly (ES2015+) See #4. Other times, you may want to convert an array-like object into a true array. Doing that is surprisingly easy: Use the slice method of arrays We can use the slice method of arrays, which like the other methods mentioned above is "intentionally generic" and so can be used with array-like objects, like this: So for instance, if we want to convert a NodeList into a true array, we could do this: See the Caveat for host-provided objects below. In particular, note that this will fail in IE8 and earlier, which don't let you use host-provided objects as this like that. Use spread syntax (...) It's also possible to use ES2015's spread syntax with JavaScript engines that support this feature. Like for-of, this uses the iterator provided by the object (see #4 in the previous section): So for instance, if we want to convert a NodeList into a true array, with spread syntax this becomes quite succinct: Use Array.from Array.from (spec) | (MDN) (ES2015+, but easily polyfilled) creates an array from an array-like object, optionally passing the entries through a mapping function first. So: Or if you wanted to get an array of the tag names of the elements with a given class, you'd use the mapping function: If you use Array.prototype functions with host-provided array-like objects (DOM lists and other things provided by the browser rather than the JavaScript engine), you need to be sure to test in your target environments to make sure the host-provided object behaves properly. Most do behave properly (now), but it's important to test. The reason is that most of the Array.prototype methods you're likely to want to use rely on the host-provided object giving an honest answer to the abstract [[HasProperty]] operation. As of this writing, browsers do a very good job of this, but the 5.1 spec did allow for the possibility a host-provided object may not be honest. It's in §8.6.2, several paragraphs below the big table near the beginning of that section), where it says: Host objects may implement these internal methods in any manner unless specified otherwise; for example, one possibility is that [[Get]] and [[Put]] for a particular host object indeed fetch and store property values but [[HasProperty]] always generates false. (I couldn't find the equivalent verbiage in the ES2015 spec, but it's bound to still be the case.) Again, as of this writing the common host-provided array-like objects in modern browsers [NodeList instances, for instance] do handle [[HasProperty]] correctly, but it's important to test.) Note: This answer is hopelessly out-of-date. For a more modern approach, look at the methods available on an array. Methods of interest might be: The standard way to iterate an array in JavaScript is a vanilla for-loop: Note, however, that this approach is only good if you have a dense array, and each index is occupied by an element. If the array is sparse, then you can run into performance problems with this approach, since you will iterate over a lot of indices that do not really exist in the array. In this case, a for .. in-loop might be a better idea. However, you must use the appropriate safeguards to ensure that only the desired properties of the array (that is, the array elements) are acted upon, since the for..in-loop will also be enumerated in legacy browsers, or if the additional properties are defined as enumerable. In ECMAScript 5 there will be a forEach method on the array prototype, but it is not supported in legacy browsers. So to be able to use it consistently you must either have an environment that supports it (for example, Node.js for server side JavaScript), or use a "Polyfill". The Polyfill for this functionality is, however, trivial and since it makes the code easier to read, it is a good polyfill to include. If you’re using the jQuery library, you can use jQuery.each: EDIT :  As per question, user want code in javascript instead of jquery so the edit is I think the reverse for loop deserves a mention here: Some developers use the reverse for loop by default, unless there is a good reason to loop forwards. Although the performance gains are usually insignificant, it sort of screams: "Just do this to every item in the list, I don't care about the order!" However in practice that is not actually a reliable indication of intent, since it is indistinguishable from those occasions when you do care about the order, and really do need to loop in reverse.  So in fact another construct would be needed to accurately express the "don't care" intent, something currently unavailable in most languages, including ECMAScript, but which could be called, for example, forEachUnordered(). If order doesn't matter, and efficiency is a concern (in the innermost loop of a game or animation engine), then it may be acceptable to use the reverse for loop as your go-to pattern.  Just remember that seeing a reverse for loop in existing code does not necessarily mean that the order irrelevant! In general for higher level code where clarity and safety are greater concerns, I previously recommended using Array::forEach as your default pattern for looping (although these days I prefer to use for..of).  Reasons to prefer forEach over a reverse loop are: Then when you do see the reverse for loop in your code, that is a hint that it is reversed for a good reason (perhaps one of the reasons described above).  And seeing a traditional forward for loop may indicate that shifting can take place. (If the discussion of intent makes no sense to you, then you and your code may benefit from watching Crockford's lecture on Programming Style & Your Brain.) There is a debate about whether for..of or forEach() are preferable: For maximum browser support, for..of requires a polyfill for iterators, making your app slightly slower to execute and slightly larger to download. For that reason (and to encourage use of map and filter), some front-end style guides ban for..of completely! But the above concerns is not applicable to Node.js applications, where for..of is now well supported. And furthermore await does not work inside forEach().  Using for..of is the clearest pattern in this case. Personally, I tend to use whatever looks easiest to read, unless performance or minification has become a major concern.  So these days I prefer to use for..of instead of forEach(), but I will always use map or filter or find or some when applicable.   (For the sake of my colleagues, I rarely use reduce.) You will notice that i-- is the middle clause (where we usually see a comparison) and the last clause is empty (where we usually see i++).  That means that i-- is also used as the condition for continuation.  Crucially, it is executed and checked before each iteration. How can it start at array.length without exploding? Because i-- runs before each iteration, on the first iteration we will actually be accessing the item at array.length - 1 which avoids any issues with Array-out-of-bounds undefined items. Why doesn't it stop iterating before index 0? The loop will stop iterating when the condition i-- evaluates to a falsey value (when it yields 0). The trick is that unlike --i, the trailing i-- operator decrements i but yields the value before the decrement.  Your console can demonstrate this: > var i = 5; [i, i--, i]; [5, 5, 4] So on the final iteration, i was previously 1 and the i-- expression changes it to 0 but actually yields 1 (truthy), and so the condition passes.  On the next iteration i-- changes i to -1 but yields 0 (falsey), causing execution to immediately drop out of the bottom of the loop. In the traditional forwards for loop, i++ and ++i are interchangeable (as Douglas Crockford points out).  However in the reverse for loop, because our decrement is also our condition expression, we must stick with i-- if we want to process the item at index 0. Some people like to draw a little arrow in the reverse for loop, and end with a wink: Credits go to WYL for showing me the benefits and horrors of the reverse for loop. Some C-style languages use foreach to loop through enumerations. In JavaScript this is done with the for..in loop structure: There is a catch. for..in will loop through each of the object's enumerable members, and the members on its prototype. To avoid reading values that are inherited through the object's prototype, simply check if the property belongs to the object: Additionally, ECMAScript 5 has added a forEach method to Array.prototype which can be used to enumerate over an array using a calback (the polyfill is in the docs so you can still use it for older browsers): It's important to note that Array.prototype.forEach doesn't break when the callback returns false. jQuery and Underscore.js provide their own variations on each to provide loops that can be short-circuited. If you want to loop over an array, use the standard three-part for loop. You can get some performance optimisations by caching myArray.length or iterating over it backwards. If you don't mind emptying the array: x will contain the last value of y and it will be removed from the array. You can also use shift() which will give and remove the first item from y. I know this is an old post, and there are so many great answers already. For a little more completeness I figured I'd throw in another one using AngularJS. Of course, this only applies if you're using Angular, obviously, nonetheless I'd like to put it anyway. angular.forEach takes 2 arguments and an optional third argument. The first argument is the object (array) to iterate over, the second argument is the iterator function, and the optional third argument is the object context (basically referred to inside the loop as 'this'. There are different ways to use the forEach loop of angular. The simplest and probably most used is Another way that is useful for copying items from one array to another is Though, you don't have to do that, you can simply do the following and it's equivalent to the previous example: Now there are pros and cons of using the angular.forEach function as opposed to the built in vanilla-flavored for loop. Pros Consider the following 2 nested loops, which do exactly the same thing. Let's say that we have 2 arrays of objects and each object contains an array of results, each of which has a Value property that's a string (or whatever). And let's say we need to iterate over each of the results and if they're equal then perform some action:  Granted this is a very simple hypothetical example, but I've written triple embedded for loops using the second approach and it was very hard to read, and write for that matter. Cons I'm sure there's various other pros and cons as well, and please feel free to add any that you see fit. I feel that, bottom line, if you need efficiency, stick with just the native for loop for your looping needs. But, if your datasets are smaller and a some efficiency is okay to give up in exchange for readability and writability, then by all means throw an angular.forEach in that bad boy. A forEach implementation (see in jsFiddle): As of ECMAScript 6:   list = [0, 1, 2, 3] for (let obj of list) {     console.log(obj) }    Where of avoids the oddities associated with in and makes it work like the for loop of any other language, and let binds i within the loop as opposed to within the function. The braces ({}) can be omitted when there is only one command (e.g. in the example above). Probably the for(i = 0; i < array.length; i++) loop is not the best choice. Why? If you have this: The method will call from array[0] to array[2]. First, this will first reference variables you don't even have, second you would not have the variables in the array, and third this will make the code bolder. Look here, it's what I use: And if you want it to be a function, you can do this: If you want to break, a little more logic: Example: It returns: There are three implementations of foreach in jQuery as follows. An easy solution now would be to use the underscore.js library. It's providing many useful tools, such as each and will automatically delegate the job to the native forEach if available. A CodePen example of how it works is: There isn't any for each loop in native JavaScript. You can either use libraries to get this functionality (I recommend Underscore.js), use a simple for in loop. However, note that there may be reasons to use an even simpler for loop (see Stack Overflow question Why is using “for…in” with array iteration such a bad idea?) This is an iterator for NON-sparse list where the index starts at 0, which is the typical scenario when dealing with document.getElementsByTagName or document.querySelectorAll) Examples of usage: Example #1 Example #2 Each p tag gets class="blue" Example #3 Every other p tag gets class="red"> Example #4 And finally the first 20 blue p tags are changed to green Caution when using string as function: the function is created out-of-context and ought to be used only where you are certain of variable scoping.  Otherwise, better to pass functions where scoping is more intuitive. There are a few ways to loop through an array in JavaScript, as below: for - it's the most common one. Full block of code for looping    var languages = ["Java", "JavaScript", "C#", "Python"]; var i, len, text; for (i = 0, len = languages.length, text = ""; i < len; i++) {     text += languages[i] + "<br>"; } document.getElementById("example").innerHTML = text; <p id="example"></p>    while - loop while a condition is through. It seems to be the fastest loop    var text = ""; var i = 0; while (i < 10) {     text +=  i + ") something<br>";     i++; } document.getElementById("example").innerHTML = text; <p id="example"></p>    do/while - also loop through a block of code while the condition is true, will run at least one time    var text = "" var i = 0;  do {     text += i + ") something <br>";     i++; } while (i < 10);  document.getElementById("example").innerHTML = text; <p id="example"></p>    Functional loops - forEach, map, filter, also reduce (they loop through the function, but they are used if you need to do something with your array, etc.   // For example, in this case we loop through the number and double them up using the map function var numbers = [65, 44, 12, 4]; document.getElementById("example").innerHTML = numbers.map(function(num){return num * 2}); <p id="example"></p>    For more information and examples about functional programming on arrays, look at the blog post Functional programming in JavaScript: map, filter and reduce. ECMAScript 5 (the version on JavaScript) to work with Arrays: forEach - Iterates through every item in the array and do whatever you need with each item. In case, more interested on operation on array using some inbuilt feature. map - It creates a new array with the result of the callback function. This method is good to be used when you need to format the elements of your array. reduce - As the name says, it reduces the array to a single value by calling the given function passing in the current element and the result of the previous execution. every - Returns true or false if all the elements in the array pass the test in the callback function. filter - Very similar to every except that filter returns an array with the elements that return true to the given function. There's no inbuilt ability to break in forEach. To interrupt execution use the Array#some like below: This works because some returns true as soon as any of the callbacks, executed in array order, returns true, short-circuiting the execution of the rest.  Original Answer see Array prototype for some I also would like to add this as a composition of a reverse loop and an answer above for someone that would like this syntax too. Pros: The benefit for this: You have the reference already in the first like that won't need to be declared later with another line. It is handy when looping trough the object array. Cons: This will break whenever the reference is false - falsey (undefined, etc.). It can be used as an advantage though. However, it would make it a little bit harder to read. And also depending on the browser it can be "not" optimized to work faster than the original one. jQuery way using $.map: Using loops with ECMAScript 6  destructuring and the spread operator Destructuring and using of the spread operator have proven quite useful for newcomers to ECMAScript 6 as being more human-readable/aesthetic, although some JavaScript veterans might consider it messy. Juniors or some other people might find it useful. The following examples will use the for...of statement and the .forEach method. Examples 6, 7, and 8 can be used with any functional loops like .map, .filter, .reduce, .sort, .every, .some. For more information about these methods, check out the Array Object. Example 1: Normal for...of loop - no tricks here.   let arrSimple = ['a', 'b', 'c'];  for (let letter of arrSimple) {   console.log(letter); }    Example 2: Split words to characters   let arrFruits = ['apple', 'orange', 'banana'];  for (let [firstLetter, ...restOfTheWord] of arrFruits) {   // Create a shallow copy using the spread operator   let [lastLetter] = [...restOfTheWord].reverse();   console.log(firstLetter, lastLetter, restOfTheWord); }    Example 3: Looping with a key and value   // let arrSimple = ['a', 'b', 'c'];  // Instead of keeping an index in `i` as per example `for(let i = 0 ; i<arrSimple.length;i++)` // this example will use a multi-dimensional array of the following format type: // `arrWithIndex: [number, string][]`  let arrWithIndex = [   [0, 'a'],   [1, 'b'],   [2, 'c'], ];  // Same thing can be achieved using `.map` method // let arrWithIndex = arrSimple.map((i, idx) => [idx, i]);  // Same thing can be achieved using `Object.entries` // NOTE: `Object.entries` method doesn't work on Internet Explorer  unless it's polyfilled // let arrWithIndex = Object.entries(arrSimple);  for (let [key, value] of arrWithIndex) {   console.log(key, value); }    Example 4: Get object properties inline   let arrWithObjects = [{     name: 'Jon',     age: 32   },   {     name: 'Elise',     age: 33   } ];  for (let { name, age: aliasForAge } of arrWithObjects) {   console.log(name, aliasForAge); }    Example 5: Get deep object properties of what you need   let arrWithObjectsWithArr = [{     name: 'Jon',     age: 32,     tags: ['driver', 'chef', 'jogger']   },   {     name: 'Elise',     age: 33,     tags: ['best chef', 'singer', 'dancer']   } ];  for (let { name, tags: [firstItemFromTags, ...restOfTags] } of arrWithObjectsWithArr) {   console.log(name, firstItemFromTags, restOfTags); }    Example 6: Is Example 3 used with .forEach   let arrWithIndex = [   [0, 'a'],   [1, 'b'],   [2, 'c'], ];  // Not to be confused here, `forEachIndex` is the real index // `mappedIndex` was created by "another user", so you can't really trust it  arrWithIndex.forEach(([mappedIndex, item], forEachIndex) => {   console.log(forEachIndex, mappedIndex, item); });    Example 7: Is Example 4 used with .forEach   let arrWithObjects = [{     name: 'Jon',     age: 32   },   {     name: 'Elise',     age: 33   } ]; // NOTE: Destructuring objects while using shorthand functions // are required to be surrounded by parentheses arrWithObjects.forEach( ({ name, age: aliasForAge }) => {   console.log(name, aliasForAge) });    Example 8: Is Example 5 used with .forEach   let arrWithObjectsWithArr = [{     name: 'Jon',     age: 32,     tags: ['driver', 'chef', 'jogger']   },   {     name: 'Elise',     age: 33,     tags: ['best chef', 'singer', 'dancer']   } ];  arrWithObjectsWithArr.forEach(({   name,   tags: [firstItemFromTags, ...restOfTags] }) => {   console.log(name, firstItemFromTags, restOfTags); });    A way closest to your idea would be to use Array.forEach() which accepts a closure function which will be executed for each element of the array. Another viable way would be to use Array.map() which works in the same way, but it also takes all values that you return and returns them in a new array (essentially mapping each element to a new one), like this: The lambda syntax doesn't usually work in Internet Explorer 10  or below. I usually use the If you are a jQuery fan and already have a jQuery file running, you should reverse the positions of the index and value parameters You can call forEach like this: forEach will iterate over the array you provide and for each iteration it will have element which holds the value of that iteration. If you need index you can get the current index by passing the i as the second parameter in the callback function for forEach.  Foreach is basically a High Order Function, Which takes another function as its parameter.  Output: You can also iterate over an array like this: If you want to use forEach(), it will look like -  If you want to use for(), it will look like -    If you want to loop through an array of objects with the arrow function:   let arr = [{name:'john', age:50}, {name:'clark', age:19}, {name:'mohan', age:26}];  arr.forEach((person)=>{   console.log('I am ' + person.name + ' and I am ' + person.age + ' old'); })    Today (2019-12-18) I perform test on my macOS v10.13.6 (High Sierra), on Chrome v 79.0, Safari v13.0.4 and Firefox v71.0 (64 bit) - conclusions about optimisation (and micro-optimisation which usually is not worth to introduce it to code because the benefit is small, but code complexity grows). It looks like the traditional for i (Aa) is a good choice to write fast code on all browsers. The other solutions, like for-of (Ad), all in group C.... are usually 2 - 10 (and more) times slower than Aa, but for small arrays it is ok to use it - for the sake of increase code clarity. The loops with array length cached in n (Ab, Bb, Be) are sometimes faster, sometimes not. Probably compilers automatically detect this situation and introduce caching. The speed differences between the cached and no-cached versions (Aa, Ba, Bd) are about ~1%, so it looks like introduce n is a micro-optimisation. The i-- like solutions where the loop starts from the last array element (Ac, Bc) are usually ~30% slower than forward solutions - probably the reason is the way of CPU memory cache working - forward memory reading is more optimal for CPU caching). Is recommended to NOT USE such solutions. In tests we calculate the sum of array elements. I perform a test for small arrays (10 elements) and big arrays (1M elements) and divide them into three groups:   let arr = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]; //let arr = Array.from(Array(1000000), (x, i) => i%10);  function Aa(a, s=0) {   for(let i=0; i<a.length; i++) {     s += a[i];   }   console.log('Aa=', s); }  function Ab(a, s=0) {   let n = a.length;   for(let i=0; i<n; i++) {     s += a[i];   }   console.log('Ab=', s); }  function Ac(a, s=0) {   for(let i=a.length; i--;) {     s += a[i];   }   console.log('Ac=', s); }  function Ad(a, s=0) {   for(let x of a) {     s += x;   }   console.log('Ad=', s); }  function Ae(a, s=0) {   for(let i in a) if (a.hasOwnProperty(i)) {     s += a[i];   }   console.log('Ae=', s); }  function Ba(a, s=0) {   let i = -1;   while(++i < a.length) {     s+= a[i];   }   console.log('Ba=', s); }  function Bb(a, s=0) {   let i = -1;   let n = a.length;   while(++i < n) {     s+= a[i];   }   console.log('Bb=', s); }  function Bc(a, s=0) {   let i = a.length;   while(i--) {     s += a[i];   }   console.log('Bc=', s); }  function Bd(a, s=0) {   let i = 0;   do {     s+= a[i]   } while (++i < a.length);   console.log('Bd=', s); }  function Be(a, s=0) {   let i = 0;   let n = a.length;   do {     s += a[i]   } while (++i < n);   console.log('Be=', s); }  function Bf(a, s=0) {   const it = a.values();    let e;   while (!(e = it.next()).done) {      s+= e.value;    }   console.log('Bf=', s); }  function Ca(a, s=0) {   a.map(x => { s+=x });   console.log('Ca=', s); }  function Cb(a, s=0) {   a.forEach(x => { s+=x });   console.log('Cb=', s); }  function Cc(a, s=0) {   a.every(x => (s += x, 1));   console.log('Cc=', s); }  function Cd(a, s=0) {   a.filter(x => { s+=x });   console.log('Cd=',s); }  function Ce(a, s=0) {   a.reduce((z, c) => { s+=c }, 0);   console.log('Ce=', s); }  function Cf(a, s=0) {   a.reduceRight((z, c) => { s += c }, 0);   console.log('Cf=', s); }  function Cg(a, s=0) {   a.some(x => { s += x } );   console.log('Cg=', s); }  function Ch(a, s=0) {   Array.from(a, x=> s += x);   console.log('Cc=', s); }   Aa(arr); Ab(arr); Ac(arr); Ad(arr); Ae(arr);  Ba(arr); Bb(arr); Bc(arr); Bd(arr); Be(arr); Bf(arr);  Ca(arr); Cb(arr); Cc(arr); Cd(arr); Ce(arr); Cf(arr); Cg(arr); Ch(arr); <p style="color: red">This snippets only PRESENTS code used for benchmark - it not perform test itself</p>    Cross browser results Results for all tested browsers browsers** Array with 10 elements Results for Chrome. You can perform the test on your machine here.  Array with 1,000,000 elements Results for Chrome. You can perform the test on your machine here  If you have a massive array you should use iterators to gain some efficiency. Iterators are a property of certain JavaScript collections (like Map, Set, String, Array). Even, for..of uses iterator under-the-hood. Iterators improve efficiency by letting you consume the items in a list one at a time as if they were a stream. What makes an iterator special is how it traverses a collection. Other loops need to load the entire collection up front in order to iterate over it, whereas an iterator only needs to know the current position in the collection.              You access the current item by calling the iterator’s next method. The next method will return the value of the current item and a boolean to indicate when you have reached the end of the collection. The following is an example of creating an iterator from an array.                Transform your regular array to iterator using values() method like this:                      const myArr = [2,3,4]  let it = myArr.values();  console.log(it.next()); console.log(it.next()); console.log(it.next()); console.log(it.next());    You can also transform your regular array to iterator using Symbol.iterator like this:            const myArr = [2,3,4]  let it = myArr[Symbol.iterator]();  console.log(it.next()); console.log(it.next()); console.log(it.next()); console.log(it.next());    You can also transform your regular array to an iterator like this:             let myArr = [8, 10, 12];  function makeIterator(array) {     var nextIndex = 0;          return {        next: function() {            return nextIndex < array.length ?                {value: array[nextIndex++], done: false} :                {done: true};        }     }; };  var it = makeIterator(myArr);  console.log(it.next().value);   // {value: 8, done: false} console.log(it.next().value);   // {value: 10, done: false} console.log(it.next().value);   // {value: 12, done: false} console.log(it.next().value);   // {value: undefined, done: true}    NOTE:                 You can read more about iteration protocol here.   As per the new updated feature ECMAScript 6 (ES6) and ECMAScript 2015, you can use the following options with loops: for loops for...in loops Array.forEach() for...of loops while loops do...while loops When iterating over an array, we often want to accomplish one of the following goals: We want to iterate over the array and create a new array: Array.prototype.map  We want to iterate over the array and don't create a new array: Array.prototype.forEach  for..of loop In JavaScript, there are many ways of accomplishing both of these goals. However, some are more convenient than others. Below you can find some commonly used methods (the most convenient IMO) to accomplish array iteration in JavaScript. map() is a function located on Array.prototype which can transform every element of an array and then returns a new array. map() takes as an argument a callback function and works in the following manner:   let arr = [1, 2, 3, 4, 5];  let newArr = arr.map((element, index, array) => {   return element * 2; })  console.log(arr); console.log(newArr);    The callback which we have passed into map() as an argument gets executed for every element. Then an array gets returned which has the same length as the original array. In this new array element is transformed by the callback function passed in as an argument to map(). The distinct difference between map and another loop mechanism like forEach and a for..of loop is that map returns a new array and leaves the old array intact (except if you explicitly manipulate it with thinks like splice). Also, note that the map function's callback provides the index number of the current iteration as a second argument. Furthermore, does the third argument provide the array on which map was called? Sometimes these properties can be very useful. forEach is a function which is located on Array.prototype which takes a callback function as an argument. It then executes this callback function for every element in the array. In contrast to the map() function, the forEach function returns nothing (undefined). For example:   let arr = [1, 2, 3, 4, 5];  arr.forEach((element, index, array) => {    console.log(element * 2);    if (index === 4) {     console.log(array)   }   // index, and oldArray are provided as 2nd and 3th argument by the callback  })  console.log(arr);    Just like the map function, the forEach callback provides the index number of the current iteration as a second argument. Also, does the third argument provide the array on which forEach was called? The for..of loop loops through every element of an array (or any other iterable object). It works in the following manner:   let arr = [1, 2, 3, 4, 5];  for(let element of arr) {   console.log(element * 2); }    In the above example, element stands for an array element and arr is the array which we want to loop. Note that the name element is arbitrary, and we could have picked any other name like 'el' or something more declarative when this is applicable. Don't confuse the for..in loop with the for..of loop. for..in will loop through all enumerable properties of the array whereas the for..of loop will only loop through the array elements. For example:   let arr = [1, 2, 3, 4, 5];  arr.foo = 'foo';  for(let element of arr) {   console.log(element); }  for(let element in arr) {   console.log(element); }   
__label__sql __label__join __label__database __label__outer-join __label__inner-join Also how do LEFT JOIN, RIGHT JOIN and FULL JOIN fit in? Assuming you're joining on columns with no duplicates, which is a very common case: An inner join of A and B gives the result of A intersect B, i.e. the inner part of a Venn diagram intersection. An outer join of A and B gives the results of A union B, i.e. the outer parts of a Venn diagram union. Examples Suppose you have two tables, with a single column each, and data as follows: Note that (1,2) are unique to A, (3,4) are common, and (5,6) are unique to B. Inner join An inner join using either of the equivalent queries gives the intersection of the two tables, i.e. the two rows they have in common. Left outer join A left outer join will give all rows in A, plus any common rows in B. Right outer join A right outer join will give all rows in B, plus any common rows in A. Full outer join A full outer join will give you the union of A and B, i.e. all the rows in A and all the rows in B. If something in A doesn't have a corresponding datum in B, then the B portion is null, and vice versa. The Venn diagrams don't really do it for me. They don't show any distinction between a cross join and an inner join, for example, or more generally show any distinction between different types of join predicate or provide a framework for reasoning about how they will operate. There is no substitute for understanding the logical processing and it is relatively straightforward to grasp anyway. (NB: In practice the query optimiser may find more efficient ways of executing the query than the purely logical description above but the final result must be the same) I'll start off with an animated version of a full outer join. Further explanation follows.  Source Tables  First start with a CROSS JOIN (AKA Cartesian Product). This does not have an ON clause and simply returns every combination of rows from the two tables. SELECT A.Colour, B.Colour FROM A CROSS JOIN B  Inner and Outer joins have an "ON" clause predicate. SELECT A.Colour, B.Colour FROM A INNER JOIN B ON A.Colour = B.Colour The above is the classic equi join.   The inner join condition need not necessarily be an equality condition and it need not reference columns from both (or even either) of the tables. Evaluating A.Colour NOT IN ('Green','Blue') on each row of the cross join returns.  SELECT A.Colour, B.Colour FROM A INNER JOIN B ON 1 =1 The join condition evaluates to true for all rows in the cross join result so this is just the same as a cross join. I won't repeat the picture of the 16 rows again. Outer Joins are logically evaluated in the same way as inner joins except that if a row from the left table (for a left join) does not join with any rows from the right hand table at all it is preserved in the result with NULL values for the right hand columns.  This simply restricts the previous result to only return the rows where B.Colour IS NULL. In this particular case these will be the rows that were preserved as they had no match in the right hand table and the query returns the single red row not matched in table B. This is known as an anti semi join. It is important to select a column for the IS NULL test that is either not nullable or for which the join condition ensures that any NULL values will be excluded in order for this pattern to work correctly and avoid just bringing back rows which happen to have a NULL value for that column in addition to the un matched rows.  Right outer joins act similarly to left outer joins except they preserve non matching rows from the right table and null extend the left hand columns.  Full outer joins combine the behaviour of left and right joins and preserve the non matching rows from both the left and the right tables.  No rows in the cross join match the 1=0 predicate. All rows from both sides are preserved using normal outer join rules with NULL in the columns from the table on the other side.  With a minor amend to the preceding query one could simulate a UNION ALL of the two tables.  Note that the WHERE clause (if present) logically runs after the join. One common error is to perform a left outer join and then include a WHERE clause with a condition on the right table that ends up excluding the non matching rows. The above ends up performing the outer join...  ... And then the "Where" clause runs. NULL= 'Green' does not evaluate to true so the row preserved by the outer join ends up discarded (along with the blue one) effectively converting the join back to an inner one.  If the intention was to include only rows from B where Colour is Green and all rows from A regardless the correct syntax would be  See these examples run live at SQLFiddle.com. Joins are used to combine the data from two tables, with the result being a new, temporary table. Joins are performed based on something called a predicate, which specifies the condition to use in order to perform a join.  The difference between an inner join and an outer join is that an inner join will return only the rows that actually match based on the join predicate. For eg- Lets consider Employee and Location table:  Inner Join:- Inner join creates a new result table by combining column values of two tables (Employee and Location) based upon the join-predicate. The query compares each row of Employee with each row of Location to find all pairs of rows which satisfy the join-predicate. When the join-predicate is satisfied by matching non-NULL values, column values for each matched pair of rows of Employee and Location are combined into a result row. Here’s what the SQL for an inner join will look like: Now, here is what the result of running that SQL would look like:  Outer Join:- An outer join does not require each record in the two joined tables to have a matching record. The joined table retains each record—even if no other matching record exists. Outer joins subdivide further into left outer joins and right outer joins, depending on which table's rows are retained (left or right). Left Outer Join:- The result of a left outer join (or simply left join) for tables Employee and Location always contains all records of the "left" table (Employee), even if the join-condition does not find any matching record in the "right" table (Location). Here is what the SQL for a left outer join would look like, using the tables above: Now, here is what the result of running this SQL would look like:  Right Outer Join:- A right outer join (or right join) closely resembles a left outer join, except with the treatment of the tables reversed. Every row from the "right" table (Location) will appear in the joined table at least once. If no matching row from the "left" table (Employee) exists, NULL will appear in columns from Employee for those records that have no match in Location. This is what the SQL looks like: Using the tables above, we can show what the result set of a right outer join would look like:  Full Outer Joins:- Full Outer Join or Full Join is to retain the nonmatching information by including nonmatching rows in the results of a join, use a full outer join. It includes all rows from both tables, regardless of whether or not the other table has a matching value. Image Source MySQL 8.0 Reference Manual - Join Syntax Oracle Join operations Retrieve the matched rows only, that is, A intersect B.  Select all records from the first table, and any records in the second table that match the joined keys.  Select all records from the second table, and any records in the first table that match the joined keys.  Inner and outer joins SQL examples and the Join block SQL: JOINS In simple words: An inner join retrieve the matched rows only. Whereas an outer join retrieve the matched rows from one table and all rows in other table ....the result depends on which one you are using: Left: Matched rows in the right table and all rows in the left table Right: Matched rows in the left table and all rows in the right table or  Full: All rows in all tables. It doesn't matter if there is a match or not A inner join only shows rows if there is a matching record on the other (right) side of the join. A (left) outer join shows rows for each record on the left hand side, even if there are no matching rows on the other (right) side of the join. If there is no matching row, the columns for the other (right) side would show NULLs. Inner joins require that a record with a related ID exist in the joined table. Outer joins will return records for the left side even if nothing exists for the right side. For instance, you have an Orders and an OrderDetails table. They are related by an "OrderID". Orders OrderDetails The request will only return Orders that also have something in the OrderDetails table. If you change it to OUTER LEFT JOIN then it will return records from the Orders table even if they have no OrderDetails records. You can use this to find Orders that do not have any OrderDetails indicating a possible orphaned order by adding a where clause like WHERE OrderDetails.OrderID IS NULL. In simple words : Inner join -> Take ONLY common records from parent and child tables WHERE primary key of Parent table matches Foreign key in Child table. Left join -> pseudo code Right join : Exactly opposite of left join . Put name of table in LEFT JOIN at right side in Right join , you get same output as LEFT JOIN. Outer join : Show all records in Both tables No matter what. If records in Left table are not matching to right table based on Primary , Forieign key , use NULL value as result of join . Example :  Lets assume now for 2 tables 1.employees  , 2.phone_numbers_employees Here , employees table is Master table , phone_numbers_employees is child table(it contains emp_id as foreign key which connects employee.id so its child table.) Inner joins Take the records of 2 tables ONLY IF Primary key of employees table(its id) matches Foreign key of Child table phone_numbers_employees(emp_id). So query would be : Here take only matching rows on primary key = foreign key as explained above.Here non matching rows on primary key = foreign key are skipped as result of join. Left joins : Left join retains all rows of the left table, regardless of whether there is a row that matches on the right table. Outer joins : Diagramatically it looks like :  You use INNER JOIN to return all rows from both tables where there is a match. i.e. In the resulting table all the rows and columns will have values. In OUTER JOIN the resulting table may have empty columns. Outer join may be either LEFT or RIGHT. LEFT OUTER JOIN returns all the rows from the first table, even if there are no matches in the second table. RIGHT OUTER JOIN returns all the rows from the second table, even if there are no matches in the first table. INNER JOIN requires there is at least a match in comparing the two tables. For example, table A and table B which implies A ٨ B (A intersection B). LEFT OUTER JOIN and LEFT JOIN are the same. It gives all the records matching in both tables and all possibilities of the left table. Similarly, RIGHT OUTER JOIN and RIGHT JOIN are the same. It gives all the records matching in both tables and all possibilities of the right table. FULL JOIN is the combination of LEFT OUTER JOIN and RIGHT OUTER JOIN without duplication. The answer is in the meaning of each one, so in the results. Note :   In SQLite there is no RIGHT OUTER JOIN or FULL OUTER JOIN.   And also in MySQL there is no FULL OUTER JOIN. My answer is based on above Note. When you have two tables like these:  CROSS JOIN / OUTER JOIN : You can have all of those tables data with CROSS JOIN or just with , like this: INNER JOIN : When you want to add a filter to above results based on a relation like table1.id = table2.id you can use INNER JOIN: LEFT [OUTER] JOIN : When you want to have all rows of one of tables in the above result -with same relation- you can use LEFT JOIN: (For RIGHT JOIN just change place of tables) FULL OUTER JOIN : When you also want to have all rows of the other table in your results you can use FULL OUTER JOIN: Well, as your need you choose each one that covers your need ;). Inner join.  A join is combining the rows from two tables. An inner join attempts to match up the two tables based on the criteria you specify in the query, and only returns the rows that match. If a row from the first table in the join matches two rows in the second table, then two rows will be returned in the results. If there’s a row in the first table that doesn’t match a row in the second, it’s not returned; likewise, if there’s a row in the second table that doesn’t match a row in the first, it’s not returned. Outer Join.  A left join attempts to find match up the rows from the first table to rows in the second table. If it can’t find a match, it will return the columns from the first table and leave the columns from the second table blank (null). I don't see much details about performance and optimizer in the other answers. Sometimes it is good to know that only INNER JOIN is associative which means the optimizer has the most option to play with it. It can reorder the join order to make it faster keeping the same result. The optimizer can use the most join modes. Generally it is a good practice to try to use INNER JOIN instead of the different kind of joins. (Of course if it is possible considering the expected result set.) There are a couple of good examples and explanation here about this strange associative behavior:  For example: Having criticized the much-loved red-shaded Venn diagram, I thought it only fair to post my own attempt. Although @Martin Smith's answer is the best of this bunch by a long way, his only shows the key column from each table, whereas I think ideally non-key columns should also be shown. The best I could do in the half hour allowed, I still don't think it adequately shows that the nulls are there due to absence of key values in TableB or that OUTER JOIN is actually a union rather than a join:  The precise algorithm for INNER JOIN, LEFT/RIGHT OUTER JOIN are as following:  Note: the condition specified in ON clause could be anything, it is not required to use Primary Keys (and you don't need to always refer to Columns from both tables)! For example:   Note: Left Join = Left Outer Join, Right Join = Right Outer Join. Simplest Definitions Inner Join: Returns matched records from both tables. Full Outer Join: Returns matched and unmatched records from both tables with null for unmatched records from Both Tables. Left Outer Join: Returns matched and unmatched records only from table on Left Side. Right Outer Join: Returns matched and unmatched records only from table on Right Side. In-Short Matched + Left Unmatched + Right Unmatched = Full Outer Join Matched + Left Unmatched = Left Outer Join Matched + Right Unmatched = Right Outer Join Matched = Inner Join In Simple Terms, 1.INNER JOIN OR EQUI JOIN : Returns the resultset that matches only the condition in both the tables. 2.OUTER JOIN : Returns the resultset of all the values from both the tables even if there is condition match or not.  3.LEFT JOIN : Returns the resultset of all the values from left table and only rows that match the condition in right table. 4.RIGHT JOIN : Returns the resultset of all the values from right table and only rows that match the condition in left table. 5.FULL JOIN : Full Join and Full outer Join are same. Please see the answer by Martin Smith for a better illustations and explanations of the different joins, including and especially differences between FULL OUTER JOIN, RIGHT OUTER JOIN and LEFT OUTER JOIN. These two table form a basis for the representation of the JOINs below:   The result will be the Cartesian products of all combinations. No JOIN condition required:  INNER JOIN is the same as simply: JOIN  The result will be combinations that satisfies the required JOIN condition:  LEFT OUTER JOIN is the same as LEFT JOIN  The result will be everything from citizen even if there are no matches in postalcode. Again a JOIN condition is required:  All examples have been run on an Oracle 18c. They're available at dbfiddle.uk which is also where screenshots of tables came from. CROSS JOIN resulting in rows as The General Idea/INNER JOIN: Using CROSS JOIN to get the result of a LEFT OUTER JOIN requires tricks like adding in a NULL row. It's omitted. INNER JOIN becomes a cartesian products. It's the same as The General Idea/CROSS JOIN: This is where the inner join can really be seen as the cross join with results not matching the condition removed. Here none of the resulting rows are removed. Using INNER JOIN to get the result of a LEFT OUTER JOIN also requires tricks. It's omitted. LEFT JOIN results in rows as The General Idea/CROSS JOIN: LEFT JOIN results in rows as The General Idea/INNER JOIN: An image internet search on "sql join cross inner outer" will show a multitude of Venn diagrams. I used to have a printed copy of one on my desk. But there are issues with the representation. Venn diagram are excellent for set theory, where an element can be in one or both sets. But for databases, an element in one "set" seem, to me, to be a row in a table, and therefore not also present in any other tables. There is no such thing as one row present in multiple tables. A row is unique to the table. Self joins are a corner case where each element is in fact the same in both sets. But it's still not free of any of the issues below. The set A represents the set on the left (the citizen table) and the set B is the set on the right (the postalcode table) in below discussion. Every element in both sets are matched with every element in the other set, meaning we need A amount of every B elements and B amount of every A elements to properly represent this Cartesian product. Set theory isn't made for multiple identical elements in a set, so I find Venn diagrams to properly represent it impractical/impossible. It doesn't seem that UNION fits at all. The rows are distinct. The UNION is 7 rows in total. But they're incompatible for a common SQL results set. And this is not how a CROSS JOIN works at all:  Trying to represent it like this:  ..but now it just looks like an INTERSECTION, which it's certainly not. Furthermore there's no element in the INTERSECTION that is actually in any of the two distinct sets. However, it looks very much like the searchable results similar to this:  For reference one searchable result for CROSS JOINs can be seen at Tutorialgateway. The INTERSECTION, just like this one, is empty. The value of an element depends on the JOIN condition. It's possible to represent this under the condition that every row becomes unique to that condition. Meaning id=x is only true for one row. Once a row in table A (citizen) matches multiple rows in table B (postalcode) under the JOIN condition, the result has the same problems as the CROSS JOIN: The row needs to be represented multiple times, and the set theory isn't really made for that. Under the condition of uniqueness, the diagram could work though, but keep in mind that the JOIN condition determines the placement of an element in the diagram. Looking only at the values of the JOIN condition with the rest of the row just along for the ride:  This representation falls completely apart when using an INNER JOIN with a ON 1 = 1 condition making it into a CROSS JOIN. With a self-JOIN, the rows are in fact idential elements in both tables, but representing the tables as both A and B isn't very suitable. For example a common self-JOIN condition that makes an element in A to be matching a different element in B is ON A.parent = B.child, making the match from A to B on seperate elements. From the examples that would be a SQL like this:  Meaning Smith is the leader of both Green and Jensen. Again the troubles begin when one row has multiple matches to rows in the other table. This is further complicated because the OUTER JOIN can be though of as to match the empty set. But in set theory the union of any set C and an empty set, is always just C. The empty set adds nothing. The representation of this LEFT OUTER JOIN is usually just showing all of A to illustrate that rows in A are selected regardless of whether there is a match or not from B. The "matching elements" however has the same problems as the illustration above. They depend on the condition. And the empty set seems to have wandered over to A:  Finding all rows from a CROSS JOIN with Smith and postalcode on the Moon:  Now the Venn diagram isn't used to reflect the JOIN. It's used only for the WHERE clause:  ..and that makes sense. As explained an INNER JOIN is not really an INTERSECT. However INTERSECTs can be used on results of seperate queries. Here a Venn diagram makes sense, because the elements from the seperate queries are in fact rows that either belonging to just one of the results or both. Intersect will obviously only return results where the row is present in both queries. This SQL will result in the same row as the one above WHERE, and the Venn diagram will also be the same: An OUTER JOIN is not a UNION. However UNION work under the same conditions as INTERSECT, resulting in a return of all results combining both SELECTs: which is equivalent to: ..and gives the result:  Also here a Venn diagram makes sense:  An important note is that these only work when the structure of the results from the two SELECT's are the same, enabling a comparison or union. The results of these two will not enable that: ..trying to combine the results with UNION gives a For further interest read Say NO to Venn Diagrams When Explaining JOINs and sql joins as venn diagram. Both also cover EXCEPT. There is a lot of misinformation out there on this topic, including here on Stack Overflow. left join on (aka left outer join on) returns inner join on rows union all unmatched left table rows extended by nulls. right join (on aka  right outer join on) returns inner join on rows union all unmatched right table rows extended by nulls. full join on (aka full outer join on) returns inner join on rowsunion all unmatched left table rows extended by nulls union all unmatched right table rows extended by nulls. (SQL Standard 2006 SQL/Foundation 7.7 Syntax Rules 1, General Rules 1 b, 3 c & d, 5 b.) So don't outer join until you know what underlying inner join is involved. Find out what rows inner join returns. Inner join - An inner join using either of the equivalent queries gives the intersection of the two tables, i.e. the two rows they have in common. Left outer join - A left outer join will give all rows in A, plus any common rows in B. Full outer join - A full outer join will give you the union of A and B, i.e. All the rows in A and all the rows in B. If something in A doesn't have a corresponding datum in B, then the B portion is null, and vice versay 1.Inner Join: Also called as Join. It returns the rows present in both the Left table, and right table only if there is a match. Otherwise, it returns zero records. Example:  2.Full Outer Join: Also called as Full Join. It returns all the rows present in both the Left table, and right table. Example:  3.Left Outer join: Or simply called as Left Join. It returns all the rows present in the Left table and matching rows from the right table (if any). 4.Right Outer Join: Also called as Right Join. It returns matching rows from the left table (if any), and all the rows present in the Right table.  Advantages of Joins Examples Suppose you have two tables, with a single column each, and data as follows: Note that (1,2,7,8) are unique to A, (3,4) are common, and (5,6) are unique to B. The INNER JOIN keyword selects all rows from both the tables as long as the condition satisfies. This keyword will create the result-set by combining all rows from both the tables where the condition satisfies i.e value of the common field will be the same.  Result: This join returns all the rows of the table on the left side of the join and matching rows for the table on the right side of the join. The rows for which there is no matching row on the right side, the result-set will contain null. LEFT JOIN is also known as LEFT OUTER JOIN.  Result:  Result: FULL (OUTER) JOIN: FULL JOIN creates the result-set by combining the result of both LEFT JOIN and RIGHT JOIN. The result-set will contain all the rows from both the tables. The rows for which there is no matching, the result-set will contain NULL values.  Result: Consider below 2 tables: EMP Department Mostly written as just JOIN in sql queries. It returns only the matching records between the tables. As you see above, Jose is not printed from EMP in the output as it's dept_id 6 does not find a match in the Department table.  Similarly, HR and R&D rows are not printed from Department table as they didn't find a match in the Emp table.  So, INNER JOIN or just JOIN, returns only matching rows. This returns all records from the LEFT table and only matching records from the RIGHT table. So, if you observe the above output, all records from the LEFT table(Emp) are printed with just matching records from RIGHT table. HR and R&D rows are not printed from Department table as they didn't find a match in the Emp table on dept_id. So, LEFT JOIN returns ALL rows from Left table and only matching rows from RIGHT table.  Can also check DEMO here. The difference between inner join and outer join is as follow: Joins are more easily explained with an example:  To simulate persons and emails stored in separate tables, Table A and Table B are joined by Table_A.id = Table_B.name_id Inner Join  Only matched ids' rows are shown. Outer Joins  Matched ids and not matched rows for Table A are shown.  Matched ids and not matched rows for Table B are shown.  Matched ids and not matched rows from both Tables are shown. Note: Full outer join is not available on MySQL There are a lot of good answers here with very accurate relational algebra examples.  Here is a very simplified answer that might be helpful for amateur or novice coders with SQL coding dilemmas. Basically, more often than not, JOIN queries boil down to two cases: For a SELECT of a subset of A data:
__label__git __label__version-control __label__git-checkout I have made some changes to a file which has been committed a few times as part of a group of files, but now want to reset/revert the changes on it back to a previous version.  I have done a git log along with a git diff to find the revision I need, but just have no idea how to get the file back to its former state in the past. Assuming the hash of the commit you want is c5f567: The git checkout man page gives more information. If you want to revert to the commit before c5f567, append ~1 (where 1 is the number of commits you want to go back, it can be anything): As a side note, I've always been uncomfortable with this command because it's used for both ordinary things (changing between branches) and unusual, destructive things (discarding changes in the working directory). You can quickly review the changes made to a file using the diff command: Then to revert a specific file to that commit use the reset command: You may need to use the --hard option if you have local modifications. A good workflow for managaging waypoints is to use tags to cleanly mark points in your timeline. I can't quite understand your last sentence but what you may want is diverge a branch from a previous point in time. To do this, use the handy checkout command: You can then rebase that against your mainline when you are ready to merge those changes: You can use any reference to a git commit, including the SHA-1 if that's most convenient. The point is that the command looks like this: git checkout [commit-ref] -- [filename] That will reset foo to HEAD. You can also: for one revision back, etc. And to revert to last committed version, which is most frequently needed, you can use this simpler command. I had the same issue just now and I found this answer easiest to understand (commit-ref is the SHA value of the change in the log you want to go back to): This will put that old version in your working directory and from there you can commit it if you want. If you know how many commits you need to go back, you can use: This assumes that you're on the master branch, and the version you want is 5 commits back. I think I've found it....from http://www-cs-students.stanford.edu/~blynn/gitmagic/ch02.html Sometimes you just want to go back and forget about every change past a certain point because they're all wrong.  Start with: $ git log which shows you a list of recent commits, and their SHA1 hashes.  Next, type: $ git reset --hard SHA1_HASH to restore the state to a given commit and erase all newer commits from the record permanently. This worked for me: Then commit the change: You have to be careful when you say "rollback".  If you used to have one version of a file in commit $A, and then later made two changes in two separate commits $B and $C (so what you are seeing is the third iteration of the file), and if you say "I want to roll back to the first one", do you really mean it?   If  you want to get rid of the changes both the second and the third iteration, it is very simple: and then you commit the result.  The command asks "I want to check out the file from the state recorded by the commit $A". On the other hand, what you meant is to get rid of the change the second iteration (i.e. commit $B) brought in, while keeping what commit $C did to the file, you would want to revert $B Note that whoever created commit $B may not have been very disciplined and may have committed totally unrelated change in the same commit, and this revert may touch files other than file you see offending changes, so you may want to check the result carefully after doing so. As of git v2.23.0 there's a new git restore method which is supposed to assume part of what git checkout was responsible for (even the accepted answer mentions that git checkout is quite confusing). See highlights of changes on github blog. The default behaviour of this command is to restore the state of a working tree with the content coming from the source parameter (which in your case will be a commit hash). So based on Greg Hewgill's answer (assuming the commit hash is c5f567) the command would look like this: Or if you want to restore to the content of one commit before c5f567: Amusingly, git checkout foo will not work if the working copy is in a directory named foo; however, both git checkout HEAD foo and git checkout ./foo will: Here's how rebase works: Assume you have The first two commands ... commit     git checkout      git rebase master ... check out the branch of changes you want to apply to the master branch. The rebase command takes the commits from <my branch> (that are not found in master) and reapplies them to the head of master. In other words, the parent of the first commit in <my branch> is no longer a previous commit in the master history, but the current head of master. The two commands are the same as: It might be easier to remember this command as both the "base" and "modify" branches are explicit. . The final history result is: The final two commands ... ... do a fast-forward merge to apply all <my branch> changes onto master. Without this step, the rebase commit does not get added to master. The final result is: master and <my branch> both reference B'. Also, from this point it is safe to delete the <my branch> reference. First Reset Head For Target File Second Checkout That File git-aliases, awk and shell-functions to the rescue! where <N> is the number of revisions of the file to rollback for file <filename>. For example, to checkout the immediate previous revision of a single file x/y/z.c, run Add the following to your gitconfig The command basically Essentially, all that one would manually do in this situation, wrapped-up in one beautiful, efficient git-alias - git-prevision Many suggestions here, most along the lines of git checkout $revision -- $file. A couple of obscure alternatives: And also, I use this a lot just to see a particular version temporarily: or (OBS: $file needs to be prefixed with ./ if it is a relative path for git show $revision:$file to work) And the even more weird: 2.Git revert file to a specific branch I have to plug EasyGit here, which is a wrapper to make git more approachable to novices without confusing seasoned users. One of the things it does is give more meanings to git revert. In this case, you would simply say: eg revert foo/bar foo/baz In the case that you want to revert a file to a previous commit (and the file you want to revert already committed) you can use or Then just stage and commit the "new" version. Armed with the knowledge that a commit can have two parents in the case of a merge, you should know that HEAD^1 is the first parent and HEAD~1 is the second parent. Either will work if there is only one parent in the tree. Note, however, that git checkout ./foo and git checkout HEAD ./foo are not exactly the same thing; case in point: (The second add stages the file in the index, but it does not get committed.) Git checkout ./foo means revert path ./foo from the index; adding HEAD instructs Git to revert that path in the index to its HEAD revision before doing so. For me none of the reply seemed really clear and therefore I would like to add mine which seems super easy.  I have a commit abc1 and after it I have done several (or one modification) to a file file.txt. Now say that I messed up something in the file file.txt and I want to go back to a previous commit abc1. 1.git checkout file.txt : this will remove local changes, if you don't need them 2.git checkout abc1 file.txt : this will bring your file to your wanted version 3.git commit -m "Restored file.txt to version abc1" : this will commit your reversion. Between the step 2 and 3 of course you can do git status to understand what is going on. Usually you should see the file.txt already added and that is why there is no need of a git add. In order to go to a previous commit version of the file, get the commit number, say eb917a1 then  If you just need to go back to the last commited version This will simply take you to the last committed state of the file Many answers here claims to use git reset ... <file> or git checkout ... <file> but by doing so, you will loose every modifications on <file> committed after the commit you want to revert. If you want to revert changes from one commit on a single file only, just as git revert would do but only for one file (or say a subset of the commit files), I suggest to use both git diff and git apply like that (with <sha> = the hash of the commit you want to revert) : Basically, it will first generate a patch corresponding to the changes you want to revert, and then reverse-apply the patch to drop those changes. Of course, it shall not work if reverted lines had been modified by any commit between <sha1> and HEAD (conflict). git checkout ref|commitHash -- filePath e.g.  This is a very simple step. Checkout file to the commit id we want, here one commit id before, and then just git commit amend and we are done. This is very handy. If we want to bring any file to any prior commit id at the top of commit, we can easily do. Use git log to obtain the hash key for specific version and then use git checkout <hashkey> Note: Do not forget to type the hash before the last one. Last hash points your current position (HEAD) and changes nothing. Obviously someone either needs to write an intelligible book on git, or git needs to be better explained in the documentation.  Faced with this same problem I guessed that  would undo the last commit which is seemed to do. Ian You can do it in 4 steps: What you need to type in your terminal: good luck Will revert a given commit.  It sounds like you think git revert only affects the most recent commit. That doesn't solve your problem, if you want to revert a change in a specific file and that commit changed more than that file. if you commit a wrong file in your last commits follow the instruction :    
__label__javascript __label__checkbox __label__html __label__jquery I need to check the checked property of a checkbox and perform an action based on the checked property using jQuery. For example, if the age checkbox is checked, then I need to show a textbox to enter age, else hide the textbox. But the following code returns false by default:   if ($('#isAgeSelected').attr('checked')) {   $("#txtAge").show(); } else {   $("#txtAge").hide(); } <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <input type="checkbox" id="isAgeSelected"/> <div id="txtAge" style="display:none">   Age is selected </div>    How do I successfully query the checked property? How do I successfully query the checked property? The checked property of a checkbox DOM element will give you the checked state of the element. Given your existing code, you could therefore do this: However, there's a much prettier way to do this, using toggle:   $('#isAgeSelected').click(function() {     $("#txtAge").toggle(this.checked); }); <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <input type="checkbox" id="isAgeSelected"/> <div id="txtAge" style="display:none">Age is something</div>    Use jQuery's is() function: Using jQuery > 1.6  Using the new property method: jQuery 1.6+ jQuery 1.5 and below Any version of jQuery All credit goes to Xian. I am using this and this is working absolutely fine: Note: If the checkbox is checked it will return true otherwise undefined, so better check for the "TRUE" value. Use:       $("#planned_checked").change(function() {         if($(this).prop('checked')) {             alert("Checked Box Selected");         } else {             alert("Checked Box deselect");         }     }); <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script> <input type="checkbox" name="planned_checked" checked id="planned_checked"> Planned    Since jQuery 1.6, the behavior of jQuery.attr() has changed and users are encouraged  not to use it to retrieve an element's checked state. Instead, you should use jQuery.prop(): Two other possibilities are: This worked for me: Where isAgeSelected is the id of the control. Also, @karim79's answer works fine. I am not sure what I missed at the time I tested it. Note, this is answer uses Microsoft Ajax, not jQuery If you are using an updated version of jquery, you must go for .prop method to resolve your issue: $('#isAgeSelected').prop('checked') will return true if checked and false if unchecked. I confirmed it and I came across this issue earlier. $('#isAgeSelected').attr('checked') and $('#isAgeSelected').is('checked') is returning undefined which is not a worthy answer for the situation. So do as given below. Hope it helps :)- Thanks. Use: This can help if you want that the required action has to be done only when you check the box not at the time you remove the check. Using the Click event handler for the checkbox property is unreliable, as the checked property can change during the execution of the event handler itself! Ideally, you'd want to put your code into a change event handler such as it is fired every time the value of the check box is changed (independent of how it's done so). I believe you could do this: I decided to post an answer on how to do that exact same thing without jQuery. Just because I'm a rebel. First you get both elements by their ID. Then you assign the checkboxe's onchange event a function that checks whether the checkbox got checked and sets the hidden property of the age text field appropriately. In that example using the ternary operator. Here is a fiddle for you to test it. Addendum If cross-browser compatibility is an issue then I propose to set the CSS display property to none and inline. Slower but cross-browser compatible. I ran in to the exact same issue. I have an ASP.NET checkbox In the jQuery code I used the following selector to check if the checkbox was checked or not, and it seems to work like a charm. I'm sure you can also use the ID instead of the CssClass, I hope this helps you. This code will help you There are many ways to check if a checkbox is checked or not: Way to check using jQuery Check example or also document: http://api.jquery.com/attr/ http://api.jquery.com/prop/ This works for me: This is some different method to do the same thing:   $(document).ready(function (){      $('#isAgeSelected').click(function() {         // $("#txtAge").toggle(this.checked);          // Using a pure CSS selector         if ($(this.checked)) {             alert('on check 1');         };          // Using jQuery's is() method         if ($(this).is(':checked')) {             alert('on checked 2');         };          //  // Using jQuery's filter() method         if ($(this).filter(':checked')) {             alert('on checked 3');         };     }); }); <script src="http://code.jquery.com/jquery-1.9.1.js"></script> <input type="checkbox" id="isAgeSelected"/> <div id="txtAge" style="display:none">Age is something</div>    You can try the change event of checkbox to track the :checked state change.   $("#isAgeSelected").on('change', function() {   if ($("#isAgeSelected").is(':checked'))     alert("checked");   else {     alert("unchecked");   } }); <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <input type="checkbox" id="isAgeSelected" /> <div id="txtAge" style="display:none">   Age is selected </div>    Use this: The length is greater than zero if the checkbox is checked. My way of doing this is: This returns true if the input is checked and false if it is not.  You can use: Both of them should work. 1) If your HTML markup is: attr used: If prop is used: 2) If your HTML markup is: attr used: Prop used: This example is for button. Try the following: The top answer didn't do it for me. This did though: Basically when the element #li_13 is clicked, it checks if the element # agree (which is the checkbox) is checked by using the .attr('checked') function. If it is then fadeIn the #saveForm element, and if not fadeOut the saveForm element. Though you have proposed a JavaScript solution for your problem (displaying a textbox when a checkbox is checked), this problem could be solved just by css. With this approach, your form works for users who have disabled JavaScript. Assuming that you have the following HTML: You can use the following CSS to achieve the desired functionality: For other scenarios, you may think of appropriate CSS selectors. Here is a Fiddle to demonstrate this approach. I am using this: Toggle: 0/1 or else
__label__validation __label__javascript __label__email-validation __label__email __label__regex Is there a regular expression to validate an email address in JavaScript? Using regular expressions is probably the best way. You can see a bunch of tests here (taken from chromium) Here's the example of regular expresion that accepts unicode: But keep in mind that one should not rely only upon JavaScript validation. JavaScript can easily be disabled. This should be validated on the server side as well. Here's an example of the above in action:   function validateEmail(email) {   const re = /^(([^<>()[\]\\.,;:\s@\"]+(\.[^<>()[\]\\.,;:\s@\"]+)*)|(\".+\"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$/;   return re.test(email); }  function validate() {   const $result = $("#result");   const email = $("#email").val();   $result.text("");    if (validateEmail(email)) {     $result.text(email + " is valid :)");     $result.css("color", "green");   } else {     $result.text(email + " is not valid :(");     $result.css("color", "red");   }   return false; }  $("#validate").on("click", validate); <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>  <form>   <p>Enter an email address:</p>   <input id='email'>   <button type='submit' id='validate'>Validate!</button> </form>  <h2 id='result'></h2>    I've slightly modified Jaymon's answer for people who want really simple validation in the form of: The regular expression: Example JavaScript function:   function validateEmail(email)      {         var re = /\S+@\S+\.\S+/;         return re.test(email);     }      console.log(validateEmail('anystring@anystring.anystring'));    Just for completeness, here you have another RFC 2822 compliant regex The official standard is known as RFC 2822. It describes the syntax that valid email addresses must adhere to. You can (but you shouldn't — read on) implement it with this regular expression: (?:[a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*|"(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21\x23-\x5b\x5d-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])*")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\[(?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?|[a-z0-9-]*[a-z0-9]:(?:[\x01-\x08\x0b\x0c\x0e-\x1f\x21-\x5a\x53-\x7f]|\\[\x01-\x09\x0b\x0c\x0e-\x7f])+)\]) (...) We get a more practical implementation of RFC 2822 if we omit the syntax using double quotes and square brackets. It will still match 99.99% of all email addresses in actual use today. [a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])? A further change you could make is to allow any two-letter country code top level domain, and only specific generic top level domains. This regex filters dummy email addresses like asdf@adsf.adsf. You will need to update it as new top-level domains are added. [a-z0-9!#$%&'*+/=?^_`{|}~-]+(?:\.[a-z0-9!#$%&'*+/=?^_`{|}~-]+)*@(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\.)+(?:[A-Z]{2}|com|org|net|gov|mil|biz|info|mobi|name|aero|jobs|museum)\b So even when following official standards, there are still trade-offs to be made. Don't blindly copy regular expressions from online libraries or discussion forums. Always test them on your own data and with your own applications. Emphasis mine Wow, there are lots of complexity here. If all you want to do is just catch the most obvious syntax errors, I would do something like this: It usually catches the most obvious errors that the user makes and assures that the form is mostly right, which is what JavaScript validation is all about. There's something you have to understand the second you decide to use a regular expression to validate emails: It's probably not a good idea. Once you have come to terms with that, there are many implementations out there that can get you halfway there, this article sums them up nicely. In short, however, the only way to be absolutely, positively sure that what the user entered is in fact an email is to actually send an email and see what happens. Other than that it's all just guesses. HTML5 itself has email validation. If your browser supports HTML5 then you can use the following code. jsFiddle link From the HTML5 spec: A valid e-mail address is a string that matches the email production of the following ABNF, the character set for which is Unicode. This requirement is a willful violation of RFC 5322, which defines a syntax for e-mail addresses that is simultaneously too strict (before the "@" character), too vague (after the "@" character), and too lax (allowing comments, whitespace characters, and quoted strings in manners unfamiliar to most users) to be of practical use here. The following JavaScript- and Perl-compatible regular expression is an implementation of the above definition. I have found this to be the best solution: It allows the following formats: It's clearly versatile and allows the all-important international characters, while still enforcing the basic anything@anything.anything format. It will block spaces which are technically allowed by RFC, but they are so rare that I'm happy to do this. In modern browsers you can build on top of @Sushil's answer with pure JavaScript and the DOM: I've put together an example in the fiddle http://jsfiddle.net/boldewyn/2b6d5/. Combined with feature detection and the bare-bones validation from Squirtle's Answer, it frees you from the regular expression massacre and does not bork on old browsers. This is the correct RFC822 version. JavaScript can match a regular expression: Here's an RFC22 regular expression for emails: All email addresses contain an 'at' (i.e. @) symbol. Test that necessary condition: Don't bother with anything more complicated. Even if you could perfectly determine whether an email is RFC-syntactically valid, that wouldn't tell you whether it belongs to the person who supplied it. That's what really matters. To test that, send a validation message. Correct validation of email address in compliance with the RFCs is not something that can be achieved with a one-liner regular expression. An article with the best solution I've found in PHP is What is a valid email address?. Obviously, it has been ported to Java. I think the function is too complex to be ported and used in JavaScript. JavaScript/node.js port: https://www.npmjs.com/package/email-addresses. A good practice is to validate your data on the client, but double-check the validation on the server. With this in mind, you can simply check whether a string looks like a valid email address on the client and perform the strict check on the server. Here's the JavaScript function I use to check if a string looks like a valid mail address: Explanation: lastAtPos < lastDotPos: Last @ should be before last . since @ cannot be part of server name (as far as I know). lastAtPos > 0: There should be something (the email username) before the last @. str.indexOf('@@') == -1: There should be no @@ in the address. Even if @ appears as the last character in email username, it has to be quoted so " would be between that @ and the last @ in the address. lastDotPos > 2: There should be at least three characters before the last dot, for example a@b.com. (str.length - lastDotPos) > 2: There should be enough characters after the last dot to form a two-character domain. I'm not sure if the brackets are necessary. This was stolen from http://codesnippets.joyent.com/posts/show/1917 Do this: Why?  It's based on RFC 2822, which is a standard ALL email addresses MUST adhere to.  And I'm not sure why you'd bother with something "simpler"... you're gonna copy and paste it anyway ;) Often when storing email addresses in the database I make them lowercase and, in practice, regexs can usually be marked case insensitive. In those cases this is slightly shorter: Here's an example of it being used in JavaScript (with the case insensitive flag i at the end). Note: Technically some emails can include quotes in the section before the @ symbol with escape characters inside the quotes (so your email user can be obnoxious and contain stuff like @ and "..." as long as it's written in quotes). NOBODY DOES THIS EVER! It's obsolete. But, it IS included in the true RFC 2822 standard, and omitted here. More info: http://www.regular-expressions.info/email.html I'm really looking forward to solve this problem. So I modified email validation regular expression above  Original /^(([^<>()\[\]\\.,;:\s@"]+(\.[^<>()\[\]\\.,;:\s@"]+)*)|(".+"))@((\[[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}\.[0-9]{1,3}])|(([a-zA-Z\-0-9]+\.)+[a-zA-Z]{2,}))$/ Modified /^(([^<>()\[\]\.,;:\s@\"]+(\.[^<>()\[\]\.,;:\s@\"]+)*)|(\".+\"))@(([^<>()\.,;\s@\"]+\.{0,1})+[^<>()\.,;:\s@\"]{2,})$/ to pass the examples in Wikipedia Email Address. And you can see the result in here.  You should not use regular expressions to validate an input string to check if it's an email. It's too complicated and would not cover all the cases. Now since you can only cover 90% of the cases, write something like: You can refine it. For instance, 'aaa@' is valid. But overall you get the gist. And don't get carried away... A simple 90% solution is better than 100% solution that does not work.  The world needs simpler code... Simply check out if the entered email address is valid or not using HTML. There isn't any need to write a function for validation. It's hard to get an email validator 100% correct. The only real way to get it correct would be to send a test email to the account. That said, there are a few basic checks that can help make sure that you're getting something reasonable. Some things to improve: Instead of new RegExp, just try writing the regexp out like this: Second, check to make sure that a period comes after the @ sign, and make sure that there are characters between the @s and periods.  This is how node-validator does it: Use this code inside your validator function: Else you can use jQuery. Inside rules define: Almost all answers to this questions suggest using Regex to validate emails addresses. I think Regex is only good for a rudimentary validation. It seems that the checking validation of email addresses is actually two separate problems: 1- Validation of email format: Making sure if the email complies with the format and pattern of emails in RFC 5322 and if the TLD actually exists. A list of all valid TLDs can be found here. For example, although the address example@example.ccc will pass the regex, it is not a valid email, because ccc is not a top-level domain by IANA. 2- Making sure the email actually exists: For doing this, the only option is to send the users an email. Regex update 2018! try this typscript version complete more info https://git.io/vhEfc Regex for validating email address Here is a very good discussion about using regular expressions to validate email addresses; "Comparing E-mail Address Validating Regular Expressions" Here is the current top expression, that is JavaScript compatible, for reference purposes: Apparently, that's it: Taken from http://fightingforalostcause.net/misc/2006/compare-email-regex.php on Oct 1 '10. But, of course, that's ignoring internationalization. In contrast to squirtle, here is a complex solution, but it does a mighty fine job of validating emails properly: Use like so: My knowledge of regular expressions is not that good. That's why I check the general syntax with a simple regular expression first and check more specific options with other functions afterwards. This may not be not the best technical solution, but this way I'm way more flexible and faster. The most common errors I've come across are spaces (especially at the beginning and end) and occasionally a double dot.  I was looking for a Regex in JS that passes all Email Address test cases: email@example.com Valid email firstname.lastname@example.com  Email contains dot in the address field email@subdomain.example.com Email contains dot with subdomain firstname+lastname@example.com   Plus sign is considered valid character email@192.0.2.123 Domain is valid IP address email@[192.0.2.123] Square bracket around IP address is considered valid “email”@example.com Quotes around email is considered valid 1234567890@example.com Digits in address are valid email@domain-one.example Dash in domain name is valid _______@example.com Underscore in the address field is valid email@example.name .name is valid Top Level Domain name email@example.co.jp Dot in Top Level Domain name also considered valid (using co.jp as example here) firstname-lastname@example.com Dash in address field is valid Here we go : http://regexr.com/3f07j OR regex: Wikipedia standard mail syntax : https://en.wikipedia.org/wiki/Email_address#Examples https://fr.wikipedia.org/wiki/Adresse_%C3%A9lectronique#Syntaxe_exacte Show this test : https://regex101.com/r/LHJ9gU/1
__label__javascript __label__replace __label__string I have this string in my Javascript code: Doing: seems to only remove the first occurrence of abc in the string above. How can I replace all occurrences of it? Update: In the latest versions of most popular browsers, you can use replaceAll as shown here: But check Can I use or another compatibility table first to make sure the browsers you're targeting have added support for it first. For Node and compatibility with older/non-current browsers: Note: Don't use the following solution in performance critical code. As an alternative to regular expressions for a simple literal string, you could use The general pattern is This used to be faster in some cases than using replaceAll and a regular expression, but that doesn't seem to be the case anymore in modern browsers. Benchmark: https://jsperf.com/replace-all-vs-split-join Conclusion: If you have a performance critical use case (e.g processing hundreds of strings), use the Regexp method. But for most typical use cases, this is well worth not having to worry about special characters. As of August 2020, greenfield browsers have support for the String.replaceAll() method defined by the ECMAScript 2021 language specification. For older/legacy browser support, the below still applies. In response to comment: In response to Click Upvote's comment, you could simplify it even more: Note: Regular expressions contain special (meta) characters, and as such it is dangerous to blindly pass an argument in the find function above without pre-processing it to escape those characters.  This is covered in the Mozilla Developer Network's JavaScript Guide on Regular Expressions, where they present the following utility function (which has changed at least twice since this answer was originally written, so make sure to check the MDN site for potential updates): So in order to make the replaceAll() function above safer, it could be modified to the following if you also include escapeRegExp: For the sake of completeness, I got to thinking about which method I should use to do this. There are basically two ways to do this as suggested by the other answers on this page. Note: In general, extending the built-in prototypes in JavaScript is generally not recommended. I am providing as extensions on the String prototype simply for purposes of illustration, showing different implementations of a hypothetical standard method on the String built-in prototype. Not knowing too much about how regular expressions work behind the scenes in terms of efficiency, I tended to lean toward the split and join implementation in the past without thinking about performance. When I did wonder which was more efficient, and by what margin, I used it as an excuse to find out. On my Chrome Windows 8 machine, the regular expression based implementation is the fastest, with the split and join implementation being 53% slower. Meaning the regular expressions are twice as fast for the lorem ipsum input I used. Check out this benchmark running these two implementations against each other. As noted in the comment below by @ThomasLeduc and others, there could be an issue with the regular expression-based implementation if search contains certain characters which are reserved as special characters in regular expressions. The implementation assumes that the caller will escape the string beforehand or will only pass strings that are without the characters in the table in Regular Expressions (MDN). MDN also provides an implementation to escape our strings. It would be nice if this was also standardized as RegExp.escape(str), but alas, it does not exist: We could call escapeRegExp within our String.prototype.replaceAll implementation, however, I'm not sure how much this will affect the performance (potentially even for strings for which the escape is not needed, like all alphanumeric strings). Using a regular expression with the g flag set will replace all: Here's a string prototype function based on the accepted answer: EDIT  If your find will contain special characters then you need to escape them: Fiddle: http://jsfiddle.net/cdbzL/ Update: It's somewhat late for an update, but since I just stumbled on this question, and noticed that my previous answer is not one I'm happy with. Since the question involved replaceing a single word, it's incredible nobody thought of using word boundaries (\b) This is a simple regex that avoids replacing parts of words in most cases. However, a dash - is still considered a word boundary. So conditionals can be used in this case to avoid replacing strings like cool-cat: basically, this question is the same as the question here: Javascript replace " ' " with " '' " @Mike, check the answer I gave there... regexp isn't the only way to replace multiple occurrences of a subsrting, far from it. Think flexible, think split! Alternatively, to prevent replacing word parts -which the approved answer will do, too! You can get around this issue using regular expressions that are, I admit, somewhat more complex and as an upshot of that, a tad slower, too: The output is the same as the accepted answer, however, using the /cat/g expression on this string: Oops indeed, this probably isn't what you want. What is, then? IMHO, a regex that only replaces 'cat' conditionally. (ie not part of a word), like so: My guess is, this meets your needs. It's not fullproof, of course, but it should be enough to get you started. I'd recommend reading some more on these pages. This'll prove useful in perfecting this expression to meet your specific needs. http://www.javascriptkit.com/jsref/regexp.shtml http://www.regular-expressions.info Final addition: Given that this question still gets a lot of views, I thought I might add an example of .replace used with a callback function. In this case, it dramatically simplifies the expression and provides even more flexibility, like replacing with correct capitalisation or replacing both cat and cats in one go: Match against a global regular expression: For replacing a single time use: For replacing multiple times use: These are the most common and readable methods. Method 1: Method 2: Method 3: Method 4: Output: Or try the replaceAll function from here: What are useful JavaScript methods that extends built-in objects? EDIT: Clarification about replaceAll availability The 'replaceAll' method is added to String's prototype. This means it will be available for all string objects/literals. E.g. Using RegExp in JavaScript could do the job for you, just simply do something like below code, don't forget the /g after which standout for global: If you think of reuse, create a function to do that for you, but it's not recommended as it's only one line function, but again if you heavily use this, you can write something like this: and simply use it in your code over and over like below: But as I mention earlier, it won't make a huge difference in terms of lines to be written or performance, only caching the function may effect some faster performance on long strings and also a good practice of DRY code if you want to reuse. Say you want to replace all the 'abc' with 'x': I was trying to think about something more simple than modifying the string prototype. Use a regular expression: Replacing single quotes: Today 27.12.2019 I perform tests on macOS v10.13.6 (High Sierra) for the chosen solutions. Conclusions I also create my own solution. It looks like currently it is the shortest one which does the question job:   str = "Test abc test test abc test test test abc test test abc"; str = str.split`abc`.join``  console.log(str);    The tests were performed on Chrome 79.0, Safari 13.0.4 and Firefox 71.0 (64 bit). The tests RA and RB use recursion. Results  You can run tests on your machine HERE. Results for Chrome:  The recursive solutions RA and RB gives RangeError: Maximum call stack size exceeded For 1M characters they even break Chrome  I try to perform tests for 1M characters for other solutions, but E,F,G,H takes so much time that browser ask me to break script so I shrink test string to 275K characters. You can run tests on your machine HERE. Results for Chrome  Code used in tests   var t="Test abc test test abc test test test abc test test abc"; // .repeat(5000) var log = (version,result) => console.log(`${version}: ${result}`);   function A(str) {   return str.split('abc').join(''); }  function B(str) {   return str.split`abc`.join``; // my proposition }   function C(str) {   return str.replace(/abc/g, ''); }  function D(str) {   return str.replace(new RegExp("abc", "g"), ''); }  function E(str) {   while (str.indexOf('abc') !== -1) { str = str.replace('abc', ''); }   return str; }  function F(str) {   while (str.indexOf('abc') !== -1) { str = str.replace(/abc/, ''); }   return str; }  function G(str) {   while(str.includes("abc")) { str = str.replace('abc', ''); }   return str; }  // src: https://stackoverflow.com/a/56989553/860099 function H(str) {     let i = -1     let find = 'abc';     let newToken = '';      if (!str)     {         if ((str == null) && (find == null)) return newToken;         return str;     }      while ((         i = str.indexOf(             find, i >= 0 ? i + newToken.length : 0         )) !== -1     )     {         str = str.substring(0, i) +             newToken +             str.substring(i + find.length);     }     return str; }  // src: https://stackoverflow.com/a/22870785/860099 function RA(string, prevstring) {   var omit = 'abc';   var place = '';   if (prevstring && string === prevstring)     return string;   prevstring = string.replace(omit, place);   return RA(prevstring, string) }  // src: https://stackoverflow.com/a/26107132/860099 function RB(str) {   var find = 'abc';   var replace = '';   var i = str.indexOf(find);   if (i > -1){     str = str.replace(find, replace);     i = i + replace.length;     var st2 = str.substring(i);     if(st2.indexOf(find) > -1){       str = str.substring(0,i) + RB(st2, find, replace);     }   }   return str; }     log('A ', A(t)); log('B ', B(t)); log('C ', C(t)); log('D ', D(t)); log('E ', E(t)); log('F ', F(t)); log('G ', G(t)); log('H ', H(t)); log('RA', RA(t)); // use reccurence log('RB', RB(t)); // use reccurence <p style="color:red">This snippet only presents codes used in tests. It not perform test itself!<p>    //loop it until number occurrences comes to 0. OR simply copy/paste worked better for me than the above answers. so new RegExp("abc", 'g') creates a RegExp what matches all occurence ('g' flag) of the text ("abc"). The second part is what gets replaced to, in your case empty string (""). str is the string, and we have to override it, as replace(...) just returns result, but not overrides. In some cases you might want to use that. This is the fastest version that doesn't use regular expressions. Revised jsperf It is almost twice as fast as the split and join method. As pointed out in a comment here, this will not work if your omit variable contains place, as in: replaceAll("string", "s", "ss"), because it will always be able to replace another occurrence of the word. There is another jsperf with variants on my recursive replace that go even faster (http://jsperf.com/replace-all-vs-split-join/12)! If what you want to find is already in a string, and you don't have a regex escaper handy, you can use join/split:       function replaceMulti(haystack, needle, replacement)     {         return haystack.split(needle).join(replacement);     }      someString = 'the cat looks like a cat';     console.log(replaceMulti(someString, 'cat', 'dog'));     I like this method (it looks a little cleaner): The simplest way to this without using any regex is split and join like the code here: http://jsfiddle.net/ANHR9/  If the string contain similar pattern like abccc, you can use this: The previous answers are way too complicated. Just use the replace function like this: Example:   var str = "Test abc test test abc test test test abc test test abc";  var res = str.replace(/[abc]+/g, "");  console.log(res);    As of August 2020 there is a Stage 4 proposal to ECMAScript that adds the replaceAll method to String. It's now supported in Chrome 85+, Edge 85+, Firefox 77+, Safari 13.1+. The usage is the same as the replace method: Here's an example usage: It's supported in most modern browsers, but there exist polyfills: It is supported in the V8 engine behind an experimental flag --harmony-string-replaceall. Read more on the V8 website. If you are trying to ensure that the string you are looking for won't exist even after the replacement, you need to use a loop. For example: When complete, you will still have 'test abc'! The simplest loop to solve this would be: But that runs the replacement twice for each cycle. Perhaps (at risk of being voted down) that can be combined for a slightly more efficient but less readable form: This can be particularly useful when looking for duplicate strings. For example, if we have 'a,,,b' and we wish to remove all duplicate commas. [In that case, one could do .replace(/,+/g,','), but at some point the regex gets complex and slow enough to loop instead.] Although people have mentioned the use of regex but there's a better approach if you want to replace the text irrespective of the case of the text. Like uppercase or lowercase. Use below syntax You can refer the detailed example here. Just add /g  to /g means global
__label__wsdl __label__soap __label__actionscript __label__coldfusion __label__apache-flex We have an employee whose surname is Null. Our employee lookup application is killed when that last name is used as the search term (which happens to be quite often now). The error received (thanks Fiddler!) is: Cute, huh? The parameter type is string. I am using: Note that the error does not occur when calling the webservice as an object from a ColdFusion page. At first I thought this was a coercion bug where null was getting coerced to "null" and a test of "null" == null was passing. It's not. I was close, but so very, very wrong. Sorry about that! I've since done lots of fiddling on wonderfl.net and tracing through the code in mx.rpc.xml.*. At line 1795 of XMLEncoder (in the 3.5 source), in setValue, all of the XMLEncoding boils down to  which is essentially the same as: This code, according to my original fiddle, returns an empty XML element. But why?  Cause  According to commenter Justin Mclean on bug report FLEX-33664, the following is the culprit (see last two tests in my fiddle which verify this): When currentChild.appendChild is passed the string "null", it first converts it to a root XML element with text null, and then tests that element against the null literal. This is a weak equality test, so either the XML containing null is coerced to the null type, or the null type is coerced to a root xml element containing the string "null", and the test passes where it arguably should fail. One fix might be to always use strict equality tests when checking XML (or anything, really) for "nullness." Solution The only reasonable workaround I can think of, short of fixing this bug in every damn version of ActionScript, is to test fields for "null" and escape them as CDATA values.  CDATA values are the most appropriate way to mutate an entire text value that would otherwise cause encoding/decoding problems. Hex encoding, for instance, is meant for individual characters. CDATA values are preferred when you're escaping the entire text of an element. The biggest reason for this is that it maintains human readability. On the xkcd note, the Bobby Tables website has good advice for avoiding the improper interpretation of user data (in this case, the string "Null") in SQL queries in various languages, including ColdFusion. It is not clear from the question that this is the source of the problem, and given the solution noted in a comment to the first answer (embedding the parameters in a structure) it seems likely that it was something else. The problem could be in Flex's SOAP encoder. Try extending the SOAP encoder in your Flex application and debug the program to see how the null value is handled.  My guess is, it's passed as NaN (Not a Number). This will mess up the SOAP message unmarshalling process sometime (most notably in the JBoss 5 server...). I remember extending the SOAP encoder and performing an explicit check on how NaN is handled.  @doc_180 had the right concept, except he is focused on numbers, whereas the original poster had issues with strings. The solution is to change the mx.rpc.xml.XMLEncoder file. This is line 121: (I looked at Flex 4.5.1 SDK; line numbers may differ in other versions.) Basically, the validation fails because 'content is null' and therefore your argument is not added to the outgoing SOAP Packet; thus causing the missing parameter error. You have to extend this class to remove the validation.  Then there is a big snowball up the chain, modifying SOAPEncoder to use your modified XMLEncoder, and then modifying Operation to use your modified SOAPEncoder, and then moidfying WebService to use your alternate Operation class. I spent a few hours on it, but I need to move on. It'll probably take a day or two. You may be able to just fix the XMLEncoder line and do some monkey patching to use your own class. I'll also add that if you switch to using RemoteObject/AMF with ColdFusion, the null is passed without problems. 11/16/2013 update: I have one more recent addition to my last comment about RemoteObject/AMF. If you are using ColdFusion 10; then properties with a null value on an object are removed from the server-side object.  So, you have to check for the properties existence before accessing it or you will get a runtime error. Check like this: This is a change in behavior from ColdFusion 9; where the null properties would turn into empty strings. Edit 12/6/2013 Since there was a question about how nulls are treated, here is a quick sample application to demonstrate how a string "null" will relate to the reserved word null. The trace output is: null string is not equal to null reserved word using the != condition null string is not equal to null reserved word using the == condition null string is not equal to null reserved word using the === condition Translate all characters into their hex-entity equivalents. In this case, Null would be converted into &#4E;&#75;&#6C;&#6C; Stringifying a null value in ActionScript will give the string "NULL". My suspicion is that someone has decided that it is, therefore, a good idea to decode the string "NULL" as null, causing the breakage you see here -- probably because they were passing in null objects and getting strings in the database, when they didn't want that (so be sure to check for that kind of bug, too). As a hack, you could consider having a special handling on the client side, converting 'Null' string to something that will never occur, for example, XXNULLXX and converting back on the server.  It is not pretty, but it may solve the issue for such a boundary case. Well, I guess that Flex' implementation of the SOAP Encoder seems to serialize null values incorrectly. Serializing them as a String Null doesn't seem to be a good solution. The formally correct version seems to be to pass a null value as: So the value of "Null" would be nothing else than a valid string, which is exactly what you are looking for. I guess getting this fixed in Apache Flex shouldn't be that hard to get done. I would recommend opening a Jira issue or to contact the guys of the apache-flex mailinglist. However this would only fix the client side. I can't say if ColdFusion will be able to work with null values encoded this way. See also Radu Cotescu's blog post How to send null values in soapUI requests. It's a kludge, but assuming there's a minimum length for SEARCHSTRING, for example 2 characters, substring the SEARCHSTRING parameter at the second character and pass it as two parameters instead: SEARCHSTRING1 ("Nu") and SEARCHSTRING2 ("ll"). Concatenate them back together when executing the query to the database.
__label__symbols __label__php __label__logic __label__arguments __label__operators This is a collection of questions that come up every now and then about syntax in PHP. This is also a Community Wiki, so everyone is invited to participate in maintaining this list. It used to be hard to find questions about operators and other syntax tokens.¹ The main idea is to have links to existing questions on Stack Overflow, so it's easier for us to reference them, not to copy over content from the PHP Manual. Note: Since January 2013, Stack Overflow does support special characters. Just surround the search terms by quotes, e.g. [php] "==" vs "===" If you have been pointed here by someone because you have asked such a question, please find the particular syntax below. The linked pages to the PHP manual along with the linked questions will likely answer your question then. If so, you are encouraged to upvote the answer. This list is not meant as a substitute to the help others provided. If your particular token is not listed below, you might find it in the List of Parser Tokens. & Bitwise Operators or References =& References &= Bitwise Operators && Logical Operators % Arithmetic Operators !! Logical Operators @ Error Control Operators ?: Ternary Operator ?? Null Coalesce Operator (since PHP 7) ?string ?int ?array ?bool ?float Nullable return type declaration (since PHP 7.1) : Alternative syntax for control structures, Ternary Operator :: Scope Resolution Operator \ Namespaces -> Classes And Objects => Arrays ^ Bitwise Operators >> Bitwise Operators << Bitwise Operators <<< Heredoc or Nowdoc = Assignment Operators == Comparison Operators === Comparison Operators !== Comparison Operators != Comparison Operators <> Comparison Operators <=> Comparison Operators (since PHP 7.0) | Bitwise Operators || Logical Operators ~ Bitwise Operators + Arithmetic Operators, Array Operators += and -= Assignment Operators ++ and -- Incrementing/Decrementing Operators .= Assignment Operators . String Operators , Function Arguments , Variable Declarations $$ Variable Variables ` Execution Operator <?= Short Open Tags [] Arrays (short syntax since PHP 5.4) <? Opening and Closing tags ... Argument unpacking (since PHP 5.6) ** Exponentiation (since PHP 5.6) # One-line shell-style comment ?-> NullSafe Operator Calls (since PHP 8.0) Incrementing / Decrementing Operators ++ increment operator -- decrement operator These can go before or after the variable. If put before the variable, the increment/decrement operation is done to the variable first then the result is returned. If put after the variable, the variable is first returned, then the increment/decrement operation is done. For example: Live example In the case above ++$i is used, since it is faster. $i++ would have the same results. Pre-increment is a little bit faster because it really increments the variable and after that 'returns' the result. Post-increment creates a special variable, copies there the value of the first variable and only after the first variable is used, replaces its value with second's. However, you must use $apples--, since first, you want to display the current number of apples, and then you want to subtract one from it. You can also increment letters in PHP: Once z is reached aa is next, and so on. Note that character variables can be incremented but not decremented and even so only plain ASCII characters (a-z and A-Z) are supported. Stack Overflow Posts: What is a bit? A bit is a representation of 1 or 0. Basically OFF(0) and ON(1) What is a byte? A byte is made up of 8 bits and the highest value of a byte is 255, which would mean every bit is set. We will look at why a byte's maximum value is 255. This representation of 1 Byte 1 + 2 + 4 + 8 + 16 + 32 + 64 + 128 = 255 (1 Byte)  This would output the number 8. Why? Well let's see using our table example. So you can see from the table the only bit they share together is the 8 bit. Second example The two shared bits are 32 and 4, which when added together return 36. This would output the number 11. Why? You will notice that we have 3 bits set, in the 8, 2, and 1 columns. Add those up: 8+2+1=11. The spaceship operator <=> is the latest comparison operator added in PHP 7. It is a non-associative binary operator with the same precedence as equality operators (==, !=, ===, !==). This operator allows for simpler three-way comparison between left-hand and right-hand operands. The operator results in an integer expression of: e.g. A good practical application of using this operator would be in comparison type callbacks that are expected to return a zero, negative, or positive integer based on a three-way comparison between two values. The comparison function passed to usort is one such example. _ Alias for gettext()  The underscore character '_' as in _() is an alias to the gettext() function.  Magic constants: Although these are not just symbols but important part of this token family. There are eight magical constants that change depending on where they are used. __LINE__: The current line number of the file. __FILE__: The full path and filename of the file. If used inside an include, the name of the included file is returned. Since PHP 4.0.2, __FILE__ always contains an absolute path with symlinks resolved whereas in older versions it contained relative path under some circumstances. __DIR__: The directory of the file. If used inside an include, the directory of the included file is returned. This is equivalent to dirname(__FILE__). This directory name does not have a trailing slash unless it is the root directory. (Added in PHP 5.3.0.) __FUNCTION__: The function name. (Added in PHP 4.3.0) As of PHP 5 this constant returns the function name as it was declared (case-sensitive). In PHP 4 its value is always lowercased. __CLASS__: The class name. (Added in PHP 4.3.0) As of PHP 5 this constant returns the class name as it was declared (case-sensitive). In PHP 4 its value is always lowercased. The class name includes the namespace it was declared in (e.g. Foo\Bar). Note that as of PHP 5.4 __CLASS__ works also in traits. When used in a trait method, __CLASS__ is the name of the class the trait is used in. __TRAIT__: The trait name. (Added in PHP 5.4.0) As of PHP 5.4 this constant returns the trait as it was declared (case-sensitive). The trait name includes the namespace it was declared in (e.g. Foo\Bar). __METHOD__: The class method name. (Added in PHP 5.0.0) The method name is returned as it was declared (case-sensitive). __NAMESPACE__: The name of the current namespace (case-sensitive). This constant is defined in compile-time (Added in PHP 5.3.0). Source instanceof is used to determine whether a PHP variable is an instantiated object of a certain class. The above example will output: Reason: Above Example $a is a object of the mclass so use only a mclass data not instance of with the sclass The above example will output: The above example will output: and operator and or operator have lower precedence than assignment operator =. This means that $a = true and false; is equivalent to ($a = true) and false. In most cases you will probably want to use && and ||, which behave in a way known from languages like C, Java or JavaScript. Examples for <=> Spaceship operator (PHP 7, Source: PHP Manual): Integers, Floats, Strings, Arrays & objects for Three-way comparison of variables. {} Curly braces And some words about last post Null coalescing operator (??) This operator has been added in PHP 7.0 for the common case of needing to use a ternary operator in conjunction with isset(). It returns its first operand if it exists and is not NULL; otherwise it returns its second operand. PHP Strings: PHP Strings can be specified in four ways not just two ways: 1) Single Quote Strings:  2) Double Quote Strings:  3) Heredoc:  4) Nowdoc (since PHP 5.3.0):  QUESTION: What does => mean? ANSWER: => Is the symbol we humans decided to use to separate "Key" => "Value" pairs in Associative Arrays. ELABORATING: To understand this, we have to know what Associative Arrays are. The first thing that comes up when a conventional programmer thinks of an array (in PHP) would be something similar to: Where as, if we wanted to call the array in some later part of the code, we could do: So far so good. However, as humans, we might find it hard to remember that index [0] of the array is the value of the year 2016, index [1] of the array is a greetings, and index [2] of the array is a simple integer value. The alternative we would then have is to use what is called an Associative Array. An Associative array has a few differences from a Sequential Array (which is what the previous cases were since they increment the index used in a predetermined sequence, by incrementing by 1 for each following value). Differences (between a sequential and associative array): Durring the declaration of an Associative Array, you don't only include the value of what you want to put in the array, but you also put the index value (called the key) which you want to use when calling the array in later parts of the code. The following syntax is used during it's declaration: "key" => "value". When using the Associative Array, the key value would then be placed inside the index of the array to retrieve the desired value. For instance: And now, to receive the same output as before, the key value would be used in the arrays index: FINAL POINT: So from the above example, it is pretty easy to see that the => symbol is used to express the relationship of an Associative Array between each of the key and value pairs in an array DURING the initiation of the values within the array. Question: What does "&" mean here in PHP? Makes life more easier once we get used to it..(check example below carefully) & usually checks bits that are set in both $a and $b are set. have you even noticed how these calls works? So behind all above is game of bitwise operator and bits. One usefull case of these is easy configurations like give below, so a single integer field can store thousands of combos for you.  Most people have already read the docs but didn't reliase the real world use case of these bitwise operators. ==  is used for check equality without considering variable data-type === is used for check equality for both the variable value and data-type $a = 5 if ($a == 5)  - will evaluate to true if ($a == '5') - will evaluate to true, because while comparing this both value PHP internally convert that string value into integer and then compare both values if ($a === 5) - will evaluate to true if ($a === '5') - will evaluate to false, because value is 5, but this value 5 is not an integer. Not the catchiest name for an operator, but PHP 7 brings in the rather handy null coalesce so I thought I'd share an example. In PHP 5, we already have a ternary operator, which tests a value, and then returns the second element if that returns true and the third if it doesn't: There is also a shorthand for that which allows you to skip the second element if it's the same as the first one: echo $count ?: 10; // also outputs 10 In PHP 7 we additionally get the ?? operator which rather than indicating extreme confusion which is how I would usually use two question marks together instead allows us to chain together a string of values. Reading from left to right, the first value which exists and is not null is the value that will be returned. This construct is useful for giving priority to one or more values coming perhaps from user input or existing configuration, and safely falling back on a given default if that configuration is missing. It's kind of a small feature but it's one that I know I'll be using as soon as my applications upgrade to PHP 7. PHP 7 adds support for return type declarations. Similarly to argument type declarations, return type declarations specify the type of value that will be returned from a function. The same types are available for return type declarations as are available for argument type declarations. Strict typing also has an effect on return type declarations. In the default weak mode, returned values will be coerced to the correct type if they are not already of that type. In strong mode, the returned value must be of the correct type, otherwise, a TypeError will be thrown. As of PHP 7.1.0, return values can be marked as nullable by prefixing the type name with a question mark (?). This signifies that the function returns either the specified type or NULL. Source PHP has an operator "..." (Three dots) which is referred as Splat Operator. It is used to pass arbitrary number of parameters in a function and this type of function is called Variadic Functions. Let’s take examples to use of "..." (Three dots). Example 1: Each arguments of calculateNumbers() function pass through $params as an array when use "… ". There are many different ways to use "… "  operator. Below some examples: Example 2: Example 3: Example 4: Added in PHP 8.0 It's the NullSafe Operator, it returns null in case you try to invoke functions or get values from null. Nullsafe operator can be chained and can be used both on the methods and properties. Nullsafe operator doesn't work with array keys: In PHP8 it's been accepted this new operator,  you can find the documentation here. ?-> it's the NullSafe Operator, it returns null in case you try to invoke functions or get values from null... Examples:
__label__git __label__repository __label__git-branch __label__git-remote __label__git-push I want to be able to do the following: Create a local branch based on some other (remote or local) branch (via git branch or git checkout -b) Push the local branch to the remote repository (publish), but make it trackable so git pull and git push will work immediately. How do I do that? I know about --set-upstream in Git 1.7, but that is a post-creation action. I want to find a way to make a similar change when pushing the branch to the remote repository. In Git 1.7.0 and later, you can checkout a new branch: Edit files, add and commit. Then push with the -u (short for --set-upstream) option: Git will set up the tracking information during the push. If you are not sharing your repo with others, this is useful to push all your branches to the remote, and --set-upstream tracking correctly for you: (Not exactly what the OP was asking for, but this one-liner is pretty popular) If you are sharing your repo with others this isn't really good form as you will clog up the repo with all your dodgy experimental branches. Prior to the introduction of git push -u, there was no git push option to obtain what you desire. You had to add new configuration statements. If you create a new branch using: You can use the git config command to avoid editing directly the .git/config file. Or you can edit manually the .git/config file to had tracking information to this branch. Simply put, to create a new local branch, do: To push it to the remote repository, do: A slight variation of the solutions already given here: Create a local branch based on some other (remote or local) branch: Push the local branch to the remote repository (publish), but make it trackable so git pull and git push will work immediately Using HEAD is a "handy way to push the current branch to the same name on the remote". Source: https://git-scm.com/docs/git-push In Git terms, HEAD (in uppercase) is a reference to the top of the current branch (tree). The -u option is just short for --set-upstream. This will add an upstream tracking reference for the current branch. you can verify this by looking in your .git/config file:  I simply do over an already cloned project. Git creates a new branch named remoteBranchToBeCreated under my commits I did in localBranch. Edit: this changes your current local branch's (possibly named localBranch) upstream to origin/remoteBranchToBeCreated. To fix that, simply type: or So your current local branch now tracks origin/localBranch back. I suppose that you have already cloned a project like: Then in your local copy, create a new branch and check it out: Supposing that you made a "git bare --init" on your server and created the myapp.git, you should: After that, users should be able to NOTE: I'm assuming that you have your server up and running. If it isn't, it won't work. A good how-to is here. Add a remote branch: Check if everything is good (fetch origin and list remote branches): Create a local branch and track the remote branch: Update everything: edit Outdated, just use git push -u origin $BRANCHNAME Use git publish-branch from William's miscellaneous Git tools. OK, no Ruby, so - ignoring the safeguards! - take the last three lines of the script and create a bash script, git-publish-branch: Then run git-publish-branch REMOTENAME BRANCHNAME, where REMOTENAME is usually origin (you may modify the script to take origin as default, etc...) To create a new branch by branching off from an existing branch and then push this new branch to repository using This creates and pushes all local commits to a newly created remote branch origin/<new_branch> For GitLab version prior to 1.7, use: (name_branch, ex: master) To push it to the remote repository, do: (name_new_branch, example: feature) I made an alias so that whenever I create a new branch, it will push and track the remote branch accordingly. I put following chunk into the .bash_profile file: Usage: just type gcb thuy/do-sth-kool with thuy/do-sth-kool is my new branch name. Building slightly upon the answers here, I've wrapped this process up as a simple Bash script, which could of course be used as a Git alias as well. The important addition to me is that this prompts me to run unit tests before committing and passes in the current branch name by default. You can do it in 2 steeps:  1. Use the checkout for create the local branch: Work with your Branch as you want. 2. Use the push command to autocreate the branch and send the code to the remote repository: There are mutiple ways to do this but I think that this way is really simple. For greatest flexibility, you could use a custom Git command. For example, create the following Python script somewhere in your $PATH under the name git-publish and make it executable: Then git publish -h will show you usage information: I think this is the simplest alias, add to your ~/.gitconfig You just run and... it publishes the branch
__label__operating-system __label__path __label__directory __label__exception __label__python What is the most elegant way to check if the directory a file is going to be written to exists, and if not, create the directory using Python? Here is what I tried: Somehow, I missed os.path.exists (thanks kanja, Blair, and Douglas). This is what I have now: Is there a flag for "open", that makes this happen automatically? On Python ≥ 3.5, use pathlib.Path.mkdir: For older versions of Python, I see two answers with good qualities, each with a small flaw, so I will give my take on it: Try os.path.exists, and consider os.makedirs for the creation. As noted in comments and elsewhere, there's a race condition – if the directory is created between the os.path.exists and the os.makedirs calls, the os.makedirs will fail with an OSError. Unfortunately, blanket-catching OSError and continuing is not foolproof, as it will ignore a failure to create the directory due to other factors, such as insufficient permissions, full disk, etc. One option would be to trap the OSError and examine the embedded error code (see Is there a cross-platform way of getting information from Python’s OSError): Alternatively, there could be a second os.path.exists, but suppose another created the directory after the first check, then removed it before the second one – we could still be fooled.  Depending on the application, the danger of concurrent operations may be more or less than the danger posed by other factors such as file permissions. The developer would have to know more about the particular application being developed and its expected environment before choosing an implementation. Modern versions of Python improve this code quite a bit, both by exposing FileExistsError (in 3.3+)... ...and by allowing a keyword argument to os.makedirs called exist_ok (in 3.2+). pathlib.Path.mkdir as used above recursively creates the directory and does not raise an exception if the directory already exists. If you don't need or want the parents to be created, skip the parents argument. Using pathlib: If you can, install the current pathlib backport named pathlib2. Do not install the older unmaintained backport named pathlib. Next, refer to the Python 3.5+ section above and use it the same. If using Python 3.4, even though it comes with pathlib, it is missing the useful exist_ok option. The backport is intended to offer a newer and superior implementation of mkdir which includes this missing option. Using os: os.makedirs as used above recursively creates the directory and does not raise an exception if the directory already exists. It has the optional exist_ok argument only if using Python 3.2+, with a default value of False. This argument does not exist in Python 2.x up to 2.7. As such, there is no need for manual exception handling as with Python 2.7. Using pathlib: If you can, install the current pathlib backport named pathlib2. Do not install the older unmaintained backport named pathlib. Next, refer to the Python 3.5+ section above and use it the same. Using os: While a naive solution may first use os.path.isdir followed by os.makedirs, the solution above reverses the order of the two operations. In doing so, it prevents a common race condition having to do with a duplicated attempt at creating the directory, and also disambiguates files from directories. Note that capturing the exception and using errno is of limited usefulness because OSError: [Errno 17] File exists, i.e. errno.EEXIST, is raised for both files and directories. It is more reliable simply to check if the directory exists. mkpath creates the nested directory, and does nothing if the directory already exists. This works in both Python 2 and 3. Per Bug 10948, a severe limitation of this alternative is that it works only once per python process for a given path. In other words, if you use it to create a directory, then delete the directory from inside or outside Python, then use mkpath again to recreate the same directory, mkpath will simply silently use its invalid cached info of having previously created the directory, and will not actually make the directory again. In contrast, os.makedirs doesn't rely on any such cache. This limitation may be okay for some applications. With regard to the directory's mode, please refer to the documentation if you care about it. Using try except and the right error code from errno module gets rid of the race condition and is cross-platform: In other words, we try to create the directories, but if they already exist we ignore the error. On the other hand, any other error gets reported. For example, if you create dir 'a' beforehand and remove all permissions from it, you will get an OSError raised with errno.EACCES (Permission denied, error 13). I would personally recommend that you use os.path.isdir() to test instead of os.path.exists(). If you have: And a foolish user input: ... You're going to end up with a directory named filename.etc when you pass that argument to os.makedirs() if you test with os.path.exists(). Check os.makedirs:  (It makes sure the complete path exists.)  To handle the fact the directory might exist, catch OSError. (If exist_ok is False (the default), an OSError is raised if the target directory already exists.) Starting from Python 3.5, pathlib.Path.mkdir has an exist_ok flag: This recursively creates the directory and does not raise an exception if the directory already exists. (just as os.makedirs got an exist_ok flag starting from python 3.2 e.g os.makedirs(path, exist_ok=True)) Note: when i posted this answer none of the other answers mentioned exist_ok... You give a particular file at a certain path and you pull the directory from the file path. Then after making sure you have the directory, you attempt to open a file for reading. To comment on this code: We want to avoid overwriting the builtin function, dir. Also, filepath or perhaps fullfilepath is probably a better semantic name than filename so this would be better written: Your end goal is to open this file, you initially state, for writing, but you're essentially approaching this goal (based on your code) like this, which opens the file for reading: Why would you make a directory for a file that you expect to be there and be able to read?  Just attempt to open the file. If the directory or file isn't there, you'll get an IOError with an associated error number: errno.ENOENT will point to the correct error number regardless of your platform. You can catch it if you want, for example: This is probably what you're wanting. In this case, we probably aren't facing any race conditions. So just do as you were, but note that for writing, you need to open with the w mode (or a to append). It's also a Python best practice to use the context manager for opening files. However, say we have several Python processes that attempt to put all their data into the same directory. Then we may have contention over creation of the directory. In that case it's best to wrap the makedirs call in a try-except block. Try the os.path.exists function I have put the following down. It's not totally foolproof though. Now as I say, this is not really foolproof, because we have the possiblity of failing to create the directory, and another process creating it during that period. Check if a directory exists and create it if necessary? The direct answer to this is, assuming a simple situation where you don't expect other users or processes to be messing with your directory: or if making the directory is subject to race conditions (i.e. if after checking the path exists, something else may have already made it) do this: But perhaps an even better approach is to sidestep the resource contention issue, by using temporary directories via tempfile: Here's the essentials from the online doc: There's a new Path object (as of 3.4) with lots of methods one would want to use with paths - one of which is mkdir. (For context, I'm tracking my weekly rep with a script. Here's the relevant parts of code from the script that allow me to avoid hitting Stack Overflow more than once a day for the same data.) First the relevant imports: We don't have to deal with os.path.join now - just join path parts with a /: Then I idempotently ensure the directory exists - the exist_ok argument shows up in Python 3.5: Here's the relevant part of the documentation: If exist_ok is true, FileExistsError exceptions will be ignored (same behavior as the POSIX mkdir -p command), but only if the last path component is not an existing non-directory file. Here's a little more of the script - in my case, I'm not subject to a race condition, I only have one process that expects the directory (or contained files) to be there, and I don't have anything trying to remove the directory.  Path objects have to be coerced to str before other APIs that expect str paths can use them. Perhaps Pandas should be updated to accept instances of the abstract base class, os.PathLike. In Python 3.4 you can also use the brand new pathlib module: In Python3, os.makedirs supports setting exist_ok. The default setting is False, which means an OSError will be raised if the target directory already exists. By setting exist_ok to True, OSError (directory exists) will be ignored and the directory will not be created. In Python2, os.makedirs doesn't support setting exist_ok. You can use the approach in heikki-toivonen's answer: The relevant Python documentation suggests the use of the EAFP coding style (Easier to Ask for Forgiveness than Permission). This means that the code is better than the alternative The documentation suggests this exactly because of the race condition discussed in this question. In addition, as others mention here, there is a performance advantage in querying once instead of twice the OS. Finally, the argument placed forward, potentially, in favour of the second code in some cases --when the developer knows the environment the application is running-- can only be advocated in the special case that the program has set up a private environment for itself (and other instances of the same program). Even in that case, this is a bad practice and can lead to long useless debugging. For example, the fact we set the permissions for a directory should not leave us with the impression permissions are set appropriately for our purposes. A parent directory could be mounted with other permissions. In general, a program should always work correctly and the programmer should not expect one specific environment. For a one-liner solution, you can use IPython.utils.path.ensure_dir_exists(): From the documentation: Ensure that a directory exists. If it doesn’t exist, try to create it and protect against a race condition if another process is doing the same. You can use mkpath Note that it will create the ancestor directories as well.  It works for Python 2 and 3. I use os.path.exists(), here is a Python 3 script that can be used to check if a directory exists, create one if it does not exist, and delete it if it does exist (if desired). It prompts users for input of the directory and can be easily modified. I found this Q/A and I was initially puzzled by some of the failures and errors I was getting. I am working in Python 3 (v.3.5 in an Anaconda virtual environment on an Arch Linux x86_64 system). Consider this directory structure: Here are my experiments/notes, which clarifies things: Conclusion: in my opinion, "Method 2" is more robust. [1] How can I create a directory if it does not exist? [2] https://docs.python.org/3/library/os.html#os.makedirs Use this command check and create dir Why not use subprocess module if running on a machine that supports command  mkdir with -p option ?  Works on python 2.7 and python 3.6 Should do the trick on most systems. In situations where portability doesn't matter (ex, using docker) the solution is a clean 2 lines. You also don't have to add logic to check if directories exist or not. Finally, it is safe to re-run without any side effects If you need error handling: I saw Heikki Toivonen and A-B-B's answers and thought of this variation. You can use os.listdir for this: Call the function create_dir() at the entry point of your program/project. You have to set the full path before creating the directory: This works for me and hopefully, it will works for you as well If you consider the following:  means a directory (path) exists AND is a directory. So for me this way does what I need. So I can make sure it is folder (not a file) and exists. Where your code here is use the (touch) command This will check if the file is there if it is not then it will create it. Under Linux you can create directory in one line: You can both create a file, and all its parent directories in 1 command with fastcore extension to pathlib: path.mk_write(data)
__label__regex-negation __label__regex I know it's possible to match a word and then reverse the matches using other tools (e.g. grep -v). However, is it possible to match lines that do not contain a specific word, e.g. hede, using a regular expression?  The notion that regex doesn't support inverse matching is not entirely true. You can mimic this behavior by using negative look-arounds: The regex above will match any string, or line without a line break, not containing the (sub)string 'hede'. As mentioned, this is not something regex is "good" at (or should do), but still, it is possible.  And if you need to match line break chars as well, use the DOT-ALL modifier (the trailing s in the following pattern): or use it inline: (where the /.../ are the regex delimiters, i.e., not part of the pattern) If the DOT-ALL modifier is not available, you can mimic the same behavior with the character class [\s\S]: A string is just a list of n characters. Before, and after each character, there's an empty string. So a list of n characters will have n+1 empty strings. Consider the string "ABhedeCD": where the e's are the empty strings. The regex (?!hede). looks ahead to see if there's no substring "hede" to be seen, and if that is the case (so something else is seen), then the . (dot) will match any character except a line break. Look-arounds are also called zero-width-assertions because they don't consume any characters. They only assert/validate something.  So, in my example, every empty string is first validated to see if there's no "hede" up ahead, before a character is consumed by the . (dot). The regex (?!hede). will do that only once, so it is wrapped in a group, and repeated zero or more times: ((?!hede).)*. Finally, the start- and end-of-input are anchored to make sure the entire input is consumed: ^((?!hede).)*$ As you can see, the input "ABhedeCD" will fail because on e3, the regex (?!hede) fails (there is "hede" up ahead!). Note that the solution to does not start with “hede”: is generally much more efficient than the solution to does not contain “hede”: The former checks for “hede” only at the input string’s first position, rather than at every position. If you're just using it for grep, you can use grep -v hede to get all lines which do not contain hede. ETA Oh, rereading the question, grep -v is probably what you meant by "tools options". Answer: Explanation: ^the beginning of the string, ( group and capture to \1 (0 or more times (matching the most amount possible)), (?! look ahead to see if there is not,   hede your string,   ) end of look-ahead,  . any character except \n, )* end of \1   (Note: because you are using a quantifier on this capture, only the LAST repetition of the captured pattern will be stored in \1) $ before an optional \n, and the end of the string The given answers are perfectly fine, just an academic point: Regular Expressions in the meaning of theoretical computer sciences ARE NOT ABLE do it like this. For them it had to look something like this: This only does a FULL match. Doing it for sub-matches would even be more awkward. If you want the regex test to only fail if the entire string matches, the following will work: e.g. -- If you want to allow all values except "foo" (i.e. "foofoo", "barfoo", and "foobar" will pass, but "foo" will fail), use: ^(?!foo$).* Of course, if you're checking for exact equality, a better general solution in this case is to check for string equality, i.e.  You could even put the negation outside the test if you need any regex features (here, case insensitivity and range matching): The regex solution at the top of this answer may be helpful, however, in situations where a positive regex test is required (perhaps by an API). FWIW, since regular languages (aka rational languages) are closed under complementation, it's always possible to find a regular expression (aka rational expression) that negates another expression. But not many tools implement this. Vcsn supports this operator (which it denotes {c}, postfix). You first define the type of your expressions: labels are letter (lal_char) to pick from a to z for instance (defining the alphabet when working with complementation is, of course, very important), and the "value" computed for each word is just a Boolean: true the word is accepted, false, rejected. In Python: then you enter your expression: convert this expression to an automaton:  finally, convert this automaton back to a simple expression. where + is usually denoted |, \e denotes the empty word, and [^] is usually written . (any character).  So, with a bit of rewriting ()|h(ed?)?|([^h]|h([^e]|e([^d]|d([^e]|e.)))).*. You can see this example here, and try Vcsn online there. Here's a good explanation of why it's not easy to negate an arbitrary regex. I have to agree with the other answers, though: if this is anything other than a hypothetical question, then a regex is not the right choice here. With negative lookahead, regular expression can match something not contains specific pattern. This is answered and explained by Bart Kiers. Great explanation! However, with Bart Kiers' answer, the lookahead part will test 1 to 4 characters ahead while matching any single character. We can avoid this and let the lookahead part check out the whole text, ensure there is no 'hede', and then the normal part (.*) can eat the whole text all at one time. Here is the improved regex: Note the (*?) lazy quantifier in the negative lookahead part is optional, you can use (*) greedy quantifier instead, depending on your data: if 'hede' does present and in the beginning half of the text, the lazy quantifier can be faster; otherwise, the greedy quantifier be faster. However if 'hede' does not present, both would be equal slow. Here is the demo code. For more information about lookahead, please check out the great article: Mastering Lookahead and Lookbehind. Also, please check out RegexGen.js, a JavaScript Regular Expression Generator that helps to construct complex regular expressions. With RegexGen.js, you can construct the regex in a more readable way: I decided to evaluate some of the presented Options and compare their performance, as well as use some new Features. Benchmarking on .NET Regex Engine: http://regexhero.net/tester/ The first 7 lines should not match, since they contain the searched Expression, while the lower 7 lines should match! Results are Iterations per second as the median of 3 runs - Bigger Number = Better Since .NET doesn't support action Verbs (*FAIL, etc.) I couldn't test the solutions P1 and P2. I tried to test most proposed solutions, some Optimizations are possible for certain words. For Example if the First two letters of the search string are not the Same, answer 03 can be expanded to ^(?>[^R]+|R+(?!egex Hero))*$ resulting in a small performance gain. But the overall most readable and performance-wise fastest solution seems to be 05 using a conditional statement or 04 with the possesive quantifier. I think the Perl solutions should be even faster and more easily readable. Not regex, but I've found it logical and useful to use serial greps with pipe to eliminate noise. eg.  search an apache config file without all the comments- and The logic of serial grep's is (not a comment) and (matches dir) with this, you avoid to test a lookahead on each positions: equivalent to (for .net): Old answer: Aforementioned (?:(?!hede).)* is great because it can be anchored. But the following would suffice in this case: This simplification is ready to have "AND" clauses added: Here's how I'd do it: Accurate and more efficient than the other answers. It implements Friedl's "unrolling-the-loop" efficiency technique and requires much less backtracking. An, in my opinon, more readable variant of the top answer:  Basically, "match at the beginning of the line if and only if it does not have 'hede' in it" - so the requirement translated almost directly into regex. Of course, it's possible to have multiple failure requirements: Details: The ^ anchor ensures the regex engine doesn't retry the match at every location in the string, which would match every string. The ^ anchor in the beginning is meant to represent the beginning of the line. The grep tool matches each line one at a time, in contexts where you're working with a multiline string, you can use the "m" flag: or If you want to match a character to negate a word similar to negate character class: For example, a string: Do not use: Use: Notice "(?!bbb)." is neither lookbehind nor lookahead, it's lookcurrent, for example: Since no one else has given a direct answer to the question that was asked, I'll do it. The answer is that with POSIX grep, it's impossible to literally satisfy this request: The reason is that POSIX grep is only required to work with Basic Regular Expressions, which are simply not powerful enough for accomplishing that task (they are not capable of parsing all regular languages, because of lack of alternation). However, GNU grep implements extensions that allow it. In particular, \| is the alternation operator in GNU's implementation of BREs. If your regular expression engine supports alternation, parentheses and the Kleene star, and is able to anchor to the beginning and end of the string, that's all you need for this approach. Note however that negative sets [^ ... ] are very convenient in addition to those, because otherwise, you need to replace them with an expression of the form (a|b|c| ... ) that lists every character that is not in the set, which is extremely tedious and overly long, even more so if the whole character set is Unicode. Thanks to formal language theory, we get to see how such an expression looks like. With GNU grep, the answer would be something like: (found with Grail and some further optimizations made by hand). You can also use a tool that implements Extended Regular Expressions, like egrep, to get rid of the backslashes: Here's a script to test it (note it generates a file testinput.txt in the current directory). Several of the expressions presented fail this test. In my system it prints: as expected. For those interested in the details, the technique employed is to convert the regular expression that matches the word into a finite automaton, then invert the automaton by changing every acceptance state to non-acceptance and vice versa, and then converting the resulting FA back to a regular expression. As everyone has noted, if your regular expression engine supports negative lookahead, the regular expression is much simpler. For example, with GNU grep: However, this approach has the disadvantage that it requires a backtracking regular expression engine. This makes it unsuitable in installations that are using secure regular expression engines like RE2, which is one reason to prefer the generated approach in some circumstances. Using Kendall Hopkins' excellent FormalTheory library, written in PHP, which provides a functionality similar to Grail, and a simplifier written by myself, I've been able to write an online generator of negative regular expressions given an input phrase (only alphanumeric and space characters currently supported): http://www.formauri.es/personal/pgimeno/misc/non-match-regex/ For hede it outputs: which is equivalent to the above. The OP did not specify or Tag the post to indicate the context (programming language, editor, tool) the Regex will be used within.   For me, I sometimes need to do this while editing a file using Textpad.   Textpad supports some Regex, but does not support lookahead or lookbehind, so it takes a few steps.   If I am looking to retain all lines that Do NOT contain the string hede, I would do it like this: 1. Search/replace the entire file to add a unique "Tag" to the beginning of each line containing any text.  2. Delete all lines that contain the string hede (replacement string is empty):     3. At this point, all remaining lines Do NOT contain the string hede. Remove the unique "Tag" from all lines (replacement string is empty):    Now you have the original text with all lines containing the string hede removed.  If I am looking to Do Something Else to only lines that Do NOT contain the string hede, I would do it like this: 1. Search/replace the entire file to add a unique "Tag" to the beginning of each line containing any text.  2. For all lines that contain the string hede, remove the unique "Tag":     3. At this point, all lines that begin with the unique "Tag", Do NOT contain the string hede. I can now do my Something Else to only those lines.  4. When I am done, I remove the unique "Tag" from all lines (replacement string is empty):    Since the introduction of ruby-2.4.1, we can use the new Absent Operator in Ruby’s Regular Expressions from the official doc Thus, in your case ^(?~hede)$ does the job for you Through PCRE verb (*SKIP)(*F) This would completely skips the line which contains the exact string hede and matches all the remaining lines. DEMO Execution of the parts: Let us consider the above regex by splitting it into two parts. Part before the | symbol. Part shouldn't be matched.  Part after the | symbol. Part should be matched.  PART 1  Regex engine will start its execution from the first part. Explanation: So the line which contains the string hede would be matched. Once the regex engine sees the following (*SKIP)(*F) (Note: You could write (*F) as (*FAIL)) verb, it skips and make the match to fail. | called alteration or logical OR operator added next to the PCRE verb which inturn matches all the boundaries exists between each and every character on all the lines except the line contains the exact string hede. See the demo here. That is, it tries to match the characters from the remaining string. Now the regex in the second part would be executed. PART 2 Explanation: .* In the Multiline mode, . would match any character except newline or carriage return characters. And * would repeat the previous character zero or more times. So .* would match the whole line. See the demo here. Hey why you added .* instead of .+ ? Because .* would match a blank line but .+ won't match a blank. We want to match all the lines except hede , there may be a possibility of blank lines also in the input . so you must use .* instead of .+ . .+ would repeat the previous character one or more times. See .* matches a blank line here. $ End of the line anchor is not necessary here. Another option is that to add a positive look-ahead and check if hede is anywhere in the input line, then we would negate that, with an expression similar to: with word boundaries. The expression is explained on the top right panel of regex101.com, if you wish to explore/simplify/modify it, and in this link, you can watch how it would match against some sample inputs, if you like. jex.im visualizes regular expressions:  It may be more maintainable to two regexes in your code, one to do the first match, and then if it matches run the second regex to check for outlier cases you wish to block for example ^.*(hede).* then have appropriate logic in your code. OK, I admit this is not really an answer to the posted question posted and it may also use slightly more processing than a single regex. But for developers who came here looking for a fast emergency fix for an outlier case then this solution should not be overlooked. The TXR Language supports regex negation. A more complicated example: match all lines that start with a and end with z, but do not contain the substring hede: Regex negation is not particularly useful on its own but when you also have intersection, things get interesting, since you have a full set of boolean set operations: you can express "the set which matches this, except for things which match that". The below function will help you get your desired output As long as you are dealing with lines, simply mark the negative matches and target the rest. In fact, I use this trick with sed because ^((?!hede).)*$ looks not supported by it. Mark the negative match: (e.g. lines with hede), using a character not included in the whole text at all. An emoji could probably be a good choice for this purpose. Target the rest (the unmarked strings: e.g. lines without hede). Suppose you want to keep only the target and delete the rest (as you want): Suppose you want to delete the target: Mark the negative match: (e.g. lines with hede), using a character not included in the whole text at all. An emoji could probably be a good choice for this purpose. Target the rest (the unmarked strings: e.g. lines without hede). Suppose you want to delete the target: Remove the mark: I wanted to add another example for if you are trying to match an entire line that contains string X, but does not also contain string Y. For example, let's say we want to check if our URL / string contains "tasty-treats", so long as it does not also contain "chocolate" anywhere. This regex pattern would work (works in JavaScript too) (global, multiline flags in example) Interactive Example: https://regexr.com/53gv4 (These urls contain "tasty-treats" and also do not contain "chocolate") (These urls contain "chocolate" somewhere - so they won't match even though they contain "tasty-treats") ^((?!hede).)*$ is an elegant solution, except since it consumes characters you won't be able to combine it with other criteria. For instance, say you wanted to check for the non-presence of "hede" and the presence of "haha." This solution would work because it won't consume characters: Here's a method that I haven't seen used before: First, it tries to find "hede" somewhere in the line. If successful, at this point, (*COMMIT) tells the engine to, not only not backtrack in the event of a failure, but also not to attempt any further matching in that case. Then, we try to match something that cannot possibly match (in this case, ^). If a line does not contain "hede" then the second alternative, an empty subpattern, successfully matches the subject string. This method is no more efficient than a negative lookahead, but I figured I'd just throw it on here in case someone finds it nifty and finds a use for it for other, more interesting applications. A simpler solution is to use the not operator ! Your if statement will need to match "contains" and not match "excludes".   I believe the designers of RegEx anticipated the use of not operators. Maybe you'll find this on Google while trying to write a regex that is able to match segments of a line (as opposed to entire lines) which do not contain a substring. Tooke me a while to figure out, so I'll share: Given a string:   <span class="good">bar</span><span class="bad">foo</span><span class="ugly">baz</span>  I want to match <span> tags which do not contain the substring "bad". /<span(?:(?!bad).)*?> will match <span class=\"good\"> and <span class=\"ugly\">. Notice that there are two sets (layers) of parentheses: Demo in Ruby:
__label__html __label__alignment __label__css __label__centering How can I horizontally center a <div> within another <div> using CSS? You can apply this CSS to the inner <div>: Of course, you don't have to set the width to 50%. Any width less than the containing <div> will work. The margin: 0 auto is what does the actual centering. If you are targeting Internet Explorer 8 (and later), it might be better to have this instead: It will make the inner element center horizontally and it works without setting a specific width. Working example here:   #inner {   display: table;   margin: 0 auto;   border: 1px solid black; }  #outer {   border: 1px solid red;   width:100% } <div id="outer">   <div id="inner">Foo foo</div> </div>    With flexbox it is very easy to style the div horizontally and vertically centered.   #inner {     border: 1px solid black; }  #outer {   border: 1px solid red;   width:100%;   display: flex;   justify-content: center; } <div id="outer">   <div id="inner">Foo foo</div> </div>    To align the div vertically centered, use the property align-items: center. If you don't want to set a fixed width on the inner div you could do something like this:   #outer {   width: 100%;   text-align: center; }  #inner {   display: inline-block; } <div id="outer">       <div id="inner">Foo foo</div> </div>    That makes the inner div into an inline element that can be centered with text-align. The best approaches are with CSS 3.   #outer {   width: 100%;   /* Firefox */   display: -moz-box;   -moz-box-pack: center;   -moz-box-align: center;   /* Safari and Chrome */   display: -webkit-box;   -webkit-box-pack: center;   -webkit-box-align: center;   /* W3C */   display: box;   box-pack: center;   box-align: center; }  #inner {   width: 50%; } <div id="outer">   <div id="inner">Foo foo</div> </div>    According to your usability you may also use the box-orient, box-flex, box-direction properties. Flex: Link 2 Link 3 Link 4 And this explains why the box model is the best approach: Suppose that your div is 200 pixels wide: Make sure the parent element is positioned, i.e., relative, fixed, absolute, or sticky. If you don't know the width of your div, you can use transform:translateX(-50%); instead of the negative margin. https://jsfiddle.net/gjvfxxdj/ With CSS calc(), the code can get even simpler:   The principle is still the same; put the item in the middle and compensate for the width. I've created this example to show how to vertically and horizontally align. The code is basically this: and... And it will stay in the center even when you resize your screen. Some posters have mentioned the CSS 3 way to center using display:box. This syntax is outdated and shouldn't be used anymore. [See also this post]. So just for completeness here is the latest way to center in CSS 3 using the Flexible Box Layout Module. So if you have simple markup like: ...and you want to center your items within the box, here's what you need on the parent element (.box):   .box {   display: flex;   flex-wrap: wrap;   /* Optional. only if you want the items to wrap */   justify-content: center;   /* For horizontal alignment */   align-items: center;   /* For vertical alignment */ } * {   margin: 0;   padding: 0; } html, body {   height: 100%; } .box {   height: 200px;   display: flex;   flex-wrap: wrap;   justify-content: center;   align-items: center;   border: 2px solid tomato; } .box div {   margin: 0 10px;   width: 100px; } .item1 {   height: 50px;   background: pink; } .item2 {   background: brown;   height: 100px; } .item3 {   height: 150px;   background: orange; } <div class="box">   <div class="item1">A</div>   <div class="item2">B</div>   <div class="item3">C</div> </div>    If you need to support older browsers which use older syntax for flexbox here's a good place to look. If you don't want to set a fixed width and don't want the extra margin, add display: inline-block to your element. You can use: Horizontally and vertically. It works with reasonably modern browsers (Firefox, Safari/WebKit, Chrome, Internet Explorer 10, Opera, etc.)   .content {   position: absolute;   left: 50%;   top: 50%;   -webkit-transform: translate(-50%, -50%);   transform: translate(-50%, -50%); } <div class="content">This works with any content</div>    Tinker with it further on Codepen or on JSBin. Set the width and set margin-left and margin-right to auto. That's for horizontal only, though. If you want both ways, you'd just do it both ways. Don't be afraid to experiment; it's not like you'll break anything. It cannot be centered if you don't give it a width. Otherwise, it will take, by default, the whole horizontal space. CSS 3's box-align property I recently had to center a "hidden" div (i.e., display:none;) that had a tabled form within it that needed to be centered on the page. I wrote the following jQuery code to display the hidden div and then update the CSS content to the automatic generated width of the table and change the margin to center it.  (The display toggle is triggered by clicking on a link, but this code wasn't necessary to display.) NOTE: I'm sharing this code, because Google brought me to this Stack Overflow solution and everything would have worked except that hidden elements don't have any width and can't be resized/centered until after they are displayed.   $(function(){   $('#inner').show().width($('#innerTable').width()).css('margin','0 auto'); }); <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <div id="inner" style="display:none;">   <form action="">     <table id="innerTable">       <tr><td>Name:</td><td><input type="text"></td></tr>       <tr><td>Email:</td><td><input type="text"></td></tr>       <tr><td>Email:</td><td><input type="submit"></td></tr>     </table>   </form> </div>    The way I usually do it is using absolute position: The outer div doesn't need any extra propertites for this to work. For Firefox and Chrome:   <div style="width:100%;">   <div style="width: 50%; margin: 0px auto;">Text</div> </div>    For Internet Explorer, Firefox, and Chrome:   <div style="width:100%; text-align:center;">   <div style="width: 50%; margin: 0px auto; text-align:left;">Text</div> </div>    The text-align: property is optional for modern browsers, but it is necessary in Internet Explorer Quirks Mode for legacy browsers support. Use:   #outerDiv {   width: 500px; }  #innerDiv {   width: 200px;   margin: 0 auto; } <div id="outerDiv">   <div id="innerDiv">Inner Content</div> </div>    Another solution for this without having to set a width for one of the elements is using the CSS 3 transform attribute. The trick is that translateX(-50%) sets the #inner element 50 percent to the left of its own width. You can use the same trick for vertical alignment. Here's a Fiddle showing horizontal and vertical alignment. More information is on Mozilla Developer Network. Chris Coyier who wrote an excellent post on 'Centering in the Unknown' on his blog. It's a roundup of multiple solutions. I posted one that isn't posted in this question. It has more browser support than the Flexbox solution, and you're not using display: table; which could break other things. I recently found an approach: Both elements must be the same width to function correctly. For example, see this link and the snippet below:   div#outer {   height: 120px;   background-color: red; }  div#inner {   width: 50%;   height: 100%;   background-color: green;   margin: 0 auto;   text-align: center; /* For text alignment to center horizontally. */   line-height: 120px; /* For text alignment to center vertically. */ } <div id="outer" style="width:100%;">   <div id="inner">Foo foo</div> </div>    If you have a lot of children under a parent, so your CSS content must be like this example on fiddle. The HTML content look likes this: Then see this example on fiddle. In my experience, the best way to center a box horizontally is to apply the following properties:   .container {   width: 100%;   height: 120px;   background: #CCC;   text-align: center; }  .centered-content {   display: inline-block;   background: #FFF;   padding: 20px;   border: 1px solid #000; } <div class="container">   <div class="centered-content">     Center this!   </div> </div>    See also this Fiddle! In my experience, the best way to center a box both vertically and horizontally is to use an additional container and apply the following properties:   .outer-container {   display: table;   width: 100%;   height: 120px;   background: #CCC; }  .inner-container {   display: table-cell;   vertical-align: middle;   text-align: center; }  .centered-content {   display: inline-block;   background: #FFF;   padding: 20px;   border: 1px solid #000; } <div class="outer-container">   <div class="inner-container">     <div class="centered-content">       Center this!     </div>   </div> </div>    See also this Fiddle! The easiest way:   #outer {   width: 100%;   text-align: center; } #inner {   margin: auto;   width: 200px; } <div id="outer">   <div id="inner">Blabla</div> </div>    If width of the content is unknown you can use the following method. Suppose we have these two elements: Suppose the computed width of the elements are 1000 pixels and 300 pixels respectively. Proceed as follows: Demo:   body {   font: medium sans-serif; }  .outer {   overflow: hidden;   background-color: papayawhip; }  .center-helper {   display: inline-block;   position: relative;   left: 50%;   background-color: burlywood; }  .inner {   display: inline-block;   position: relative;   left: -50%;   background-color: wheat; } <div class="outer">   <div class="center-helper">     <div class="inner">       <h1>A div with no defined width</h1>       <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.<br>           Duis condimentum sem non turpis consectetur blandit.<br>           Donec dictum risus id orci ornare tempor.<br>           Proin pharetra augue a lorem elementum molestie.<br>           Nunc nec justo sit amet nisi tempor viverra sit amet a ipsum.</p>     </div>   </div> </div>    Applying text-align: center the inline contents are centered within the line box. However since the inner div has by default width: 100% you have to set a specific width or use one of the following:   #inner {   display: inline-block; }  #outer {   text-align: center; } <div id="outer">   <div id="inner">Foo foo</div> </div>    Using margin: 0 auto is another option and it is more suitable for older browsers compatibility. It works together with display: table.   #inner {   display: table;   margin: 0 auto; } <div id="outer">   <div id="inner">Foo foo</div> </div>    display: flex behaves like a block element and lays out its content according to the flexbox model. It works with justify-content: center. Please note: Flexbox is compatible with most of the browsers but not all. See display: flex not working on Internet Explorer for a complete and up to date list of browsers compatibility.   #inner {   display: inline-block; }  #outer {   display: flex;   justify-content: center; } <div id="outer">   <div id="inner">Foo foo</div> </div>    transform: translate lets you modify the coordinate space of the CSS visual formatting model. Using it, elements can be translated, rotated, scaled, and skewed. To center horizontally it require position: absolute and left: 50%.   #inner {   position: absolute;   left: 50%;   transform: translate(-50%, 0%); } <div id="outer">   <div id="inner">Foo foo</div> </div>    The tag <center> is the HTML alternative to text-align: center. It works on older browsers and most of the new ones but it is not considered a good practice since this feature is obsolete and has been removed from the Web standards.   #inner {   display: inline-block; } <div id="outer">   <center>     <div id="inner">Foo foo</div>   </center> </div>    You can do something like this This will also align the #inner vertically. If you don't want to, remove the display and vertical-align properties; Here is what you want in the shortest way.  JSFIDDLE Flex have more than 97% browser support coverage and might be the best way to solve these kind of problems within few lines: You can use display: flex for your outer div and to horizontally center you have to add justify-content: center or you can visit w3schools - CSS flex Property for more ideas. This method also works just fine: For the inner <div>, the only condition is that its height and width must not be larger than the ones of its container. Well, I managed to find a solution that maybe will fit all situations, but uses JavaScript: Here's the structure: And here's the JavaScript snippet: If you want to use it in a responsive approach, you can add the following: One option existed that I found: Everybody says to use: But there is another option. Set this property for the parent div. It works perfectly anytime: And see, child go center. And finally CSS for you:
__label__url __label__http __label__urn __label__uri __label__rfc3986 People talk about URLs, URIs, and URNs as if they're different things, but they look the same to the naked eye. What are the distinguishable differences between them? From RFC 3986: A URI can be further classified as a locator, a name, or both.  The      term "Uniform Resource Locator" (URL) refers to the subset of URIs      that, in addition to identifying a resource, provide a means of      locating the resource by describing its primary access mechanism      (e.g., its network "location").  The term "Uniform Resource Name"      (URN) has been used historically to refer to both URIs under the      "urn" scheme [RFC2141], which are required to remain globally unique      and persistent even when the resource ceases to exist or becomes      unavailable, and to any other URI with the properties of a name. So all URLs are URIs (actually not quite - see below), and all URNs are URIs - but URNs and URLs are different, so you can't say that all URIs are URLs. EDIT: I had previously thought that all URLs are valid URIs, but as per comments: Not "all URLs are URIs". It depends on the interpretation of the RFC. For example in Java the URI parser does not like [ or ] and that's because the spec says "should not" and not "shall not". So that muddies the waters further, unfortunately. If you haven't already read Roger Pate's answer, I'd advise doing so as well. URIs identify and URLs locate; however, locators are also identifiers, so every URL is also a URI, but there are URIs which are not URLs. This is my name, which is an identifier. It is like a URI, but cannot be a URL, as it tells you nothing about my location or how to contact me. In this case it also happens to identify at least 5 other people in the USA alone. This is a locator, which is an identifier for that physical location. It is like both a URL and URI (since all URLs are URIs), and also identifies me indirectly as "resident of..". In this case it uniquely identifies me, but that would change if I get a roommate. I say "like" because these examples do not follow the required syntax. From Wikipedia: In computing, a Uniform Resource Locator (URL) is a subset of the Uniform Resource Identifier (URI) that specifies where an identified resource is available and the mechanism for retrieving it. In popular usage and in many technical documents and verbal discussions it is often incorrectly used as a synonym for URI, ... [emphasis mine] Because of this common confusion, many products and documentation incorrectly use one term instead of the other, assign their own distinction, or use them synonymously. My name, Roger Pate, could be like a URN (Uniform Resource Name), except those are much more regulated and intended to be unique across both space and time. Because I currently share this name with other people, it's not globally unique and would not be appropriate as a URN.  However, even if no other family used this name, I'm named after my paternal grandfather, so it still wouldn't be unique across time.  And even if that wasn't the case, the possibility of naming my descendants after me make this unsuitable as a URN. URNs are different from URLs in this rigid uniqueness constraint, even though they both share the syntax of URIs. URIs are a standard for identifying documents using a short string of numbers, letters, and symbols.   They are defined by RFC 3986 - Uniform Resource Identifier (URI): Generic Syntax.  URLs, URNs, and URCs are all types of URI.   Contains information about how to fetch a resource from its location.  For example: URLs always start with a protocol (http) and usually contain information such as the network host name (example.com) and often a document path (/foo/mypage.html). URLs may have query parameters and fragment identifiers. Identifies a resource by a unique and persistent name, but doesn't necessarily tell you how to locate it on the internet.   It usually starts with the prefix urn:  For example: URNs can identify ideas and concepts.  They are not restricted to identifying documents.   When a URN does represent a document, it can be translated into a URL by a "resolver".  The document can then be downloaded from the URL. Points to meta data about a document rather than to the document itself.   An example of a URC is one that points to the HTML source code of a page like: view-source:http://example.com/ Rather than locating it on the internet, or naming it, data can be placed directly into a URI.  An example would be data:,Hello%20World.   The W3 spec for HTML says that the href of an anchor tag can contain a URI, not just a URL.  You should be able to put in a URN such as <a href="urn:isbn:0451450523">.  Your browser would then resolve that URN to a URL and download the book for you. Not that I know of, but modern web browser do implement the data URI scheme. No.   Both relative and absolute URLs are URLs (and URIs.) No.   Both URLs with and without query parameters are URLs (and URIs.) No.   Both URLs with and without fragment identifiers are URLs (and URIs.) No. URLs are defined to be a strict subset of URIs.   If a parser allows a character in a URL but not in a URI, there is a bug in the parser.   The specs go into great detail about which characters are allowed in which parts of URLs and URIs.   Some characters may be allowed only in some parts of the URL, but characters alone are not a difference between URLs and URIs. Yes.  The W3C realized that there is a ton of confusion about this.  They issued a URI clarification document that says that it is now OK to use the terms URL and URI interchangeably (to mean URI).  It is no longer useful to strictly segment URIs into different types such as URL, URN, and URC. The definition of URN is now looser than what I stated above.  The latest RFC on URIs says that any URI can now be a URN (regardless of whether it starts with urn:) as long as it has "the properties of a name."  That is: It is globally unique and persistent even when the resource ceases to exist or becomes unavailable.  An example: The URIs used in HTML doctypes such as http://www.w3.org/TR/html4/strict.dtd.   That URI would continue to name the HTML4 transitional doctype even if the page on the w3.org website were deleted.  In summary: a URI identifies, a URL identifies and locates. Consider a specific edition of Shakespeare's play Romeo and Juliet, of which you have a digital copy on your home network. You could identify the text as urn:isbn:0-486-27557-4. That would be a URI, but more specifically a URN* because it names the text. You could also identify the text as file://hostname/sharename/RomeoAndJuliet.pdf. That would also be a URI, but more specifically a URL because it locates the text. *Uniform Resource Name (Note that my example is adapted from Wikipedia) These are some very well-written but long-winded answers. Here is the difference as far as CodeIgniter is concerned: URL - http://example.com/some/page.html URI - /some/page.html Put simply, URL is the full way to indentify any resource anywhere and can have different protocols like FTP, HTTP, SCP, etc. URI is a resource on the current domain, so it needs less information to be found. In every instance that CodeIgniter uses the word URL or URI this is the difference they are talking about, though in the grand-scheme of the web, it is not 100% correct. First of all get your mind out of confusion and take it simple and you will understand. URI => Uniform Resource Identifier Identifies a complete address of resource i-e location, name or both. URL => Uniform Resource Locator Identifies location of the resource. URN => Uniform Resource Name Identifies the name of the resource Example We have address https://www.google.com/folder/page.html where, URI(Uniform Resource Identifier) => https://www.google.com/folder/page.html URL(Uniform Resource Locator) => https://www.google.com/ URN(Uniform Resource Name) => /folder/page.html URI => (URL + URN) or URL only or URN only A small addition to the answers already posted, here's a Venn's diagram to sum up the theory (from Prateek Joshi's beautiful explanation):  And an example (also from Prateek's website):  Identity = Name with Location Every URL(Uniform Resource Locator) is a URI(Uniform Resource Identifier), abstractly speaking, but every URI is not a URL. There is another subcategory of URI is URN (Uniform Resource Name), which is a named resource but do not specify how to locate them, like mailto, news, ISBN is URIs.  Source  URN:  URL:  Analogy: To reach a person: Driving(protocol others SMS, email, phone), Address(hostname other phone-number, emailid) and person name(object name with a relative path). This is one of the most confusing and possibly irrelevant topics I've encountered as a web professional. As I understand it, a URI is a description of something, following an accepted format, that can define both or either the unique name (identification) of something or its location. There are two basic subsets: I tend to think of URNs as being similar to GUIDs.  They are simply a standardized methodology for providing unique names for things.  As in the namespace declarative that uses a company's name—it's not like there is a resource sitting on a server somewhere to correspond to that line of text—it simply uniquely identifies something. I also tend to completely avoid the term URI and discuss things only in terms of URL or URN as appropriate, because it causes so much confusion.  The question we should really try answering for people isn't so much the semantics, but how to identify when encountering the terms whether or not there is any practical difference in them that will change the approach to a programming situation.  For example, if someone corrects me in conversation and says, "oh, that's not a URL it's a URI" I know they're full of it.  If someone says, "we're using a URN to define the resource," I'm more likely to understand we are only naming it uniquely, not locating it on a server. If I'm way off base, please let me know! URI => http://en.wikipedia.org/wiki/Uniform_Resource_Identifier URL's are a subset of URI's (which also contain URNs). Basically, a URI is a general identifier, where a URL specifies a location and a URN specifies a name. Another example I like to use when thinking about URIs is the xmlns attribute of an XML document: In this case com.mycompany.mynode would be a URI that uniquely identifies the "myPrefix" namespace for all of the elements that use it within my XML document. This is NOT a URL because it is only used to identify, not to locate something per se. They're the same thing. A URI is a generalization of a URL. Originally, URIs were planned to be divided into URLs (addresses) and URNs (names) but then there was little difference between a URL and URI and http URIs were used as namespaces even though they didn't actually locate any resources. Due to difficulties to clearly distinguish between URI and URL, as far as I remember W3C does not make a difference any longer between URI and URL (http://www.w3.org/Addressing/).  URI, URL, URN As the image above indicates, there are three distinct components at play here. It’s usually best to go to the source when discussing matters like these, so here’s an exerpt from Tim Berners-Lee, et. al. in   RFC 3986: Uniform Resource Identifier (URI): Generic Syntax: A Uniform Resource Identifier (URI) is a compact sequence of   characters that identifies an abstract or physical resource. A URI can be further classified as a locator, a name, or both. The   term “Uniform Resource Locator” (URL) refers to the subset of URIs   that, in addition to identifying a resource, provide a means of   locating the resource by describing its primary access mechanism   (e.g., its network “location”). URI is kind of the super class of URL's and URN's. Wikipedia has a fine article about them with links to the right set of RFCs. Wikipedia will give all the information you need here. Quoting from http://en.wikipedia.org/wiki/URI: A URL is a URI that, in addition to identifying a resource, provides means of acting upon or obtaining a representation of the resource by describing its primary access mechanism or network "location". URL A URL is a specialization of URI that defines the network location of a specific resource. Unlike a URN, the URL defines how the resource can be obtained. We use URLs every day in the form of http://example.com etc. But a URL doesn't have to be an HTTP URL, it can be ftp://example.com etc., too. URI A URI identifies a resource either by location, or a name, or both. More often than not, most of us use URIs that defines a location to a resource. The fact that a URI can identify a resources by both name and location has lead to a lot of the confusion in my opinion. A URI has two specializations known as URL and URN. Difference between URL and URI A URI is an identifier for some resource, but a URL gives you specific information as to obtain that resource. A URI is a URL and as one commenter pointed out, it is now considered incorrect to use URL when describing applications. Generally, if the URL describes both the location and name of a resource, the term to use is URI. Since this is generally the case most of us encounter everyday, URI is the correct term. As per RFC 3986, URIs are comprised of the following pieces: The URI describes the protocol for accessing a resource (path) or application (query) on a server (authority).  All the URLs are URIs, and all the URNs are URIs, but all the URIs are not URLs. Please refer for more details: Wikipedia A URI identifies a resource either by location, or a name, or both. More often than not, most of us use URIs that defines a location to a resource. The fact that a URI can identify a resources by both name and location has lead to a lot of the confusion in my opinion. A URI has two specializations known as URL and URN. A URL is a specialization of URI that defines the network location of a specific resource. Unlike a URN, the URL defines how the resource can be obtained. We use URLs every day in the form of http://stackoverflow.com, etc. But a URL doesn’t have to be an HTTP URL, it can be ftp://example.com, etc. Although the terms URI and URL are strictly defined, many use the terms for other things than they are defined for. Let’s take Apache for example. If http://example.com/foo is requested from an Apache server, you’ll have the following environment variables set: With mod_rewrite enabled, you will also have these variables: This might be the reason for some of the confusion. See this document. Specifically,  a URL is a type of URI that identifies a resource via a representation of its primary access mechanism (e.g., its network "location"), rather than by some other attributes it may have. It's not an extremely clear term, really. After reading through the posts, I find some very relevant comments. In short, the confusion between the URL and URI definitions is based in part on which definition depends on which and also informal use of the word URI in software development. By definition URL is a subset of URI [RFC2396]. URI contain URN and URL. Both URI and URL each have their own specific syntax that confers upon them the status of being either URI or URL.  URN are for uniquely identifying a resource while URL are for locating a resource. Note that a resource can have more than one URL but only a single URN.[RFC2611] As web developers and programmers we will almost always be concerned with URL and therefore URI. Now a URL is specifically defined to have all the parts  scheme:scheme-specific-part, like for example https://stackoverflow.com/questions. This is a URL and it is also a URI. Now consider a relative link embedded in the page such as ../index.html. This is no longer a URL by definition. It is still what is referred to as a "URI-reference" [RFC2396].  I believe that when the word URI is used to refer to relative paths, "URI-reference" is actually what is being thought of. So informally, software systems use URI to refer to relative pathing and URL for the absolute address. So in this sense, a relative path is no longer a URL but still URI. Here is my simplification: URN: unique resource name, i.e. "what" (eg urn:issn:1234-5678 ). This is meant to be unique .. as in no two different docs can have the same urn. A bit like "uuid" URL: "where" to find it ( eg https://google.com/pub?issnid=1234-5678 .. or                               ftp://somesite.com/doc8.pdf ) URI: can be either a URN or a URL. This fuzzy definition is thanks to RFC 3986 produced by W3C and IETF. The definition of URI has changed over the years, so it makes sense for most people to be confused. However, you can now take solace in the fact that you can refer to http://somesite.com/something as either a URL or URI  ... an you will be right either way (at least fot the time being anyway...) I was wondering about the same thing and I've found this: http://docs.kohanaphp.com/helpers/url. You can see a clear example using the url::current() method. If you have this URL: http://example.com/kohana/index.php/welcome/home.html?query=string then using url:current() gives you the URI which, according to the documentation, is: welcome/home URIs came about from the need to identify resources on the Web, and other Internet resources such as electronic mailboxes in a uniform and coherent way. So, one can introduce a new type of widget: URIs to identify widget resources or use tel: URIs to have web links cause telephone calls to be made when invoked.   Some URIs provide information to locate a resource (such as a DNS host name and a path on that machine), while some are used as pure resource names. The URL is reserved for identifiers that are resource locators, including 'http' URLs such as http://stackoverflow.com, which identifies the web page at the given path on the host. Another example is 'mailto' URLs, such as mailto:fred@mail.org, which identifies the mailbox at the given address. URNs are URIs that are used as pure resource names rather than locators. For example, the URI: mid:0E4FC272-5C02-11D9-B115-000A95B55BC8@stackoverflow.com is a URN that identifies the email message containing it in its 'Message-Id' field. The URI serves to distinguish that message from any other email message. But it does not itself provide the message's address in any store.  In order to answer this I'll lean on an answer I modified to another question. A good example of a URI is how you identify an Amazon S3 resource. Let's take: s3://www-example-com/index.html [fig. 1] which I created as a cached copy of http://www.example.com/index.html [fig. 2] in Amazon's S3-US-West-2 datacenter. Even if StackOverflow would allow me to hyperlink to the s3:// protocol scheme, it wouldn't do you any good in locating the resource. Because it Identifies a Resource, fig. 1 is a valid URI. It is also a valid URN, because Amazon requires that the bucket (their term for the authority portion of the URI) be unique across datacenters. It is helpful in locating it, but it does not indicate the datacenter. Therefore it does not work as a URL. So, how do URI, URL, and URN differ in this case? NOTE: RFC 3986 defines URIs as scheme://authority/path?query#fragment Easy to explain: Lets assume the following URI is your Name URL is your address with your name in-order to communicate with you. my name is Loyola Loyola is URI  my address is TN, Chennai 600001. TN, Chennai 600 001, Loyola is URL Hope you understand,  Now lets see a precise example http://www.google.com/fistpage.html in the above you can communicate with a page called firstpage.html (URI) using following http://www.google.com/fistpage.html(URL). Hence URI is subset of URL but not vice-versa.  The best (technical) summary imo is this one IRI, URI, URL, URN and their differences from Jan Martin Keil: Everybody dealing with the Semantic Web repeatedly comes across the terms IRI, URI, URL and URN. Nevertheless, I frequently observe that there is some confusion about their exact meaning. And, of course, others noticed that as well (see e.g. RFC3305 or search on Google). To be honest, I even was confused myself at the outset. But actually the issue is not that complex. Let’s have a look on the definitions of the mentioned terms to see what the differences are: A Uniform Resource Identifier is a compact sequence of characters that identifies an abstract or physical resource. The set of characters is limited to US-ASCII excluding some reserved characters. Characters outside the set of allowed characters can be represented using Percent-Encoding. A URI can be used as a locator, a name, or both. If a URI is a locator, it describes a resource’s primary access mechanism. If a URI is a name, it identifies a resource by giving it a unique name. The exact specifications of syntax and semantics of a URI depend on the used Scheme that is defined by the characters before the first colon. [RFC3986] A Uniform Resource Name is a URI in the scheme urn intended to serve as persistent, location-independent, resource identifier. Historically, the term also referred to any URI. [RFC3986] A URN consists of a Namespace Identifier (NID) and a Namespace Specific String (NSS): urn:: The syntax and semantics of the NSS is specific specific for each NID. Beside the registered NIDs, there exist several more NIDs, that did not go through the official registration process. [RFC2141] A Uniform Resource Locator is a URI that, in addition to identifying a resource, provides a means of locating the resource by describing its primary access mechanism [RFC3986]. As there is no exact definition of URL by means of a set of Schemes, "URL is a useful but informal concept", usually referring to a subset of URIs that do not contain URNs [RFC3305]. An Internationalized Resource Identifier is defined similarly to a URI, but the character set is extended to the Universal Coded Character Set. Therefore, it can contain any Latin and non Latin characters except the reserved characters. Instead of extending the definition of URI, the term IRI was introduced to allow for a clear distinction and avoid incompatibilities. IRIs are meant to replace URIs in identifying resources in situations where the Universal Coded Character Set is supported. By definition, every URI is an IRI. Furthermore, there is a defined surjective mapping of IRIs to URIs: Every IRI can be mapped to exactly one URI, but different IRIs might map to the same URI. Therefore, the conversion back from a URI to an IRI may not produce the original IRI. [RFC3987] RDF explicitly allows to use IRIs to name entities [RFC3987]. This means that we can use almost every character in entity names. On the other hand, we often have to deal with early state software. Thus, it is not unlikely to run into problems using non ASCII characters. Therefore, I suggest to avoid non URI names for entities and recommend to use http URIs [LINKED-DATA]. To put it briefly: only use URLs to name your entities. Of course, we can refer to existing entities named by a URN. However, we should avoid to newly create this kind of identifiers. I found: A uniform resource identifier(URI) represents something of a big picture. You can split URIs/ URIs can be classified as locators (uniform resource locators- URL), or as names (uniform resource name-URN), or either both. So basically, a URN functions like a person's name and the URL depicts that person's address. So long story short, a URN defines an item's identity, while the URL provides defines the method for finding it, finally encapsulating these two concepts is the URI The answer is ambiguous. In Java it is frequently used in this way: An Uniform Resource Locator (URL) is the term used to identify an Internet resource including the scheme( http, https, ftp, news, etc.). For instance What is the difference between a URI, a URL and a URN? An Uniform Resource Identifier (URI) is used to identify a single document in the Web Server: For instance /questions/176264/whats-the-difference-between-a-uri-and-a-url In Java servlets, the URI frequently refers to the document without the web application context.
__label__javascript __label__angularjs __label__jquery Suppose I'm familiar with developing client-side applications in jQuery, but now I'd like to start using AngularJS. Can you describe the paradigm shift that is necessary? Here are a few questions that might help you frame an answer: I'm not looking for a detailed comparison between jQuery and AngularJS. In jQuery, you design a page, and then you make it dynamic. This is because jQuery was designed for augmentation and has grown incredibly from that simple premise. But in AngularJS, you must start from the ground up with your architecture in mind. Instead of starting by thinking "I have this piece of the DOM and I want to make it do X", you have to start with what you want to accomplish, then go about designing your application, and then finally go about designing your view. Similarly, don't start with the idea that jQuery does X, Y, and Z, so I'll just add AngularJS on top of that for models and controllers. This is really tempting when you're just starting out, which is why I always recommend that new AngularJS developers don't use jQuery at all, at least until they get used to doing things the "Angular Way". I've seen many developers here and on the mailing list create these elaborate solutions with jQuery plugins of 150 or 200 lines of code that they then glue into AngularJS with a collection of callbacks and $applys that are confusing and convoluted; but they eventually get it working! The problem is that in most cases that jQuery plugin could be rewritten in AngularJS in a fraction of the code, where suddenly everything becomes comprehensible and straightforward. The bottom line is this: when solutioning, first "think in AngularJS"; if you can't think of a solution, ask the community; if after all of that there is no easy solution, then feel free to reach for the jQuery. But don't let jQuery become a crutch or you'll never master AngularJS. First know that single-page applications are applications. They're not webpages. So we need to think like a server-side developer in addition to thinking like a client-side developer. We have to think about how to divide our application into individual, extensible, testable components. So then how do you do that? How do you "think in AngularJS"? Here are some general principles, contrasted with jQuery. In jQuery, we programmatically change the view. We could have a dropdown menu defined as a ul like so: In jQuery, in our application logic, we would activate it with something like: When we just look at the view, it's not immediately obvious that there is any functionality here. For small applications, that's fine. But for non-trivial applications, things quickly get confusing and hard to maintain. In AngularJS, though, the view is the official record of view-based functionality. Our ul declaration would look like this instead: These two do the same thing, but in the AngularJS version anyone looking at the template knows what's supposed to happen. Whenever a new member of the development team comes on board, she can look at this and then know that there is a directive called dropdownMenu operating on it; she doesn't need to intuit the right answer or sift through any code. The view told us what was supposed to happen. Much cleaner. Developers new to AngularJS often ask a question like: how do I find all links of a specific kind and add a directive onto them. The developer is always flabbergasted when we reply: you don't. But the reason you don't do that is that this is like half-jQuery, half-AngularJS, and no good. The problem here is that the developer is trying to "do jQuery" in the context of AngularJS. That's never going to work well. The view is the official record. Outside of a directive (more on this below), you never, ever, never change the DOM. And directives are applied in the view, so intent is clear. Remember: don't design, and then mark up. You must architect, and then design. This is by far one of the most awesome features of AngularJS and cuts out a lot of the need to do the kinds of DOM manipulations I mentioned in the previous section. AngularJS will automatically update your view so you don't have to! In jQuery, we respond to events and then update content. Something like: For a view that looks like this: Apart from mixing concerns, we also have the same problems of signifying intent that I mentioned before. But more importantly, we had to manually reference and update a DOM node. And if we want to delete a log entry, we have to code against the DOM for that too. How do we test the logic apart from the DOM? And what if we want to change the presentation? This a little messy and a trifle frail. But in AngularJS, we can do this: And our view can look like this: But for that matter, our view could look like this: And now instead of using an unordered list, we're using Bootstrap alert boxes. And we never had to change the controller code! But more importantly, no matter where or how the log gets updated, the view will change too. Automatically. Neat! Though I didn't show it here, the data binding is two-way. So those log messages could also be editable in the view just by doing this: <input ng-model="entry.msg" />. And there was much rejoicing. In jQuery, the DOM is kind of like the model. But in AngularJS, we have a separate model layer that we can manage in any way we want, completely independently from the view. This helps for the above data binding, maintains separation of concerns, and introduces far greater testability. Other answers mentioned this point, so I'll just leave it at that. And all of the above tie into this over-arching theme: keep your concerns separate. Your view acts as the official record of what is supposed to happen (for the most part); your model represents your data; you have a service layer to perform reusable tasks; you do DOM manipulation and augment your view with directives; and you glue it all together with controllers. This was also mentioned in other answers, and the only thing I would add pertains to testability, which I discuss in another section below. To help us out with separation of concerns is dependency injection (DI). If you come from a server-side language (from Java to PHP) you're probably familiar with this concept already, but if you're a client-side guy coming from jQuery, this concept can seem anything from silly to superfluous to hipster. But it's not. :-) From a broad perspective, DI means that you can declare components very freely and then from any other component, just ask for an instance of it and it will be granted. You don't have to know about loading order, or file locations, or anything like that. The power may not immediately be visible, but I'll provide just one (common) example: testing. Let's say in our application, we require a service that implements server-side storage through a REST API and, depending on application state, local storage as well. When running tests on our controllers, we don't want to have to communicate with the server - we're testing the controller, after all. We can just add a mock service of the same name as our original component, and the injector will ensure that our controller gets the fake one automatically - our controller doesn't and needn't know the difference. Speaking of testing... This is really part of section 3 on architecture, but it's so important that I'm putting it as its own top-level section. Out of all of the many jQuery plugins you've seen, used, or written, how many of them had an accompanying test suite? Not very many because jQuery isn't very amenable to that. But AngularJS is. In jQuery, the only way to test is often to create the component independently with a sample/demo page against which our tests can perform DOM manipulation. So then we have to develop a component separately and then integrate it into our application. How inconvenient! So much of the time, when developing with jQuery, we opt for iterative instead of test-driven development. And who could blame us? But because we have separation of concerns, we can do test-driven development iteratively in AngularJS! For example, let's say we want a super-simple directive to indicate in our menu what our current route is. We can declare what we want in the view of our application: Okay, now we can write a test for the non-existent when-active directive: And when we run our test, we can confirm that it fails. Only now should we create our directive: Our test now passes and our menu performs as requested. Our development is both iterative and test-driven. Wicked-cool. You'll often hear "only do DOM manipulation in a directive". This is a necessity. Treat it with due deference! But let's dive a little deeper... Some directives just decorate what's already in the view (think ngClass) and therefore sometimes do DOM manipulation straight away and then are basically done. But if a directive is like a "widget" and has a template, it should also respect separation of concerns. That is, the template too should remain largely independent from its implementation in the link and controller functions. AngularJS comes with an entire set of tools to make this very easy; with ngClass we can dynamically update the class; ngModel allows two-way data binding; ngShow and ngHide programmatically show or hide an element; and many more - including the ones we write ourselves. In other words, we can do all kinds of awesomeness without DOM manipulation. The less DOM manipulation, the easier directives are to test, the easier they are to style, the easier they are to change in the future, and the more re-usable and distributable they are. I see lots of developers new to AngularJS using directives as the place to throw a bunch of jQuery. In other words, they think "since I can't do DOM manipulation in the controller, I'll take that code put it in a directive". While that certainly is much better, it's often still wrong. Think of the logger we programmed in section 3. Even if we put that in a directive, we still want to do it the "Angular Way". It still doesn't take any DOM manipulation! There are lots of times when DOM manipulation is necessary, but it's a lot rarer than you think! Before doing DOM manipulation anywhere in your application, ask yourself if you really need to. There might be a better way. Here's a quick example that shows the pattern I see most frequently. We want a toggleable button. (Note: this example is a little contrived and a skosh verbose to represent more complicated cases that are solved in exactly the same way.) There are a few things wrong with this: This directive can be rewritten (even for very complicated cases!) much more simply like so: Again, the template stuff is in the template, so you (or your users) can easily swap it out for one that meets any style necessary, and the logic never had to be touched. Reusability - boom! And there are still all those other benefits, like testing - it's easy! No matter what's in the template, the directive's internal API is never touched, so refactoring is easy. You can change the template as much as you want without touching the directive. And no matter what you change, your tests still pass. w00t! So if directives aren't just collections of jQuery-like functions, what are they? Directives are actually extensions of HTML. If HTML doesn't do something you need it to do, you write a directive to do it for you, and then use it just as if it was part of HTML. Put another way, if AngularJS doesn't do something out of the box, think how the team would accomplish it to fit right in with ngClick, ngClass, et al. Don't even use jQuery. Don't even include it. It will hold you back. And when you come to a problem that you think you know how to solve in jQuery already, before you reach for the $, try to think about how to do it within the confines the AngularJS. If you don't know, ask! 19 times out of 20, the best way to do it doesn't need jQuery and to try to solve it with jQuery results in more work for you. In jQuery, selectors are used to find DOM elements and then bind/register event handlers to them. When an event triggers, that (imperative) code executes to update/change the DOM. In AngularJS, you want to think about views rather than DOM elements. Views are (declarative) HTML that contain AngularJS directives. Directives set up the event handlers behind the scenes for us and give us dynamic databinding. Selectors are rarely used, so the need for IDs (and some types of classes) is greatly diminished. Views are tied to models (via scopes). Views are a projection of the model. Events change models (that is, data, scope properties), and the views that project those models update "automatically." In AngularJS, think about models, rather than jQuery-selected DOM elements that hold your data. Think about views as projections of those models, rather than registering callbacks to manipulate what the user sees. jQuery employs unobtrusive JavaScript - behavior (JavaScript) is separated from the structure (HTML). AngularJS uses controllers and directives (each of which can have their own controller, and/or compile and linking functions) to remove behavior from the view/structure (HTML).  Angular also has services and filters to help separate/organize your application. See also https://stackoverflow.com/a/14346528/215945 One approach to designing an AngularJS application: You can do a lot with jQuery without knowing about how JavaScript prototypal inheritance works. When developing AngularJS applications, you will avoid some common pitfalls if you have a good understanding of JavaScript inheritance. Recommended reading: What are the nuances of scope prototypal / prototypical inheritance in AngularJS? AngularJS and jQuery adopt very different ideologies. If you're coming from jQuery you may find some of the differences surprising. Angular may make you angry. This is normal, you should push through. Angular is worth it. jQuery gives you a toolkit for selecting arbitrary bits of the DOM and making ad-hoc changes to them. You can do pretty much anything you like piece by piece. AngularJS instead gives you a compiler. What this means is that AngularJS reads your entire DOM from top to bottom and treats it as code, literally as instructions to the compiler. As it traverses the DOM, It looks for specific directives (compiler directives) that tell the AngularJS compiler how to behave and what to do. Directives are little objects full of JavaScript which can match against attributes, tags, classes or even comments.  When the Angular compiler determines that a piece of the DOM matches a particular directive, it calls the directive function, passing it the DOM element, any attributes, the current $scope (which is a local variable store), and some other useful bits. These attributes may contain expressions which can be interpreted by the Directive, and which tell it how to render, and when it should redraw itself. Directives can then in turn pull in additional Angular components such as controllers, services, etc. What comes out the bottom of the compiler is a fully formed web application, wired up and ready to go. This means that Angular is Template Driven. Your template drives the JavaScript, not the other way around. This is a radical reversal of roles, and the complete opposite of the unobtrusive JavaScript we have been writing for the last 10 years or so. This can take some getting used to. If this sounds like it might be over-prescriptive and limiting, nothing could be farther from the truth. Because AngularJS treats your HTML as code, you get HTML level granularity in your web application. Everything is possible, and most things are surprisingly easy once you make a few conceptual leaps. Let's get down to the nitty gritty. Angular and jQuery do different things. AngularJS gives you a set of tools to produce web applications. jQuery mainly gives you tools for modifying the DOM. If jQuery is present on your page, AngularJS will use it automatically. If it isn't, AngularJS ships with jQuery Lite, which is a cut down, but still perfectly usable version of jQuery. Misko likes jQuery and doesn't object to you using it. However you will find as you advance that you can get a pretty much all of your work done using a combination of scope, templates and directives, and you should prefer this workflow where possible because your code will be more discrete, more configurable, and more Angular. If you do use jQuery, you shouldn't be sprinkling it all over the place. The correct place for DOM manipulation in AngularJS is in a directive. More on these later. jQuery is typically applied unobtrusively. Your JavaScript code is linked in the header (or the footer), and this is the only place it is mentioned. We use selectors to pick out bits of the page and write plugins to modify those parts. The JavaScript is in control. The HTML has a completely independent existence. Your HTML remains semantic even without JavaScript. Onclick attributes are very bad practice. One of the first things your will notice about AngularJS is that custom attributes are everywhere. Your HTML will be littered with ng attributes, which are essentially onClick attributes on steroids. These are directives (compiler directives), and are one of the main ways in which the template is hooked to the model. When you first see this you might be tempted to write AngularJS off as old school intrusive JavaScript (like I did at first). In fact, AngularJS does not play by those rules. In AngularJS, your HTML5 is a template. It is compiled by AngularJS to produce your web page. This is the first big difference. To jQuery, your web page is a DOM to be manipulated. To AngularJS, your HTML is code to be compiled. AngularJS reads in your whole web page and literally compiles it into a new web page using its built in compiler. Your template should be declarative; its meaning should be clear simply by reading it. We use custom attributes with meaningful names. We make up new HTML elements, again with meaningful names. A designer with minimal HTML knowledge and no coding skill can read your AngularJS template and understand what it is doing. He or she can make modifications. This is the Angular way. One of the first questions I asked myself when starting AngularJS and running through the tutorials is "Where is my code?". I've written no JavaScript, and yet I have all this behaviour. The answer is obvious. Because AngularJS compiles the DOM, AngularJS is treating your HTML as code. For many simple cases it's often sufficient to just write a template and let AngularJS compile it into an application for you. Your template drives your application. It's treated as a DSL. You write AngularJS components, and AngularJS will take care of pulling them in and making them available at the right time based on the structure of your template. This is very different to a standard MVC pattern, where the template is just for output. It's more similar to XSLT than Ruby on Rails for example. This is a radical inversion of control that takes some getting used to. Stop trying to drive your application from your JavaScript. Let the template drive the application, and let AngularJS take care of wiring the components together. This also is the Angular way. With jQuery your HTML page should contain semantic meaningful content. If the JavaScript is turned off (by a user or search engine) your content remains accessible. Because AngularJS treats your HTML page as a template. The template is not supposed to be semantic as your content is typically stored in your model which ultimately comes from your API. AngularJS compiles your DOM with the model to produce a semantic web page. Your HTML source is no longer semantic, instead, your API and compiled DOM are semantic. In AngularJS, meaning lives in the model, the HTML is just a template, for display only. At this point you likely have all sorts of questions concerning SEO and accessibility, and rightly so. There are open issues here. Most screen readers will now parse JavaScript. Search engines can also index AJAXed content. Nevertheless, you will want to make sure you are using pushstate URLs and you have a decent sitemap. See here for a discussion of the issue: https://stackoverflow.com/a/23245379/687677 Separation of concerns (SOC) is a pattern that grew up over many years of web development for a variety of reasons including SEO, accessibility and browser incompatibility. It looks like this: Again, AngularJS does not play by their rules. In a stroke, AngularJS does away with a decade of received wisdom and instead implements an MVC pattern in which the template is no longer semantic, not even a little bit. It looks like this: MVC and SOC are not on opposite ends of the same scale, they are on completely different axes. SOC makes no sense in an AngularJS context. You have to forget it and move on. If, like me, you lived through the browser wars, you might find this idea quite offensive. Get over it, it'll be worth it, I promise. Plugins extend jQuery. AngularJS Directives extend the capabilities of your browser. In jQuery we define plugins by adding functions to the jQuery.prototype. We then hook these into the DOM by selecting elements and calling the plugin on the result. The idea is to extend the capabilities of jQuery. For example, if you want a carousel on your page, you might define an unordered list of figures, perhaps wrapped in a nav element. You might then write some jQuery to select the list on the page and restyle it as a gallery with timeouts to do the sliding animation. In AngularJS, we define directives. A directive is a function which returns a JSON object. This object tells AngularJS what DOM elements to look for, and what changes to make to them. Directives are hooked in to the template using either attributes or elements, which you invent. The idea is to extend the capabilities of HTML with new attributes and elements. The AngularJS way is to extend the capabilities of native looking HTML. You should write HTML that looks like HTML, extended with custom attributes and elements. If you want a carousel, just use a <carousel /> element, then define a directive to pull in a template, and make that sucker work. The tendency with jQuery is to write great big plugins like lightbox which we then configure by passing in numerous values and options. This is a mistake in AngularJS. Take the example of a dropdown. When writing a dropdown plugin you might be tempted to code in click handlers, perhaps a function to add in a chevron which is either up or down, perhaps change the class of the unfolded element, show hide the menu, all helpful stuff. Until you want to make a small change. Say you have a menu that you want to unfold on hover. Well now we have a problem. Our plugin has wired in our click handler for us, we're going to need to add a configuration option to make it behave differently in this specific case. In AngularJS we write smaller directives. Our dropdown directive would be ridiculously small. It might maintain the folded state, and provide methods to fold(), unfold() or toggle(). These methods would simply update $scope.menu.visible which is a boolean holding the state. Now in our template we can wire this up: Need to update on mouseover? The template drives the application so we get HTML level granularity. If we want to make case by case exceptions, the template makes this easy. JQuery plugins are created in a closure. Privacy is maintained within that closure. It's up to you to maintain your scope chain within that closure. You only really have access to the set of DOM nodes passed in to the plugin by jQuery, plus any local variables defined in the closure and any globals you have defined. This means that plugins are quite self contained. This is a good thing, but can get restrictive when creating a whole application. Trying to pass data between sections of a dynamic page becomes a chore. AngularJS has $scope objects. These are special objects created and maintained by AngularJS in which you store your model. Certain directives will spawn a new $scope, which by default inherits from its wrapping $scope using JavaScript prototypical inheritance. The $scope object is accessible in the controller and the view. This is the clever part. Because the structure of $scope inheritance roughly follows the structure of the DOM, elements have access to their own scope, and any containing scopes seamlessly, all the way up to the global $scope (which is not the same as the global scope). This makes it much easier to pass data around, and to store data at an appropriate level. If a dropdown is unfolded, only the dropdown $scope needs to know about it. If the user updates their preferences, you might want to update the global $scope, and any nested scopes listening to the user preferences would automatically be alerted. This might sound complicated, in fact, once you relax into it, it's like flying. You don't need to create the $scope object, AngularJS instantiates and configures it for you, correctly and appropriately based on your template hierarchy. AngularJS then makes it available to your component using the magic of dependency injection (more on this later). In jQuery you make all your DOM changes by hand. You construct new DOM elements programatically. If you have a JSON array and you want to put it to the DOM, you must write a function to generate the HTML and insert it. In AngularJS you can do this too, but you are encouraged to make use of data binding. Change your model, and because the DOM is bound to it via a template your DOM will automatically update, no intervention required. Because data binding is done from the template, using either an attribute or the curly brace syntax, it's super easy to do. There's little cognitive overhead associated with it so you'll find yourself doing it all the time. Binds the input element to $scope.user.name. Updating the input will update the value in your current scope, and vice-versa. Likewise: will output the user name in a paragraph. It's a live binding, so if the $scope.user.name value is updated, the template will update too. In jQuery making an Ajax call is fairly simple, but it's still something you might think twice about. There's the added complexity to think about, and a fair chunk of script to maintain. In AngularJS, Ajax is your default go-to solution and it happens all the time, almost without you noticing. You can include templates with ng-include. You can apply a template with the simplest custom directive. You can wrap an Ajax call in a service and create yourself a GitHub service, or a Flickr service, which you can access with astonishing ease. In jQuery, if we want to accomplish a small non-dom related task such as pulling a feed from an API, we might write a little function to do that in our closure. That's a valid solution, but what if we  want to access that feed often? What if we want to reuse that code in another application? AngularJS gives us service objects. Services are simple objects that contain functions and data. They are always singletons, meaning there can never be more than one of them. Say we want to access the Stack Overflow API, we might write a StackOverflowService which defines methods for doing so. Let's say we have a shopping cart. We might define a ShoppingCartService which maintains our cart and contains methods for adding and removing items. Because the service is a singleton, and is shared by all other components, any object that needs to can write to the shopping cart and pull data from it. It's always the same cart. Service objects are self-contained AngularJS components which we can use and reuse as we see fit. They are simple JSON objects containing functions and Data. They are always singletons, so if you store data on a service in one place, you can get that data out somewhere else just by requesting the same service. AngularJS manages your dependencies for you. If you want an object, simply refer to it and AngularJS will get it for you. Until you start to use this, it's hard to explain just what a massive time boon this is. Nothing like AngularJS DI exists inside jQuery. DI means that instead of writing your application and wiring it together, you instead define a library of components, each identified by a string. Say I have a component called 'FlickrService' which defines methods for pulling JSON feeds from Flickr. Now, if I want to write a controller that can access Flickr, I just need to refer to the 'FlickrService' by name when I declare the controller. AngularJS will take care of instantiating the component and making it available to my controller. For example, here I define a service: Now when I want to use that service I just refer to it by name like this: AngularJS will recognise that a FlickrService object is needed to instantiate the controller, and will provide one for us. This makes wiring things together very easy, and pretty much eliminates any tendency towards spagettification. We have a flat list of components, and AngularJS hands them to us one by one as and when we need them. jQuery says very little about how you should organise your code. AngularJS has opinions. AngularJS gives you modules into which you can place your code. If you're writing a script that talks to Flickr for example, you might want to create a Flickr module to wrap all your Flickr related functions in. Modules can include other modules (DI). Your main application is usually a module, and this should include all the other modules your application will depend on. You get simple code reuse, if you want to write another application based on Flickr, you can just include the Flickr module and voila, you have access to all your Flickr related functions in your new application. Modules contain AngularJS components. When we include a module, all the components in that module become available to us as a simple list identified by their unique strings. We can then inject those components into each other using AngularJS's dependency injection mechanism. AngularJS and jQuery are not enemies. It's possible to use jQuery within AngularJS very nicely. If you're using AngularJS well (templates, data-binding, $scope, directives, etc.) you will find you need a lot less jQuery than you might otherwise require. The main thing to realise is that your template drives your application. Stop trying to write big plugins that do everything. Instead write little directives that do one thing, then write a simple template to wire them together. Think less about unobtrusive JavaScript, and instead think in terms of HTML extensions. I got so excited about AngularJS, I wrote a short book on it which you're very welcome to read online http://nicholasjohnson.com/angular-book/. I hope it's helpful. Can you describe the paradigm shift that is necessary? Imperative vs Declarative With jQuery you tell the DOM what needs to happen, step by step. With AngularJS you describe what results you want but not how to do it. More on this here. Also, check out Mark Rajcok's answer. How do I architect and design client-side web apps differently?  AngularJS is an entire client-side framework that uses the MVC pattern (check out their graphical representation). It greatly focuses on separation of concerns. What is the biggest difference? What should I stop doing/using; what should I start doing/using instead? jQuery is a library  AngularJS is a beautiful client-side framework, highly testable, that combines tons of cool stuff such as MVC, dependency injection, data binding and much more.  It focuses on separation of concerns and testing (unit testing and end-to-end testing), which facilitates test-driven development. The best way to start is going through their awesome tutorial. You can go through the steps in a couple of hours; however, in case you want to master the concepts behind the scenes, they include a myriad of reference for further reading. Are there any server-side considerations/restrictions? You may use it on existing applications where you are already using pure jQuery. However, if you want to fully take advantage of the AngularJS features you may consider coding the server side using a RESTful approach. Doing so will allow you to leverage their resource factory, which creates an abstraction of your server side RESTful API and makes server-side calls (get, save, delete, etc.) incredibly easy. To describe the "paradigm shift", I think a short answer can suffice. In jQuery, you typically  use selectors to find elements, and then wire them up: $('#id .class').click(doStuff); In AngularJS, you use directives to mark the elements directly, to wire them up: <a ng-click="doStuff()"> AngularJS doesn't need (or want) you to find elements using selectors - the primary difference between AngularJS's jqLite versus full-blown jQuery is that jqLite does not support selectors.   So when people say "don't include jQuery at all", it's mainly because they don't want you to use selectors; they want you to learn to use directives instead. Direct, not select! jQuery makes ridiculously long JavaScript commands like getElementByHerpDerp shorter and cross-browser. AngularJS allows you to make your own HTML tags/attributes that do things which work well with dynamic web applications (since HTML was designed for static pages). Saying "I have a jQuery background how do I think in AngularJS?" is like saying "I have an HTML background how do I think in JavaScript?" The fact that you're asking the question shows you most likely don't understand the fundamental purposes of these two resources. This is why I chose to answer the question by simply pointing out the fundamental difference rather than going through the list saying "AngularJS makes use of directives whereas jQuery uses CSS selectors to make a jQuery object which does this and that etc....". This question does not require a lengthy answer. jQuery is a way to make programming JavaScript in the browser easier. Shorter, cross-browser commands, etc. AngularJS extends HTML, so you don't have to put <div> all over the place just to make an application. It makes HTML actually work for applications rather than what it was designed for, which is static, educational web pages. It accomplishes this in a roundabout way using JavaScript, but fundamentally it is an extension of HTML, not JavaScript. jQuery: you think a lot about 'QUERYing the DOM' for DOM elements and doing something. AngularJS: THE model is the truth, and you always think from that ANGLE. For example, when you get data from THE server which you intend to display in some format in the DOM, in jQuery, you need to '1. FIND' where in the DOM you want to place this data, the '2. UPDATE/APPEND' it there by creating a new node or just setting its innerHTML. Then when you want to update this view, you then '3. FIND' the location and '4. UPDATE'. This cycle of find and update all done within the same context of getting and formatting data from server is gone in AngularJS. With AngularJS you have your model (JavaScript objects you are already used to) and the value of the model tells you about the model (obviously) and about the view, and an operation on the model automatically propagates to the view, so you don't have to think about it. You will find yourself in AngularJS no longer finding things in the DOM. To put in another way, in jQuery, you need to think about CSS selectors, that is, where is the div or td that has a class or attribute, etc., so that I can get their HTML or color or value, but in AngularJS, you will find yourself thinking like this: what model am I dealing with, I will set the model's value to true. You are not bothering yourself of whether the view reflecting this value is a checked box or resides in a td element (details you would have often needed to think about in jQuery). And with DOM manipulation in AngularJS, you find yourself adding directives and filters, which you can think of as valid HTML extensions. One more thing you will experience in AngularJS: in jQuery you call the jQuery functions a lot, in AngularJS, AngularJS will call your functions, so AngularJS will 'tell you how to do things', but the benefits are worth it, so learning AngularJS usually means learning what AngularJS wants or the way AngularJS requires that you present your functions and it will call it accordingly. This is one of the things that makes AngularJS a framework rather than a library. Those are some very nice, but lengthy answers. To sum up my experiences: jQuery is a DOM manipulation library. AngularJS is an MV* framework. In fact, AngularJS is one of the few JavaScript MV* frameworks (many JavaScript MVC tools still fall under the category library). Being a framework, it hosts your code and takes ownership of decisions about what to call and when! AngularJS itself includes a jQuery-lite edition within it. So for some basic DOM selection/manipulation, you really don't have to include the jQuery library (it saves many bytes to run on the network.) AngularJS has the concept of "Directives" for DOM manipulation and designing reusable UI components, so you should use it whenever you feel the need of doing DOM manipulation related stuff (directives are only place where you should write jQuery code while using AngularJS). AngularJS involves some learning curve (more than jQuery :-). -->For any developer coming from jQuery background, my first advice would be to "learn JavaScript as a first class language before jumping onto a rich framework like AngularJS!" I learned the above fact the hard way. Good luck. They're apples and oranges. You don't want to compare them. They're two different things. AngularJs has already jQuery lite built in which allows you to perform basic DOM manipulation without even including the full blown jQuery version.  jQuery is all about DOM manipulation. It solves all the cross browser pain otherwise you will have to deal with but it's not a framework that allows you to divide your app into components like AngularJS.  A nice thing about AngularJs is that it allows you to separate/isolate the DOM manipulation in the directives.  There are built-in directives ready for you to use such as ng-click. You can create your own custom directives that will contain all your view logic or DOM manipulation so you don't end up mingle DOM manipulation code in the controllers or services that should take care of the business logic. Angular breaks down your app into  - Controllers - Services - Views - etc. and there is one more thing, that's the directive. It's an attribute  you can attach to any DOM element and you can go nuts with jQuery within it without worrying about your jQuery ever conflicts with AngularJs components or messes up with its architecture.  I heard from a meetup I attended, one of the founders of Angular said they worked really hard to separate out the DOM manipulation so do not try to include them back in.  Listen to the podcast JavaScript Jabber: Episode #32 that features the original creators of AngularJS: Misko Hevery & Igor Minar. They talk a lot about what it's like to come to AngularJS from other JavaScript backgrounds, especially jQuery. One of the points made in the podcast made a lot of things click for me with respects to your question: MISKO: [...] one of the things we thought about very hardly in Angular is, how do we provide lots of escape hatches so that you can get out and basically figure out a way out of this. So to us, the answer is this thing called “Directives”. And with directives, you essentially become a regular little jQuery JavaScript, you can do whatever you want. IGOR: So think of directive as the instruction to the compiler that tells it whenever you come across this certain element or this CSS in the template, and you keep this kind of code and that code is in charge of the element and everything below that element in the DOM tree. A transcript of the entire episode is available at the link provided above. So, to directly answer your question: AngularJS is -very- opinionated and is a true MV* framework. However, you can still do all of the really cool stuff you know and love with jQuery inside of directives. It's not a matter of "How do I do what I used to in jQuery?" as much as it's a matter of "How do I supplement AngularJS with all of the stuff I used to do in jQuery?" It's really two very different states of mind. I find this question interesting, because my first serious exposure to JavaScript programming was Node.js and AngularJS. I never learned jQuery, and I guess that's a good thing, because I don't have to unlearn anything. In fact, I actively avoid jQuery solutions to my problems, and instead, solely look for an "AngularJS way" to solve them. So, I guess my answer to this question would essentially boil down to, "think like someone who never learned jQuery" and avoid any temptation to incorporate jQuery directly (obviously AngularJS uses it to some extent behind the scenes). AngularJS and jQuery: AngularJs and JQuery are completely different at every level except the JQLite functionality and you will see it once you start learning the AngularJs core features (I explained it below). AngularJs is a client side framework that offers to build the independent client side application. JQuery is a client side library that play around the DOM. AngularJs Cool Principle - If you want some changes on your UI think from model data change perspective. Change your data and UI will re-render itself. You need not to play around DOM each time unless and until it is hardly required and that should also be handled through Angular Directives. To answer this question, I want to share my experience on the first enterprise application with AngularJS. These are the most awesome features that Angular provide where we start changing our jQuery mindset and we get the Angular like a framework and not the library. Two-way data binding is amazing:  I had a grid with all functionality UPDATE, DELTE, INSERT. I have a data object that binds the grid's model using ng-repeat. You only need to write a single line of simple JavaScript code for delete and insert and that's it. grid automatically updates as the grid model changes instantly. Update functionality is real time, no code for it. You feel amazing!!! Reusable directives are super: Write directives in one place and use it throughout the application. OMG!!! I used these directive for paging, regex, validations, etc. It is really cool! Routing is strong: It's up to your implementation how you want to use it, but it requires very few lines of code to route the request to specify HTML and controller (JavaScript) Controllers are great: Controllers take care of their own HTML, but this separation works well for common functionality well as. If you want to call the same function on the click of a button on master HTML, just write the same function name in each controller and write individual code. Plugins: There are many other similar features like showing an overlay in your app. You don't need to write code for it, just use an overlay plugin available as wc-overlay, and this will automatically take care of all XMLHttpRequest (XHR) requests. Ideal for RESTful architecture: Being a complete frameworks makes AngularJS great to work with a RESTful architecture. To call REST CRUD APIs is very easier and  Services: Write common codes using services and less code in controllers. Sevices can be used to share common functionalities among the controllers. Extensibility: Angular has extended the HTML directives using angular directives. Write expressions inside html and evaluate them on runtime. Create your own directives and services and use them in another project without any extra effort. As a JavaScript MV* beginner and purely focusing on the application architecture (not the server/client-side matters), I would certainly recommend the following resource (which I am surprised wasn't mentioned yet): JavaScript Design Patterns, by Addy Osmani, as an introduction to different JavaScript Design Patterns. The terms used in this answer are taken from the linked document above. I'm not going to repeat what was worded really well in the accepted answer. Instead, this answer links back to the theoretical backgrounds which power AngularJS (and other libraries). Like me, you will quickly realize that AngularJS (or Ember.js, Durandal, & other MV* frameworks for that matter) is one complex framework assembling many of the different JavaScript design patterns. I found it easier also, to test (1) native JavaScript code and (2) smaller libraries for each one of these patterns separately before diving into one global framework. This allowed me to better understand which crucial issues a framework adresses (because you are personally faced with the problem). For example: NB: This list is not complete, nor 'the best libraries'; they just happen to be the libraries I used. These libraries also include more patterns, the ones mentioned are just their main focuses or original intents. If you feel something is missing from this list, please do mention it in the comments, and I will be glad to add it. Actually, if you're using AngularJS, you don't need jQuery anymore. AngularJS itself has the binding and directive, which is a very good "replacement" for most things you can do with jQuery. I usually develop mobile applications using AngularJS and Cordova. The ONLY thing from jQuery I needed is the Selector. By googling, I see that there is a standalone jQuery selector module out there. It's Sizzle.  And I decided to make a tiny code snippet that help me quickly start a website using AngularJS with the power of jQuery Selector (using Sizzle). I shared my code here: https://github.com/huytd/Sizzular
__label__git __label__directory __label__git-add How can I add an empty directory (that contains no files) to a Git repository? Another way to make a directory stay (almost) empty (in the repository) is to create a .gitignore file inside that directory that contains these four lines: Then you don't have to get the order right the way that you have to do in m104's solution. This also gives the benefit that files in that directory won't show up as "untracked" when you do a git status. Making @GreenAsJade's comment persistent: I think it's worth noting that this solution does precisely what the question asked for, but is not perhaps what many people looking at this question will have been looking for. This solution guarantees that the directory remains empty. It says "I truly never want files checked in here". As opposed to "I don't have any files to check in here, yet, but I need the directory here, files may be coming later". You can't.  See the Git FAQ. Currently the design of the git index   (staging area) only permits files to   be listed, and nobody competent enough   to make the change to allow empty   directories has cared enough about   this situation to remedy it. Directories are added automatically   when adding files inside them. That   is, directories never have to be added   to the repository, and are not tracked   on their own. You can say "git add <dir>" and it   will add files in there. If you really need a directory to   exist in checkouts you should create a   file in it. .gitignore works well for   this purpose; you can leave it empty,   or fill in the names of files you   expect to show up in the directory. Create an empty file called .gitkeep in the directory, and add that. You could always put a README file in the directory with an explanation of why you want this, otherwise empty, directory in the repository. On Linux, this creates an empty file named .keep. For what it's worth, this name is agnostic to Git. Secondly, as another user has noted, the .git prefix convention can be reserved for files and directories that Git itself uses for configuration purposes. Alternatively, as noted in another answer, the directory can contain a descriptive README.md file instead. Either way this requires that the presence of the file won't cause your application to break. First things first: An empty directory cannot be part of a tree under the Git versioning system. It simply won't be tracked. But there are scenarios in which "versioning" empty directories can be meaningful, for example: Many users suggest: While both solutions surely work I find them inconsistent with a meaningful approach to Git versioning. Use an empty file called .gitkeep in order to force the presence of the folder in the versioning system. Although it may seem not such a big difference: You use a file that has the single purpose of keeping the folder. You don't put there any info you don't want to put. For instance, you should use READMEs as, well, READMEs with useful information, not as an excuse to keep the folder. Separation of concerns is always a good thing, and you can still add a .gitignore to ignore unwanted files. Naming it .gitkeep makes it very clear and straightforward from the filename itself (and also to other developers, which is good for a shared project and one of the core purposes of a Git repository) that this file is I've seen the .gitkeep approach adopted by very important frameworks like Laravel, Angular-CLI. As described in other answers, Git is unable to represent empty directories in its staging area. (See the Git FAQ.) However, if, for your purposes, a directory is empty enough if it contains a .gitignore file only, then you can create .gitignore files in empty directories only via: Andy Lester is right, but if your directory just needs to be empty, and not empty empty, you can put an empty .gitignore file in there as a workaround. As an aside, this is an implementation issue, not a fundamental Git storage design problem. As has been mentioned many times on the Git mailing list, the reason that this has not been implemented is that no one has cared enough to submit a patch for it, not that it couldn’t or shouldn’t be done. The Ruby on Rails log folder creation way:  Now the log directory will be included in the tree. It is super-useful when deploying, so you won't have to write a routine to make log directories. The logfiles can be kept out by issuing,  but you probably knew that. Git does not track empty directories. See the Git FAQ for more explanation. The suggested workaround is to put a .gitignore file in the empty directory. I do not like that solution, because the .gitignore is "hidden" by Unix convention. Also there is no explanation why the directories are empty. I suggest to put a README file in the empty directory explaining why the directory is empty and why it needs to be tracked in Git. With the README file in place, as far as Git is concerned, the directory is no longer empty. The real question is why do you need the empty directory in git? Usually you have some sort of build script that can create the empty directory before compiling/running. If not then make one. That is a far better solution than putting empty directories in git. So you have some reason why you need an empty directory in git. Put that reason in the README file. That way other developers (and future you) know why the empty directory needs to be there. You will also know that you can remove the empty directory when the problem requiring the empty directory has been solved. To list every empty directory use the following command: To create placeholder READMEs in every empty directory: To ignore everything in the directory except the README file put the following lines in your .gitignore: Alternatively, you could just exclude every README file from being ignored: To list every README after they are already created: WARNING: This tweak is not truly working as it turns out. Sorry for the inconvenience. Original post below: I found a solution while playing with Git internals! Create your empty directory: Add it to the index using a plumbing command and the empty tree SHA-1: Type the command and then enter the second line. Press Enter and then Ctrl + D to terminate your input. Note: the format is mode [SPACE] type [SPACE] SHA-1hash [TAB] path (the tab is important, the answer formatting does not preserve it). That's it! Your empty folder is in your index. All you have to do is commit. This solution is short and apparently works fine (see the EDIT!), but it is not that easy to remember... The empty tree SHA-1 can be found by creating a new empty Git repository, cd into it and issue git write-tree, which outputs the empty tree SHA-1. EDIT: I've been using this solution since I found it. It appears to work exactly the same way as creating a submodule, except that no module is defined anywhere. This leads to errors when issuing git submodule init|update. The problem is that git update-index rewrites the 040000 tree part into 160000 commit. Moreover, any file placed under that path won't ever be noticed by Git, as it thinks they belong to some other repository. This is nasty as it can easily be overlooked! However, if you don't already (and won't) use any Git submodules in your repository, and the "empty" folder will remain empty or if you want Git to know of its existence and ignore its content, you can go with this tweak. Going the usual way with submodules takes more steps that this tweak. Let's say you need an empty directory named tmp : In other words, you need to add the .gitignore file to the index before you can tell Git to ignore it (and everything else in the empty directory). Maybe adding an empty directory seems like it would be the path of least resistance because you have scripts that expect that directory to exist (maybe because it is a target for generated binaries).  Another approach would be to modify your scripts to create the directory as needed. In this example, you might check in a (broken) symbolic link to the directory so that you can access it without the ".generated" prefix (but this is optional). When you want to clean up your source tree you can just: If you take the oft-suggested approach of checking in an almost-empty folder, you have the minor complexity of deleting the contents without also deleting the ".gitignore" file. You can ignore all of your generated files by adding the following to your root .gitignore: I've been facing the issue with empty directories, too. The problem with using placeholder files is that you need to create them, and delete them, if they are not necessary anymore (because later on there were added sub-directories or files. With big source trees managing these placeholder files can be cumbersome and error prone. This is why I decided to write an open source tool which can manage the creation/deletion of such placeholder files automatically. It is written for .NET platform and runs under Mono (.NET for Linux) and Windows. Just have a look at: http://code.google.com/p/markemptydirs You can't and unfortunately will never be able to. This is a decision made by Linus Torvald himself. He knows what's good for us.  There is a rant out there somewhere I read once.  I found Re: Empty directories.., but maybe there is another one. You have to live with the workarounds...unfortunately. I like the answers by @Artur79 and @mjs so I've been using a combination of both and made it a standard for our projects. However, only a handful of our developers work on Mac or Linux. A lot work on Windows and I could not find an equivalent simple one-liner to accomplish the same there. Some were lucky enough to have Cygwin installed for other reasons, but prescribing Cygwin just for this seemed overkill. Edit for a better solution So, since most of our developers already have Ant installed, the first thing I thought of was to put together an Ant build file to accomplish this independently of the platform. This can still be found here However, I later thought It would be better to make this into a small utility command, so I recreated it using Python and published it to the PyPI here. You can install it by simply running: It will allow you to create and remove .gitkeep files recursively, and it will also allow you to add messages to them for your peers to understand why those directories are important. This last bit is bonus. I thought it would be nice if the .gitkeep files could be self-documenting. I hope you find it useful. When you add a .gitignore file, if you are going to put any amount of content in it (that you want Git to ignore) you might want to add a single line with just an asterisk * to make sure you don't add the ignored content accidentally.  There's no way to get Git to track directories, so the only solution is to add a placeholder file within the directory that you want Git to track. The file can be named and contain anything you want, but most people use an empty file named .gitkeep (although some people prefer the VCS-agnostic .keep). The prefixed . marks it as a hidden file. Another idea would be to add a README file explaining what the directory will be used for. As mentioned it's not possible to add empty directories, but here is a one liner that adds empty .gitignore files to all directories.  ruby -e 'require "fileutils" ; Dir.glob(["target_directory","target_directory/**"]).each { |f| FileUtils.touch(File.join(f, ".gitignore")) if File.directory?(f) }' I have stuck this in a Rakefile for easy access. The solution of Jamie Flournoy works great. Here is a bit enhanced version to keep the .htaccess : With this solution you are able to commit a empty folder, for example /log, /tmp or /cache and the folder will stay empty. Many have already answered this question. Just adding a PowerShell version here. Find all the empty folders in the directory Add a empty .gitkeep file in there I always build a function to check for my desired folder structure and build it for me within the project. This gets around this problem as the empty folders are held in Git by proxy. This is in PHP, but I am sure most languages support the same functionality, and because the creation of the folders is taken care of by the application, the folders will always be there. Here is a hack, but it's funny that it works (Git 2.2.1). Similar to what @Teka suggested, but easier to remember: Now, you have a directory that gets created when commit is checked out. An interesting thing though is that if you look at the content of tree object of this file you'll get: fatal: Not a valid object name   b64338b90b4209263b50244d18278c0999867193 I wouldn't encourage to use it though since it may stop working in the future versions of Git. Which may leave your repository corrupted. Reading @ofavre's and @stanislav-bashkyrtsev's answers using broken GIT submodule references to create the GIT directories, I'm surprised that nobody has suggested yet this simple amendment of the idea to make the whole thing sane and safe: Rather than hacking a fake submodule into GIT, just add an empty real one. A GIT repository with exactly one commit: No message, no committed files. To add an empty directory to you GIT repo: To convert all existing empty directories to submodules: Git will store the latest commit hash when creating the submodule reference, so you don't have to worry about me (or GitLab) using this to inject malicious files. Unfortunately I have not found any way to force which commit ID is used during checkout, so you'll have to manually check that the reference commit ID is e84d7b81f0033399e325b8037ed2b801a5c994e0 using git submodule status after adding the repo. Still not a native solution, but the best we probably can have without somebody getting their hands really, really dirty in the GIT codebase. You should be able to recreate this exact commit using (in an empty directory): Creating reproducible GIT commits is surprisingly hard… If you want to add a folder that will house a lot of transient data in multiple semantic directories, then one approach is to add something like this to your root .gitignore... /app/data/**/*.* !/app/data/**/*.md Then you can commit descriptive README.md files (or blank files, doesn't matter, as long as you can target them uniquely like with the *.md in this case) in each directory to ensure that the directories all remain part of the repo but the files (with extensions) are kept ignored. LIMITATION: .'s are not allowed in the directory names! You can fill up all of these directories with xml/images files or whatever and add more directories under /app/data/ over time as the storage needs for your app develop (with the README.md files serving to burn in a description of what each storage directory is for exactly). There is no need to further alter your .gitignore or decentralise by creating a new .gitignore for each new directory. Probably not the smartest solution but is terse gitignore-wise and always works for me. Nice and simple! ;)  An easy way to do this is by adding a .gitkeep file to the directory you wish to (currently) keep empty.  See this SOF answer for further info - which also explains why some people find the competing convention of adding a .gitignore file (as stated in many answers here) confusing. Sometimes you have to deal with bad written libraries or software, which need a "real" empty and existing directory. Putting a simple .gitignore or .keep might break them and cause a bug. The following might help in these cases, but no guarantee... First create the needed directory: Then you add a broken symbolic link to this directory (but on any other case than the described use case above, please use a README with an explanation): To ignore files in this directory, you can add it in your root .gitignore: To add the ignored file, use a parameter to force it: After the commit you have a broken symbolic link in your index and git creates the directory. The broken link has some advantages, since it is no regular file and points to no regular file. So it even fits to the part of the question "(that contains no files)", not by the intention but by the meaning, I guess: This commands shows an empty result, since no files are present in this directory. So most applications, which get all files in a directory usually do not see this link, at least if they do a "file exists" or a "is readable". Even some scripts will not find any files there: But I strongly recommend to use this solution only in special circumstances, a good written README in an empty directory is usually a better solution. (And I do not know if this works with a windows filesystem...) Adding one more option to the fray. Assuming you would like to add a directory to git that, for all purposes related to git, should remain empty and never have it's contents tracked, a .gitignore as suggested numerous times here, will do the trick. The format, as mentioned, is: Now, if you want a way to do this at the command line, in one fell swoop, while inside the directory you want to add, you can execute: Myself, I have a shell script that I use to do this.  Name the script whatever you whish, and either add it somewhere in your include path, or reference it directly: With this, you can either execute it from within the directory you wish to add, or reference the directory as it's first and only parameter: Another option (in response to a comment by @GreenAsJade), if you want to track an empty folder that MAY contain tracked files in the future, but will be empty for now, you can ommit the * from the .gitignore file, and check that in.  Basically, all the file is saying is "do not ignore me", but otherwise, the directory is empty and tracked. Your .gitignore file would look like: That's it, check that in, and you have an empty, yet tracked, directory that you can track files in at some later time. The reason I suggest keeping that one line in the file is that it gives the .gitignore purpose. Otherwise, some one down the line may think to remove it.  It may help if you place a comment above the line. You can't. This is an intentional design decision by the Git maintainers. Basically, the purpose of a Source Code Management System like Git is managing source code and empty directories aren't source code. Git is also often described as a content tracker, and again, empty directories aren't content (quite the opposite, actually), so they are not tracked. You can save this code as create_readme.php and run the PHP code from the root directory of your Git project.  It will add README files to all directories that are empty so those directories would be then added to the index. Then do
__label__javascript __label__guid __label__uuid I'm trying to create globally-unique identifiers in JavaScript.  I'm not sure what routines are available on all browsers, how "random" and seeded the built-in random number generator is, etc. The GUID / UUID should be at least 32 characters and should stay in the ASCII range to avoid trouble when passing them around. UUIDs (Universally Unique IDentifier), also known as GUIDs (Globally Unique IDentifier), according to RFC 4122, are identifiers designed to provide certain uniqueness guarantees. While it is possible to implement RFC-compliant UUIDs in a few lines of JavaScript code (e.g., see @broofa's answer, below) there are several common pitfalls: Thus, developers writing code for production environments are encouraged to use a rigorous, well-maintained implementation such as the uuid module. For an RFC4122 version 4 compliant solution, this one-liner(ish) solution is the most compact I could come up with:   function uuidv4() {   return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {     var r = Math.random() * 16 | 0, v = c == 'x' ? r : (r & 0x3 | 0x8);     return v.toString(16);   }); }  console.log(uuidv4());    Update, 2015-06-02:  Be aware that UUID uniqueness relies heavily on the underlying random number generator (RNG).  The solution above uses Math.random() for brevity, however Math.random() is not guaranteed to be a high-quality RNG.  See Adam Hyland's excellent writeup on Math.random() for details.  For a more robust solution, consider using the uuid module, which uses higher quality RNG APIs. Update, 2015-08-26: As a side-note, this gist describes how to determine how many IDs can be generated before reaching a certain probability of collision.  For example, with 3.26x1015 version 4 RFC4122 UUIDs you have a 1-in-a-million chance of collision. Update, 2017-06-28: A good article from Chrome developers discussing the state of Math.random PRNG quality in Chrome, Firefox, and Safari.  tl;dr - As of late-2015 it's "pretty good", but not cryptographic quality.  To address that issue, here's an updated version of the above solution that uses ES6, the crypto API, and a bit of JavaScript wizardry I can't take credit for:   function uuidv4() {   return ([1e7]+-1e3+-4e3+-8e3+-1e11).replace(/[018]/g, c =>     (c ^ crypto.getRandomValues(new Uint8Array(1))[0] & 15 >> c / 4).toString(16)   ); }  console.log(uuidv4());    Update, 2020-01-06: There is a proposal in the works for a standard uuid module as part of the JavaScript language I really like how clean Broofa's answer is, but it's unfortunate that poor implementations of Math.random leave the chance for collision.   Here's a similar RFC4122 version 4 compliant solution that solves that issue by offsetting the first 13 hex numbers by a hex portion of the timestamp, and once depleted offsets by a hex portion of the microseconds since pageload.  That way, even if Math.random is on the same seed, both clients would have to generate the UUID the exact same number of microseconds since pageload (if high-perfomance time is supported) AND at the exact same millisecond (or 10,000+ years later) to get the same UUID:      function generateUUID() { // Public Domain/MIT     var d = new Date().getTime();//Timestamp     var d2 = (performance && performance.now && (performance.now()*1000)) || 0;//Time in microseconds since page-load or 0 if unsupported     return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {         var r = Math.random() * 16;//random number between 0 and 16         if(d > 0){//Use timestamp until depleted             r = (d + r)%16 | 0;             d = Math.floor(d/16);         } else {//Use microseconds since page-load if supported             r = (d2 + r)%16 | 0;             d2 = Math.floor(d2/16);         }         return (c === 'x' ? r : (r & 0x3 | 0x8)).toString(16);     }); }  console.log(generateUUID())     Here's a fiddle to test. broofa's answer is pretty slick, indeed - impressively clever, really...  rfc4122 compliant, somewhat readable, and compact.  Awesome! But if you're looking at that regular expression, those many replace() callbacks, toString()'s and Math.random() function calls (where he's only using 4 bits of the result and wasting the rest), you may start to wonder about performance.  Indeed, joelpt even decided to toss out RFC for generic GUID speed with generateQuickGUID. But, can we get speed and RFC compliance?  I say, YES!  Can we maintain readability?  Well...  Not really, but it's easy if you follow along. But first, my results, compared to broofa, guid (the accepted answer), and the non-rfc-compliant generateQuickGuid: So by my 6th iteration of optimizations, I beat the most popular answer by over 12X, the accepted answer by over 9X, and the fast-non-compliant answer by 2-3X.  And I'm still rfc4122 compliant. Interested in how?  I've put the full source on http://jsfiddle.net/jcward/7hyaC/3/ and on http://jsperf.com/uuid-generator-opt/4 For an explanation, let's start with broofa's code:   function broofa() {     return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(c) {         var r = Math.random()*16|0, v = c == 'x' ? r : (r&0x3|0x8);         return v.toString(16);     }); }  console.log(broofa())    So it replaces x with any random hex digit, y with random data (except forcing the top 2 bits to 10 per the RFC spec), and the regex doesn't match the - or 4 characters, so he doesn't have to deal with them.  Very, very slick. The first thing to know is that function calls are expensive, as are regular expressions (though he only uses 1, it has 32 callbacks, one for each match, and in each of the 32 callbacks it calls Math.random() and v.toString(16)). The first step toward performance is to eliminate the RegEx and its callback functions and use a simple loop instead.  This means we have to deal with the - and 4 characters whereas broofa did not.  Also, note that we can use String Array indexing to keep his slick String template architecture:   function e1() {     var u='',i=0;     while(i++<36) {         var c='xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'[i-1],r=Math.random()*16|0,v=c=='x'?r:(r&0x3|0x8);         u+=(c=='-'||c=='4')?c:v.toString(16)     }     return u; }  console.log(e1())    Basically, the same inner logic, except we check for - or 4, and using a while loop (instead of replace() callbacks) gets us an almost 3X improvement! The next step is a small one on the desktop but makes a decent difference on mobile.  Let's make fewer Math.random() calls and utilize all those random bits instead of throwing 87% of them away with a random buffer that gets shifted out each iteration.  Let's also move that template definition out of the loop, just in case it helps:   function e2() {     var u='',m='xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx',i=0,rb=Math.random()*0xffffffff|0;     while(i++<36) {         var c=m[i-1],r=rb&0xf,v=c=='x'?r:(r&0x3|0x8);         u+=(c=='-'||c=='4')?c:v.toString(16);rb=i%8==0?Math.random()*0xffffffff|0:rb>>4     }     return u }  console.log(e2())    This saves us 10-30% depending on platform.  Not bad.  But the next big step gets rid of the toString function calls altogether with an optimization classic - the look-up table.  A simple 16-element lookup table will perform the job of toString(16) in much less time:   function e3() {     var h='0123456789abcdef';     var k='xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx';     /* same as e4() below */ } function e4() {     var h=['0','1','2','3','4','5','6','7','8','9','a','b','c','d','e','f'];     var k=['x','x','x','x','x','x','x','x','-','x','x','x','x','-','4','x','x','x','-','y','x','x','x','-','x','x','x','x','x','x','x','x','x','x','x','x'];     var u='',i=0,rb=Math.random()*0xffffffff|0;     while(i++<36) {         var c=k[i-1],r=rb&0xf,v=c=='x'?r:(r&0x3|0x8);         u+=(c=='-'||c=='4')?c:h[v];rb=i%8==0?Math.random()*0xffffffff|0:rb>>4     }     return u }  console.log(e4())    The next optimization is another classic.  Since we're only handling 4-bits of output in each loop iteration, let's cut the number of loops in half and process 8-bits each iteration.  This is tricky since we still have to handle the RFC compliant bit positions, but it's not too hard.  We then have to make a larger lookup table (16x16, or 256) to store 0x00 - 0xff, and we build it only once, outside the e5() function.   var lut = []; for (var i=0; i<256; i++) { lut[i] = (i<16?'0':'')+(i).toString(16); } function e5() {     var k=['x','x','x','x','-','x','x','-','4','x','-','y','x','-','x','x','x','x','x','x'];     var u='',i=0,rb=Math.random()*0xffffffff|0;     while(i++<20) {         var c=k[i-1],r=rb&0xff,v=c=='x'?r:(c=='y'?(r&0x3f|0x80):(r&0xf|0x40));         u+=(c=='-')?c:lut[v];rb=i%4==0?Math.random()*0xffffffff|0:rb>>8     }     return u }  console.log(e5())    I tried an e6() that processes 16-bits at a time, still using the 256-element LUT, and it showed the diminishing returns of optimization.  Though it had fewer iterations, the inner logic was complicated by the increased processing, and it performed the same on desktop, and only ~10% faster on mobile. The final optimization technique to apply - unroll the loop.  Since we're looping a fixed number of times, we can technically write this all out by hand.  I tried this once with a single random variable r that I kept re-assigning, and performance tanked.  But with four variables assigned random data up front, then using the lookup table, and applying the proper RFC bits, this version smokes them all:   var lut = []; for (var i=0; i<256; i++) { lut[i] = (i<16?'0':'')+(i).toString(16); } function e7() {     var d0 = Math.random()*0xffffffff|0;     var d1 = Math.random()*0xffffffff|0;     var d2 = Math.random()*0xffffffff|0;     var d3 = Math.random()*0xffffffff|0;     return lut[d0&0xff]+lut[d0>>8&0xff]+lut[d0>>16&0xff]+lut[d0>>24&0xff]+'-'+     lut[d1&0xff]+lut[d1>>8&0xff]+'-'+lut[d1>>16&0x0f|0x40]+lut[d1>>24&0xff]+'-'+     lut[d2&0x3f|0x80]+lut[d2>>8&0xff]+'-'+lut[d2>>16&0xff]+lut[d2>>24&0xff]+     lut[d3&0xff]+lut[d3>>8&0xff]+lut[d3>>16&0xff]+lut[d3>>24&0xff]; }  console.log(e7())    Modualized:  http://jcward.com/UUID.js - UUID.generate() The funny thing is, generating 16 bytes of random data is the easy part.  The whole trick is expressing it in String format with RFC compliance, and it's most tightly accomplished with 16 bytes of random data, an unrolled loop and lookup table. I hope my logic is correct -- it's very easy to make a mistake in this kind of tedious bit-work.  But the outputs look good to me.  I hope you enjoyed this mad ride through code optimization! Be advised: my primary goal was to show and teach potential optimization strategies.  Other answers cover important topics such as collisions and truly random numbers, which are important for generating good UUIDs. Here's some code based on RFC 4122, section 4.4 (Algorithms for Creating a UUID from Truly Random or Pseudo-Random Number).   document.getElementById("unique").innerHTML =   Math.random().toString(36).substring(2) + (new Date()).getTime().toString(36); <div id="unique"> </div>    If ID's are generated more than 1 millisecond apart, they are 100% unique. If two ID's are generated at shorter intervals, and assuming that the random method is truly random, this would generate ID's that are 99.99999999999999% likely to be globally unique (collision in 1 of 10^15) You can increase this number by adding more digits, but to generate 100% unique ID's you will need to use a global counter. if you need RFC compatibility, this formatting will pass as a valid version 4 GUID:   let u = Date.now().toString(16)+Math.random().toString(16)+'0'.repeat(16); let guid = [u.substr(0,8), u.substr(8,4), '4000-8' + u.substr(13,3), u.substr(16,12)].join('-'); document.getElementById("unique").innerHTML = guid; <div id="unique"> </div>    Edit: The above code follow the intention, but not the letter of the RFC. Among other discrepancies it's a few random digits short. (Add more random digits if you need it) The upside is that this it's really fast :) You can test validity of your GUID here Fastest GUID like string generator method in the format XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX. This does not generate standard-compliant GUID. Ten million executions of this implementation take just 32.5 seconds, which is the fastest I've ever seen in a browser (the only solution without loops/iterations). The function is as simple as: To test the performance, you can run this code: I'm sure most of you will understand what I did there, but maybe there is at least one person that will need an explanation: The algorithm: The assembly: Link to this post on my blog Enjoy! :-) Here is a totally non-compliant but very performant implementation to generate an ASCII-safe GUID-like unique identifier. Generates 26 [a-z0-9] characters, yielding a UID that is both shorter and more unique than RFC compliant GUIDs. Dashes can be trivially added if human-readability matters. Here are usage examples and timings for this function and several of this question's other answers. The timing was performed under Chrome m25, 10 million iterations each. Here is the timing code. Here is a combination of the top voted answer, with a workaround for Chrome's collisions: It is on jsbin if you want to test it. Here's a solution dated Oct. 9, 2011 from a comment by user jed at https://gist.github.com/982883: This accomplishes the same goal as the current highest-rated answer, but in 50+ fewer bytes by exploiting coercion, recursion, and exponential notation. For those curious how it works, here's the annotated form of an older version of the function: From sagi shkedy's technical blog: There are other methods that involve using an ActiveX control, but stay away from these! I thought it was worth pointing out that no GUID generator can guarantee unique keys (check the Wikipedia article). There is always a chance of collisions. A GUID simply offers a large enough universe of keys to reduce the change of collisions to almost nil. You can use node-uuid (https://github.com/kelektiv/node-uuid) Simple, fast generation of RFC4122 UUIDS. Features: Install Using NPM: Or Using uuid via browser: Download Raw File (uuid v1): https://raw.githubusercontent.com/kelektiv/node-uuid/master/v1.js Download Raw File (uuid v4): https://raw.githubusercontent.com/kelektiv/node-uuid/master/v4.js Want even smaller? Check this out: https://gist.github.com/jed/982883 Usage: ES6: This version is based on Briguy37's answer and some bitwise operators to extract nibble sized windows from the buffer. It should adhere to the RFC Type 4 (random) schema, since I had problems last time parsing non-compliant UUIDs with Java's UUID. This creates a version 4 UUID (created from pseudo random numbers): Here is a sample of the UUIDs generated: Simple JavaScript module as a combination of best answers in this question.   var crypto = window.crypto || window.msCrypto || null; // IE11 fix  var Guid = Guid || (function() {    var EMPTY = '00000000-0000-0000-0000-000000000000';    var _padLeft = function(paddingString, width, replacementChar) {     return paddingString.length >= width ? paddingString : _padLeft(replacementChar + paddingString, width, replacementChar || ' ');   };    var _s4 = function(number) {     var hexadecimalResult = number.toString(16);     return _padLeft(hexadecimalResult, 4, '0');   };    var _cryptoGuid = function() {     var buffer = new window.Uint16Array(8);     window.crypto.getRandomValues(buffer);     return [_s4(buffer[0]) + _s4(buffer[1]), _s4(buffer[2]), _s4(buffer[3]), _s4(buffer[4]), _s4(buffer[5]) + _s4(buffer[6]) + _s4(buffer[7])].join('-');   };    var _guid = function() {     var currentDateMilliseconds = new Date().getTime();     return 'xxxxxxxx-xxxx-4xxx-yxxx-xxxxxxxxxxxx'.replace(/[xy]/g, function(currentChar) {       var randomChar = (currentDateMilliseconds + Math.random() * 16) % 16 | 0;       currentDateMilliseconds = Math.floor(currentDateMilliseconds / 16);       return (currentChar === 'x' ? randomChar : (randomChar & 0x7 | 0x8)).toString(16);     });   };    var create = function() {     var hasCrypto = crypto != 'undefined' && crypto !== null,       hasRandomValues = typeof(window.crypto.getRandomValues) != 'undefined';     return (hasCrypto && hasRandomValues) ? _cryptoGuid() : _guid();   };    return {     newGuid: create,     empty: EMPTY   }; })();  // DEMO: Create and show GUID console.log(Guid.newGuid());    Usage: Guid.newGuid() "c6c2d12f-d76b-5739-e551-07e6de5b0807" Guid.empty "00000000-0000-0000-0000-000000000000" Well, this has a bunch of answers already, but unfortunately there's not a "true" random in the bunch. The version below is an adaptation of broofa's answer, but updated to include a "true" random function that uses crypto libraries where available, and the Alea() function as a fallback. JavaScript project on GitHub - https://github.com/LiosK/UUID.js UUID.js The RFC-compliant UUID generator for JavaScript. See RFC 4122 http://www.ietf.org/rfc/rfc4122.txt. Features Generates RFC 4122 compliant UUIDs. Version 4 UUIDs (UUIDs from random numbers) and version 1 UUIDs   (time-based UUIDs) are available. UUID object allows a variety of access to the UUID including access to   the UUID fields. Low timestamp resolution of JavaScript is compensated by random   numbers.  I wanted to understand broofa's answer, so I expanded it and added comments: For those wanting an RFC 4122 version 4 compliant solution with speed considerations (few calls to Math.random()):   var rand = Math.random;  function UUID() {     var nbr, randStr = "";     do {         randStr += (nbr = rand()).toString(16).substr(3, 6);     } while (randStr.length < 30);     return (         randStr.substr(0, 8) + "-" +         randStr.substr(8, 4) + "-4" +         randStr.substr(12, 3) + "-" +         ((nbr*4|0)+8).toString(16) + // [89ab]         randStr.substr(15, 3) + "-" +         randStr.substr(18, 12)     ); }  console.log( UUID() );    The above function should have a decent balance between speed and randomness. I adjusted my own UUID/GUID generator with some extras here. I'm using the following Kybos random number generator to be a bit more cryptographically sound. Below is my script with the Mash and Kybos methods from baagoe.com excluded. ES6 sample If you just need a random 128 bit string in no particular format you can use: Which will return something like 2350143528-4164020887-938913176-2513998651. Just another more readable variant with just two mutations. The native URL.createObjectURL is generating an uuid. You can take advantage of this. I know, it is an old question. Just for completeness, if your environment is SharePoint, there is a utility function called SP.Guid.newGuid (MSDN link which creates a new GUID. This function is inside the sp.init.js file. If you rewrite this function (to remove some other dependencies from other private functions), it looks like this: The better way: Minimized: This one is based on date, and add a random suffix to "ensure" uniqueness. Works well for css identifiers. It always returns something like and is easy to hack: uid-139410573297741 Simple code that uses crypto.getRandomValues(a) on supported browsers (IE11+, iOS7+, FF21+, Chrome, Android Chrome). Avoids using Math.random() because that can cause collisions (for example 20 collisions for 4000 generated uuids in a real situation by Muxa). Notes: OK, using uuid package, it support for version 1, 3, 4 and 5 UUIDs do: and then: You can also do it with fully-specified options: For more info, visit the npm page here
__label__git __label__git-remote __label__github I pulled a project from GitHub a few days ago. I've since discovered that there are several forks on GitHub, and I neglected to note which one I took originally. How can I determine which of those forks I pulled? If you want only the remote URL, or if your are not connected to a network that can reach the remote repo: If you require full output and you are on a network that can reach the remote repo where the origin resides : When using git clone (from GitHub, or any source repository for that matter) the default name for the source of the clone is "origin". Using git remote show will display the information about this remote name. The first few lines should show: If you want to use the value in the script, you would use the first command listed in this answer. Should you want this for scripting purposes, you can get only the URL with You can try: It will print all your remotes' fetch/push URLs. To get the answer: This is better than reading the configuration; refer to the man page for git-ls-remote: --get-url Expand the URL of the given remote repository taking into account   any "url.<base>.insteadOf" config setting (See git-config(1)) and   exit without talking to the remote. As pointed out by @Jefromi, this option was added in v1.7.5 and not documented until v1.7.12.2 (2012-09). With Git 2.7 (release January 5th, 2015), you have a more coherent solution using git remote: (nice pendant of git remote set-url origin <newurl>) See commit 96f78d3 (16 Sep 2015) by Ben Boeckel (mathstuf). (Merged by Junio C Hamano -- gitster -- in commit e437cbd, 05 Oct 2015): Expanding insteadOf is a part of ls-remote --url and there is no way   to expand pushInsteadOf as well.   Add a get-url subcommand to be able to query both as well as a way to get all configured URLs. Retrieves the URLs for a remote.   Configurations for insteadOf and pushInsteadOf are expanded here.   By default, only the first URL is listed. Before git 2.7, you had: To summarize, there are at least four ways: (The following was tried for the official Linux repository) Least information: and More information: Even more information: I think you can find it under .git/config and remote["origin"] if you didn't manipulate that. Short answer: or, an alternative for pure quick scripts: Some info: I ended up with: $ git remote show -n origin, which  seems to be fastest. With -n it will not fetch remote heads (AKA branches). You don't need that type of info, right? http://www.kernel.org/pub//software/scm/git/docs/git-remote.html You can apply | grep -i fetch to all three versions to show only the fetch URL. If you require pure speed, then use: Thanks to @Jefromi for pointing that out. For me, this is the easier way (less typing): actually, I've that into an alias called s that does: You can add to your profile with: alias s='git remote -v && git status' I can never remember all the parameters to Git commands, so I just put an alias in the ~/.gitconfig file that makes more sense to me, so I can remember it, and it results in less typing: After reloading the terminal, you can then just type: Here are a few more of my frequently used ones: I also highly recommend git-extras which has a git info command which provides much more detailed information on the remote and local branches. The Git URL will be inside the Git configuration file. The value corresponds to the key url. For Mac and Linux use the commands below: For Windows open the below file in any text editor and find the value for key url. Note: This will work even if you are offline or the remote git server has been taken down. I basically use: It works for Git Bash command console or CMD command console in Windows. That said, it works with version 2.x of Git. I prefer this one as it is easier to remember: It will list all useful information such as: This command will give all information related to your repository. The upstream's remote may not be called "origin", so here's a variation: Or: For more useful variables there's: For ssh:// repositories: For git:// repositories: To supplement the other answers:  If the remote has for some reason been changed and so doesn't reflect the original origin, the very first entry in the reflog (i.e. the last entry displayed by the command git reflog) should indicate where the repo was originally cloned from. e.g. (Bear in mind that the reflog may be purged, so this isn't guaranteed to work.) With git remote show origin you have to be in the projects directory. But if you want to determine the URLs from anywhere else you could use: If you'll need this command often, you could define an alias in your .bashrc or .bash_profile with MacOS. So you just need to call giturl in the Git root folder in order to simply obtain its URL. If you extend this alias like this you get only the plain URL without the preceding  "url=" in  url=http://example.com/repo.git you get more possibilities in its usage: Example On Mac you could call open $(giturl) to open the URL in the standard browser. Or chrome $(giturl) to open it with the Chrome browser on Linux. A simple way is to open the .git/config file: To edit: vim .git/config or nano .git/config Print arbitrarily named remote fetch URLs: If you do not know the name of the upstream remote for a branch, you can look that up first by inspecting the upstream branch name that the current branch was built upon. Use git rev-parse like this: This shows that upstream branch that was the source for the current branch. This can be parsed to get the remote name like this: Now take that and pipe it to git ls-remote and you'll get the URL of the upstream remote that is the source of the current branch: Now it should be noted, that this is not necessarily the same as the source remote repository that was cloned from. In many cases however it will be enough. Usage: Output: You cloned your repo with SSH clone. But you want to get http url to open it in the browser or share it: GitHub or GitLab doesn’t matter. easy just use this command where you .git folder placed git config --get remote.origin.url if you are connected to network git remote show origin it will show you the URL that a local Git repository was originally cloned from. hope this help
__label__git __label__remote-branch __label__git-branch __label__git-clone I have a master and a development branch, both pushed to GitHub. I've cloned, pulled, and fetched, but I remain unable to get anything other than the master branch back. I'm sure I'm missing something obvious, but I have read the manual and I'm getting no joy at all. First, clone a remote Git repository and cd into it: Next, look at the local branches in your repository: But there are other branches hiding in your repository! You can see these using the -a flag: If you just want to take a quick peek at an upstream branch, you can check it out directly: But if you want to work on that branch, you'll need to create a local tracking branch which is done automatically by: and you will see Here, "new branch" simply means that the branch is taken from the index and created locally for you.  As the previous line tells you, the branch is being set up to track the remote branch, which usually means the origin/branch_name branch. Now, if you look at your local branches, this is what you'll see: You can actually track more than one remote repository using git remote. At this point, things are getting pretty crazy, so run gitk to see what's going on: If you have many remote branches that you want to fetch at once, do: Now you can checkout any branch as you need to, without hitting the remote repository. Note: This will not create working copies of any non-checked out branches, which is what the question was asking. For that, see This Bash script helped me out: It will create tracking branches for all remote branches, except master (which you probably got from the original clone command). I think you might still need to do a  to be sure. One liner: git branch -a | grep -v HEAD | perl -ne 'chomp($_); s|^\*?\s*||; if (m|(.+)/(.+)| && not $d{$2}) {print qq(git branch --track $2 $1/$2\n)} else {$d{$_}=1}' | csh -xfs   As usual: test in your setup before copying rm -rf universe as we know it  Credits for one-liner go to user cfi Using the --mirror option seems to copy the remote tracking branches properly. However, it sets up the repository as a bare repository, so you have to turn it back into a normal repository afterwards. Reference: Git FAQ: How do I clone a repository with all remotely tracked branches?  You can easily switch to a branch without using the fancy "git checkout -b somebranch origin/somebranch" syntax.  You can do: Git will automatically do the right thing: Git will check whether a branch with the same name exists in exactly one remote, and if it does, it tracks it the same way as if you had explicitly specified that it's a remote branch. From the git-checkout man page of Git 1.8.2.1: If <branch> is not found but there does exist a tracking branch in exactly one remote (call it <remote>) with a matching name, treat as equivalent to Regarding, $ git checkout -b experimental origin/experimental using or the more verbose but easier to remember might be better, in terms of tracking a remote repository.  The fetch that you are doing should get all the remote branches, but it won't create local branches for them. If you use gitk, you should see the remote branches described as "remotes/origin/dev" or something similar. To create a local branch based on a remote branch, do something like: Which should return something like: Now, when you are on the dev branch, "git pull" will update your local dev to the same point as the remote dev branch.  Note that it will fetch all branches, but only pull the one you are on to the top of the tree. Use aliases. Though there aren't any native Git one-liners, you can define your own as and then use it as When you do "git clone git://location", all branches and tags are fetched. In order to work on top of a specific remote branch, assuming it's the origin remote: Better late than never, but here is the best way to do this: At this point you have a complete copy of the remote repo with all of it's branches (verify with git branch).  You can use --mirror instead of --bare if your remote repo has remotes of its own. git clone downloads all remote branches but still considers them "remote", even though the files are located in your new repository. There's one exception to this, which is that the cloning process creates a local branch called "master" from the remote branch called "master". By default, git branch only shows local branches, which is why you only see "master". git branch -a shows all branches, including remote branches. If you actually want to work on a branch, you'll probably want a "local" version of it. To simply create local branches from remote branches (without checking them out and thereby changing the contents of your working directory), you can do that like this: In this example, branchone is the name of a local branch you're creating based on origin/branchone; if you instead want to create local branches with different names, you can do this: Once you've created a local branch, you can see it with git branch (remember, you don't need -a to see local branches). This isn't too much complicated, very simple and straight forward steps are as follows; git fetch origin This will bring all the remote branches to your local. git branch -a This will show you all the remote branches. git checkout --track origin/<branch you want to checkout> Verify whether you are in the desired branch by the following command; The output will like this; Notice the * sign that denotes the current branch. Just do this: You see, 'git clone git://example.com/myprojectt' fetches everything, even the branches, you just have to checkout them, then your local branch will be created. You only need to use "git clone" to get all branches. Even though you only see master branch, you can use "git branch -a" to see all branches. And you can switch to any branch which you already have. Don't worry that after you "git clone", you don't need to connect with the remote repo, "git branch -a" and "git checkout " can be run successfully when you close your wifi. So it is proved that when you do "git clone", it already has copied all branches from the remote repo. After that, you don't need the remote repo, your local already has all branches' codes.   A git clone is supposed to copy the entire repository.  Try cloning it, and then run git branch -a.  It should list all the branches.  If then you want to switch to branch "foo" instead of "master", use git checkout foo. all the answers I saw here are valid but there is a much cleaner way to clone a repository and to pull all the branches at once.  When you clone a repository all the information of the branches is actually downloaded but the branches are hidden. With the command you can show all the branches of the repository, and with the command you can then "download" them manually one at a time.  However, when you want to clone a repo with a lot of branches all the ways illustrated are above are lengthy and tedious in respect to a much cleaner and quicker way that I am going to show, though it's a bit complicated. You need three steps to accomplish this: create a new empty folder on your machine and clone a mirror copy of the .git folder from the repository: the local repository inside the folder my_repo_folder is still empty, there is just a hidden .git folder now that you can see with a "ls -alt" command from the terminal. switch this repository from an empty (bare) repository to a regular repository by switching the boolean value "bare" of the git configurations to false: Grab everything that inside the current folder and create all the branches on the local machine, therefore making this a normal repo.  So now you can just type the command "git branch" and you can see that all the branches are downloaded.  This is the quick way in which you can clone a git repository with all the branches at once, but it's not something you wanna do for every single project in this way.  Use my tool git_remote_branch (you need Ruby installed on your machine). It's built specifically to make remote branch manipulations dead easy. Each time it does an operation on your behalf, it prints it in red at the console. Over time, they finally stick into your brain :-) If you don't want grb to run commands on your behalf, just use the 'explain' feature. The commands will be printed to your console instead of executed for you. Finally, all commands have aliases, to make memorization easier. Note that this is alpha software ;-) Here's the help when you run grb help: Cloning from a local repo will not work with git clone & git fetch: a lot of branches/tags will remain unfetched. To get a clone with all branches and tags. To get a clone with all branches and tags but also with a working copy: Looking at one of answers to the question I noticed that it's possible to shorten it: But beware, if one of remote branches is named as e.g. admin_master  it won't get downloaded! Thanks to bigfish for original idea OK, when you clone your repo, you have all branches there... If you just do git branch, they are kind of hidden... So if you'd like to see all branches name, just simply add --all flag like this: git branch --all or git branch -a If you just checkout to the branch, you get all you need. But how about if the branch created by someone else after you clone? In this case, just do: git fetch and check all branches again... If you like to fetch and checkout at the same time, you can do: git fetch && git checkout your_branch_name Also created the image below for you to simplify what I said:  These code will pull all remote branches code to local repo. For copy-paste into command line: For more readibility:  This will: Based on answer of VonC. None of these answers cut it, except user nobody is on the right track. I was having trouble with moving a repo from one server/system to another. When I cloned the repo, it only created a local branch for master so when I pushed to the new remote, only master branch was pushed. So I found these two methods VERY useful. Hope they help someone else. Method 1: Method 2: I wrote this small Powershell functions to be able to checkout all my git branches, that are on origin remote. More git functions can be found on my git settings repo Here's an answer that uses awk.  This method should suffice if used on a new repo. Existing branches will simply be checked out, or declared as already in it, but filters can be added to avoid the conflicts. It can also be modified so it calls an explicit git checkout -b <branch> -t <remote>/<branch> command. This answer follows Nikos C.'s idea. Alternatively we can specify the remote branch instead.  This is based on murphytalk's answer. It throws fatal error messages on conflicts but I see them harmless. Both commands can be aliased. Using nobody's answer as reference, we can have the following commands to create the aliases: Personally I'd use track-all or track-all-branches. If you’re looking for a self-contained clone or backup that includes all remote branches and commit logs, use: The accepted answer of git branch -a only shows the remote branches. If you attempt to checkout the branches you'll be unable to  unless you still have network access to the origin server. Credit: Gabe Kopley's for suggesting using git pull --all. Note: Of course, if you no longer have network access to the remote/origin server, remote/origin branches will not have any updates reflected in them. Their revisions will reflect commits from the date and time you performed the 2 commands above. To checkout ALL your clone branches to local branches with one command, use one of the bash commands below: OR If your repo has nested branches then this command will take that into account: I needed to do exactly the same. Here is my Ruby script. git clone --mirror on the original repo works well for this. Here is another short one-liner command which creates local branches for all remote branches: It works also properly if tracking local branches are already created. You can call it after the first git clone or any time later. If you do not need to have master branch checked out after cloning, use  Git usually (when not specified) fetches all branches and/or tags (refs, see: git ls-refs) from one or more other repositories along with the objects necessary to complete their histories. In other words it fetches the objects which are reachable by the objects that are already downloaded. See: What does git fetch really do? Sometimes you may have branches/tags which aren't directly connected to the current one, so git pull --all/git fetch --all won't help in that case, but you can list them by: and fetch them manually by knowing the ref names. So to fetch them all, try: The --depth=10000 parameter may help if you've shallowed repository. Then check all your branches again: If above won't help, you need to add missing branches manually to the tracked list (as they got lost somehow): by git remote set-branches like: so it may appear under remotes/origin after fetch: If you still cannot get anything other than the master branch, check the followings:
__label__git __label__git-remote __label__url I have a repo (origin) on a USB key that I cloned on my hard drive (local). I moved "origin" to a NAS and successfully tested cloning it from here. I would like to know if I can change the URI of "origin" in the settings of "local" so it will now pull from the NAS, and not from the USB key. For now, I can see two solutions: push everything to the usb-orign, and copy it to the NAS again (implies a lot of work due to new commits to nas-origin); add a new remote to "local" and delete the old one (I fear I'll break my history). You can (see git help remote) or you can edit .git/config and change the URLs there. You're not in any danger of losing history unless you do something very silly (and if you're worried, just make a copy of your repo, since your repo is your history.) Changing a remote's URL Change Host for a Git Origin Server from: http://pseudofish.com/blog/2010/06/28/change-host-for-a-git-origin-server/ Hopefully this isn’t something you need to do. The server that I’ve been using to collaborate on a few git projects with had the domain name expire. This meant finding a way of migrating the local repositories to get back in sync. Update: Thanks to @mawolf for pointing out there is an easy way with recent git versions (post Feb, 2010): See the man page for details. If you’re on an older version, then try this: As a caveat, this works only as it is the same server, just with different names. Assuming that the new hostname is newhost.com, and the old one was oldhost.com, the change is quite simple. Edit the .git/config file in your working directory. You should see something like: Change oldhost.com to newhost.com, save the file and you’re done. From my limited testing (git pull origin; git push origin; gitx) everything seems in order. And yes, I know it is bad form to mess with git internals. (alternatively, open .git/config, look for [remote "origin"], and edit the url = line. You can check it worked by examining the remotes: Next time you push, you'll have to specify the new upstream branch, e.g.: See also: GitHub: Changing a remote's URL Open Terminal. Ist Step:- Change the current working directory to your local project. 2nd Step:- List your existing remotes in order to get the name of the remote you want to change. git remote -v Change your remote's URL from HTTPS to SSH with the git remote set-url command. 3rd Step:-  git remote set-url origin git@github.com:USERNAME/REPOSITORY.git 4th Step:- Now Verify that the remote URL has changed. git remote -v Verify new remote URL  Write the below command from your repo terminal: Refer this link for more details about changing the url in the remote. As seen here, git remote set-url {name} {url} ex) git remote set-url origin https://github.com/myName/GitTest.git if you cloned your local will automatically consist, remote URL where it gets  cloned. you can check  it using git remote -v if you want to made change in it, here, origin - your branch  if you want to overwrite existing branch you can still use it.. it will override your existing ... it will do, for you... To check git remote connection: Now, set the local repository to remote git:  Now to make it upstream or push use following code: git push --set-upstream origin master -f In the Git Bash, enter the command: git remote set-url origin https://NewRepoLink.git Enter the Credentials Done I worked: You have a lot of ways to do that: Console  Just be sure that you've opened it in a place where a repository is. Config It is placed in .git/config (same folder as repository) TortoiseGit   Then just edit URL. SourceTree Click on the "Settings" button on the toolbar to open the Repository Settings window. Click "Add" to add a remote repository path to the repository. A "Remote details" window will open. Enter a name for the remote path. Enter the URL/Path for the remote repository Enter the username for the hosting service for the remote repository. Click 'OK' to add the remote path. Back on the Repository Settings window, click 'OK'. The new remote path should be added on the repository now. If you need to edit an already added remote path, just click the 'Edit' button. You should be directed to the "Remote details" window where you can edit the details (URL/Path/Host Type) of the remote path. To remove a remote repository path, click the 'Remove' button   ref. Support  Troubleshooting :  You may encounter these errors when trying to changing a remote. No such remote '[name]' This error means that the remote you tried to change doesn't exist: git remote set-url sofake https://github.com/octocat/Spoon-Knife fatal: No such remote 'sofake' Check that you've correctly typed the remote name. Reference : https://help.github.com/articles/changing-a-remote-s-url/ Change remote git URI to git@github.com rather than https://github.com Example: The benefit is that you may do git push automatically when you use ssh-agent : Put a script file $HOME/.ssh/agent to let it runs ssh-add using expect as below: Navigate to the project root of the local repository and check for existing remotes: If your repository is using SSH you will see something like: And if your repository is using HTTPS you will see something like: Changing the URL is done with git remote set-url. Depending on the output of git remote -v, you can change the URL in the following manner: In case of SSH, you can change the URL from REPOSITORY.git to NEW_REPOSITORY.git like: And in case of HTTPS, you can change the URL from REPOSITORY.git to NEW_REPOSITORY.git like: NOTE: If you've changed your GitHub username, you can follow the same process as above to update the change in the username associated with your repository. You would only have to update the USERNAME in the git remote set-url command. If you're using TortoiseGit then follow the below steps: Your branch and all your local commits will remain intact and you can keep working as you were before. To change the remote upstream: git remote set-url origin <url> To add more upstreams: git remote add newplace <url> So you can choose where to work git push origin <branch> or git push newplace <branch> You can change the url by editing the config file. Go to your project root: Then edit the url field and set your new url.  Save the changes. You can verify the changes by using the command. Removing a remote Use the git remote rm command to remove a remote URL from your repository.    An alternative approach is to rename the 'old' origin (in the example below I name it simply old-origin) and adding a new one. This might be the desired approach if you still want to be able to push to the old origin every now and then: And in case you need to push your local state to the new origin: For those who want to make this change from Visual Studio 2019 Open Team Explorer (Ctrl+M) Home -> Settings Git -> Repository Settings Remotes -> Edit  If your repository is private then Reference check your privilege  in my case i need to check  my username  i have two or three repository with seperate credentials. problem is my permission i have two private git server and repositories  this second account is admin of that new repo and first one is my default user account and i should grant permission to first 
__label__checkbox __label__javascript __label__checked __label__jquery __label__selected I'd like to do something like this to tick a checkbox using jQuery: or Does such a thing exist? Use .prop(): If you're working with just one element, you can always just access the underlying HTMLInputElement and modify its .checked property: The benefit to using the .prop() and .attr() methods instead of this is that they will operate on all matched elements. The .prop() method is not available, so you need to use .attr(). Note that this is the approach used by jQuery's unit tests prior to version 1.6 and is preferable to using $('.myCheckbox').removeAttr('checked'); since the latter will, if the box was initially checked, change the behaviour of a call to .reset() on any form that contains it – a subtle but probably unwelcome behaviour change. For more context, some incomplete discussion of the changes to the handling of the checked attribute/property in the transition from 1.5.x to 1.6 can be found in the version 1.6 release notes and the Attributes vs. Properties section of the .prop() documentation. Use: And if you want to check if a checkbox is checked or not: This is the correct way of checking and unchecking checkboxes with jQuery, as it is cross-platform standard, and will allow form reposts. By doing this, you are using JavaScript standards for checking and unchecking checkboxes, so any browser that properly implements the "checked" property of the checkbox element will run this code flawlessly. This should be all major browsers, but I am unable to test previous to Internet Explorer 9. The Problem (jQuery 1.6): Once a user clicks on a checkbox, that checkbox stops responding to the "checked" attribute changes. Here is an example of the checkbox attribute failing to do the job after someone has clicked the checkbox (this happens in Chrome). Fiddle The Solution: By using JavaScript's "checked" property on the DOM elements, we are able to solve the problem directly, instead of trying to manipulate the DOM into doing what we want it to do. Fiddle This plugin will alter the checked property of any elements selected by jQuery, and successfully check and uncheck checkboxes under all circumstances. So, while this may seem like an over-bearing solution, it will make your site's user experience better, and help prevent user frustration. Alternatively, if you do not want to use a plugin, you can use the following code snippets: You can do or If you have custom code in the onclick event for the checkbox that you want to fire, use this one instead: You can uncheck by removing the attribute entirely: You can check all checkboxes like this: You can also extend the $.fn object with new methods: Then you can just do: Or you may want to give them more unique names like mycheck() and myuncheck()  in case you use some other library that uses those names. The last one will fire the click event for the checkbox, the others will not. So if you have custom code in the onclick event for the checkbox that you want to fire, use the last one. To check a checkbox you should use or and to uncheck a check box you should always set it to false: If you do it removes the attribute all together and therefore you will not be able to reset the form. BAD DEMO jQuery 1.6. I think this is broken. For 1.6 I am going to make a new post on that. NEW WORKING DEMO jQuery 1.5.2 works in Chrome. Both demos use This selects elements that have the specified attribute with a value containing the given substring "ckbItem": It will select all elements that contain ckbItem in its name attribute. Assuming that the question is... Remember that in a typical checkbox set, all input tags have the same name, they differ by the attribute value:  there are no ID for each input of the set. Xian's answer can be extended with a more specific selector, using the following line of code: I'm missing the solution. I'll always use: To check a checkbox using jQuery 1.6 or higher just do this: To uncheck, use: Here' s what I like to use to toggle a checkbox using jQuery: If you're using jQuery 1.5 or lower: To uncheck, use: Here is a way to do it without jQuery   function addOrAttachListener(el, type, listener, useCapture) {   if (el.addEventListener) {     el.addEventListener(type, listener, useCapture);   } else if (el.attachEvent) {     el.attachEvent("on" + type, listener);   } };  addOrAttachListener(window, "load", function() {   var cbElem = document.getElementById("cb");   var rcbElem = document.getElementById("rcb");   addOrAttachListener(cbElem, "click", function() {     rcbElem.checked = cbElem.checked;   }, false); }, false); <label>Click Me!   <input id="cb" type="checkbox" /> </label> <label>Reflection:   <input id="rcb" type="checkbox" /> </label>    Here is code for checked and unchecked with a button: Update: Here is the same code block using the newer Jquery 1.6+ prop method, which replaces attr: Try this: We can use elementObject with jQuery for getting the attribute checked: We can use this for all jQuery versions without any error. Update: Jquery 1.6+ has the new prop method which replaces attr, e.g.: If you are using PhoneGap doing application development, and you have a value on the button that you want to show instantly, remember to do this I found that without the span, the interface will not update no matter what you do. Here is the code and demo for how to check multiple check boxes... http://jsfiddle.net/tamilmani/z8TTt/ Another possible solution: As @livefree75 said: jQuery 1.5.x and below You can also extend the $.fn object with new methods: But in new versions of jQuery, we have to use something like this: jQuery 1.6+ Then you can just do: If using mobile and you want the interface to update and show the checkbox as unchecked, use the following: Be aware of memory leaks in Internet Explorer prior to Internet Explorer 9, as the jQuery documentation states: In Internet Explorer prior to version 9, using .prop() to set a DOM   element property to anything other than a simple primitive value   (number, string, or boolean) can cause memory leaks if the property is   not removed (using .removeProp()) before the DOM element is removed   from the document. To safely set values on DOM objects without memory   leaks, use .data(). For jQuery 1.6+ For jQuery 1.5.x and below To check, To check and uncheck  This is probably the shortest and easiest solution: or Even shorter would be: Here is a jsFiddle as well. Plain JavaScript is very simple and much less overhead: Example here I couldn't get it working using: Both true and false would check the checkbox. What worked for me was: When you checked a checkbox like; it might not be enough. You should also call the function below; Especially when you removed the checkbox checked attribute. Here's the complete answer using jQuery I test it and it works 100% :D In case you use ASP.NET MVC, generate many checkboxes and later have to select/unselect all using JavaScript you can do the following. HTML JavaScript
__label__arrays __label__javascript-objects __label__javascript __label__algorithm __label__time-complexity What is the most concise and efficient way to find out if a JavaScript array contains a value? This is the only way I know to do it: Is there a better and more concise way to accomplish this? Modern browsers have Array#includes, which does exactly that and is widely supported by everyone except IE:   console.log(['joe', 'jane', 'mary'].includes('jane')); //true    You can also use Array#indexOf, which is less direct, but doesn't require polyfills for outdated browsers.   console.log(['joe', 'jane', 'mary'].indexOf('jane') >= 0); //true    Many frameworks also offer similar methods: Notice that some frameworks implement this as a function, while others add the function to the array prototype. Update from 2019: This answer is from 2008 (11 years old!) and is not relevant for modern JS usage. The promised performance improvement was based on a benchmark done in browsers of that time. It might not be relevant to modern JS execution contexts. If you need an easy solution, look for other answers. If you need the best performance, benchmark for yourself in the relevant execution environments. As others have said, the iteration through the array is probably the best way, but it has been proven that a decreasing while loop is the fastest way to iterate in JavaScript. So you may want to rewrite your code as follows: Of course, you may as well extend Array prototype: And now you can simply use the following: indexOf maybe, but it's a "JavaScript extension to the ECMA-262 standard; as such it may not be present in other implementations of the standard." Example: AFAICS Microsoft does not offer some kind of alternative to this, but you can add similar functionality to arrays in Internet Explorer (and other browsers that don't support indexOf) if you want to, as a quick Google search reveals (for example, this one). ECMAScript 7 introduces Array.prototype.includes. It can be used like this: It also accepts an optional second argument fromIndex: Unlike indexOf, which uses Strict Equality Comparison, includes compares using SameValueZero equality algorithm. That means that you can detect if an array includes a NaN: Also unlike indexOf, includes does not skip missing indices: Currently it's still a draft but can be polyfilled to make it work on all browsers. The top answers assume primitive types but if you want to find out if an array contains an object with some trait, Array.prototype.some() is an elegant solution: The nice thing about it is that the iteration is aborted once the element is found so unnecessary iteration cycles are spared. Also, it fits nicely in an if statement since it returns a boolean: * As jamess pointed out in the comment, at the time of this answer, September 2018, Array.prototype.some() is fully supported: caniuse.com support table Let's say you've defined an array like so: Below are three ways of checking whether there is a 3 in there. All of them return either true or false. Here's a JavaScript 1.6 compatible implementation of Array.indexOf: Use: Extending the JavaScript Array object is a really bad idea because you introduce new properties (your custom methods) into for-in loops which can break existing scripts. A few years ago the authors of the Prototype library had to re-engineer their library implementation to remove just this kind of thing. If you don't need to worry about compatibility with other JavaScript running on your page, go for it, otherwise, I'd recommend the more awkward, but safer free-standing function solution. One-liner: Thinking out of the box for a second, if you are making this call many many times, it is vastly more efficient to use an associative array a Map to do lookups using a hash function. https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Map I use the following: Today 2020.01.07 I perform tests on MacOs HighSierra 10.13.6 on Chrome v78.0.0, Safari v13.0.4 and Firefox v71.0.0 for 15 chosen solutions. Conclusions  I perform 2 tests cases: for array with 10 elements, and array with 1 milion elements. In both cases we put searched element in the array middle.   let log = (name,f) => console.log(`${name}: 3-${f(arr,'s10')}  's7'-${f(arr,'s7')}  6-${f(arr,6)} 's3'-${f(arr,'s3')}`)  let arr = [1,2,3,4,5,'s6','s7','s8','s9','s10']; //arr = new Array(1000000).fill(123); arr[500000]=7;  function A(a, val) {     var i = -1;     var n = a.length;     while (i++<n) {        if (a[i] === val) {            return true;        }     }     return false; }  function B(a, val) {     var i = a.length;     while (i--) {        if (a[i] === val) {            return true;        }     }     return false; }  function C(a, val) {     for (var i = 0; i < a.length; i++) {         if (a[i] === val) return true;     }     return false; }  function D(a,val) {     var len = a.length;     for(var i = 0 ; i < len;i++)     {         if(a[i] === val) return true;     }     return false; }   function E(a, val){     var n = a.length-1;   var t = n/2;   for (var i = 0; i <= t; i++) {         if (a[i] === val || a[n-i] === val) return true;   }   return false; }  function F(a,val) { 	return a.includes(val); }  function G(a,val) { 	return a.indexOf(val)>=0; }  function H(a,val) { 	return !!~a.indexOf(val); }  function I(a, val) {   return a.findIndex(x=> x==val)>=0; }  function J(a,val) { 	return a.some(x=> x===val); }  function K(a, val) {   const s = JSON.stringify(val);   return a.some(x => JSON.stringify(x) === s); }  function L(a,val) { 	return !a.every(x=> x!==val); }  function M(a, val) {   return !!a.find(x=> x==val); }  function N(a,val) { 	return a.filter(x=>x===val).length > 0; }  function O(a, val) {   return new Set(a).has(val); }  log('A',A); log('B',B); log('C',C); log('D',D); log('E',E); log('F',F); log('G',G); log('H',H); log('I',I); log('J',J); log('K',K); log('L',L); log('M',M); log('N',N); log('O',O); This shippet only presents functions used in performance tests - it not perform tests itself!    Array small - 10 elements You can perform tests in your machine HERE  Array big - 1.000.000 elements You can perform tests in your machine HERE  Array.prototype.some() was added to the ECMA-262 standard in the 5th edition A hopefully faster bidirectional indexOf / lastIndexOf alternative While the new method includes is very nice, the support is basically zero for now. It's long time that I was thinking of way to replace the slow indexOf/lastIndexOf functions. A performant way has already been found, looking at the top answers. From those I chose the contains function posted by @Damir Zekic which should be the fastest one. But it also states that the benchmarks are from 2008 and so are outdated. I also prefer while over for, but for not a specific reason I ended writing the function with a for loop. It could be also done with a while --. I was curious if the iteration was much slower if I check both sides of the array while doing it. Apparently no, and so this function is around two times faster than the top voted ones. Obviously it's also faster than the native one. This in a real world environment, where you never know if the value you are searching is at the beginning or at the end of the array. When you know you just pushed an array with a value, using lastIndexOf remains probably the best solution, but if you have to travel through big arrays and the result could be everywhere, this could be a solid solution to make things faster. Bidirectional indexOf/lastIndexOf http://jsperf.com/bidirectionalindexof As test I created an array with 100k entries. Three queries: at the beginning, in the middle & at the end of the array. I hope you also find this interesting and test the performance. Note: As you can see I slightly modified the contains function to reflect the indexOf & lastIndexOf output (so basically true with the index and false with -1). That shouldn't harm it. The function can also be easily modified to return true or false or even the object, string or whatever it is. And here is the while variant: I think that the simple calculation to get the reflected index in an array is so simple that it's two times faster than doing an actual loop iteration. Here is a complex example doing three checks per iteration, but this is only possible with a longer calculation which causes the slowdown of the code. http://jsperf.com/bidirectionalindexof/2 If you are using JavaScript 1.6 or later (Firefox 1.5 or later) you can use Array.indexOf.  Otherwise, I think you are going to end up with something similar to your original code. Returns array index if found, or -1 if not found If you are checking repeatedly for existence of an object in an array you should maybe look into We use this snippet (works with objects, arrays, strings): Usage: Solution that works in all modern browsers: Usage: IE6+ solution: Usage: Array.indexOf and Array.includes (as well as most of the answers here) only compare by reference and not by value. Non-optimized ES6 one-liner: Note: Comparing objects by value will work better if the keys are in the same order, so to be safe you might sort the keys first with a package like this one: https://www.npmjs.com/package/sort-keys Updated the contains function with a perf optimization. Thanks itinance for pointing it out. Use lodash's some function. It's concise, accurate and has great cross platform support. The accepted answer does not even meet the requirements. Requirements: Recommend most concise and efficient way to find out if a JavaScript array contains an object. Accepted Answer: My recommendation: Notes:  $.inArray works fine for determining whether a scalar value exists in an array of scalars... ... but the question clearly asks for an efficient way to determine if an object is contained in an array. In order to handle both scalars and objects, you could do this: ECMAScript 6 has an elegant proposal on find. The find method executes the callback function once for each element   present in the array until it finds one where callback returns a true   value. If such an element is found, find immediately returns the value   of that element. Otherwise, find returns undefined. callback is   invoked only for indexes of the array which have assigned values; it   is not invoked for indexes which have been deleted or which have never   been assigned values. Here is the MDN documentation on that. The find functionality works like this. You can use this in ECMAScript 5 and below by defining the function. While array.indexOf(x)!=-1 is the most concise way to do this (and has been supported by non-Internet Explorer browsers for over decade...), it is not O(1), but rather O(N), which is terrible. If your array will not be changing, you can convert your array to a hashtable, then do table[x]!==undefined or ===undefined: Demo: (Unfortunately, while you can create an Array.prototype.contains to "freeze" an array and store a hashtable in this._cache in two lines, this would give wrong results if you chose to edit your array later. JavaScript has insufficient hooks to let you keep this state, unlike Python for example.) One can use Set that has the method "has()":   function contains(arr, obj) {       var proxy = new Set(arr);       if (proxy.has(obj))         return true;       else         return false;     }      var arr = ['Happy', 'New', 'Year'];     console.log(contains(arr, 'Happy'));    Use: Demo To know exactly what the tilde ~ do at this point, refer to this question What does a tilde do when it precedes an expression?. OK, you can just optimise your code to get the result!  There are many ways to do this which are cleaner and better, but I just wanted to get your pattern and apply to that using JSON.stringify, just simply do something like this in your case: Simple solution for this requirement is using find() If you're having array of objects like below, Then you can check whether the object with your value is already present or not if data is null then no admin, else it will return the existing object like below. Then you can find the index of that object in the array and replace the object using below code. you will get value like below hope this will help anyone. By no means the best, but I was just getting creative and adding to the repertoire.   Object.defineProperty(Array.prototype, 'exists', {   value: function(element, index) {      var index = index || 0      return index === this.length ? -1 : this[index] === element ? index : this.exists(element, ++index)   } })   // Outputs 1 console.log(['one', 'two'].exists('two'));  // Outputs -1 console.log(['one', 'two'].exists('three'));  console.log(['one', 'two', 'three', 'four'].exists('four'));    Surprised that this question still doesn't have latest syntax added, adding my 2 cents. Let's say we have array of Objects arrObj and we want to search obj in it. Array.prototype.indexOf -> (returns index or -1) is generally used for finding index of element in array. This can also be used for searching object but only works if you are passing reference to same object. Array.prototype.includes -> (returns true or false) Array.prototype.find -> (takes callback, returns first value/object that returns true in CB). Array.prototype.findIndex -> (takes callback, returns index of first value/object that returns true in CB). Since find and findIndex takes a callback, we can be fetch any object(even if we don't have the reference) from array by creatively setting the true condition.       function countArray(originalArray) {           	var compressed = [];     	// make a copy of the input array     	var copyArray = originalArray.slice(0);           	// first loop goes over every element     	for (var i = 0; i < originalArray.length; i++) {           		var count = 0;	     		// loop over every element in the copy and see if it's the same     		for (var w = 0; w < copyArray.length; w++) {     			if (originalArray[i] == copyArray[w]) {     				// increase amount of times duplicate is found     				count++;     				// sets item to undefined     				delete copyArray[w];     			}     		}           		if (count > 0) {     			var a = new Object();     			a.value = originalArray[i];     			a.count = count;     			compressed.push(a);     		}     	}           	return compressed;     };          // It should go something like this:          var testArray = new Array("dog", "dog", "cat", "buffalo", "wolf", "cat", "tiger", "cat");     var newArray = countArray(testArray);     console.log(newArray);   
__label__stream __label__inputstream __label__string __label__io __label__java If you have a java.io.InputStream object, how should you process that object and produce a String? Suppose I have an InputStream that contains text data, and I want to convert it to a String, so for example I can write that to a log file. What is the easiest way to take the InputStream and convert it to a String? A nice way to do this is using Apache commons IOUtils to copy the InputStream into a StringWriter... something like or even Alternatively, you could use ByteArrayOutputStream if you don't want to mix your Streams and Writers Summarize other answers I found 11 main ways to do this (see below). And I wrote some performance tests (see results below): Ways to convert an InputStream to a String: Using IOUtils.toString (Apache Utils)  Using CharStreams (Guava)  Using Scanner (JDK)  Using Stream API (Java 8). Warning: This solution converts different line breaks (like \r\n) to \n. Using parallel Stream API (Java 8). Warning: This solution converts different line breaks (like \r\n) to \n.     Using InputStreamReader and StringBuilder (JDK)  Using StringWriter and IOUtils.copy (Apache Commons) Using ByteArrayOutputStream and inputStream.read (JDK)   Using BufferedReader (JDK). Warning: This solution converts different line breaks (like \n\r) to line.separator system property (for example, in Windows to "\r\n"). Using BufferedInputStream and ByteArrayOutputStream (JDK)  Using inputStream.read() and StringBuilder (JDK). Warning: This solution has problems with Unicode, for example with Russian text (works correctly only with non-Unicode text) Warning: Solutions 4, 5 and 9 convert different line breaks to one. Solution 11 can't work correctly with Unicode text Performance tests Performance tests for small String (length = 175), url in github (mode = Average Time, system = Linux, score 1,343 is the best): Performance tests for big String (length = 50100), url in github (mode = Average Time, system = Linux, score 200,715 is the best): Graphs (performance tests depending on Input Stream length in Windows 7 system)  Performance test (Average Time) depending on Input Stream length in Windows 7 system: Here's a way using only the standard Java library (note that the stream is not closed, your mileage may vary). I learned this trick from "Stupid Scanner tricks" article. The reason it works is because Scanner iterates over tokens in the stream, and in this case we separate tokens using "beginning of the input boundary" (\A), thus giving us only one token for the entire contents of the stream. Note, if you need to be specific about the input stream's encoding, you can provide the second argument to Scanner constructor that indicates what character set to use (e.g. "UTF-8"). Hat tip goes also to Jacob, who once pointed me to the said article. Apache Commons allows: Of course, you could choose other character encodings besides UTF-8. Also see: (documentation) Taking into account file one should first get a java.io.Reader instance. This can then be read and added to a StringBuilder (we don't need StringBuffer if we are not accessing it in multiple threads, and StringBuilder is faster). The trick here is that we work in blocks, and as such don't need other buffering streams. The block size is parameterized for run-time performance optimization. Use: If you are using Google-Collections/Guava you could do the following: Note that the second parameter (i.e. Charsets.UTF_8) for the InputStreamReader isn't necessary, but it is generally a good idea to specify the encoding if you know it (which you should!) This is the best pure Java solution that fits perfectly for Android and any other JVM. This solution works amazingly well... it is simple, fast, and works on small and large streams just the same!! (see benchmark above.. No. 8) For completeness here is Java 9 solution: This uses the readAllBytes method which was added to Java 9. Use: Here's the most elegant, pure-Java (no library) solution I came up with after some experimentation: I did a benchmark upon 14 distinct answers here (sorry for not providing credits but there are too many duplicates). The result is very surprising. It turns out that Apache IOUtils is the slowest and ByteArrayOutputStream is the fastest solutions: So first here is the best method: Time in milliseconds I'd use some Java 8 tricks. Essentially the same as some other answers except more succinct. I ran some timing tests because time matters, always. I attempted to get the response into a String 3 different ways. (shown below) I left out try/catch blocks for the sake readability. To give context, this is the preceding code for all 3 approaches: 1) 2) 3) So, after running 500 tests on each approach with the same request/response data, here are the numbers. Once again, these are my findings and your findings may not be exactly the same, but I wrote this to give some indication to others of the efficiency differences of these approaches. Ranks: Approach #1 Approach #3 - 2.6% slower than #1 Approach #2 - 4.3% slower than #1 Any of these approaches is an appropriate solution for grabbing a response and creating a String from it. Pure Java solution using Streams, works since Java 8. As mentioned by Christoffer Hammarström below other answer it is safer to explicitly specify the Charset. I.e. The InputStreamReader constructor can be changes as follows: Here's more-or-less sampath's answer, cleaned up a bit and represented as a function: If you were feeling adventurous, you could mix Scala and Java and end up with this: Mixing Java and Scala code and libraries has it's benefits. See full description here: Idiomatic way to convert an InputStream to a String in Scala If you can't use Commons IO (FileUtils/IOUtils/CopyUtils), here's an example using a BufferedReader to read the file line by line: Or if you want raw speed I'd propose a variation on what Paul de Vrieze suggested (which avoids using a StringWriter (which uses a StringBuffer internally): Make sure to close the streams at end if you use Stream Readers EDIT: On JDK 7+, you can use try-with-resources construct. This is an answer adapted from org.apache.commons.io.IOUtils source code, for those who want to have the apache implementation but do not want the whole library. This one is nice because: How to do it? For JDK 9 Another one, for all the Spring users: The utility methods in org.springframework.util.StreamUtils are similar to the ones in FileCopyUtils, but they leave the stream open when done. Use the java.io.InputStream.transferTo(OutputStream) supported in Java 9 and the ByteArrayOutputStream.toString(String) which takes the charset name: Here is the complete method for converting InputStream into String without using any third party library. Use StringBuilder for single threaded environment otherwise use StringBuffer.   Here's how to do it using just the JDK using byte array buffers. This is actually how the commons-io IOUtils.copy() methods all work. You can replace byte[] with char[] if you're copying from a Reader instead of an InputStream. Kotlin users simply do: whereas  is Kotlin standard library’s built-in extension method.  The easiest way in JDK is with the following code snipplets.  Here's my Java 8 based solution, which uses the new Stream API to collect all lines from an InputStream: In terms of reduce, and concat it can be expressed in Java 8 as:
__label__git __label__undo How do I reset my local branch to be just like the branch on the remote repository? I did: But when I run a git status, Can you please tell me why I have these 'modified'? I haven't touched these files? If I did, I want to remove those. Setting your branch to exactly match the remote branch can be done in two steps: Update @2020 (if you have main branch instead of master in remote repo) If you want to save your current branch's state before doing this (just in case), you can do: Now your work is saved on the branch "my-saved-work" in case you decide you want it back (or want to look at it later or diff it against your updated branch). Note that the first example assumes that the remote repo's name is "origin" and that the branch named "master" in the remote repo matches the currently checked-out branch in your local repo. BTW, this situation that you're in looks an awful lot like a common case where a push has been done into the currently checked out branch of a non-bare repository. Did you recently push into your local repo? If not, then no worries -- something else must have caused these files to unexpectedly end up modified. Otherwise, you should be aware that it's not recommended to push into a non-bare repository (and not into the currently checked-out branch, in particular). I needed to do (the solution in the accepted answer): Followed by: to remove local files To see what files will be removed (without actually removing them): First, use git reset to reset to the previously fetched HEAD of the corresponding upstream branch: The advantage of specifying @{u} or its verbose form @{upstream} is that the name of the remote repo and branch don't have to be explicitly specified. On Windows or with PowerShell, specify "@{u}" (with double quotes). Next, as needed, use git clean to remove untracked files, optionally also with -x: Finally, as needed, get the latest changes: git reset --hard HEAD actually only resets to the last committed state. In this case HEAD refers to the HEAD of your branch.  If you have several commits, this won't work.. What you probably want to do, is reset to the head of origin or whatever you remote repository is called. I'd probably just do something like  Be careful though. Hard resets cannot easily be undone. It is better to do as Dan suggests, and branch off a copy of your changes before resetting.  All of the above suggests are right, but often to really reset your project, you also need to remove even files that are in your .gitignore. To get the moral equivalent of erasing your project directory and re-cloning from the remote is: Warning: git clean -x -d -f is irreversible and you may lose files and data (e.g. things you have ignored using .gitignore). Use the commands below. These commands will remove all untracked files from local git too The question mixes two issues here: The one-stop-answer is: This is something I face regularly, & I've generalised the script Wolfgang provided above to work with any branch I also added an "are you sure" prompt, & some feedback output Provided that the remote repository is origin, and that you're interested in branch_name: Also, you go for reset the current branch of origin to HEAD. How it works: git fetch origin downloads the latest from remote without trying to merge or rebase anything. Then the git reset resets the <branch_name> branch to what you just fetched. The --hard option changes all the files in your working tree to match the files in origin/branch_name. I did: to totally reset branch note, you should checkout to another branch to be able to delete required branch Here is a script that automates what the most popular answer suggests ... See https://stackoverflow.com/a/13308579/1497139 for an improved version that supports branches If you had a problem as me, that you have already committed some changes, but now, for any reason you want to get rid of it, the quickest way is to use git reset like this: I had 2 not needed commits, hence the number 2. You can change it to your own number of commits to reset. So answering your question - if you're 5 commits ahead of remote repository HEAD, you should run this command: Notice that you will lose the changes you've made, so be careful! The answer was underrated (-d to remove directories). Thanks! Previous answers assume that the branch to be reset is the current branch (checked out). In comments, OP hap497 clarified that the branch is indeed checked out, but this is not explicitly required by the original question. Since there is at least one "duplicate" question, Reset branch completely to repository state, which does not assume that the branch is checked out, here's an alternative: If branch "mybranch" is not currently checked out, to reset it to remote branch "myremote/mybranch"'s head, you can use this low-level command: This method leaves the checked out branch as it is, and the working tree untouched. It simply moves mybranch's head to another commit, whatever is given as the second argument. This is especially helpful if multiple branches need to be updated to new remote heads. Use caution when doing this, though, and use gitk or a similar tool to double check source and destination. If you accidentally do this on the current branch (and git will not keep you from this), you may become confused, because the new branch content does not match the working tree, which did not change (to fix, update the branch again, to where it was before). This is what I use often: Note that it is good practice not to make changes to your local master/develop branch, but instead checkout to another branch for any change, with the branch name prepended by the type of change, e.g. feat/, chore/, fix/, etc. Thus you only need to pull changes, not push any changes from master. Same thing for other branches that others contribute to. So the above should only be used if you have happened to commit changes to a branch that others have committed to, and need to reset. Otherwise in future avoid pushing to a branch that others push to, instead checkout and push to the said branch via the checked out branch. If you want to reset your local branch to the latest commit in the upstream branch, what works for me so far is: Check your remotes, make sure your upstream and origin are what you expect, if not as expected then use git remote add upstream <insert URL>, e.g. of the original GitHub repo that you forked from, and/or git remote add origin <insert URL of the forked GitHub repo>. On GitHub, you can also checkout the branch with the same name as the local one, in order to save the work there, although this isn't necessary if origin develop has the same changes as the local saved-work branch. I'm using the develop branch as an example, but it can be any existing branch name. Then if you need to merge these changes with another branch while where there are any conflicts, preserving the changes in develop, use: While use to preserve branch_name's conflicting changes. Otherwise use a mergetool with git mergetool. With all the changes together: Note that instead of upstream/develop you could use a commit hash, other branch name, etc. Use a CLI tool such as Oh My Zsh to check that your branch is green indicating that there is nothing to commit and the working directory is clean (which is confirmed or also verifiable by git status). Note that this may actually add commits compared to upstream develop if there is anything automatically added by a commit, e.g. UML diagrams, license headers, etc., so in that case, you could then pull the changes on origin develop to upstream develop, if needed. If you want to go back to the HEAD state for both the working directory and the index, then you should git reset --hard HEAD, rather than to HEAD^. (This may have been a typo, just like the single versus double dash for --hard.) As for your specific question as to why those files appear in the status as modified, it looks like perhaps you did a soft reset instead of a hard reset.  This will cause the files that were changed in the HEAD commit to appear as if they were staged, which is likely what you are seeing here. No amount of reset and cleaning seemed to have any effect on untracked and modified files in my local git repo (I tried all the options above). My only solution to this was to rm the local repo and re-clone it from the remote. Fortunately I didn't have any other branches I cared about. xkcd: Git Only 3 commands will make it work The only solution that works in all cases that I've seen is to delete and reclone. Maybe there's another way, but obviously this way leaves no chance of old state being left there, so I prefer it. Bash one-liner you can set as a macro if you often mess things up in git: * assumes your .git files aren't corrupt Have you forgotten to create a feature-branch and have committed directly on master by mistake?  You can create the feature branch now and set master back without affecting the worktree (local filesystem) to avoid triggering builds, tests and trouble with file-locks: If you don't mind saving your local changes, yet still want to update your repository to match origin/HEAD, you can simply stash your local changes and then pull:
__label__c++ __label__c++-faq This question attempts to collect the few pearls among the dozens of bad C++ books that are published every year. Unlike many other programming languages, which are often picked up on the go from tutorials found on the Internet, few are able to quickly pick up C++ without studying a well-written C++ book. It is way too big and complex for doing this. In fact, it is so big and complex, that there are very many very bad C++ books out there. And we are not talking about bad style, but things like sporting glaringly obvious factual errors and promoting abysmally bad programming styles. Please edit the accepted answer to provide quality books and an approximate skill level — preferably after discussing your addition in the C++ chat room. (The regulars might mercilessly undo your work if they disagree with a recommendation.) Add a short blurb/description about each book that you have personally read/benefited from. Feel free to debate quality, headings, etc. Books that meet the criteria will be added to the list.  Books that have reviews by the Association of C and C++ Users (ACCU) have links to the review. *Note: FAQs and other resources can be found in the C++ tag info and under c++-faq.  C++ Primer * (Stanley Lippman, Josée Lajoie, and Barbara E. Moo)  (updated for C++11) Coming at 1k pages, this is a very thorough introduction into C++ that covers just about everything in the language in a very accessible format and in great detail. The fifth edition (released August 16, 2012) covers C++11. [Review] * Not to be confused with C++ Primer Plus (Stephen Prata), with a significantly less favorable review. Programming: Principles and Practice Using C++ (Bjarne Stroustrup, 2nd Edition - May 25, 2014) (updated for C++11/C++14) An introduction to programming using C++ by the creator of the language. A good read, that assumes no previous programming experience, but is not only for beginners. A Tour of C++ (Bjarne Stroustrup) (2nd edition for C++17) The “tour” is a quick (about 180 pages and 14 chapters) tutorial overview of all of standard C++ (language and standard library, and using C++11) at a moderately high level for people who already know C++ or at least are experienced programmers. This book is an extended version of the material that constitutes Chapters 2-5 of The C++ Programming Language, 4th edition. Accelerated C++ (Andrew Koenig and Barbara Moo, 1st Edition - August 24, 2000)  This basically covers the same ground as the C++ Primer, but does so on a fourth of its space. This is largely because it does not attempt to be an introduction to programming, but an introduction to C++ for people who've previously programmed in some other language. It has a steeper learning curve, but, for those who can cope with this, it is a very compact introduction to the language. (Historically, it broke new ground by being the first beginner's book to use a modern approach to teaching the language.) Despite this, the C++ it teaches is purely C++98. [Review] Effective C++ (Scott Meyers, 3rd Edition - May 22, 2005)  This was written with the aim of being the best second book C++ programmers should read, and it succeeded. Earlier editions were aimed at programmers coming from C, the third edition changes this and targets programmers coming from languages like Java. It presents ~50 easy-to-remember rules of thumb along with their rationale in a very accessible (and enjoyable) style. For C++11 and C++14 the examples and a few issues are outdated and Effective Modern C++ should be preferred. [Review] Effective Modern C++ (Scott Meyers) This is basically the new version of Effective C++, aimed at C++ programmers making the transition from C++03 to C++11 and C++14. Effective STL (Scott Meyers)  This aims to do the same to the part of the standard library coming from the STL what Effective C++ did to the language as a whole: It presents rules of thumb along with their rationale. [Review] More Effective C++ (Scott Meyers) Even more rules of thumb than Effective C++. Not as important as the ones in the first book, but still good to know. Exceptional C++ (Herb Sutter)  Presented as a set of puzzles, this has one of the best and thorough discussions of the proper resource management and exception safety in C++ through Resource Acquisition is Initialization (RAII) in addition to in-depth coverage of a variety of other topics including the pimpl idiom, name lookup, good class design, and the C++ memory model. [Review] More Exceptional C++ (Herb Sutter)  Covers additional exception safety topics not covered in Exceptional C++, in addition to discussion of effective object-oriented programming in C++ and correct use of the STL. [Review] Exceptional C++ Style (Herb Sutter)  Discusses generic programming, optimization, and resource management; this book also has an excellent exposition of how to write modular code in C++ by using non-member functions and the single responsibility principle. [Review] C++ Coding Standards (Herb Sutter and Andrei Alexandrescu) “Coding standards” here doesn't mean “how many spaces should I indent my code?”  This book contains 101 best practices, idioms, and common pitfalls that can help you to write correct, understandable, and efficient C++ code. [Review] C++ Templates: The Complete Guide (David Vandevoorde and Nicolai M. Josuttis) This is the book about templates as they existed before C++11.  It covers everything from the very basics to some of the most advanced template metaprogramming and explains every detail of how templates work (both conceptually and at how they are implemented) and discusses many common pitfalls.  Has excellent summaries of the One Definition Rule (ODR) and overload resolution in the appendices. A second edition covering C++11, C++14 and C++17 has been already published. [Review] C++ 17 - The Complete Guide (Nicolai M. Josuttis) This book describes all the new features introduced in the C++17 Standard covering everything from the simple ones like 'Inline Variables', 'constexpr if' all the way up to 'Polymorphic Memory Resources' and 'New and Delete with overaligned Data'. [Review] C++ in Action (Bartosz Milewski). This book explains C++ and its features by building an application from ground up. [Review] Functional Programming in C++ (Ivan Čukić). This book introduces functional programming techniques to modern C++ (C++11 and later). A very nice read for those who want to apply functional programming paradigms to C++. Professional C++ (Marc Gregoire) Provides a comprehensive and detailed tour of the C++ language implementation replete with professional tips and concise but informative in-text examples, emphasizing C++17 features. Modern C++ Design (Andrei Alexandrescu)  A groundbreaking book on advanced generic programming techniques.  Introduces policy-based design, type lists, and fundamental generic programming idioms then explains how many useful design patterns (including small object allocators, functors, factories, visitors, and multi-methods) can be implemented efficiently, modularly, and cleanly using generic programming. [Review] C++ Template Metaprogramming (David Abrahams and Aleksey Gurtovoy) C++ Concurrency In Action (Anthony Williams) A book covering C++11 concurrency support including the thread library, the atomics library, the C++ memory model, locks and mutexes, as well as issues of designing and debugging multithreaded applications. A second edition covering C++14 and C++17 has been already published. [Review] Advanced C++ Metaprogramming (Davide Di Gennaro) A pre-C++11 manual of TMP techniques, focused more on practice than theory.  There are a ton of snippets in this book, some of which are made obsolete by type traits, but the techniques, are nonetheless useful to know.  If you can put up with the quirky formatting/editing, it is easier to read than Alexandrescu, and arguably, more rewarding.  For more experienced developers, there is a good chance that you may pick up something about a dark corner of C++ (a quirk) that usually only comes about through extensive experience. The C++ Programming Language (Bjarne Stroustrup) (updated for C++11) The classic introduction to C++ by its creator. Written to parallel the classic K&R, this indeed reads very much like it and covers just about everything from the core language to the standard library, to programming paradigms to the language's philosophy. [Review] Note: All releases of the C++ standard are tracked in the question "Where do I find the current C or C++ standard documents?". C++ Standard Library Tutorial and Reference (Nicolai Josuttis) (updated for C++11) The introduction and reference for the C++ Standard Library. The second edition (released on April 9, 2012) covers C++11. [Review] The C++ IO Streams and Locales (Angelika Langer and Klaus Kreft)  There's very little to say about this book except that, if you want to know anything about streams and locales, then this is the one place to find definitive answers. [Review] C++11/14/17/… References: The C++11/14/17 Standard (INCITS/ISO/IEC 14882:2011/2014/2017) This, of course, is the final arbiter of all that is or isn't C++. Be aware, however, that it is intended purely as a reference for experienced users willing to devote considerable time and effort to its understanding. The C++17 standard is released in electronic form for 198 Swiss Francs. The C++17 standard is available, but seemingly not in an economical form – directly from the ISO it costs 198 Swiss Francs (about $200 US). For most people, the final draft before standardization is more than adequate (and free). Many will prefer an even newer draft, documenting new features that are likely to be included in C++20. Overview of the New C++ (C++11/14) (PDF only) (Scott Meyers) (updated for C++14) These are the presentation materials (slides and some lecture notes) of a three-day training course offered by Scott Meyers, who's a highly respected author on C++. Even though the list of items is short, the quality is high. The C++ Core Guidelines (C++11/14/17/…) (edited by Bjarne Stroustrup and Herb Sutter) is an evolving online document consisting of a set of guidelines for using modern C++ well. The guidelines are focused on relatively higher-level issues, such as interfaces, resource management, memory management and concurrency affecting application architecture and library design. The project was announced at CppCon'15 by Bjarne Stroustrup and others and welcomes contributions from the community. Most guidelines are supplemented with a rationale and examples as well as discussions of possible tool support. Many rules are designed specifically to be automatically checkable by static analysis tools. The C++ Super-FAQ (Marshall Cline, Bjarne Stroustrup and others) is an effort by the Standard C++ Foundation to unify the C++ FAQs previously maintained individually by Marshall Cline and Bjarne Stroustrup and also incorporating new contributions. The items mostly address issues at an intermediate level and are often written with a humorous tone. Not all items might be fully up to date with the latest edition of the C++ standard yet. cppreference.com (C++03/11/14/17/…) (initiated by Nate Kohl) is a wiki that summarizes the basic core-language features and has extensive documentation of the C++ standard library. The documentation is very precise but is easier to read than the official standard document and provides better navigation due to its wiki nature. The project documents all versions of the C++ standard and the site allows filtering the display for a specific version. The project was presented by Nate Kohl at CppCon'14. Note: Some information contained within these books may not be up-to-date or no longer considered best practice. The Design and Evolution of C++ (Bjarne Stroustrup)  If you want to know why the language is the way it is, this book is where you find answers. This covers everything before the standardization of C++. Ruminations on C++ - (Andrew Koenig and Barbara Moo) [Review] Advanced C++ Programming Styles and Idioms (James Coplien)  A predecessor of the pattern movement, it describes many C++-specific “idioms”. It's certainly a very good book and might still be worth a read if you can spare the time, but quite old and not up-to-date with current C++. Large Scale C++ Software Design (John Lakos)  Lakos explains techniques to manage very big C++ software projects. Certainly, a good read, if it only was up to date. It was written long before C++ 98 and misses on many features (e.g. namespaces) important for large-scale projects. If you need to work in a big C++ software project, you might want to read it, although you need to take more than a grain of salt with it. The first volume of a new edition is released in 2019. Inside the C++ Object Model (Stanley Lippman)  If you want to know how virtual member functions are commonly implemented and how base objects are commonly laid out in memory in a multi-inheritance scenario, and how all this affects performance, this is where you will find thorough discussions of such topics. The Annotated C++ Reference Manual (Bjarne Stroustrup, Margaret A. Ellis) This book is quite outdated in the fact that it explores the 1989 C++ 2.0 version - Templates, exceptions, namespaces and new casts were not yet introduced. Saying that however, this book goes through the entire C++ standard of the time explaining the rationale, the possible implementations, and features of the language. This is not a book to learn programming principles and patterns on C++, but to understand every aspect of the C++ language. Thinking in C++ (Bruce Eckel, 2nd Edition, 2000).  Two volumes; is a tutorial style free set of intro level books. Downloads: vol 1, vol 2. Unfortunately they're marred by a number of trivial errors (e.g. maintaining that temporaries are automatically const), with no official errata list. A partial 3rd party errata list is available at http://www.computersciencelab.com/Eckel.htm, but it is apparently not maintained. Scientific and Engineering C++: An Introduction to Advanced Techniques and Examples (John Barton and Lee Nackman) It is a comprehensive and very detailed book that tried to explain and make use of all the features available in C++, in the context of numerical methods. It introduced at the time several new techniques, such as the Curiously Recurring Template Pattern (CRTP, also called Barton-Nackman trick). It pioneered several techniques such as dimensional analysis and automatic differentiation. It came with a lot of compilable and useful code, ranging from an expression parser to a Lapack wrapper. The code is still available online. Unfortunately, the books have become somewhat outdated in the style and C++ features, however, it was an incredible tour-de-force at the time (1994, pre-STL). The chapters on dynamics inheritance are a bit complicated to understand and not very useful. An updated version of this classic book that includes move semantics and the lessons learned from the STL would be very nice.
__label__unix-timestamp __label__datetime __label__date __label__javascript __label__timestamp How can I get a timestamp in JavaScript? Something similar to Unix timestamp, that is, a single number that represents the current time and date. Either as a number or a string. A unary operator like plus triggers the valueOf method in the Date object and it returns the timestamp (without any alteration). Details: On almost all current browsers you can use Date.now() to get the UTC timestamp in milliseconds; a notable exception to this is IE8 and earlier (see compatibility table). You can easily make a shim for this, though: To get the timestamp in seconds, you can use: Or alternatively you could use: Which should be slightly faster, but also less readable (also see this answer). I would recommend using Date.now() (with compatibility shim). It's slightly better because it's shorter & doesn't create a new Date object. However, if you don't want a shim & maximum compatibility, you could use the "old" method to get the timestamp in milliseconds: Which you can then convert to seconds like this: And you can also use the valueOf method which we showed above: Timestamp in Milliseconds   var timeStampInMs = window.performance && window.performance.now && window.performance.timing && window.performance.timing.navigationStart ? window.performance.now() + window.performance.timing.navigationStart : Date.now();  console.log(timeStampInMs, Date.now());    I like this, because it is small: I also like this, because it is just as short and is compatible with modern browsers, and over 500 people voted that it is better:  JavaScript works with the number of milliseconds since the epoch whereas most other languages work with the seconds. You could work with milliseconds but as soon as you pass a value to say PHP, the PHP native functions will probably fail. So to be sure I always use the seconds, not milliseconds. This will give you a Unix timestamp (in seconds): This will give you the milliseconds since the epoch (not Unix timestamp):  I provide multiple solutions with descriptions in this answer. Feel free to ask questions if anything is unclear PS: sadly someone merged this to the top answer without giving credit. Quick and dirty solution: Warning: it might break in 2038 and return negative numbers if you do the |0 magic. Use Math.floor() instead by that time Math.floor() solution: Some nerdy alternative by Derek 朕會功夫 taken from the comments below this answer: Polyfill to get Date.now() working: To get it working in IE you could do this (Polyfill from MDN): If you do not care about the year / day of week / daylight saving time you could strip it away and use this after 2038:  Some output of how it will look: Of course it will break daylight saving time but depending on what you   are building this might be useful to you if you need to do binary   operations on timestamps after int32 will break in 2038. This will also return negative values but only if the user of that PC   you are running your code on is changing their PC's clock at least to   31th of december of the previous year. If you just want to know the relative time from the point of when the code was run through first you could use something like this: In case you are using jQuery you could use $.now() as described in jQuery's Docs which makes the polyfill obsolete since $.now() internally does the same thing: (new Date).getTime() If you are just happy about jQuery's version consider upvoting this answer since I did not find it myself. Now a tiny explaination of what |0 does: By providing |, you tell the interpreter to do a binary OR operation. Bit operations require absolute numbers which turns the decimal result from Date.now() / 1000 into an integer.   During that conversion, decimals are removed, resulting in the same result as using Math.floor() but using less code. Be warned though: it will convert a 64 bit double to a 32 bit integer. This will result in information loss when dealing with huge numbers. Timestamps will break after 2038 due to 32 bit integer overflow. For further information about Date.now follow this link: Date.now() @ MDN  jQuery provides its own method to get the timestamp: (besides it just implements (new Date).getTime() expression) REF: http://api.jquery.com/jQuery.now/ In addition to the other options, if you want a dateformat ISO, you get can get it directly   console.log(new Date().toISOString());      console.log(new Date().valueOf()); // returns the number of milliseconds since the epoch    Just to add up, here's a function to return a timestamp string in Javascript.  Example: 15:06:38 PM Date, a native object in JavaScript is the way we get all data about time. Just be careful in JavaScript the timestamp depends on the client computer set, so it's not 100% accurate timestamp. To get the best result, you need to get the timestamp from the server-side.  Anyway, my preferred way is using vanilla. This is a common way of doing it in JavaScript: In MDN it's mentioned as below: The Date.now() method returns the number of milliseconds elapsed since   1 January 1970 00:00:00 UTC.   Because now() is a static method of Date, you always use it as Date.now(). If you using a version below ES5, Date.now(); not works and you need to use: Today - 2020.04.23 I perform tests for chosen solutions. I tested on MacOs High Sierra 10.13.6 on Chrome 81.0, Safari 13.1, Firefox 75.0  Results for chrome  You can perform test on your machine HERE Code used in tests is presented in below snippet   function A() {   return new Date().getTime(); }  function B() {   return new Date().valueOf(); }  function C() {   return +new Date(); }  function D() {   return new Date()*1; }  function E() {   return Date.now(); }  function F() {   return Number(new Date()); }  function G() {   // this solution returns time counted from loading the page.   // (and on Chrome it gives better precission)   return performance.now();  }    // TEST  log = (n,f) => console.log(`${n} : ${f()}`);  log('A',A); log('B',B); log('C',C); log('D',D); log('E',E); log('F',F); log('G',G); This snippet only presents code used in external benchmark    One I haven't seen yet  Another one I haven't seen yet is The Date.getTime() method can be used with a little tweak: The value returned by the getTime method is the number of milliseconds   since 1 January 1970 00:00:00 UTC. Divide the result by 1000 to get the Unix timestamp, floor if necessary: The Date.valueOf() method is functionally equivalent to Date.getTime(), which makes it possible to use arithmetic operators on date object to achieve identical results. In my opinion, this approach affects readability. The code Math.floor(new Date().getTime() / 1000) can be shortened to new Date / 1E3 | 0. Consider to skip direct getTime() invocation and use | 0 as a replacement for Math.floor() function. It's also good to remember 1E3 is a shorter equivalent for 1000 (uppercase E is preferred than lowercase to indicate 1E3 as a constant). As a result you get the following:   var ts = new Date / 1E3 | 0;  console.log(ts);    For a timestamp with microsecond resolution, there's performance.now: This could for example yield 1436140826653.139, while Date.now only gives 1436140826653. I highly recommend using moment.js. To get the number of milliseconds since UNIX epoch, do  To get the number of seconds since UNIX epoch, do You can also convert times like so: I do that all the time. No pun intended. To use moment.js in the browser: For more details, including other ways of installing and using MomentJS, see their docs You can only use        var timestamp = new Date().getTime();     console.log(timestamp);    to get the current timestamp. No need to do anything extra. Here is a simple function to generate timestamp in the format: mm/dd/yy hh:mi:ss    // The Current Unix Timestamp // 1443534720 seconds since Jan 01 1970. (UTC)  // seconds console.log(Math.floor(new Date().valueOf() / 1000)); // 1443534720 console.log(Math.floor(Date.now() / 1000)); // 1443534720 console.log(Math.floor(new Date().getTime() / 1000)); // 1443534720  // milliseconds console.log(Math.floor(new Date().valueOf())); // 1443534720087 console.log(Math.floor(Date.now())); // 1443534720087 console.log(Math.floor(new Date().getTime())); // 1443534720087  // jQuery // seconds console.log(Math.floor($.now() / 1000)); // 1443534720 // milliseconds console.log($.now()); // 1443534720087 <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script>    Any browsers not supported Date.now, you can use this for get current date time: This one has a solution : which converts unixtime stamp to tim in js try this I learned a really cool way of converting a given Date object to a Unix timestamp from the source code of JQuery Cookie the other day. Here's an example: If want a basic way to generate a timestamp in Node.js this works well. Our team is using this to bust cache in a localhost environment. The output is /dist/css/global.css?v=245521377 where 245521377 is the timestamp generated by hrtime().  Hopefully this helps, the methods above can work as well but I found this to be the simplest approach for our needs in Node.js. This seems to work. If it is for logging purposes, you can use ISOString new Date().toISOString() "2019-05-18T20:02:36.694Z" For lodash and underscore users, use _.now. Moment.js can abstract away a lot of the pain in dealing with Javascript Dates.  See: http://momentjs.com/docs/#/displaying/unix-timestamp/ As of writing this, the top answer is 9 years old, and a lot has changed since then - not least, we have near universal support for a non-hacky solution: If you want to be absolutely certain that this won't break in some ancient (pre ie9) browser, you can put it behind a check, like so: This will return the milliseconds since epoch time, of course, not seconds. MDN Documentation on Date.now more simpler way:
__label__javascript __label__ajax __label__json __label__security Why does Google prepend while(1); to their (private) JSON responses? For example, here's a response while turning a calendar on and off in Google Calendar: I would assume this is to prevent people from doing an eval() on it, but all you'd really have to do is replace the while and then you'd be set. I would assume the eval prevention is to make sure people write safe JSON parsing code. I've seen this used in a couple of other places, too, but a lot more so with Google (Mail, Calendar, Contacts, etc.) Strangely enough, Google Docs starts with &&&START&&& instead, and Google Contacts seems to start with while(1); &&&START&&&. What's going on here? It prevents JSON hijacking, a major JSON security issue that is formally fixed in all major browsers since 2011 with ECMAScript 5. Contrived example: say Google has a URL like mail.google.com/json?action=inbox which returns the first 50 messages of your inbox in JSON format. Evil websites on other domains can't make AJAX requests to get this data due to the same-origin policy, but they can include the URL via a <script> tag. The URL is visited with your cookies, and by overriding the global array constructor or accessor methods they can have a method called whenever an object (array or hash) attribute is set, allowing them to read the JSON content. The while(1); or &&&BLAH&&& prevents this: an AJAX request at mail.google.com will have full access to the text content, and can strip it away. But a <script> tag insertion blindly executes the JavaScript without any processing, resulting in either an infinite loop or a syntax error. This does not address the issue of cross-site request forgery. It prevents disclosure of the response through JSON hijacking. In theory, the content of HTTP responses are protected by the Same Origin Policy: pages from one domain cannot get any pieces of information from pages on the other domain (unless explicitly allowed). An attacker can request pages on other domains on your behalf, e.g. by using a <script src=...> or <img> tag, but it can't get any information about the result (headers, contents). Thus, if you visit an attacker's page, it couldn't read your email from gmail.com. Except that when using a script tag to request JSON content, the JSON is executed as JavaScript in an attacker's controlled environment. If the attacker can replace the Array or Object constructor or some other method used during object construction, anything in the JSON would pass through the attacker's code, and be disclosed. Note that this happens at the time the JSON is executed as JavaScript, not at the time it's parsed. There are multiple countermeasures: By placing a while(1); statement before the JSON data, Google makes sure that the JSON data is never executed as JavaScript. Only a legitimate page could actually get the whole content, strip the while(1);, and parse the remainder as JSON. Things like for(;;); have been seen at Facebook for instance, with the same results. Similarly, adding invalid tokens before the JSON, like &&&START&&&, makes sure that it is never executed. This is OWASP recommended way to protect from JSON hijacking and is the less intrusive one. Similarly to the previous counter-measures, it makes sure that the JSON is never executed as JavaScript. A valid JSON object, when not enclosed by anything, is not valid in JavaScript: This is however valid JSON: So, making sure you always return an Object at the top level of the response makes sure that the JSON is not valid JavaScript, while still being valid JSON. As noted by @hvd in the comments, the empty object {} is valid JavaScript, and knowing the object is empty may itself be valuable information. The OWASP way is less intrusive, as it needs no client library changes, and transfers valid JSON. It is unsure whether past or future browser bugs could defeat this, however.  As noted by @oriadam, it is unclear whether data could be leaked in a parse error through an error handling or not (e.g. window.onerror). Google's way requires a client library in order for it to support automatic de-serialization and can be considered to be safer with regard to browser bugs. Both methods require server side changes in order to avoid developers accidentally sending vulnerable JSON. This is to ensure some other site can't do nasty tricks to try to steal your data. For example, by replacing the array constructor, then including this JSON URL via a <script> tag, a malicious third-party site could steal the data from the JSON response. By putting a while(1); at the start, the script will hang instead. A same-site request using XHR and a separate JSON parser, on the other hand, can easily ignore the while(1); prefix. That would be to make it difficult for a third-party to insert the JSON response into an HTML document with the <script> tag. Remember that the <script> tag is exempt from the Same Origin Policy. Note: as of 2019, many of the old vulnerabilities that lead to the preventative measures discussed in this question are no longer an issue in modern browsers.  I'll leave the answer below as a historical curiosity, but really the whole topic has changed radically since 2010 (!!) when this was asked. It prevents it from being used as the target of a simple <script> tag. (Well, it doesn't prevent it, but it makes it unpleasant.)  That way bad guys can't just put that script tag in their own site and rely on an active session to make it possible to fetch your content. edit — note the comment (and other answers). The issue has to do with subverted built-in facilities, specifically the Object and Array constructors. Those can be altered such that otherwise innocuous JSON, when parsed, could trigger attacker code. Since the <script> tag is exempted from the Same Origin Policy which is a security necessity in the web world, while(1) when added to the JSON response prevents misuse of it in the <script> tag.  After authentication is in place, JSON hijacking protection can take a   variety of forms. Google appends while(1) into their JSON data, so   that if any malicious script evaluates it, the malicious script enters   an infinite loop. Reference: Web Security Testing Cookbook: Systematic Techniques to Find Problems Fast As this is a High traffic post i hope to provide here an answer slightly more undetermined to the original question and thus to provide further background on a JSON Hijacking attack and its consequences JSON Hijacking as the name suggests is an attack similar to Cross-Site Request Forgery where an attacker can access cross-domain sensitive JSON data from applications that return sensitive data as array literals to GET requests. An example of a JSON call returning an array literal is shown below: This attack can be achieved in 3 major steps: Step 1: Get an authenticated user to visit a malicious page. Step 2: The malicious page will try and access sensitive data from the application that the user is logged into.This can be done by embedding a script tag in an HTML page since the same-origin policy does not apply to script tags. The browser will make a GET request to json_server.php and any authentication cookies of the user will be sent along with the request. Step 3: At this point while the malicious site has executed the script it does not have access to any sensitive data. Getting access to the data can be achieved by using an object prototype setter. In the code below an object prototypes property is being bound to the defined function when an attempt is being made to set the "ccnum" property. At this point the malicious site has successfully hijacked the sensitive financial data (ccnum) returned byjson_server.php JSON It should be noted that not all browsers support this method; the proof of concept was done on Firefox 3.x.This method has now been deprecated and replaced by the useObject.defineProperty There is also a variation of this attack that should work on all browsers where full named JavaScript (e.g. pi=3.14159) is returned instead of a JSON array. There are several ways in which JSON Hijacking can be prevented: Since SCRIPT tags can only generate HTTP GET requests, only return JSON objects to POST requests. Prevent the web browser from interpreting the JSON object as valid JavaScript code. Implement Cross-Site Request Forgery protection by requiring that a predefined random value be required for all JSON requests. so as you can see While(1) comes under the last option. In the most simple terms, while(1)  is an infinite loop which will run till a break statement is issued explicitly. And thus what would be described as a lock for the key to be applied (google break statement). Therefore a JSON hijacking, in which the Hacker has no key will be consistently dismissed.Alas, If you read the JSON block with a parser, the while(1) loop is ignored. So in conclusion, the while(1) loop can more easily visualised as a simple break statement cipher that google can use to control flow of data. However the key word in that statement is the word 'simple'. The usage of authenticated infinite loops has been thankfully removed from basic practice in the years since 2010 due to its absolute decimation of CPU usage when isolated (and the fact the internet has moved away from forcing through crude 'quick-fixes'). Today instead the codebase has preventative measures embedded and the system is not crucial nor effective anymore. (part of this is the move away from JSON Hijacking to more fruitful datafarming techniques that i wont go into at present) *
__label__git __label__undo __label__git-merge Within my master branch, I did a git merge some-other-branch locally, but never pushed the changes to origin master. I didn't mean to merge, so I'd like to undo it. When doing a git status after my merge, I was getting this message: Based upon some instructions I found, I tried running but now I'm getting this message with git status: I don't want my branch to be ahead by any number of commits. How do I get back to that point? With git reflog check which commit is one prior the merge (git reflog will be a better option than git log). Then you can reset it using: There's also another way: It will get you back 1 commit. Be aware that any modified and uncommitted/unstashed files will be reset to their unmodified state. To keep them either stash changes away or see --merge option below.   As @Velmont suggested below in his answer, in this direct case using: might yield better results, as it should preserve your changes. ORIG_HEAD will point to a commit directly before merge has occurred, so you don't have to hunt for it yourself. A further tip is to use the --merge switch instead of --hard since it doesn't reset files unnecessarily: --merge Resets the index and updates the files in the working tree that are different between <commit> and HEAD, but keeps those which are different between the index and working tree (i.e. which have changes which have not been added).  Assuming your local master was not ahead of origin/master, you should be able to do Then your local master branch should look identical to origin/master. See chapter 4 in the Git book and the original post by Linus Torvalds. To undo a merge that was already pushed: Be sure to revert the revert if you're committing the branch again, like Linus said. It is strange that the simplest command was missing. Most answers work, but undoing the merge you just did, this is the easy and safe way: The ref ORIG_HEAD will point to the original commit from before the merge. (The --merge option has nothing to do with the merge. It's just like git reset --hard ORIG_HEAD, but safer since it doesn't touch uncommitted changes.) With newer Git versions, if you have not committed the merge yet and you have a merge conflict, you can simply do: From man git merge: [This] can only be run after the merge has resulted in conflicts. git merge --abort will abort the merge process and try to reconstruct the pre-merge state. You should reset to the previous commit. This should work: Or even HEAD^^ to revert that revert commit. You can always give a full SHA reference if you're not sure how many steps back you should take. In case when you have problems and your master branch didn't have any local changes, you can reset to origin/master. Lately, I've been using git reflog to help with this. This mostly only works if the merge JUST happened, and it was on your machine.  git reflog might return something like: The first line indicates that a merge occurred. The 2nd line is the time before my merge. I simply git reset --hard 43b6032 to force this branch to track from before the merge, and carry-on. With modern Git, you can: Older syntax: Old-school: But actually, it is worth noticing that git merge --abort is only equivalent to git reset --merge given that MERGE_HEAD is present. This can be read in the Git help for merge command. After a failed merge, when there is no MERGE_HEAD, the failed merge can be undone with git reset --merge, but not necessarily with git merge --abort, so they are not only old and new syntax for the same thing. Personally I find git reset --merge much more powerful and useful in everyday work, so that's the one I always use. Okay, the answers other people here gave me were close, but it didn't work. Here's what I did. Doing this... ...gave me the following status. I then had to type in the same git reset command several more times.  Each time I did that, the message changed by one as you can see below. At this point, I saw the status message changed, so I tried doing a git pull, and that seemed to work: So long story short, my commands came down to this: You have to change your HEAD, Not yours of course but git HEAD.... So before answering let's add some background, explaining what is this HEAD. HEAD is simply a reference to the current commit (latest) on the current branch. There can only be a single HEAD at any given time. (excluding git worktree) The content of HEAD is stored inside .git/HEAD and it contains the 40 bytes SHA-1 of the current commit. If you are not on the latest commit - meaning that HEAD is pointing to a prior commit in history its called detached HEAD.  On the command line, it will look like this- SHA-1 instead of the branch name since the HEAD is not pointing to the tip of the current branch  This will checkout new branch pointing to the desired commit. This command will checkout to a given commit. At this point, you can create a branch and start to work from this point on. You can always use the reflog as well. git reflog will display any change which updated the HEAD and checking out the desired reflog entry will set the HEAD back to this commit.  Every time the HEAD is modified there will be a new entry in the reflog This will get you back to your desired commit  "Move" your HEAD back to the desired commit. "Undo" the given commit or commit range. The reset command will "undo" any changes made in the given commit. A new commit with the undo patch will be committed while the original commit will remain in the history as well. This schema illustrates which command does what. As you can see there reset && checkout modify the HEAD.  You could use git reflog to find the previous checkout. Sometimes that's a good state you want to return back to. Concretely, If you are in a middle of merging you can always abort it git merge --abort I was able to resolve this problem with a single command that doesn't involve looking up a commit id. The accepted answer didn't work for me but this command achieved the results I was looking for. It can be done multiple ways.  1) Abort Merge If you are in-between a bad merge (mistakenly done with wrong branch), and wanted to avoid the merge to go back to the branch latest as below: 2) Reset HEAD to remote branch If you are working from remote develop branch, you can reset HEAD to the last commit on remote branch as below: 3) Delete current branch, and checkout again from the remote repository Considering, you are working on develop branch in local repo, that syncs with remote/develop branch, you can do as below: If you didn't commit it yet, you can only use It will undo the merge (and everything that you did). Got to this question also looking to revert to match origin (ie, NO commits ahead of origin). Researching further, found there's a reset command for exactly that: git reset --hard @{u} Note: @{u} is shorthand for origin/master. (And, of course, you need that remote repository for this to work.) The simplest answer is the one given by odinho - Velmont First do git reset --merge ORIG_HEAD For those looking to reset after changes are pushed, do this (Because this is the first post seen for any git reset merge questions) git push origin HEAD --force This will reset in a way that you won't get the merged changes back again after pull. You can use only two commands to revert a merge or restart by a specific commit: Good luck and go ahead! Just for an extra option to look at, I've been mostly following the branching model described here: http://nvie.com/posts/a-successful-git-branching-model/ and as such have been merging with --no-ff (no fast forward) usually.   I just read this page as I'd accidentally merged a testing branch instead of my release branch with master for deploying (website, master is what is live).  The testing branch has two other branches merged to it and totals about six commits.   So to revert the whole commit I just needed one git reset --hard HEAD^ and it reverted the whole merge.  Since the merges weren't fast forwarded the merge was a block and one step back is "branch not merged". If your merge and the corresponding commits were not pushed yet, you can always switch to another branch, delete the original one and re-create it. For example, I accidentally merged a develop branch into master and wanted to undo that. Using the following steps: Voila! Master is at the same stage as origin, and your mis-merged state is erased. If branches are murge and not pushed. Then bellow git reset command will work to undo the merge: git reset --merge ORIG_HEAD If you want a command-line solution, I suggest to just go with MBO's answer. If you're a newbie, you might like the graphical approach: Strategy: Create a new branch from where everything was good. Rationale: Reverting a merge is hard. There are too many solutions, depending on many factors such as whether you've committed or pushed your merge or if there were new commits since your merge. Also you still need to have a relatively deep understanding of git to adapt these solutions to your case. If you blindly follow some instructions, you can end up with an "empty merge" where nothing will be merged, and further merge attempts will make Git tell you "Already up to date". Solution: Let's say you want to merge dev into feature-1. Find the revision that you want to receive the merge: Check it out (go back in time): Create a new branch from there and check it out: Now you can restart your merge: Merge: git merge dev Fix your merge conflicts. Commit: git commit When you're satisfied with the results, delete the old branch: git branch --delete feature-1 Just create new branch, then cherry-pick desired commits to it. Its saver and simpler then resets described in many answers above I think you can do git rebase -i [hash] [branch_name]  where [hash] is the identifying hash for however far back you want to rewind plus one (or however many commits back you want to go) and then delete the lines for the commits in the editor that you don't want any more. Save the file. Exit. Pray. And it should be rewound. You might have to do a git reset --hard, but it should be good at this point. You can also use this to pull specific commits out of a stack, if you don't want to keep them in your history, but that can leave your repository in a state that you probably don't want. If you committed the merge: First, make sure that you've committed everything. Then reset your repository to the previous working state: or using --hard (this will remove all local, not committed changes!): Use the hash which was there before your wrongly merged commit. Check which commits you'd like to re-commit on the top of the previous correct version by: Apply your right commits on the top of the right version of your repository by: By using cherry-pick (the changes introduced by some existing commits) Or by cherry-picking the range of commits by: First checking the right changes before merging them: First checking the right changes before merging them: where this is the range of the correct commits which you've committed (excluding wrongly committed merge). git stash git branch -d the_local_branch git checkout -t <name of remote> git stash apply This worked for me..!! If you notice that you need to revert immediately after the merge and you haven't done anything else after the merge attempt, you can just issue this command: git reset --hard HEAD@{1}. Essentially, your merge sha will be pointing to HEAD@{0} if nothing else was committed after the merge and so HEAD@{1} will be the previous point before the merge. The simplest of the simplest chance, much simpler than anything said here: Remove your local branch (local, not remote) and pull it again. This way you'll undo the changes on your master branch and anyone will be affected by the change you don't want to push. Start it over.
__label__href __label__performance __label__javascript __label__html __label__optimization The following are two methods of building a link that has the sole purpose of running JavaScript code. Which is better, in terms of functionality, page load speed, validation purposes, etc.?   function myJsFunc() {     alert("myJsFunc"); } <a href="#" onclick="myJsFunc();">Run JavaScript Code</a>    or   function myJsFunc() {     alert("myJsFunc"); }  <a href="javascript:void(0)" onclick="myJsFunc();">Run JavaScript Code</a>    I use javascript:void(0). Three reasons. Encouraging the use of # amongst a team of developers inevitably leads to some using the return value of the function called like this: But then they forget to use return doSomething() in the onclick and just use doSomething(). A second reason for avoiding # is that the final return false; will not execute if the called function throws an error. Hence the developers have to also remember to handle any error appropriately in the called function. A third reason is that there are cases where the onclick event property is assigned dynamically.  I prefer to be able to call a function or assign it dynamically without having to code the function specifically for one method of attachment or another. Hence my onclick (or on anything) in HTML markup look like this: OR Using javascript:void(0) avoids all of the above headaches, and I haven't found any examples of a downside. So if you're a lone developer then you can clearly make your own choice, but if you work as a team you have to either state: Use href="#", make sure onclick always contains return false; at the end, that any called function does not throw an error and if you attach a function dynamically to the onclick property make sure that as well as not throwing an error it returns false. OR Use href="javascript:void(0)" The second is clearly much easier to communicate. Neither.   If you can have an actual URL that makes sense use that as the HREF.  The onclick won't fire if someone middle-clicks on your link to open a new tab or if they have JavaScript disabled. If that is not possible, then you should at least inject the anchor tag into the document with JavaScript and the appropriate click event handlers.   I realize this isn't always possible, but in my opinion it should be striven for in developing any public website. Check out Unobtrusive JavaScript and Progressive enhancement (both Wikipedia). Doing <a href="#" onclick="myJsFunc();">Link</a> or <a href="javascript:void(0)" onclick="myJsFunc();">Link</a> or whatever else that contains an onclick attribute - was okay back five years ago, though now it can be a bad practice. Here's why: It promotes the practice of obtrusive JavaScript - which has turned out to be difficult to maintain and difficult to scale. More on this in Unobtrusive JavaScript. You're spending your time writing incredibly overly verbose code - which has very little (if any) benefit to your codebase. There are now better, easier, and more maintainable and scalable ways of accomplishing the desired result. Just don't have a href attribute at all! Any good CSS reset would take care of the missing default cursor style, so that is a non-issue. Then attach your JavaScript functionality using graceful and unobtrusive  best practices - which are more maintainable as your JavaScript logic stays in JavaScript, instead of in your markup - which is essential when you start developing large scale JavaScript applications which require your logic to be split up into blackboxed components and templates. More on this in Large-scale JavaScript Application Architecture   // Cancel click event $('.cancel-action').click(function(){     alert('Cancel action occurs!'); });  // Hover shim for Internet Explorer 6 and Internet Explorer 7. $(document.body).on('hover','a',function(){     $(this).toggleClass('hover'); }); a { cursor: pointer; color: blue; } a:hover,a.hover { text-decoration: underline; } <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.1/jquery.min.js"></script> <a class="cancel-action">Cancel this action</a>    For a scalable, blackboxed, Backbone.js component example - see this working jsfiddle example here. Notice how we utilize unobtrusive JavaScript practices, and in a tiny amount of code have a component that can be repeated across the page multiple times without side-effects or conflicts between the different component instances. Amazing! Omitting the href attribute on the a element will cause the element to not be accessible using tab key navigation. If you wish for those elements to be accessible via the tab key, you can set the tabindex attribute, or use button elements instead. You can easily style button elements to look like normal links as mentioned in Tracker1's answer. Omitting the href attribute on the a element will cause Internet Explorer 6 and Internet Explorer 7 to not take on the a:hover styling, which is why we have added a simple JavaScript shim to accomplish this via a.hover instead. Which is perfectly okay, as if you don't have a href attribute and no graceful degradation then your link won't work anyway - and you'll have bigger issues to worry about. If you want your action to still work with JavaScript disabled, then using an a element with a href attribute that goes to some URL that will perform the action manually instead of via an Ajax request or whatever should be the way to go. If you are doing this, then you want to ensure you do an event.preventDefault() on your click call to make sure when the button is clicked it does not follow the link. This option is called graceful degradation. '#' will take the user back to the top of the page, so I usually go with void(0). javascript:; also behaves like javascript:void(0); I would honestly suggest neither.  I would use a stylized <button></button> for that behavior.   button.link {   display: inline-block;   position: relative;   background-color: transparent;   cursor: pointer;   border: 0;   padding: 0;   color: #00f;   text-decoration: underline;   font: inherit; } <p>A button that looks like a <button type="button" class="link">link</button>.</p>    This way you can assign your onclick.  I also suggest binding via script, not using the onclick attribute on the element tag.  The only gotcha is the psuedo 3d text effect in older IEs that cannot be disabled. If you MUST use an A element, use javascript:void(0); for reasons already mentioned. NOTE: You can replace the 0 with a string such as javascript:void('Delete record 123') which can serve as an extra indicator that will show what the click will actually do. The first one, ideally with a real link to follow in case the user has JavaScript disabled. Just make sure to return false to prevent the click event from firing if the JavaScript executes. If you use Angular2, this way works: <a [routerLink]="" (click)="passTheSalt()">Click me</a>. See here https://stackoverflow.com/a/45465728/2803344 Neither if you ask me; If your "link" has the sole purpose of running some JavaScript code it doesn't qualify as a link; rather a piece of text with a JavaScript function coupled to it. I would recommend to use a <span> tag with an onclick handler attached to it and some basic CSS to immitate a link. Links are made for navigation, and if your JavaScript code isn't for navigation it should not be an <a> tag. Example:   function callFunction() { console.log("function called"); } .jsAction {     cursor: pointer;     color: #00f;     text-decoration: underline; } <p>I want to call a JavaScript function <span class="jsAction" onclick="callFunction();">here</span>.</p>    Ideally you'd do this: Or, even better, you'd have the default action link in the HTML, and you'd add the onclick event to the element unobtrusively via JavaScript after the DOM renders, thus ensuring that if JavaScript is not present/utilized you don't have useless event handlers riddling your code and potentially obfuscating (or at least distracting from) your actual content. Using just # makes some funny movements, so I would recommend to use #self if you would like to save on typing efforts of JavaScript bla, bla,. I use the following instead I agree with suggestions elsewhere stating that you should use regular URL in href attribute, then call some JavaScript function in onclick. The flaw is, that they automaticaly add return false after the call. The problem with this approach is, that if the function will not work or if there will be any problem, the link will become unclickable. Onclick event will always return false, so the normal URL will not be called. There's very simple solution. Let function return true if it works correctly. Then use the returned value to determine if the click should be cancelled or not: JavaScript HTML Note, that I negate the result of the doSomething() function. If it works, it will return true, so it will be negated (false) and the path/to/some/URL will not be called. If the function will return false (for example, the browser doesn't support something used within the function or anything else goes wrong), it is negated to true and the path/to/some/URL is called. I recommend using a <button> element instead, especially if the control is supposed to produce a change in the data. (Something like a POST.) It's even better if you inject the elements unobtrusively, a type of progressive enhancement. (See this comment.) # is better than javascript:anything, but the following is even better: HTML: JavaScript: You should always strive for graceful degradation (in the event that the user doesn't have JavaScript enabled...and when it is with specs. and budget).  Also, it is considered bad form to use JavaScript attributes and protocol directly in HTML. Unless you're writing out the link using JavaScript (so that you know it's enabled in the browser), you should ideally be providing a proper link for people who are browsing with JavaScript disabled and then prevent the default action of the link in your onclick event handler. This way those with JavaScript enabled will run the function and those with JavaScript disabled will jump to an appropriate page (or location within the same page) rather than just clicking on the link and having nothing happen. Definitely hash (#) is better because in JavaScript it is a pseudoscheme: Of course "#" with an onclick handler which prevents default action is [much] better. Moreover, a link that has the sole purpose to run JavaScript is not really "a link" unless you are sending user to some sensible anchor on the page (just # will send to top) when something goes wrong. You can simply simulate look and feel of link with stylesheet and forget about href at all. In addition, regarding cowgod's suggestion, particularly this: ...href="javascript_required.html" onclick="... This is good approach, but it doesn't distinguish between "JavaScript disabled" and "onclick fails" scenarios. I usually go for It's shorter than javascript:void(0) and does the same. I would use: Reasons: I choose use javascript:void(0), because use this could prevent right click to open the content menu. But javascript:; is shorter and does the same thing. So, when you are doing some JavaScript things with an <a /> tag and if you put href="#" as well, you can add return false at the end of the event (in case of inline event binding) like: Or you can change the href attribute with JavaScript like: or But semantically, all the above ways to achieve this are wrong (it works fine though). If any element is not created to navigate the page and that have some JavaScript things associated with it, then it should not be a <a> tag.  You can simply use a <button /> instead to do things or any other element like b, span or whatever fits there as per your need, because you are allowed to add events on all the elements. So, there is one benefit to use <a href="#">. You get the cursor pointer by default on that element when you do a href="#". For that, I think you can use CSS for this like cursor:pointer; which solves this problem also. And at the end, if you are binding the event from the JavaScript code itself, there you can do event.preventDefault() to achieve this if you are using <a> tag, but if you are not using a <a> tag for this, there you get an advantage, you don't need to do this. So, if you see, it's better not to use a tag for this kind of stuff. Don't use links for the sole purpose of running JavaScript.  The use of href="#" scrolls the page to the top; the use of void(0) creates navigational problems within the browser. Instead, use an element other than a link: And style it with CSS: It would be better to use jQuery, and omit both href="#" and href="javascript:void(0)". The anchor tag markup will be like  Simple enough! If you happen to be using AngularJS, you can use the following: Which will not do anything. In addition Usually, you should always have a fall back link to make sure that clients with JavaScript disabled still has some functionality. This concept is called unobtrusive JavaScript.  Example... Let's say you have the following search link: You can always do the following: That way, people with JavaScript disabled are directed to search.php while your viewers with JavaScript view your enhanced functionality. I personally use them in combination. For example: HTML with little bit of jQuery or But I'm using that just for preventing the page jumping to the top when the user clicks on an empty anchor. I'm rarely using onClick and other on events directly in HTML. My suggestion would be to use <span> element with the class attribute instead of an anchor. For example: Then assign the function to .link with a script wrapped in the body and just before the </body> tag or in an external JavaScript document. *Note: For dynamically created elements, use: And for dynamically created elements which are created with dynamically created elements, use: Then you can style the span element to look like an anchor with a little CSS: Here's a jsFiddle example of above aforementioned. Depending on what you want to accomplish, you could forget the onclick and just use the href: It gets around the need to return false. I don't like the # option because, as mentioned, it will take the user to the top of the page. If you have somewhere else to send the user if they don't have JavaScript enabled (which is rare where I work, but a very good idea), then Steve's proposed method works great.  Lastly, you can use javascript:void(0) if you do not want anyone to go anywhere and if you don't want to call a JavaScript function. It works great if you have an image you want a mouseover event to happen with, but there's not anything for the user to click on. When I've got several faux-links, I prefer to give them a class of 'no-link'. Then in jQuery, I add the following code: And for the HTML, the link is simply I don't like using Hash-Tags unless they're used for anchors, and I only do the above when I've got more than two faux-links, otherwise I go with javascript:void(0). Typically, I like to just avoid using a link at all and just wrap something around in a span and use that as a way to active some JavaScript code, like a pop-up or a content-reveal. I believe you are presenting a false dichotomy. These are not the only two options.  I agree with Mr. D4V360 who suggested that, even though you are using the anchor tag, you do not truly have an anchor here. All you have is a special section of a document that should behave slightly different. A <span> tag is far more appropriate. I tried both in google chrome with the developer tools, and the id="#" took 0.32 seconds. While the javascript:void(0) method took only 0.18 seconds. So in google chrome, javascript:void(0) works better and faster. I'm basically paraphrasing from this practical article using progressive enhancement. The short answer is that you never use javascript:void(0); or # unless your user interface has already inferred that JavaScript is enabled, in which case you should use javascript:void(0);. Also, do not use span as links, since that is semantically false to begin with. Using SEO friendly URL routes in your application, such as /Home/Action/Parameters is a good practice as well. If you have a link to a page that works without JavaScript first, you can enhance the experience afterward. Use a real link to a working page, then add an onlick event to enhance the presentation. Here is a sample. Home/ChangePicture is a working link to a form on a page complete with user interface and standard HTML submit buttons, but it looks nicer injected into a modal dialog with jQueryUI buttons. Either way works, depending on the browser, which satisfies mobile first development. It's nice to have your site be accessible by users with JavaScript disabled, in which case the href points to a page that performs the same action as the JavaScript being executed. Otherwise I use "#" with a "return false;" to prevent the default action (scroll to top of the page) as others have mentioned. Googling for "javascript:void(0)" provides a lot of information on this topic. Some of them, like this one mention reasons to NOT use void(0).
__label__vi __label__vim I'm stuck and cannot escape. It says: But when I type that it simply appears in the object body. Hit the Esc key to enter "Normal mode". Then you can type : to enter "Command-line mode". A colon (:) will appear at the bottom of the screen and you can type in one of the following commands. To execute a command, press the Enter key. You can also exit Vim directly from "Normal mode" by typing ZZ to save and quit (same as :x) or ZQ to just quit (same as :q!). (Note that case is important here. ZZ and zz do not mean the same thing.) Vim has extensive help - that you can access with the :help command - where you can find answers to all your questions and a tutorial for beginners. Pictures are worth a thousand Unix commands and options:  I draw this to my students each semester and they seem to grasp vi afterwards. vi is a finite state machine with only three states. Upon starting, vi goes into COMMAND mode, where you can type short, few character commands, blindly. You know what you are doing; this isn't for amateurs. When you want to actually edit text, you should go to INSERT mode with some one-character command: Now, answering the question: exiting. You can exit vi from EX mode: w and x accept a file name parameter. If you started vi with a filename, you need not give it here again. At last, the most important: how can you reach EX mode? EX mode is for long commands that you can see typing at the bottom line of the screen. From COMMAND mode, you push colon, :, and a colon will appear at the bottom line, where you can type the above commands. From INSERT mode, you need to push ESC, i.e. the Escape button, going to COMMAND mode, and then : to go to EX mode. If you are unsure, push ESC and that will bring you to command mode. So, the robust method is ESC-:-x-Enter which saves your file and quits. Before you enter a command, hit the Esc key. After you enter it, hit the Return to confirm. Esc finishes the current command and switches Vim to normal mode.   Now if you press :, the : will appear at the bottom of the screen. This confirms that you're actually typing a command and not editing the file.  Most commands have abbreviations, with optional part enclosed in brackets: c[ommand]. Commands marked with '*' are Vim-only (not implemented in Vi). Safe-quit (fails if there are unsaved changes): Prompt-quit (prompts if there are unsaved changes) Write (save) changes and quit: Discard changes and quit: Press Return to confirm the command. This answer doesn't reference all Vim write and quit commands and arguments. Indeed, they are referenced in the Vim documentation.  Vim has extensive built-in help, type Esc:helpReturn to open it.  This answer was inspired by the other one, originally authored by @dirvine and edited by other SO users. I've included more information from Vim reference, SO comments and some other sources. Differences for Vi and Vim are reflected too.  If you want to quit without saving in Vim and have Vim return a non-zero exit code, you can use :cq. I use this all the time because I can't be bothered to pinky shift for !. I often pipe things to Vim which don't need to be saved in a file. We also have an odd SVN wrapper at work which must be exited with a non-zero value in order to abort a checkin. This is for the worst-case scenario of exiting Vim if you just want out, have no idea what you've done and you don't care what will happen to the files you opened. Ctrl-cEnterEnterviEnterCtrl-\Ctrl-n:qa!Enter This should get you out most of the time. Some interesting cases where you need something like this: iCtrl-ovg (you enter insert mode, then visual mode and then operator pending mode) QappendEnter iCtrl-ogQCtrl-r=Ctrl-k (thanks to porges for this case) :set insertmode (this is a case when Ctrl-\Ctrl-n returns you to normal mode) Edit: This answer was corrected due to cases above. It used to be: EscEscEsc:qa!Enter However, that doesn't work if you have entered Ex mode. In that case you would need to do: viEnter:qa!Enter So a complete command for "I don't want to know what I've done and I don't want to save anything, I just want out now!" would be viEnterEscEscEsc:qa!Enter In case you need to exit Vim in easy mode (while using -y option) you can enter normal Vim mode by hitting Ctrl + L and then any of the normal exiting options will work. Vim has three modes of operation: Input mode, Command mode & Ex mode. Input mode - everything that you type, all keystrokes are echoed on the screen. Command mode or Escape mode - everything that you type in this mode is interpreted as a command. Ex mode - this is another editor, ex. It is a line editor. It works per line or based on a range of lines. In this mode, a : appears at the bottom of the screen. This is the ex editor. In order to exit Vim, you can exit while you are in either the ex mode or in the command mode. You cannot exit Vim when you are in input mode. Exiting from ex mode You need to be sure that you are in the Command mode. To do that, simply press the Esc key. Go to the ex mode by pressing the : key Use any of the following combinations in ex mode to exit: :q - quit :q! - quit without saving :wq - save & quit or write & quit :wq! - same as wq, but force write in case file permissions are readonly :x - write & quit :qa - quit all. useful when multiple files are opened like: vim abc.txt xyz.txt Exiting from command mode Press the escape key. You probably have done this already if you are in command mode. Press capital ZZ (shift zz) - save & exit Press capital ZQ (shift zq) - exit without saving. After hitting ESC (or cmd + C on my computer) you must hit : for the command prompt to appear. Then, you may enter quit. You may find that the machine will not allow you to quit because your information hasn't been saved. If you'd like to quit anyway, enter ! directly after the quit (i.e. :quit!). I got Vim by installing a Git client on Windows. :q wouldn't exit Vim for me. :exit did however... The q command with a number closes the given split in that position. :q<split position> or :<split position>q will close the split in that position. Let's say your Vim window layout is as follows: If you run the q1 command, it will close the first split. q2 will close the second split and vice versa. The order of split position in the quit command does not matter. :2q or :q2 will close the second split. If the split position you pass to the command is greater than the number of current splits, it will simply close the last split. For example, if you run the q100 on the above window setup where there are only three splits, it will close the last split (Split 3). The question has been asked here. Once you have made your choice of the exit command, press enter to finally quit Vim and close the editor (but not the terminal). Do note that when you press shift + “:” the editor will have the next keystrokes displayed at the bottom left of the terminal. Now if you want to simply quit, write exit or wq (save and exit)
__label__null __label__java __label__nullpointerexception I use object != null a lot to avoid NullPointerException. What is an alternative to: This to me sounds like a reasonably common problem that junior to intermediate developers tend to face at some point: they either don't know or don't trust the contracts they are participating in and defensively overcheck for nulls.  Additionally, when writing their own code, they tend to rely on returning nulls to indicate something thus requiring the caller to check for nulls. To put this another way, there are two instances where null checking comes up: Where null is a valid response in terms of the contract; and Where it isn't a valid response. (2) is easy.  Either use assert statements (assertions) or allow failure (for example,  NullPointerException).  Assertions are a highly-underused Java feature that was added in 1.4.  The syntax is: or where <condition> is a boolean expression and <object> is an object whose toString() method's output will be included in the error. An assert statement throws an Error (AssertionError) if the condition is not true.  By default, Java ignores assertions.  You can enable assertions by passing the option -ea to the JVM.  You can enable and disable assertions for individual classes and packages.  This means that you can validate code with the assertions while developing and testing, and disable them in a production environment, although my testing has shown next to no performance impact from assertions. Not using assertions in this case is OK because the code will just fail, which is what will happen if you use assertions.  The only difference is that with assertions it might happen sooner, in a more-meaningful way and possibly with extra information, which may help you to figure out why it happened if you weren't expecting it. (1) is a little harder.  If you have no control over the code you're calling then you're stuck.  If null is a valid response, you have to check for it. If it's code that you do control, however (and this is often the case), then it's a different story.  Avoid using nulls as a response.  With methods that return collections, it's easy: return empty collections (or arrays) instead of nulls pretty much all the time. With non-collections it might be harder.  Consider this as an example: if you have these interfaces: where Parser takes raw user input and finds something to do, perhaps if you're implementing a command line interface for something.  Now you might make the contract that it returns null if there's no appropriate action.  That leads the null checking you're talking about. An alternative solution is to never return null and instead use the Null Object pattern: Compare: to which is a much better design because it leads to more concise code. That said, perhaps it is entirely appropriate for the findAction() method to throw an Exception with a meaningful error message -- especially in this case where you are relying on user input.  It would be much better for the findAction method to throw an Exception than for the calling method to blow up with a simple NullPointerException with no explanation. Or if you think the try/catch mechanism is too ugly, rather than Do Nothing your default action should provide feedback to the user. If you use (or planning to use) a Java IDE like JetBrains IntelliJ IDEA, Eclipse or Netbeans or a tool like findbugs then you can use annotations to solve this problem. Basically, you've got @Nullable and @NotNull. You can use in method and parameters, like this: or The second example won't compile (in IntelliJ IDEA). When you use the first helloWorld() function in another piece of code: Now the IntelliJ IDEA compiler will tell you that the check is useless, since the helloWorld() function won't return null, ever. Using parameter if you write something like: This won't compile. Last example using @Nullable Doing this And you can be sure that this won't happen. :) It's a nice way to let the compiler check something more than it usually does and to enforce your contracts to be stronger. Unfortunately, it's not supported by all the compilers. In IntelliJ IDEA 10.5 and on, they added support for any other @Nullable @NotNull implementations. See blog post More flexible and configurable @Nullable/@NotNull annotations. If your method is called externally, start with something like this: Then, in the rest of that method, you'll know that object is not null. If it is an internal method (not part of an API), just document that it cannot be null, and that's it. Example: However, if your method just passes the value on, and the next method passes it on etc. it could get problematic. In that case you may want to check the argument as above. This really depends. If find that I often do something like this: So I branch, and do two completely different things. There is no ugly code snippet, because I really need to do two different things depending on the data. For example, should I work on the input, or should I calculate a good default value? It's actually rare for me to use the idiom "if (object != null && ...". It may be easier to give you examples, if you show examples of where you typically use the idiom. Wow, I almost hate to add another answer when we have 57 different ways to recommend the NullObject pattern, but I think that some people interested in this question may like to know that there is a proposal on the table for Java 7 to add "null-safe handling"—a streamlined syntax for if-not-equal-null logic. The example given by Alex Miller looks like this: The ?. means only de-reference the left identifier if it is not null, otherwise evaluate the remainder of the expression as null. Some people, like Java Posse member Dick Wall and the voters at Devoxx really love this proposal, but there is opposition too, on the grounds that it will actually encourage more use of null as a sentinel value. Update: An official proposal for a null-safe operator in Java 7 has been submitted under Project Coin. The syntax is a little different than the example above, but it's the same notion. Update: The null-safe operator proposal didn't make it into Project Coin. So, you won't be seeing this syntax in Java 7. You might configure your IDE to warn you about potential null dereferencing. E.g. in Eclipse, see Preferences > Java > Compiler > Errors/Warnings/Null analysis. If you want to define a new API where undefined values make sense, use the Option Pattern (may be familiar from functional languages). It has the following advantages: Java 8 has a built-in Optional class (recommended); for earlier versions, there are library alternatives, for example Guava's Optional or FunctionalJava's Option. But like many functional-style patterns, using Option in Java (even 8) results in quite some boilerplate, which you can reduce using a less verbose JVM language, e.g. Scala or Xtend. If you have to deal with an API which might return nulls, you can't do much in Java. Xtend and Groovy have the Elvis operator ?: and the null-safe dereference operator ?., but note that this returns null in case of a null reference, so it just "defers" the proper handling of null. Only for this situation - Not checking if a variable is null before invoking an equals method (a string compare example below): will result in a NullPointerException if foo doesn't exist. You can avoid that if you compare your Strings like this: With Java 8 comes the new java.util.Optional class that arguably solves some of the problem. One can at least say that it improves the readability of the code, and in the case of public APIs make the API's contract clearer to the client developer. They work like that: An optional object for a given type (Fruit) is created as the return type of a method. It can be empty or contain a Fruit object: Now look at this code where we search a list of Fruit (fruits) for a given Fruit instance: You can use the map() operator to perform a computation on--or extract a value from--an optional object.  orElse() lets you provide a fallback for missing values. Of course, the check for null/empty value is still necessary, but at least the developer is conscious that the value might be empty and the risk of forgetting to check is limited. In an API built from scratch using Optional whenever a return value might be empty, and returning a plain object only when it cannot be null (convention), the client code might abandon null checks on simple object return values... Of course Optional could also be used as a method argument, perhaps a better way to indicate optional arguments than 5 or 10 overloading methods in some cases. Optional offers other convenient methods, such as orElse that allow the use of a default value, and ifPresent that works with lambda expressions. I invite you to read this article (my main source for writing this answer) in which the NullPointerException (and in general null pointer) problematic as well as the (partial) solution brought by Optional are well explained: Java Optional Objects. Depending on what kind of objects you are checking you may be able to use some of the classes in the apache commons such as: apache commons lang and apache commons collections Example:  or (depending on what you need to check): The StringUtils class is only one of many; there are quite a few good classes in the commons that do null safe manipulation. Here follows an example of how you can use null vallidation in JAVA when you include apache library(commons-lang-2.4.jar) And if you are using Spring, Spring also has the same functionality in its package, see library(spring-2.4.6.jar)  Example on how to use this static classf from spring(org.springframework.util.Assert) You have to check for object != null only if you want to handle the case where the object may be null... There is a proposal to add new annotations in Java7 to help with null / notnull params: http://tech.puredanger.com/java7/#jsr308 I'm a fan of "fail fast" code. Ask yourself - are you doing something useful in the case where the parameter is null? If you don't have a clear answer for what your code should do in that case... I.e. it should never be null in the first place, then ignore it and allow a NullPointerException to be thrown. The calling code will make just as much sense of an NPE as it would an IllegalArgumentException, but it'll be easier for the developer to debug and understand what went wrong if an NPE is thrown rather than your code attempting to execute some other unexpected contingency logic - which ultimately results in the application failing anyway.  The Google collections framework offers a good and elegant way to achieve the null check. There is a method in a library class like this: And the usage is (with import static): Or in your example: Rather than Null Object Pattern -- which has its uses -- you might consider situations where the null object is a bug. When the exception is thrown, examine the stack trace and work through the bug. Sometimes, you have methods that operate on its parameters that define a symmetric operation: If you know b can never be null, you can just swap it. It is most useful for equals: Instead of foo.equals("bar"); better do "bar".equals(foo);. Null is not a 'problem'. It is an integral part of a complete modeling tool set. Software aims to model the complexity of the world and null bears its burden. Null indicates 'No data' or 'Unknown' in Java and the like. So it is appropriate to use nulls for these purposes. I don't prefer the 'Null object' pattern; I think it rise the 'who will guard the guardians' problem.  If you ask me what is the name of my girlfriend I'll tell you that I have no girlfriend. In the Java language I'll return null.  An alternative would be to throw meaningful exception to indicate some problem that can't be (or don't want to be) solved right there and delegate it somewhere higher in the stack to retry or report data access error to the user.   For an 'unknown question' give 'unknown answer'. (Be null-safe where this is correct from business point of view) Checking arguments for null once inside a method before usage relieves multiple callers from checking them before a call. Previous leads to normal logic flow to get no photo of a non-existent girlfriend from my photo library. And it fits with new coming Java API (looking forward) While it is rather 'normal business flow' not to find photo stored into the DB for some person, I used to use pairs like below for some other cases And don't loathe to type <alt> + <shift> + <j> (generate javadoc in Eclipse) and write three additional words for you public API. This will be more than enough for all but those who don't read documentation. or This is rather theoretical case and in most cases you should prefer java null safe API (in case it will be released in another 10 years), but NullPointerException is subclass of an Exception. Thus it is a form of Throwable that indicates conditions that a reasonable application might want to catch (javadoc)! To use the first most advantage of exceptions and separate error-handling code from 'regular' code (according to creators of Java) it is appropriate, as for me, to catch NullPointerException. Questions could arise: Q. What if getPhotoDataSource() returns null? A. It is up to business logic. If I fail to find a photo album I'll show you no photos. What if appContext is not initialized? This method's business logic puts up with this. If the same logic should be more strict then throwing an exception it is part of the business logic and explicit check for null should be used (case 3). The new Java Null-safe API fits better here to specify selectively what implies and what does not imply to be initialized to be fail-fast in case of programmer errors. Q. Redundant code could be executed and unnecessary resources could be grabbed. A. It could take place if getPhotoByName() would try to open a database connection, create PreparedStatement and use the person name as an SQL parameter at last. The approach for an unknown question gives an unknown answer (case 1) works here. Before grabbing resources the method should check parameters and return 'unknown' result if needed. Q. This approach has a performance penalty due to the try closure opening. A. Software should be easy to understand and modify firstly. Only after this, one could think about performance, and only if needed! and where needed! (source), and many others). PS. This approach will be as reasonable to use as the separate error-handling code from "regular" code principle is reasonable to use in some place. Consider the next example: PPS. For those fast to downvote (and not so fast to read documentation) I would like to say that I've never caught a null-pointer exception (NPE) in my life. But this possibility was intentionally designed by the Java creators because NPE is a subclass of Exception. We have a precedent in Java history when ThreadDeath is an Error not because it is actually an application error, but solely because it was not intended to be caught! How much NPE fits to be an Error than ThreadDeath! But it is not. Check for 'No data' only if business logic implies it. and If appContext or dataSource is not initialized unhandled runtime NullPointerException will kill current thread and will be processed by Thread.defaultUncaughtExceptionHandler (for you to define and use your favorite logger or other notification mechanizm). If not set, ThreadGroup#uncaughtException will print stacktrace to system err. One should monitor application error log and open Jira issue for each unhandled exception which in fact is application error. Programmer should fix bug somewhere in initialization stuff. Java 7 has a new java.util.Objects utility class on which there is a requireNonNull() method. All this does is throw a NullPointerException if its argument is null, but it cleans up the code a bit. Example: The method is most useful for checking just before an assignment in a constructor, where each use of it can save three lines of code: becomes Ultimately, the only way to completely solve this problem is by using a different programming language: Common "problem" in Java indeed. First, my thoughts on this: I consider that it is bad to "eat" something when NULL was passed where NULL isn't a valid value. If you're not exiting the method with some sort of error then it means nothing went wrong in your method which is not true. Then you probably return null in this case, and in the receiving method you again check for null, and it never ends, and you end up with "if != null", etc.. So, IMHO, null must be a critical error which prevents further execution (that is, where null is not a valid value). The way I solve this problem is this: First, I follow this convention: And finally, in the code, the first line of the public method goes like this: Note that addParam() returns self, so that you can add more parameters to check. Method validate() will throw checked ValidationException if any of the parameters is null (checked or unchecked is more a design/taste issue, but my ValidationException is checked). The message will contain the following text if, for example, "plans" is null: "Illegal argument value null is encountered for parameter [plans]" As you can see, the second value in the addParam() method (string) is needed for the user message, because you cannot easily detect passed-in variable name, even with reflection (not subject of this post anyway...). And yes, we know that beyond this line we will no longer encounter a null value so we just safely invoke methods on those objects. This way, the code is clean, easy maintainable and readable. In addition to using assert you can use the following: This is slightly better than: Asking that question points out that you may be interested in error handling strategies.  How and where to handle errors is a pervasive architectural question.  There are several ways to do this. My favorite: allow the Exceptions to ripple through - catch them at the 'main loop' or in some other function with the appropriate responsibilities.  Checking for error conditions and handling them appropriately can be seen as a specialized responsibility. Sure do have a look at Aspect Oriented Programming, too - they have neat ways to insert if( o == null ) handleNull() into your bytecode. Just don't ever use null. Don't allow it. In my classes, most fields and local variables have non-null default values, and I add contract statements (always-on asserts) everywhere in the code to make sure this is being enforced (since it's more succinct, and more expressive than letting it come up as an NPE and then having to resolve the line number, etc.). Once I adopted this practice, I noticed that the problems seemed to fix themselves. You'd catch things much earlier in the development process just by accident and realize you had a weak spot..  and more importantly.. it helps encapsulate different modules' concerns, different modules can 'trust' each other, and no more littering the code with if = null else constructs! This is defensive programming and results in much cleaner code in the long run. Always sanitize the data, e.g. here by enforcing rigid standards, and the problems go away. The contracts are like mini-unit tests which are always running, even in production, and when things fail, you know why, rather than a random NPE you have to somehow figure out. Guava, a very useful core library by Google, has a nice and useful API to avoid nulls. I find UsingAndAvoidingNullExplained very helpful. As explained in the wiki: Optional<T> is a way of replacing a nullable T reference with a   non-null value. An Optional may either contain a non-null T reference   (in which case we say the reference is "present"), or it may contain   nothing (in which case we say the reference is "absent"). It is never   said to "contain null." Usage: This is a very common problem for every Java developer. So there is official support in Java 8 to address these issues without cluttered code. Java 8 has introduced java.util.Optional<T>. It is a container that may or may not hold a non-null value. Java 8 has given a safer way to handle an object whose value may be null in some of the cases. It is inspired from the ideas of Haskell and Scala. In a nutshell, the Optional class includes methods to explicitly deal with the cases where a value is present or absent. However, the advantage compared to null references is that the Optional<T> class forces you to think about the case when the value is not present. As a consequence, you can prevent unintended null pointer exceptions. In above example we have a home service factory that returns a handle to multiple appliances available in the home. But these services may or may not be available/functional; it means it may result in a NullPointerException. Instead of adding a null if condition before using any service, let's wrap it in to Optional<Service>. WRAPPING TO OPTION<T> Let's consider a method to get a reference of a service from a factory. Instead of returning the service reference, wrap it with Optional. It lets the API user know that the returned service may or may not available/functional, use defensively As you see Optional.ofNullable() provides an easy way to get the reference wrapped. There are another ways to get the reference of Optional, either Optional.empty() & Optional.of(). One for returning an empty object instead of retuning null and the other to wrap a non-nullable object, respectively. SO HOW EXACTLY IT HELPS TO AVOID A NULL CHECK? Once you have wrapped a reference object, Optional provides many useful methods to invoke methods on a wrapped reference without NPE. Optional.ifPresent invokes the given Consumer with a reference if it is a non-null value. Otherwise, it does nothing. Represents an operation that accepts a single input argument and returns no result. Unlike most other functional interfaces, Consumer is expected to operate via side-effects. It is so clean and easy to understand. In the above code example, HomeService.switchOn(Service) gets invoked if the Optional holding reference is non-null. We use the ternary operator very often for checking null condition and return an alternative value or default value. Optional provides another way to handle the same condition without checking null. Optional.orElse(defaultObj) returns defaultObj if the Optional has a null value. Let's use this in our sample code: Now HomeServices.get() does same thing, but in a better way. It checks whether the service is already initialized of not. If it is then return the same or create a new New service. Optional<T>.orElse(T) helps to return a default value. Finally, here is our NPE as well as null check-free code: The complete post is NPE as well as Null check-free code … Really?. I like articles from Nat Pryce. Here are the links: In the articles there is also a link to a Git repository for a Java Maybe Type which I find interesting, but I don't think it alone could decrease the checking code bloat. After doing some research on the Internet, I think != null code bloat could be decreased mainly by careful design. I've tried the NullObjectPattern but for me is not always the best way to go. There are sometimes when a "no action" is not appropiate. NullPointerException is a Runtime exception that means it's developers fault and with enough experience it tells you exactly where is the error. Now to the answer: Try to make all your attributes and its accessors as private as possible or avoid to  expose them to the clients at all. You can have the argument values in the constructor of course, but by reducing the scope you don't let the client class pass an invalid value. If you need to modify the values, you can always create a new object.  You check the values in the constructor only once and in the rest of the methods you can be almost sure that the values are not null. Of course, experience is the better way to understand and apply this suggestion. Byte! Probably the best alternative for Java 8 or newer is to use the Optional class.  This is especially handy for long chains of possible null values. Example: Example on how to throw exception on null: Java 7 introduced the Objects.requireNonNull method which can be handy when something should be checked for non-nullness. Example: May I answer it more generally! We usually face this issue when the methods get the parameters in the way we not expected (bad method call is programmer's fault). For example: you expect to get an object, instead you get a null. You expect to get an String with at least one character, instead you get an empty String ... So there is no difference between: } or They both want to make sure that we received valid parameters, before we do any other functions. As mentioned in some other answers, to avoid above problems you can follow the Design by contract pattern. Please see http://en.wikipedia.org/wiki/Design_by_contract.  To implement this pattern in java, you can use core java annotations like javax.annotation.NotNull or use more sophisticated libraries like Hibernate Validator. Just a sample: Now you can safely develop the core function of your method without needing to check input parameters, they guard your methods from unexpected parameters. You can go a step further and make sure that only valid pojos could be created in your application. (sample from hibernate validator site) I highly disregard answers that suggest using the null objects in every situation. This pattern may break the contract and bury problems deeper and deeper instead of solving them, not mentioning that used inappropriately will create another pile of boilerplate code that will require future maintenance. In reality if something returned from a method can be null and the calling code has to make decision upon that, there should an earlier call that ensures the state.  Also keep in mind, that null object pattern will be memory hungry if used without care. For this - the instance of a NullObject should be shared between owners, and not be an unigue instance for each of these. Also I would not recommend using this pattern where the type is meant to be a primitive type representation - like mathematical entities, that are not scalars: vectors, matrices, complex numbers and POD(Plain Old Data) objects, which are meant to hold state in form of Java built-in types. In the latter case you would end up calling getter methods with arbitrary results. For example what should a NullPerson.getName() method return?  It's worth considering such cases in order to avoid absurd results. Doing this in your own code and you can avoid != null checks. Most of the time null checks seem to guard loops over collections or arrays, so just initialise them empty, you won't need any null checks. There is a tiny overhead in this, but it's worth it for cleaner code and less NullPointerExceptions. This is the most common error occurred for most of the developers. We have number of ways to handle this. Approach 1: notNull(Object object, String message)  Approach 2: Approach 3: Approach 4: 
__label__html __label__css __label__placeholder __label__html-input Chrome supports the placeholder attribute on input[type=text] elements (others probably do too). But the following CSS doesn't do anything to the placeholder's value:   input[placeholder], [placeholder], *[placeholder] {     color: red !important; } <input type="text" placeholder="Value">    But Value will still remain grey instead of red. Is there a way to change the color of the placeholder text? There are three different implementations: pseudo-elements, pseudo-classes, and nothing. Internet Explorer 9 and lower does not support the placeholder attribute at all, while Opera 12 and lower do not support any CSS selector for placeholders. The discussion about the best implementation is still going on. Note the pseudo-elements act like real elements in the Shadow DOM. A padding on an input will not get the same background color as the pseudo-element. User agents are required to ignore a rule with an unknown selector. See Selectors Level 3: a group of selectors containing an invalid selector is invalid. So we need separate rules for each browser. Otherwise the whole group would be ignored by all browsers.   ::-webkit-input-placeholder { /* WebKit, Blink, Edge */     color:    #909; } :-moz-placeholder { /* Mozilla Firefox 4 to 18 */    color:    #909;    opacity:  1; } ::-moz-placeholder { /* Mozilla Firefox 19+ */    color:    #909;    opacity:  1; } :-ms-input-placeholder { /* Internet Explorer 10-11 */    color:    #909; } ::-ms-input-placeholder { /* Microsoft Edge */    color:    #909; }  ::placeholder { /* Most modern browsers support this now. */    color:    #909; } <input placeholder="Stack Snippets are awesome!">      /* do not group these rules */ *::-webkit-input-placeholder {     color: red; } *:-moz-placeholder {     /* FF 4-18 */     color: red;     opacity: 1; } *::-moz-placeholder {     /* FF 19+ */     color: red;     opacity: 1; } *:-ms-input-placeholder {     /* IE 10+ */     color: red; } *::-ms-input-placeholder {     /* Microsoft Edge */     color: red; } *::placeholder {     /* modern browser */     color: red; } <input placeholder="hello"/> <br /> <textarea placeholder="hello"></textarea>    This will style all input and textarea placeholders. Important Note: Do not group these rules. Instead, make a separate rule for every selector (one invalid selector in a group makes the whole group invalid). You may also want to style textareas: For Bootstrap and Less users, there is a mixin .placeholder: In addition to toscho's answer I've noticed some webkit inconsistencies between Chrome 9-10 and Safari 5 with the CSS properties supported that are worth noting. Specifically Chrome 9 and 10 do not support background-color, border, text-decoration and text-transform when styling the placeholder. The full cross-browser comparison is here. For Sass users: This will work fine. DEMO HERE:   input::-webkit-input-placeholder, textarea::-webkit-input-placeholder {   color: #666; } input:-moz-placeholder, textarea:-moz-placeholder {   color: #666; } input::-moz-placeholder, textarea::-moz-placeholder {   color: #666; } input:-ms-input-placeholder, textarea:-ms-input-placeholder {   color: #666; } <input type="text" placeholder="Value" />    In Firefox and Internet Explorer, the normal input text color overrides the color property of placeholders. So, we need to  Use the new ::placeholder if you use autoprefixer. Note that the .placeholder mixin from Bootstrap is deprecated in favor of this. Example: When using autoprefixer the above will be converted to the correct code for all browsers. Cross-browser solution: Credit: David Walsh Now we have a standard way to apply CSS to an input's placeholder : ::placeholder pseudo-element from this CSS Module Level 4 Draft. I just realize something for Mozilla Firefox 19+ that the browser gives an opacity value for the placeholder, so the color will not be what you really want. I overwrite the opacity for 1, so it will be good to go. I don't remember where I've found this code snippet on the Internet (it wasn't written by me, don't remember where I've found it, nor who wrote it). Just load this JavaScript code and then edit your placeholder with CSS by calling this rule: I think this code will work because a placeholder is needed only for input type text. So this one line CSS will be enough for your need: For Bootstrap users, if you are using class="form-control", there may be a CSS specificity issue. You should get a higher priority: Or if you are using Less: If you are using Bootstrap and couldn't get this working then probably you missed the fact that Bootstrap itself adds these selectors. This is Bootstrap v3.3 we are talking about. If you are trying to change the placeholder inside a .form-control CSS class then you should override it like this: How about this   <input type="text" value="placeholder text" onfocus="this.style.color='#000';      this.value='';" style="color: #f00;" />    No CSS or placeholder, but you get the same functionality. This short and clean code: I have tried every combination here to change the color, on my mobile platform, and eventually it was: which did the trick. For SASS/SCSS user using Bourbon, it has a built-in function. CSS Output, you can also grab this portion and paste into your code. try this code for different input element different style example 1: example 2: Adding an actual very nice and simple possibility: css filters!    It will style everything, including the placeholder. The following will set both input elements on the same palette, using the hue filter for color changes. It render very well now in browsers (except ie...)   input {   filter: sepia(100%) saturate(400%) grayscale(0) contrast(200%) hue-rotate(68deg) invert(18%); } <input placeholder="Hello world!" /> <input type="date" /><br> <input type="range" /> <input type="color" />    To allow users to change it dynamically, using an input type color for changes, or to find nuances, check out this snippet: From: https://codepen.io/Nico_KraZhtest/pen/bWExEB   function stylElem() {   stylo.dataset.hue = ((parseInt(stylo.value.substring(1), 16))/46666).toFixed(0)   Array.from(document.querySelectorAll('input, audio, video')).forEach(function(e){       e.style.cssText += ";filter:sepia(100%) saturate(400%)grayscale(0)contrast(200%)hue-rotate("+ stylo.dataset.hue+"deg)invert("+(stylo.dataset.hue/3.6)+"%)"   out.innerText = e.style.cssText })()}  stylElem() body {background: black; color: white} Choose a color! <input type="color" id="stylo" oninput="stylElem()"> <br> <div id="out"></div> <p>   <input placeholder="Hello world!" />   <input type="date" /><br>   <input type="range" />  <br> <audio controls src="#"></audio> <br><br>  <video controls src="#"></video>    Css filters docs: https://developer.mozilla.org/en-US/docs/Web/CSS/filter Here is one more example:   .form-control::-webkit-input-placeholder {   color: red;   width: 250px; } h1 {   color: red; } <div class="col-sm-4">   <input class="form-control" placeholder="Enter text here.." ng-model="Email" required/> </div>    OK, placeholders behave differently in different browsers, so you need using browser prefix in your CSS to make them identical, for example Firefox gives a transparency to placeholder by default, so need to add opacity 1 to your css, plus the color, it's not a big concern most of the times, but good to have them consistent: You can change an HTML5 input's placeholder color with CSS.  If by chance, your CSS conflict, this code note working , you can use (!important) like below. Hope this will help. You can use this for input and focus style: The easiest way would be: Here is the solution with CSS selectors Compass has a mixin for this out of the box. Take your example: And in SCSS using compass: See docs for the input-placeholder mixin. A part of HTML:  I gonna show how to change color of expression of 'Enter sentence' by CSS:
__label__rest __label__http __label__definition What exactly is RESTful programming? An architectural style called REST (Representational State Transfer) advocates that web applications should use HTTP as it was originally envisioned. Lookups should use GET requests. PUT, POST, and DELETE requests should be used for mutation, creation, and deletion respectively. REST proponents tend to favor URLs, such as but the REST architecture does not require these "pretty URLs". A GET request with a parameter is every bit as RESTful. Keep in mind that GET requests should never be used for updating information. For example, a GET request for adding an item to a cart would not be appropriate. GET requests should be idempotent. That is, issuing a request twice should be no different from issuing it once. That's what makes the requests cacheable. An "add to cart" request is not idempotent—issuing it twice adds two copies of the item to the cart. A POST request is clearly appropriate in this context. Thus, even a RESTful web application needs its share of POST requests. This is taken from the excellent book Core JavaServer faces book by David M. Geary. REST is the underlying architectural principle of the web. The amazing thing about the web is the fact that clients (browsers) and servers can interact in complex ways without the client knowing anything beforehand about the server and the resources it hosts. The key constraint is that the server and client must both agree on the media used, which in the case of the web is HTML. An API that adheres to the principles of REST does not require the client to know anything about the structure of the API. Rather, the server needs to provide whatever information the client needs to interact with the service. An HTML form is an example of this: The server specifies the location of the resource and the required fields. The browser doesn't know in advance where to submit the information, and it doesn't know in advance what information to submit. Both forms of information are entirely supplied by the server. (This principle is called HATEOAS: Hypermedia As The Engine Of Application State.) So, how does this apply to HTTP, and how can it be implemented in practice? HTTP is oriented around verbs and resources. The two verbs in mainstream usage are GET and POST, which I think everyone will recognize. However, the HTTP standard defines several others such as PUT and DELETE. These verbs are then applied to resources, according to the instructions provided by the server. For example, Let's imagine that we have a user database that is managed by a web service. Our service uses a custom hypermedia based on JSON, for which we assign the mimetype application/json+userdb (There might also be an application/xml+userdb and application/whatever+userdb - many media types may be supported). The client and the server have both been programmed to understand this format, but they don't know anything about each other. As Roy Fielding points out: A REST API should spend almost all of its descriptive effort in   defining the media type(s) used for representing resources and driving   application state, or in defining extended relation names and/or   hypertext-enabled mark-up for existing standard media types. A request for the base resource / might return something like this: Request Response We know from the description of our media that we can find information about related resources from sections called "links". This is called Hypermedia controls. In this case, we can tell from such a section that we can find a user list by making another request for /user: Request Response We can tell a lot from this response. For instance, we now know we can create a new user by POSTing to /user: Request Response We also know that we can change existing data: Request Response Notice that we are using different HTTP verbs (GET, PUT, POST, DELETE etc.) to manipulate these resources, and that the only knowledge we presume on the client's part is our media definition. Further reading: (This answer has been the subject of a fair amount of criticism for missing the point. For the most part, that has been a fair critique. What I originally described was more in line with how REST was usually implemented a few years ago when I first wrote this, rather than its true meaning. I've revised the answer to better represent the real meaning.) RESTful programming is about: The last one is probably the most important in terms of consequences and overall effectiveness of REST. Overall, most of the RESTful discussions seem to center on HTTP and its usage from a browser and what not. I understand that R. Fielding coined the term when he described the architecture and decisions that lead to HTTP. His thesis is more about the architecture and cache-ability of resources than it is about HTTP. If you are really interested in what a RESTful architecture is and why it works, read his thesis a few times and read the whole thing not just Chapter 5! Next look into why DNS works. Read about the hierarchical organization of DNS and how referrals work. Then read and consider how DNS caching works. Finally, read the HTTP specifications (RFC2616 and RFC3040 in particular) and consider how and why the caching works the way that it does. Eventually, it will just click. The final revelation for me was when I saw the similarity between DNS and HTTP. After this, understanding why SOA and Message Passing Interfaces are scalable starts to click. I think that the most important trick to understanding the architectural importance and performance implications of a RESTful and Shared Nothing architectures is to avoid getting hung up on the technology and implementation details. Concentrate on who owns resources, who is responsible for creating/maintaining them, etc. Then think about the representations, protocols, and technologies. This is what it might look like. Create a user with three properties: The server responds: In the future, you can then retrieve the user information: The server responds: To modify the record (lname and age will remain unchanged): To update the record (and consequently lname and age will be NULL): A great book on REST is REST in Practice. Must reads are Representational State Transfer (REST) and REST APIs must be hypertext-driven  See Martin Fowlers article the Richardson Maturity Model (RMM) for an explanation on what an RESTful service is.   To be RESTful a Service needs to fulfill the Hypermedia as the Engine of Application State. (HATEOAS), that is, it needs to reach level 3 in the RMM, read the article for details or the slides from the qcon talk. The HATEOAS constraint is an acronym   for Hypermedia as the Engine of   Application State. This principle is   the key differentiator between a REST   and most other forms of client server   system. ... A client of a RESTful application need   only know a single fixed URL to access   it. All future actions should be   discoverable dynamically from   hypermedia links included in the   representations of the resources that   are returned from that URL.   Standardized media types are also   expected to be understood by any   client that might use a RESTful API.    (From Wikipedia, the free encyclopedia) REST Litmus Test for Web Frameworks is a similar maturity test for web frameworks. Approaching pure REST: Learning to love HATEOAS is a good collection of links.  REST versus SOAP for the Public Cloud discusses the current levels of REST usage.  REST and versioning discusses Extensibility, Versioning, Evolvability, etc.  through Modifiability What is REST? REST stands for Representational State Transfer. (It is sometimes   spelled "ReST".) It relies on a stateless, client-server, cacheable   communications protocol -- and in virtually all cases, the HTTP   protocol is used. REST is an architecture style for designing networked applications.   The idea is that, rather than using complex mechanisms such as CORBA,   RPC or SOAP to connect between machines, simple HTTP is used to make   calls between machines. In many ways, the World Wide Web itself, based on HTTP, can be viewed   as a REST-based architecture. RESTful applications use HTTP requests   to post data (create and/or update), read data (e.g., make queries),   and delete data. Thus, REST uses HTTP for all four CRUD   (Create/Read/Update/Delete) operations. REST is a lightweight alternative to mechanisms like RPC (Remote   Procedure Calls) and Web Services (SOAP, WSDL, et al.). Later, we will   see how much more simple REST is. Despite being simple, REST is fully-featured; there's basically   nothing you can do in Web Services that can't be done with a RESTful   architecture. REST is not a "standard". There will never be a W3C   recommendataion for REST, for example. And while there are REST   programming frameworks, working with REST is so simple that you can   often "roll your own" with standard library features in languages like   Perl, Java, or C#. One of the best reference I found when I try to find the simple real meaning of rest. http://rest.elkstein.org/ REST is using the various HTTP methods (mainly GET/PUT/DELETE) to manipulate data. Rather than using a specific URL to delete a method (say, /user/123/delete), you would send a DELETE request to the /user/[id] URL, to edit a user, to retrieve info on a user you send a GET request to /user/[id] For example, instead a set of URLs which might look like some of the following.. You use the HTTP "verbs" and have.. It's programming where the architecture of your system fits the REST style laid out by Roy Fielding in his thesis. Since this is the architectural style that describes the web (more or less), lots of people are interested in it. Bonus answer: No. Unless you're studying software architecture as an academic or designing web services, there's really no reason to have heard the term. I would say RESTful programming would be about creating systems (API) that follow the REST architectural style. I found this fantastic, short, and easy to understand tutorial about REST by Dr. M. Elkstein and quoting the essential part that would answer your question for the most part: Learn REST: A Tutorial REST is an architecture style for designing networked applications.   The idea is that, rather than using complex mechanisms such as CORBA,   RPC or SOAP to connect between machines, simple HTTP is used to make   calls between machines. RESTful applications use HTTP requests to post data (create and/or   update), read data (e.g., make queries), and delete data. Thus, REST   uses HTTP for all four CRUD (Create/Read/Update/Delete) operations. I don't think you should feel stupid for not hearing about REST outside Stack Overflow..., I would be in the same situation!; answers to this other SO question on Why is REST getting big now could ease some feelings. I apologize if I'm not answering the question directly, but it's easier to understand all this with more detailed examples. Fielding is not easy to understand due to all the abstraction and terminology. There's a fairly good example here: Explaining REST and Hypertext: Spam-E the Spam Cleaning Robot And even better, there's a clean explanation with simple examples here (the powerpoint is more comprehensive, but you can get most of it in the html version): http://www.xfront.com/REST.ppt or http://www.xfront.com/REST.html After reading the examples, I could see why Ken is saying that REST is hypertext-driven. I'm not actually sure that he's right though, because that /user/123 is a URI that points to a resource, and it's not clear to me that it's unRESTful just because the client knows about it "out-of-band." That xfront document explains the difference between REST and SOAP, and this is really helpful too. When Fielding says, "That is RPC. It screams RPC.", it's clear that RPC is not RESTful, so it's useful to see the exact reasons for this. (SOAP is a type of RPC.) What is REST? REST in official words, REST is an architectural style built on certain principles using the current “Web” fundamentals. There are 5 basic fundamentals of web which are leveraged to create REST services. I see a bunch of answers that say putting everything about user 123 at resource "/user/123" is RESTful. Roy Fielding, who coined the term, says REST APIs must be hypertext-driven.  In particular, "A REST API must not define fixed resource names or hierarchies". So if your "/user/123" path is hardcoded on the client, it's not really RESTful.  A good use of HTTP, maybe, maybe not.  But not RESTful.  It has to come from hypertext. The answer is very simple, there is a dissertation written by Roy Fielding.]1 In that dissertation he defines the REST principles. If an application fulfills all of those principles, then that is a REST application. The term RESTful was created because ppl exhausted the word REST by calling their non-REST application as REST. After that the term RESTful was exhausted as well. Nowadays we are talking about Web APIs and Hypermedia APIs, because the most of the so called REST applications did not fulfill the HATEOAS part of the uniform interface constraint. The REST constraints are the following: client-server architecture So it does not work with for example PUB/SUB sockets, it is based on REQ/REP. stateless communication So the server does not maintain the states of the clients. This means that you cannot use server a side session storage and you have to authenticate every request. Your clients possibly send basic auth headers through an encrypted connection. (By large applications it is hard to maintain many sessions.) usage of cache if you can So you don't have to serve the same requests again and again. uniform interface as common contract between client and server The contract between the client and the server is not maintained by the server. In other words the client must be decoupled from the implementation of the service. You can reach this state by using standard solutions, like the IRI (URI) standard to identify resources, the HTTP standard to exchange messages, standard MIME types to describe the body serialization format, metadata (possibly RDF vocabs, microformats, etc.) to describe the semantics of different parts of the message body. To decouple the IRI structure from the client, you have to send hyperlinks to the clients in hypermedia formats like (HTML, JSON-LD, HAL, etc.). So a client can use the metadata (possibly link relations, RDF vocabs) assigned to the hyperlinks to navigate the state machine of the application through the proper state transitions in order to achieve its current goal. For example when a client wants to send an order to a webshop, then it have to check the hyperlinks in the responses sent by the webshop. By checking the links it founds one described with the http://schema.org/OrderAction. The client know the schema.org vocab, so it understands that by activating this hyperlink it will send the order. So it activates the hyperlink and sends a POST https://example.com/api/v1/order message with the proper body. After that the service processes the message and responds with the result having the proper HTTP status header, for example 201 - created by success. To annotate messages with detailed metadata the standard solution to use an RDF format, for example JSON-LD with a REST vocab, for example Hydra and domain specific vocabs like schema.org or any other linked data vocab and maybe a custom application specific vocab if needed. Now this is not easy, that's why most ppl use HAL and other simple formats which usually provide only a REST vocab, but no linked data support. build a layered system to increase scalability The REST system is composed of hierarchical layers. Each layer contains components which use the services of components which are in the next layer below. So you can add new layers and components effortless.  For example there is a client layer which contains the clients and below that there is a service layer which contains a single service. Now you can add a client side cache between them. After that you can add another service instance and a load balancer, and so on... The client code and the service code won't change. code on demand to extend client functionality This constraint is optional. For example you can send a parser for a specific media type to the client, and so on... In order to do this you might need a standard plugin loader system in the client, or your client will be coupled to the plugin loader solution. REST constraints result a highly scalable system in where the clients are decoupled from the implementations of the services. So the clients can be reusable, general just like the browsers on the web. The clients and the services share the same standards and vocabs, so they can understand each other despite the fact that the client does not know the implementation details of the service. This makes possible to create automated clients which can find and utilize REST services to achieve their goals. In long term these clients can communicate to each other and trust each other with tasks, just like humans do. If we add learning patterns to such clients, then the result will be one or more AI using the web of machines instead of a single server park. So at the end the dream of Berners Lee: the semantic web and the artificial intelligence will be reality. So in 2030 we end up terminated by the Skynet. Until then ... ;-) RESTful (Representational state transfer) API programming is writing web applications in any programming language by following 5 basic software architectural style principles: In other words you're writing simple point-to-point network applications over HTTP which uses verbs such as GET, POST, PUT or DELETE by implementing RESTful architecture which proposes standardization of the interface each “resource” exposes. It is nothing that using current features of the web in a simple and effective way (highly successful, proven and distributed architecture). It is an alternative to more complex mechanisms like SOAP, CORBA and RPC.  RESTful programming conforms to Web architecture design and, if properly implemented, it allows you to take the full advantage of scalable Web infrastructure. Here is my basic outline of REST. I tried to demonstrate the thinking behind each of the components in a RESTful architecture so that understanding the concept is more intuitive. Hopefully this helps demystify REST for some people! REST (Representational State Transfer) is a design architecture that outlines how networked resources (i.e. nodes that share information) are designed and addressed. In general, a RESTful architecture makes it so that the client (the requesting machine) and the server (the responding machine) can request to read, write, and update data without the client having to know how the server operates and the server can pass it back without needing to know anything about the client. Okay, cool...but how do we do this in practice? The most obvious requirement is that there needs to be a universal language of some sort so that the server can tell the client what it is trying to do with the request and for the server to respond. But to find any given resource and then tell the client where that resource lives, there needs to be a universal way of pointing at resources. This is where Universal Resource Identifiers (URIs) come in; they are basically unique addresses to find the resources. But the REST architecture doesn’t end there! While the above fulfills the basic needs of what we want, we also want to have an architecture that supports high volume traffic since any given server usually handles responses from a number of clients. Thus, we don’t want to overwhelm the server by having it remember information about previous requests. Therefore, we impose the restriction that each request-response pair between the client and the server is independent, meaning that the server doesn’t have to remember anything about previous requests (previous states of the client-server interaction) to respond to a new request. This means that we want our interactions to be stateless. To further ease the strain on our server from redoing computations that have already been recently done for a given client, REST also allows caching. Basically, caching means to take a snapshot of the initial response provided to the client. If the client makes the same request again, the server can provide the client with the snapshot rather than redo all of the computations that were necessary to create the initial response. However, since it is a snapshot, if the snapshot has not expired--the server sets an expiration time in advance--and the response has been updated since the initial cache (i.e. the request would give a different answer than the cached response), the client will not see the updates until the cache expires (or the cache is cleared) and the response is rendered from scratch again. The last thing that you’ll often here about RESTful architectures is that they are layered. We have actually already been implicitly discussing this requirement in our discussion of the interaction between the client and server. Basically, this means that each layer in our system interacts only with adjacent layers. So in our discussion, the client layer interacts with our server layer (and vice versa), but there might be other server layers that help the primary server process a request that the client does not directly communicate with. Rather, the server passes on the request as necessary. Now, if all of this sounds familiar, then great. The Hypertext Transfer Protocol (HTTP), which defines the communication protocol via the World Wide Web is an implementation of the abstract notion of RESTful architecture (or an implementation of the abstract REST class if you're an OOP fanatic like me). In this implementation of REST, the client and server interact via GET, POST, PUT, DELETE, etc., which are part of the universal language and the resources can be pointed to using URLs. If I had to reduce the original dissertation on REST to just 3 short sentences, I think the following captures its essence: After that, it's easy to fall into debates about adaptations, coding conventions, and best practices. Interestingly, there is no mention of HTTP POST, GET, DELETE, or PUT operations in the dissertation. That must be someone's later interpretation of a "best practice" for a "uniform interface". When it comes to web services, it seems that we need some way of distinguishing WSDL and SOAP based architectures which add considerable overhead and arguably much unnecessary complexity to the interface. They also require additional frameworks and developer tools in order to implement. I'm not sure if REST is the best term to distinguish between common-sense interfaces and overly engineered interfaces such as WSDL and SOAP. But we need something. REST is an architectural pattern and style of writing distributed applications. It is not a programming style in the narrow sense. Saying you use the REST style is similar to saying that you built a house in a particular style: for example Tudor or Victorian.  Both REST as an software style and Tudor or Victorian as a home style can be defined by the qualities and constraints that make them up. For example REST must have Client Server separation where messages are self-describing. Tudor style homes have Overlapping gables and Roofs that are steeply pitched with front facing gables. You can read Roy's dissertation to learn more about the constraints and qualities that make up REST. REST unlike home styles has had a tough time being consistently and practically applied. This may have been intentional. Leaving its actual implementation up to the designer. So you are free to do what you want so as long as you meet the constraints set out in the dissertation you are creating REST Systems. Bonus: The entire web is based on REST (or REST was based on the web). Therefore as a web developer you might want aware of that although it's not necessary to write good web apps.  I think the point of restful is the separation of the statefulness into a higher layer while making use of the internet (protocol) as a stateless transport layer. Most other approaches mix things up.  It's been the best practical approach to handle the fundamental changes of programming in internet era. Regarding the fundamental changes, Erik Meijer has a discussion on show here: http://www.infoq.com/interviews/erik-meijer-programming-language-design-effects-purity#view_93197 . He summarizes it as the five effects, and presents a solution by designing the solution into a programming language. The solution, could also be achieved in the platform or system level, regardless of the language. The restful could be seen as one of the solutions that has been very successful in the current practice.  With restful style, you get and manipulate the state of the application across an unreliable internet. If it fails the current operation to get the correct and current state, it needs the zero-validation principal to help the application to continue. If it fails to manipulate the state, it usually uses multiple stages of confirmation to keep things correct. In this sense, rest is not itself a whole solution, it needs the functions in other part of the web application stack to support its working.  Given this view point, the rest style is not really tied to internet or web application. It's a fundamental solution to many of the programming situations. It is not simple either, it just makes the interface really simple, and copes with other technologies amazingly well.  Just my 2c.  Edit: Two more important aspects:  Statelessness is misleading. It is about the restful API, not the application or system. The system needs to be stateful. Restful design is about designing a stateful system based on a stateless API. Some quotes from another QA:  Idempotence: An often-overlooked part of REST is the idempotency of most verbs. That leads to robust systems and less interdependency of exact interpretations of the semantics.  Old question, newish way of answering.  There's a lot of misconception out there about this concept.  I always try to remember: I define restful programming as  An application is restful if it provides resources (being the combination of data + state transitions controls) in a media type the client understands To be a restful programmer you must be trying to build applications that allow actors to do things.  Not just exposing the database. State transition controls only make sense if the client and server agree upon a media type representation of the resource.  Otherwise there's no way to know what's a control and what isn't and how to execute a control.  IE if browsers didn't know <form> tags in html then there'd be nothing for you to submit to transition state in your browser.   I'm not looking to self promote, but i expand on these ideas to great depth in my talk http://techblog.bodybuilding.com/2016/01/video-what-is-restful-200.html . An excerpt from my talk is about the often referred to richardson maturity model, i don't believe in the levels, you either are RESTful (level 3) or you are not, but what i like to call out about it is what each level does for you on your way to RESTful  This is amazingly long "discussion" and yet quite confusing to say the least. IMO: 1) There is no such a thing as restful programing, without a big joint and lots of beer :) 2) Representational State Transfer (REST) is an architectural style specified in the dissertation of Roy Fielding. It has a number of constraints.  If your Service/Client respect those then it is RESTful. This is it.  You can summarize(significantly) the constraints to : There is another very good post which explains things nicely. A lot of answers copy/pasted valid information mixing it and adding some confusion. People talk here about levels, about RESTFul URIs(there is not such a thing!), apply HTTP methods GET,POST,PUT ... REST is not about that or not only about that. For example links - it is nice to have a beautifully looking API but at the end the client/server does not really care of the links you get/send it is the content that matters.  In the end any RESTful client should be able to consume to any RESTful service as long as the content format is known. REST defines 6 architectural constraints which make any web service – a true RESTful API. https://restfulapi.net/rest-architectural-constraints/ REST is an architectural style which is based on web-standards and the HTTP protocol (introduced in 2000). In a REST based architecture, everything is a resource(Users, Orders, Comments). A resource is accessed via a common interface based on the HTTP standard methods(GET, PUT, PATCH, DELETE etc).  In a REST based architecture you have a REST server which provides   access to the resources. A REST client can access and modify the REST   resources. Every resource should support the HTTP common operations. Resources are identified by global IDs (which are typically URIs). REST allows that resources have different representations, e.g., text, XML, JSON etc. The REST client can ask for a specific representation via the HTTP protocol (content negotiation). HTTP methods: The PUT, GET, POST and DELETE methods are typical used in REST based architectures. The following table gives an explanation of these operations. REST === HTTP analogy is not correct until you do not stress to the fact that it "MUST" be HATEOAS driven.  Roy himself cleared it here. A REST API should be entered with no prior knowledge beyond the initial URI (bookmark) and set of standardized media types that are appropriate for the intended audience (i.e., expected to be understood by any client that might use the API). From that point on, all application state transitions must be driven by client selection of server-provided choices that are present in the received representations or implied by the user’s manipulation of those representations. The transitions may be determined (or limited by) the client’s knowledge of media types and resource communication mechanisms, both of which may be improved on-the-fly (e.g., code-on-demand).  [Failure here implies that out-of-band information is driving interaction instead of hypertext.] REST stands for Representational state transfer. It relies on a stateless, client-server, cacheable communications protocol -- and in virtually all cases, the HTTP protocol is used. REST is often used in mobile applications, social networking Web sites, mashup tools and automated business processes. The REST style emphasizes that interactions between clients and services is enhanced by having a limited number of operations (verbs). Flexibility is provided by assigning resources (nouns) their own unique universal resource indicators (URIs).  Introduction about Rest Talking is more than simply exchanging information. A Protocol is actually designed so that no talking has to occur. Each party knows what their particular job is because it is specified in the protocol. Protocols allow for pure information exchange at the expense of having any changes in the possible actions. Talking, on the other hand, allows for one party to ask what further actions can be taken from the other party. They can even ask the same question twice and get two different answers, since the State of the other party may have changed in the interim. Talking is RESTful architecture. Fielding's thesis specifies the architecture that one would have to follow if one wanted to allow machines to talk to one another rather than simply communicate. There is not such notion as "RESTful programming" per se. It would be better called RESTful paradigm or even better RESTful architecture. It is not a programming language. It is a paradigm. From Wikipedia: In computing, representational state transfer (REST) is an   architectural style used for web development. The point of rest is that if we agree to use a common language for basic operations (the http verbs), the infrastructure can be configured to understand them and optimize them properly, for example, by making use of caching headers to implement caching at all levels. With a properly implemented restful GET operation, it shouldn't matter if the information comes from your server's DB, your server's memcache, a CDN, a proxy's cache, your browser's cache or your browser's local storage. The fasted, most readily available up to date source can be used. Saying that Rest is just a syntactic change from using GET requests with an action parameter to using the available http verbs makes it look like it has no benefits and is purely cosmetic. The point is to use a language that can be understood and optimized by every part of the chain. If your GET operation has an action with side effects, you have to skip all HTTP caching or you'll end up with inconsistent results. What is API Testing? API testing utilizes programming to send calls to the API and get the yield. It testing regards the segment under test as a black box. The objective of API testing is to confirm right execution and blunder treatment of the part preceding its coordination into an application. REST API REST: Representational State Transfer.   4 Commonly Used API Methods:-  Steps to Test API Manually:- To use API manually, we can use browser based REST API plugins.   Steps to Automate REST API  This is very less mentioned everywhere but the Richardson's Maturity Model is one of the best methods to actually judge how Restful is one's API. More about it here: Richardson's Maturity Model This answer is for absolute beginners, let's know about most used API architecture today. To understand Restful programming or Restful API. First, you have to understand what API is, on a very high-level API stands for Application Programming Interface, it's basically a piece of software that can be used by another piece of software in order to allow applications to talk to each other. The most widely used type of API in the globe is web APIs while an app that sends data to a client whenever a request comes in.  In fact, APIs aren't only used to send data and aren't always related to web development or javascript or python or any programming language or framework.  The application in API can actually mean many different things as long as the pice of software is relatively stand-alone. Take for example, the File System or the HTTP Modules we can say that they are small pieces of software and we can use them, we can interact with them by using their API. For example when we use the read file function for a file system module of any programming language, we are actually using the file_system_reading API. Or when we do DOM manipulation in the browser, we're are not really using the JavaScript language itself, but rather, the DOM API that browser exposes to us, so it gives us access to it. Or even another example let's say we create a class in any programming language like Java and then add some public methods or properties to it, these methods will then be the API of each object created from that class because we are giving other pieces of software the possibility of interacting with our initial piece of software, the objects in this case. S0, API has actually a broader meaning than just building web APIs. Now let's take a look at the REST Architecture to build APIs. REST which stands for Representational State Transfer is basically a way of building web APIs in a logical way, making them easy to consume for ourselves or for others. To build Restful APIs following the REST Architecture, we just need to follow a couple of principles. 1. We need to separate our API into logical resources. 2. These resources should then be exposed by using resource-based URLs. 3. To perform different actions on data like reading, creating, or deleting data the API should use the right HTTP methods and not the URL. 4. Now the data that we actually send back to the client or that we received from the client should usually use the JSON data format, were some formatting standard applied to it. 5. Finally, another important principle of EST APIs is that they must be stateless.  Separate APIs into logical resources: The key abstraction of information in REST is a resource, and therefore all the data that we wanna share in the API should be divided into logical resources. What actually is a resource? Well, in the context of REST it is an object or a representation of something which has some data associated to it. For example, applications like tour-guide tours, or users, places, or revies are of the example of logical resources. So basically any information that can be named can be a resource. Just has to name, though, not a verb.  Expose Structure: Now we need to expose, which means to make available, the data using some structured URLs, that the client can send a request to. For example something like this entire address is called the URL. and this / addNewTour is called and API Endpoint.  Our API will have many different endpoints just like bellow Each of these API will send different data back to the client on also perform different actions.  Now there is something very wrong with these endpoints here because they really don't follow the third rule which says that we should only use the HTTP methods in order to perform actions on data. So endpoints should only contain our resources and not the actions that we are performed on them because they will quickly become a nightmare to maintain. How should we use these HTTP methods in practice? Well let's see how these endpoints should actually look like starting with /getTour. So this getTour endpoint is to get data about a tour and so we should simply name the endpoint /tours and send the data whenever a get request is made to this endpoint. So in other words, when a client uses a GET HTTP method to access the endpoint,  (we only have resources in the endpoint or in the URL and no verbs because the verb is now in the HTTP method, right? The common practice to always use the resource name in the plural which is why I wrote /tours nor /tour.) The convention is that when calling endpoint /tours will get back all the tours that are in a database, but if we only want the tour with one ID, let's say seven, we add that seven after another slash(/tours/7) or in a search query (/tours?id=7), And of course, it could also be the name of a tour instead of the ID. HTTP Methods: What's really important here is how the endpoint name is the exact same name for all. The difference between PUT and PATCH-> By using PUT, the client is supposed to send the entire updated object, while with PATCH it is supposed to send only the part of the object that has been changed. By using HTTP methods users can perform basic four CRUD operations, CRUD stands for Create, Read, Update, and Delete. Now there could be a situation like a bellow:  So, /getToursByUser can simply be translated to /users/tours, for user number 3 end point will be like /users/3/tours. if we want to delete a particular tour of a particular user then the URL should be like /users/3/tours/7, here user id:3 and tour id: 7. So there really are tons of possibilities of combining resources like this.  Send data as JSON: Now about data that the client actually receives, or that the server receives from the client, usually we use the JSON Data Format.  A typical JSON might look like below:  Before sending JSON Data we usually do some simple response formatting, there are a couple of standards for this, but one of the very simple ones called Jsend. We simply create a new object, then add a status message to it in order to inform the client whether the request was a success, fail, or error. And then we put our original data into a new object called Data.  Wrapping the data into an additional object like we did here is called Enveloping, and it's a common practice to mitigate some security issues and other problems. Restful API should always be stateless: Finally a RESTful API should always be stateless meaning that, in a stateless RESTful API all state is handled on the client side no on the server. And state simply refers to a piece of data in  the application that might change over time. For example, whether a certain user is logged in or on a page with a list with several pages what the current page is? Now the fact that the state should be handled on the client means that each request must contain all the information that is necessary to process a certain request on the server. So the server should never ever have to remember the previous request in order to process the current request.  Let's say that currently we are on page five and we want to move forward to page six. Sow we could have a simple endpoint called /tours/nextPage and submit a request to server, but the server would then have to figure out what the current page is, and based on that server will send the next page to the client. In other words, the server would have to remember the previous request. This is what exactly we want to avoid in RESTful APIs. Instead of this case, we should create a /tours/page endpoint and paste the number six to it in order to request page number six /tours/page/6 . So the server doesn't have to remember anything in, all it has to do is to send back data for page number six as we requested.  Statelessness and Statefulness which is the opposite are very important concepts in computer science and applications in general
__label__git-fork __label__git __label__pull-request __label__updates __label__github I recently forked a project and applied several fixes. I then created a pull request which was then accepted. A few days later another change was made by another contributor. So my fork doesn't contain that change.  How can I get that change into my fork? Do I need to delete and re-create my fork when I have further changes to contribute? Or is there an update button? In your local clone of your forked repository, you can add the original GitHub repository as a "remote".  ("Remotes" are like nicknames for the URLs of repositories - origin is one, for example.)  Then you can fetch all the branches from that upstream repository, and rebase your work to continue working on the upstream version.  In terms of commands that might look like: If you don't want to rewrite the history of your master branch, (for example because other people may have cloned it) then you should replace the last command with git merge upstream/master.  However, for making further pull requests that are as clean as possible, it's probably better to rebase. If you've rebased your branch onto upstream/master you may need to force the push in order to push it to your own forked repository on GitHub.  You'd do that with: You only need to use the -f the first time after you've rebased. Starting in May 2014, it is possible to update a fork directly from GitHub. This still works as of September 2017, BUT it will lead to a dirty commit history. Now you have three options, but each will lead to a less-than-clean commit history. So yes, you can keep your repo updated with its upstream using the GitHub web UI, but doing so will sully your commit history. Stick to the command line instead - it's easy. Here is GitHub's official document on Syncing a fork: Before you can sync, you need to add a remote that points to the upstream repository. You may have done this when you originally forked. Tip: Syncing your fork only updates your local copy of the repository; it does not update your repository on GitHub. There are two steps required to sync your repository with the upstream: first you must fetch from the remote, then you must merge the desired branch into your local branch. Fetching from the remote repository will bring in its branches and their respective commits. These are stored in your local repository under special branches. We now have the upstream's master branch stored in a local branch, upstream/master Now that we have fetched the upstream repository, we want to merge its changes into our local branch. This will bring that branch into sync with the upstream, without losing our local changes. If your local branch didn't have any unique commits, git will instead perform a "fast-forward": Tip: If you want to update your repository on GitHub, follow the instructions here A lot of answers end up moving your fork one commit ahead of the parent repository. This answer summarizes the steps found here which will move your fork to the same commit as the parent. Change directory to your local repository. Add the parent as a remote repository, git remote add upstream <repo-location> Issue git rebase upstream/master Issue git push origin master For more information about these commands, refer to step 3. If, like me, you never commit anything directly to master, which you should really, you can do the following. From the local clone of your fork, create your upstream remote. You only need to do that once: Then whenever you want to catch up with the upstream repository master branch you need to: Assuming you never committed anything on master yourself you should be done already. Now you can push your local master to your origin remote GitHub fork. You could also rebase your development branch on your now up-to-date local master. Past the initial upstream setup and master checkout, all you need to do is run the following command to sync your master with upstream: git pull upstream master. Foreword: Your fork is the "origin" and the repository you forked from is the "upstream". Let's assume that you cloned already your fork to your computer with a command like this: If that is given then you need to continue in this order: Add the "upstream" to your cloned repository ("origin"): Fetch the commits (and branches) from the "upstream": Switch to the "master" branch of your fork ("origin"): Stash the changes of your "master" branch: Merge the changes from the "master" branch of the "upstream" into your the "master" branch of your "origin": Resolve merge conflicts if any and commit your merge Push the changes to your fork Get back your stashed changes (if any) You're done! Congratulations! GitHub also provides instructions for this topic: Syncing a fork Since November 2013 there has been an unofficial feature request open with GitHub to ask them to add a very simple and intuitive method to keep a local fork in sync with upstream: https://github.com/isaacs/github/issues/121 Note: Since the feature request is unofficial it is also advisable to contact support@github.com to add your support for a feature like this to be implemented. The unofficial feature request above could be used as evidence of the amount of interest in this being implemented. As of the date of this answer, GitHub has not (or shall I say no longer?) this feature in the web interface. You can, however, ask support@github.com to add your vote for that. In the meantime, GitHub user bardiharborow has created a tool to do just this: https://upriver.github.io/ Source is here: https://github.com/upriver/upriver.github.io If you are using GitHub for Windows or Mac then now they have a one-click feature to update forks: Actually, it is possible to create a branch in your fork from any commit of the upstream in the browser:  You can then fetch that branch to your local clone, and you won't have to push all that data back to GitHub when you push edits on top of that commit. Or use the web interface to change something in that branch. How it works (it is a guess, I don't know how exactly GitHub does it): forks share object storage and use namespaces to separate users' references. So you can access all commits through your fork, even if they did not exist by the time of forking. I update my forked repos with this one line: Use this if you dont want to add another remote endpoint to your project, as other solutions posted here.  Follow the below steps. I tried them and it helped me. Checkout to your branch Syntax: git branch yourDevelopmentBranch Example: git checkout master Pull source repository branch for getting the latest code Syntax: git pull https://github.com/tastejs/awesome-app-ideas master Example: git pull https://github.com/ORIGINAL_OWNER/ORIGINAL_REPO.git BRANCH_NAME As a complement to this answer, I was looking for a way to update all remote branches of my cloned repo (origin) from upstream branches in one go. This is how I did it. This assumes you have already configured an upstream remote pointing at the source repository (where origin was forked from) and have synced it with git fetch upstream. Then run: The first part of this command lists all heads in the upstream remote repo and removes the SHA-1 followed by refs/heads/ branch name prefix. Then for each of these branches, it pushes the local copy of the upstream remote tracking branch (refs/remotes/upstream/<branch> on local side) directly to the remote branch on origin (refs/heads/<branch> on remote side). Any of these branch sync commands may fail for one of two reasons: either the upstream branch have been rewritten, or you have pushed commits on that branch to your fork. In the first case where you haven't committed anything to the branch on your fork it is safe to push forcefully (Add the -f switch; i.e. git push -f in the command above). In the other case this is normal as your fork branch have diverged and you can't expect the sync command to work until your commits have been merged back into upstream. The "Pull" app is an automatic set-up-and-forget solution. It will sync the default branch of your fork with the upstream repository. Visit the URL, click the green "Install" button and select the repositories where you want to enable automatic synchronization. The branch is updated once per hour directly on GitHub, on your local machine you need to pull the master branch to ensure that your local copy is in sync. When you have cloned your forked repository, go to the directory path where your clone resides and the few lines in your Git Bash Terminal. And there you are good to go. All updated changes in the main repository will be pushed into your fork repository. The "fetch" command is indispensable for staying up-to-date in a project: only when performing a "git fetch" will you be informed about the changes your colleagues pushed to the remote server. You can still visit here for further queries If you set your upstream. Check with git remote -v, then this will suffice. Android Studio now has learned to work with GitHub fork repositories (you don't even have to add "upstream" remote repository by console command). Open menu VCS → Git And pay attention to the two last popup menu items: Rebase my GitHub fork Create Pull Request Try them. I use the first one to synchronize my local repository. Anyway the branches from the parent remote repository ("upstream") will be accessible in Android Studio after you click "Rebase my GitHub fork", and you will be able to operate with them easily. (I use Android Studio 3.0 with "Git integration" and "GitHub" plugins.)  There's a way to do it from GitHub's webapp. Let's go through the following example. To start with, open the repo that you want to update.  One can see the warning This branch is 157 commits behind GoogleCloudPlatform:master. On the right there are two buttons Pull request and Compare. Press Compare. As there is probably nothing to compare, press switching the base  A list of all the changes will appear and one can create a pull request by pressing the button Create pull request  Give it a title, let's say "Update repo"  And create the pull request. Once the request is created, scroll to the bottom and press Merge pull request.  Confirm the merge and that's it! That depends on the size of your repository and how you forked it. If it's quite a big repository you may have wanted to manage it in a special way (e.g. drop history). Basically, you can get differences between current and upstream versions, commit them and then cherry pick back to master. Try reading this one. It describes how to handle big Git repositories and how to upstream them with latest changes. I would like to add on to @krlmlr's answer.  Initially, the forked repository has one branch named : master. If you are working on a new feature or a fix, you would generally create a new branch feature and make the changes. If you want the forked repository to be in sync with the parent repository, you could set up a config file(pull.yml) for the Pull app (in the feature branch), like this: This keeps the master branch of the forked repo up-to-date with the parent repo.  It keeps the feature branch of the forked repo updated via the master branch of the forked repo by merging the same. This assumes that the feature branch is the default branch which contains the config file. Here two mergemethods are into play, one is hardreset which helps force sync changes in the master branch of the forked repo with the parent repo and the other method is merge. This method is used to merge changes done by you in the feature branch and changes done due to force sync in the master branch. In case of merge conflict, the pull app will allow you to choose the next course of action during the pull request. You can read about basic and advanced configs and various mergemethods here. I am currently using this configuration in my forked repo here to make sure an enhancement requested here stays updated. There are two main things on keeping a forked repository always update for good. 1. Create the branches from the fork master and do changes there. So when your Pull Request is accepted then you can safely delete the branch as your contributed code will be then live in your master of your forked repository when you update it with the upstream. By this your master will always be in clean condition to create a new branch to do another change. 2. Create a scheduled job for the fork master to do update automatically. This can be done with cron. Here is for an example code if you do it in linux. put this code on the crontab file to execute the job in hourly basis. then create the cron.sh script file and a git interaction with ssh-agent and/or expect as below Check your forked repository. From time to time it will always show this notification: This branch is even with <upstream>:master.  There may be subtler options, but it is the only way that I have any confidence that my local repository is the same as upstream. Use these commands (in lucky case) If you use GitHub Desktop, you can do it easily in just 6 steps (actually only 5). Once you open Github Desktop and choose your repository, Checkout the GIF below as an example:  Delete your remote dev from github page then apply these commands: If you want to keep your GitHub forks up to date with the respective upstreams, there also exists this probot program for GitHub specifically: https://probot.github.io/apps/pull/ which does the job. You would need to allow installation in your account and it will keep your forks up to date. How to update your forked repo on your local machine? First, check your remote/master You should have origin and upstream. For example: After that go to main: and merge from upstream to main:
__label__javascript __label__capitalize __label__letter __label__string How do I make the first letter of a string uppercase, but not change the case of any of the other letters? For example: The basic solution is:   function capitalizeFirstLetter(string) {   return string.charAt(0).toUpperCase() + string.slice(1); }  console.log(capitalizeFirstLetter('foo')); // Foo    Some other answers modify String.prototype (this answer used to as well), but I would advise against this now due to maintainability (hard to find out where the function is being added to the prototype and could cause conflicts if other code uses the same name / a browser adds a native function with that same name in future). ...and then, there is so much more to this question when you consider internationalisation, as this astonishingly good answer (buried below) shows. If you want to work with Unicode code points instead of code units (for example to handle Unicode characters outside of the Basic Multilingual Plane) you can leverage the fact that String#[@iterator] works with code points, and you can use toLocaleUpperCase to get locale-correct uppercasing:   const capitalizeFirstLetter = ([ first, ...rest ], locale = navigator.language) =>   first.toLocaleUpperCase(locale) + rest.join('')  console.log(   capitalizeFirstLetter('foo'), // Foo   capitalizeFirstLetter("𐐶𐐲𐑌𐐼𐐲𐑉"), // "𐐎𐐲𐑌𐐼𐐲𐑉" (correct!)   capitalizeFirstLetter("italya", 'tr') // İtalya" (correct in Turkish Latin!) )    For even more internationalization options, please see the original answer below. Here's a more object-oriented approach: You'd call the function, like this: With the expected output being: In CSS: Here is a shortened version of the popular answer that gets the first letter by treating the string as an array: Update: According to the comments below this doesn't work in IE 7 or below. Update 2: To avoid undefined for empty strings (see @njzk2's comment below), you can check for an empty string: Here are the fastest methods based on this jsperf test (ordered from fastest to slowest). As you can see, the first two methods are essentially comparable in terms of performance, whereas altering the String.prototype is by far the slowest in terms of performance.  For another case I need it to capitalize the first letter and lowercase the rest. The following cases made me change this function: I didn’t see any mention in the existing answers of issues related to astral plane code points or internationalization. “Uppercase” doesn’t mean the same thing in every language using a given script. Initially I didn’t see any answers addressing issues related to astral plane code points. There is one, but it’s a bit buried (like this one will be, I guess!) Most of the proposed functions look like this: However, some cased characters fall outside the BMP (basic multilingual plane, code points U+0 to U+FFFF). For example take this Deseret text: The first character here fails to capitalize because the array-indexed properties of strings don’t access “characters” or code points*. They access UTF-16 code units. This is true also when slicing — the index values point at code units. It happens to be that UTF-16 code units are 1:1 with USV code points within two ranges, U+0 to U+D7FF and U+E000 to U+FFFF inclusive. Most cased characters fall into those two ranges, but not all of them. From ES2015 on, dealing with this became a bit easier. String.prototype[@@iterator] yields strings corresponding to code points**. So for example, we can do this: For longer strings, this is probably not terribly efficient*** — we don’t really need to iterate the remainder. We could use String.prototype.codePointAt to get at that first (possible) letter, but we’d still need to determine where the slice should begin. One way to avoid iterating the remainder would be to test whether the first codepoint is outside the BMP; if it isn’t, the slice begins at 1, and if it is, the slice begins at 2. You could use bitwise math instead of > 0xFFFF there, but it’s probably easier to understand this way and either would achieve the same thing. We can also make this work in ES5 and below by taking that logic a bit further if necessary. There are no intrinsic methods in ES5 for working with codepoints, so we have to manually test whether the first code unit is a surrogate****: At the start I also mentioned internationalization considerations. Some of these are very difficult to account for because they require knowledge not only of what language is being used, but also may require specific knowledge of the words in the language. For example, the Irish digraph "mb" capitalizes as "mB" at the start of a word. Another example, the German eszett, never begins a word (afaik), but still helps illustrate the problem. The lowercase eszett (“ß”) capitalizes to “SS,” but  “SS” could lowercase to either “ß” or “ss” — you require out-of-band knowledge of the German language to know which is correct! The most famous example of these kinds of issues, probably, is Turkish. In Turkish Latin, the capital form of i is İ, while the lowercase form of I is ı — they’re two different letters. Fortunately we do have a way to account for this: In a browser, the user’s most-preferred language tag is indicated by navigator.language, a list in order of preference is found at navigator.languages, and a given DOM element’s language can be obtained (usually) with Object(element.closest('[lang]')).lang || YOUR_DEFAULT_HERE in multilanguage documents. In agents which support Unicode property character classes in RegExp, which were introduced in ES2018, we can clean stuff up further by directly expressing what characters we’re interested in: This could be tweaked a bit to also handle capitalizing multiple words in a string with fairly good accuracy. The CWU or Changes_When_Uppercased character property matches all code points which, well, change when uppercased. We can try this out with a titlecased digraph characters like the Dutch ĳ for example: As of January 2021, all major engines have implemented the Unicode property character class feature, but depending on your target support range you may not be able to use it safely yet. The last browser to introduce support was Firefox (78; June 30, 2020). You can check for support of this feature with the Kangax compat table. Babel can be used to compile RegExp literals with property references to equivalent patterns without them, but be aware that the resulting code can sometimes be enormous. You probably would not want to do this unless you’re certain the tradeoff is justified for your use case. In all likelihood, people asking this question will not be concerned with Deseret capitalization or internationalization. But it’s good to be aware of these issues because there’s a good chance you’ll encounter them eventually even if they aren’t concerns presently. They’re not “edge” cases, or rather, they’re not by-definition edge cases — there’s a whole country where most people speak Turkish, anyway, and conflating code units with codepoints is a fairly common source of bugs (especially with regard to emoji). Both strings and language are pretty complicated! * The code units of UTF-16 / UCS2 are also Unicode code points in the sense that e.g. U+D800 is technically a code point, but that’s not what it “means” here ... sort of ... though it gets pretty fuzzy. What the surrogates definitely are not, though, is USVs (Unicode scalar values). ** Though if a surrogate code unit is “orphaned” — i.e., not part of a logical pair — you could still get surrogates here, too. *** maybe. I haven’t tested it. Unless you have determined capitalization is a meaningful bottleneck, I probably wouldn’t sweat it — choose whatever you believe is most clear and readable. **** such a function might wish to test both the first and second code units instead of just the first, since it’s possible that the first unit is an orphaned surrogate. For example the input "\uD800x" would capitalize the X as-is, which may or may not be expected. This is the 2018 ECMAScript 6+ Solution:   const str = 'the Eiffel Tower'; const newStr = `${str[0].toUpperCase()}${str.slice(1)}`; console.log('Original String:', str); // the Eiffel Tower console.log('New String:', newStr); // The Eiffel Tower    If you're already (or considering) using Lodash, the solution is easy: See their documentation: https://lodash.com/docs#capitalize _.camelCase('Foo Bar'); //=> 'fooBar' https://lodash.com/docs/4.15.0#camelCase Vanilla JavaScript for first upper case: Capitalize the first letter of all words in a string:  And then: then capitalize("hello") // Hello We could get the first character with one of my favorite RegExp, looks like a cute smiley: /^./ And for all coffee-junkies: ...and for all guys who think that there's a better way of doing this, without extending native prototypes: There is a very simple way to implement it by replace. For ECMAScript 6: Result: SHORTEST 3 solutions, 1 and 2 handle cases when s string  is "", null and undefined:   let s='foo bar';  console.log( s&&s[0].toUpperCase()+s.slice(1) );  console.log( s&&s.replace(/./,s[0].toUpperCase()) );  console.log( 'foo bar'.replace(/./,x=>x.toUpperCase()) );    Use:   var str = "ruby java";  console.log(str.charAt(0).toUpperCase() + str.substring(1));    It will output "Ruby java" to the console. If you use Underscore.js or Lodash, the underscore.string library provides string extensions, including capitalize: _.capitalize(string) Converts first letter of the string to uppercase. Example: Since there are numerous answers, but none in ES2015 that would solve original problem efficiently, I came up with the following:   It seems to be easier in CSS:  This is from CSS text-transform Property (at W3Schools). Here is a function called ucfirst() (short for "upper case first letter"): You can capitalise a string by calling ucfirst("some string") -- for example, It works by splitting the string into two pieces.  On the first line it pulls out firstLetter and then on the second line it capitalises firstLetter by calling firstLetter.toUpperCase() and joins it with the rest of the string, which is found by calling str.substr(1). You might think this would fail for an empty string, and indeed in a language like C you would have to cater for this. However in JavaScript, when you take a substring of an empty string, you just get an empty string back. If you are wanting to reformat all-caps text, you might want to modify the other examples as such:  This will ensure that the following text is changed: It's always better to handle these kinds of stuff using CSS first, in general, if you can solve something using CSS, go for that first, then try JavaScript to solve your problems, so in this case try using :first-letter in CSS and apply text-transform:capitalize;  So try creating a class for that, so you can use it globally, for example: .first-letter-uppercase and add something like below in your CSS: Also the alternative option is JavaScript, so the best gonna be something like this: and call it like: If you  want to reuse it over and over, it's better attach it to javascript native String, so something like below: and call it as below:  Usage: This is a text string => This Is A Text String  Only because this is really a one-liner I will include this answer. It's an ES6-based interpolated string one-liner. Check out this solution: (You may encapsulate it in a function or even add it to the String prototype if you use it frequently.) You can do it in one line like this I found this arrow function easiest. Replace matches the first letter character (\w) of your string and converts it to uppercase. Nothing fancier is necessary.
__label__algorithm __label__sorting __label__matching __label__language-agnostic Yesterday I was pairing the socks from the clean laundry and figured out the way I was doing it is not very efficient. I was doing a naive search — picking one sock and "iterating" the pile in order to find its pair. This requires iterating over n/2 * n/4 = n2/8 socks on average. As a computer scientist I was thinking what I could do? Sorting (according to size/color/...) of course came to mind to achieve an O(NlogN) solution. Hashing or other not-in-place solutions are not an option, because I am not able to duplicate my socks (though it could be nice if I could). So, the question is basically: Given a pile of n pairs of socks, containing 2n elements (assume each sock has exactly one matching pair), what is the best way to pair them up efficiently with up to logarithmic extra space? (I believe I can remember that amount of info if needed.) I will appreciate an answer that addresses the following aspects: Sorting solutions have been proposed, but sorting is a little too much: We don't need order; we just need equality groups. So hashing would be enough (and faster). This kind of recursive hash partitioning is actually being done by SQL Server when it needs to hash join or hash aggregate over huge data sets. It distributes its build input stream into many partitions which are independent. This scheme scales to arbitrary amounts of data and multiple CPUs linearly. You don't need recursive partitioning if you can find a distribution key (hash key) that provides enough buckets that each bucket is small enough to be processed very quickly. Unfortunately, I don't think socks have such a property. If each sock had an integer called "PairID" one could easily distribute them into 10 buckets according to PairID % 10 (the last digit). The best real-world partitioning I can think of is creating a rectangle of piles: one dimension is color, the other is the pattern. Why a rectangle? Because we need O(1) random-access to piles. (A 3D cuboid would also work, but that is not very practical.) Update: What about parallelism? Can multiple humans match the socks faster? What about the element distinctness problem? As the article states, the element distinctness problem can be solved in O(N). This is the same for the socks problem (also O(N), if you need only one distribution step (I proposed multiple steps only because humans are bad at calculations - one step is enough if you distribute on md5(color, length, pattern, ...), i.e. a perfect hash of all attributes)). Clearly, one cannot go faster than O(N), so we have reached the optimal lower bound. Although the outputs are not exactly the same (in one case, just a boolean. In the other case, the pairs of socks), the asymptotic complexities are the same. As the architecture of the human brain is completely different than a modern CPU, this question makes no practical sense. Humans can win over CPU algorithms using the fact that "finding a matching pair" can be one operation for a set that isn't too big. My algorithm: At least this is what I am using in real life, and I find it very efficient. The downside is it requires a flat surface, but it's usually abundant. Case 1: All socks are identical (this is what I do in real life by the way).  Pick any two of them to make a pair. Constant time. Case 2: There are a constant number of combinations (ownership, color, size, texture, etc.). Use radix sort. This is only linear time since comparison is not required. Case 3: The number of combinations is not known in advance (general case). We have to do comparison to check whether two socks come in pair. Pick one of the O(n log n) comparison-based sorting algorithms. However in real life when the number of socks is relatively small (constant), these theoretically optimal algorithms wouldn't work well. It might take even more time than sequential search, which theoretically requires quadratic time. Non-algorithmic answer, yet "efficient" when I do it: step 1) discard all your existing socks step 2) go to Walmart and buy them by packets of 10 - n packet of white and m packets of black. No need for other colors in everyday's life. Yet times to times, I have to do this again (lost socks, damaged socks, etc.), and I hate to discard perfectly good socks too often (and I wished they kept selling the same socks reference!), so I recently took a different approach. Algorithmic answer: Consider than if you draw only one sock for the second stack of socks, as you are doing, your odds of finding the matching sock in a naive search is quite low. Why five? Usually humans are good are remembering between five and seven different elements in the working memory - a bit like the human equivalent of a RPN stack - five is a safe default. Pick up one from the stack of 2n-5. Now look for a match (visual pattern matching - humans are good at that with a small stack) inside the five you drew, if you don't find one, then add that to your five. Keep randomly picking socks from the stack and compare to your 5+1 socks for a match. As your stack grows, it will reduce your performance but raise your odds. Much faster. Feel free to write down the formula to calculate how many samples you have to draw for a 50% odds of a match. IIRC it's an hypergeometric law. I do that every morning and rarely need more than three draws - but I have n similar pairs (around 10, give or take the lost ones) of m shaped white socks. Now you can estimate the size of my stack of stocks :-) BTW, I found that the sum of the transaction costs of sorting all the socks every time I needed a pair were far less than doing it once and binding the socks. A just-in-time works better because then you don't have to bind the socks, and there's also a diminishing marginal return (that is, you keep looking for that two or three socks that when somewhere in the laundry and that you need to finish matching your socks and you lose time on that). What I do is that I pick up the first sock and put it down (say, on the edge of the laundry bowl). Then I pick up another sock and check to see if it's the same as the first sock. If it is, I remove them both. If it's not, I put it down next to the first sock. Then I pick up the third sock and compare that to the first two (if they're still there). Etc. This approach can be fairly easily be implemented in an array, assuming that "removing" socks is an option. Actually, you don't even need to "remove" socks. If you don't need sorting of the socks (see below), then you can just move them around and end up with an array that has all the socks arranged in pairs in the array. Assuming that the only operation for socks is to compare for equality, this algorithm is basically still an n2 algorithm, though I don't know about the average case (never learned to calculate that). Sorting, of course improves efficiency, especially in real life where you can easily "insert" a sock between two other socks. In computing the same could be achieved by a tree, but that's extra space. And, of course, we're back at NlogN (or a bit more, if there are several socks that are the same by sorting criteria, but not from the same pair). Other than that, I cannot think of anything, but this method does seem to be pretty efficient in real life. :) This is asking the wrong question. The right question to ask is, why am I spending time sorting socks? How much does it cost on yearly basis, when you value your free time for X monetary units of your choice? And more often than not, this is not just any free time, it's morning free time, which you could be spending in bed, or sipping your coffee, or leaving a bit early and not being caught in the traffic. It's often good to take a step back, and think a way around the problem. And there is a way! Find a sock you like. Take all relevant features into account: colour in different lighting conditions, overall quality and durability, comfort in different climatic conditions, and odour absorption. Also important is, they should not lose elasticity in storage, so natural fabrics are good, and they should be available in a plastic wrapping. It's better if there's no difference between left and right foot socks, but it's not critical. If socks are left-right symmetrical, finding a pair is O(1) operation, and sorting the socks is approximate O(M) operation, where M is the number of places in your house, which you have littered with socks, ideally some small constant number. If you chose a fancy pair with different left and right sock, doing a full bucket sort to left and right foot buckets take O(N+M), where N is the number of socks and M is same as above. Somebody else can give the formula for average iterations of finding the first pair, but worst case for finding a pair with blind search is N/2+1, which becomes astronomically unlikely case for reasonable N. This can be sped up by using advanced image recognition algorithms and heuristics, when scanning the pile of unsorted socks with Mk1 Eyeball. So, an algorithm for achieving O(1) sock pairing efficiency (assuming symmetrical sock) is: You need to estimate how many pairs of socks you will need for the rest of your life, or perhaps until you retire and move to warmer climates with no need to wear socks ever again. If you are young, you could also estimate how long it takes before we'll all have sock-sorting robots in our homes, and the whole problem becomes irrelevant. You need to find out how you can order your selected sock in bulk, and how much it costs, and do they deliver. Order the socks! Get rid of your old socks. An alternative step 3 would involve comparing costs of buying the same amount of perhaps cheaper socks a few pairs at a time over the years and adding the cost of sorting socks, but take my word for it: buying in bulk is cheaper! Also, socks in storage increase in value at the rate of stock price inflation, which is more than you would get on many investments. Then again there is also storage cost, but socks really do not take much space on the top shelf of a closet. Problem solved. So, just get new socks, throw/donate your old ones away, and live happily ever after knowing you are saving money and time every day for the rest of your life. The theoretical limit is O(n) because you need to touch each sock (unless some are already paired somehow). You can achieve O(n) with radix sort. You just need to pick some attributes for the buckets.  If you can pick a limited number of attributes, but enough attributes that can uniquely identify each pair, you should be done in O(k * n), which is O(n) if we can consider k is limited.  As a practical solution: If you have 1000 socks, with 8 colors and an average distribution, you can make 4 piles of each 125 socks in c*n time. With a threshold of 5 socks you can sort every pile in 6 runs. (Counting 2 seconds to throw a sock on the right pile it will take you little under 4 hours.) If you have just 60 socks, 3 colors and 2 sort of socks (yours / your wife's) you can sort every pile of 10 socks in 1 runs (Again threshold = 5). (Counting 2 seconds it will take you 2 min). The initial bucket sorting will speed up your process, because it divides your n socks into k buckets in c*n time so than you will only have to do c*n*log(k) work. (Not taking into account the threshold). So all in all you do about n*c*(1 + log(k)) work, where c is the time to throw a sock on a pile. This approach will be favourable compared to any c*x*n + O(1) method roughly as long as log(k) < x - 1. In computer science this can be helpful: We have a collection of n things, an order on them (length) and also an equivalence relation (extra information, for example the color of socks). The equivalence relation allows us to make a partition of the original collection, and in every equivalence class our order is still maintained. The mapping of a thing to it's equivalence class can be done in O(1), so only O(n) is needed to assign each item to a class. Now we have used our extra information and can proceed in any manner to sort every class. The advantage is that the data sets are already significantly smaller. The method can also be nested, if we have multiple equivalence relations -> make colour piles, than within every pile partition on texture, than sort on length. Any equivalence relation that creates a partition with more than 2 elements that have about even size will bring a speed improvement over sorting (provided we can directly assign a sock to its pile), and the sorting can happen very quickly on smaller data sets. You are trying to solve the wrong problem. Solution 1: Each time you put dirty socks in your laundry basket, tie them in a little knot. That way you will not have to do any sorting after the washing. Think of it like registering an index in a Mongo database. A little work ahead for some CPU savings in the future. Solution 2: If it's winter, you don't have to wear matching socks. We are programmers. Nobody needs to know, as long as it works. Solution 3: Spread the work. You want to perform such a complex CPU process asynchronously, without blocking the UI. Take that pile of socks and stuff them in a bag. Only look for a pair when you need it. That way the amount of work it takes is much less noticeable. Hope this helps! This question is actually deeply philosophical. At heart it's about whether the power of people to solve problems (the "wetware" of our brains) is equivalent to what can be accomplished by algorithms. An obvious algorithm for sock sorting is: Now the computer science in this problem is all about the steps Human beings will use various strategies to effect these. Human memory is associative, something like a hash table where feature sets of stored values are paired with the corresponding values themselves. For example, the concept of "red car" maps to all the red cars a person is capable of remembering. Someone with a perfect memory has a perfect mapping.  Most people are imperfect in this regard (and most others).  The associative map has a limited capacity. Mappings may bleep out of existence under various circumstances (one beer too many), be recorded in error ("I though her name was Betty, not Nettie"), or never be overwritten even though we observe that the truth has changed ("dad's car" evokes "orange Firebird" when we actually knew he'd traded that in for the red Camaro). In the case of socks, perfect recall means looking at a sock s always produces the memory of its sibling t, including enough information (where it is on the ironing board) to locate t in constant time.  A person with photographic memory accomplishes both 1 and 2 in constant time without fail. Someone with less than perfect memory might use a few commonsense equivalence classes based on features within his capability to track: size (papa, mama, baby), color (greenish, redish, etc.), pattern (argyle, plain, etc.), style (footie, knee-high, etc.).  So the ironing board would be divided into sections for the categories. This usually allows the category to be located in constant time by memory, but then a linear search through the category "bucket" is needed.  Someone with no memory or imagination at all (sorry) will just keep the socks in one pile and do a linear search of the whole pile. A neat freak might use numeric labels for pairs as someone suggested.  This opens the door to a total ordering, which allows the human to use exactly the same algorithms we might with a CPU: binary search, trees, hashes, etc. So the "best" algorithm depends on the qualities of the wetware/hardware/software that is running it and our willingness to "cheat" by imposing a total order on pairs.  Certainly a "best" meta-algorithm is to hire the worlds best sock-sorter: a person or machine that can aquire and quickly store a huge set N of sock attribute sets in a 1-1 associative memory with constant time lookup, insert, and delete. Both people and machines like this can be procured. If you have one, you can pair all the socks in O(N) time for N pairs, which is optimal. The total order tags allow you to use standard hashing to get the same result with either a human or hardware computer. Cost: Moving socks -> high, finding/search socks in line -> small What we want to do is reduce the number of moves, and compensate with the number of searches. Also, we can utilize the multithreded environment of the Homo Sapiens to hold more things in the descision cache.  X = Yours, Y = Your spouses From pile A of all socks: Pick two socks, place corresponding X sock in X line, and Y sock in Y line at next available position. Do until A is empty. For each line X and Y Pick the first sock in line, search along the line until it finds the corresponding sock. Put into the corresponding finished line of socks.  Optionally to step one, you pick up two sock from that line instead of two, as the caching memory is large enough we can quickly identify if either sock matches the current one on the line you are observing. If you are fortunate enough to have three arms, you could possibly parse three socks at the same time given that the memory of the subject is large enough. Do until both X and Y is empty. Done However, as this have simillar complexity as selection sort, the time taken is far less due to the speeds of I/O(moving socks) and search(searching the line for a sock). Here's an Omega(n log n) lower bound in comparison based model. (The only valid operation is comparing two socks.) Suppose that you know that your 2n socks are arranged this way: p1 p2 p3 ... pn pf(1) pf(2) ... pf(n) where f is an unknown permutation of the set {1,2,...,n}. Knowing this cannot make the problem harder. There are n! possible outputs (matchings between first and second half), which means you need log(n!) = Omega(n log n) comparisons. This is obtainable by sorting. Since you are interested in connections to element distinctness problem: proving the Omega(n log n) bound for element distinctness is harder, because the output is binary yes/no. Here, the output has to be a matching and the number of possible outputs suffices to get a decent bound. However, there's a variant connected to element distinctness. Suppose you are given 2n socks and wonder if they can be uniquely paired. You can get a reduction from ED by sending (a1, a2, ..., an) to (a1, a1, a2, a2, ..., an, an). (Parenthetically, the proof of hardness of ED is very interesting, via topology.) I think that there should be an Omega(n2) bound for the original problem if you allow equality tests only. My intuition is: Consider a graph where you add an edge after a test, and argue that if the graph is not dense the output is not uniquely determined. This is how I actually do it, for p pairs of socks (n = 2p individual socks): The worst-case scenario of this scheme is that every pair of socks is different enough that it must be matched exactly, and that the first n/2 socks you pick are all different. This is your O(n2) scenario, and it's extremely unlikely. If the number of unique types of sock t is less than the number of pairs p = n/2, and the socks in each type are alike enough (usually in wear-related terms) that any sock of that type can be paired with any other, then as I inferred above, the maximum number of socks you will ever have to compare to is t, after which the next one you pull will match one of the unpaired socks. This scenario is much more likely in the average sock drawer than the worst-case, and reduces the worst-case complexity to O(n*t) where usually t << n. Real-world approach: As rapidly as possible, remove socks from the unsorted pile one at a time and place in piles in front of you. The piles should be arranged somewhat space-efficiently, with all socks pointing the same direction; the number of piles is limited by the distance you can easily reach. The selection of a pile on which to put a sock should be -- as rapidly as possible -- by putting a sock on a pile of apparently like socks; the occasional type I (putting a sock on a pile it doesn't belong to) or type II (putting a sock in its own pile when there's an existing pile of like socks) error can be tolerated -- the most important consideration is speed. Once all the socks are in piles, rapidly go through the multi-sock piles creating pairs and removing them (these are heading for the drawer). If there are non-matching socks in the pile, re-pile them to their best (within the as-fast-as-possible constraint) pile. When all the multi-sock piles have been processed, match up remaining pairable socks that weren't paired due to type II errors. Whoosh, you're done -- and I have a lot of socks and don't wash them until a large fraction are dirty. Another practical note: I flip the top of one of a pair of socks down over the other, taking advantage of their elastic properties, so they stay together while being transported to the drawer and while in the drawer. From your question it is clear you don't have much actual experience with laundry :). You need an algorithm that works well with a small number of non-pairable socks. The answers till now don't make good use of our human pattern recognition capabilities. The game of Set provides a clue of how to do this well: put all socks in a two-dimensional space so you can both recognize them well and easily reach them with your hands. This limits you to an area of about 120 * 80 cm or so. From there select the pairs you recognize and remove them. Put extra socks in the free space and repeat. If you wash for people with easily recognizable socks (small kids come to mind), you can do a radix sort by selecting those socks first. This algorithm works well only when the  number of single socks is low Pick up a first sock and place it on a table. Now pick another sock; if it matches the first picked, place it on top of the first. If not, place it on the table a small distance from the first. Pick a third sock; if it matches either of the previous two, place it on top of them or else place it a small distance from the third. Repeat until you have picked up all the socks. In order to say how efficient it is to pair socks from a pile, we have to define the machine first, because the pairing isn't done whether by a turing nor by a random access machine, which are normally used as the basis for an algorithmic analysis. The machine is an abstraction of a the real world element called human being. It is able to read from the environment via a pair of eyes. And our machine model is able to manipulate the environment by using 2 arms. Logical and arithmetic operations are calculated using our brain (hopefully ;-)). We also have to consider the intrinsic runtime of the atomic operations that can be carried out with these instruments. Due to physical constraints, operations which are carried out by an arm or eye have non constant time complexity. This is because we can't move an endlessly large pile of socks with an arm nor can an eye see the top sock on an endlessly large pile of socks. However mechanical physics give us some goodies as well. We are not limited to move at most one sock with an arm. We can move a whole couple of them at once. So depending on the previous analysis following operations should be used in descending order: We can also make use of the fact that people only have a very limited amount of socks. So an environmental modification can involve all socks in the pile. So here is my suggestion: Operation 4 is necessary, because when spreading socks over the floor some socks may hide others. Here is the analysis of the algorithm: The algorithm terminates with high probability. This is due to the fact that one is unable to find pairs of socks in step number 2. For the following runtime analysis of pairing n pairs of socks, we suppose that at least half of the 2n socks aren't hidden after step 1. So in the average case we can find n/2 pairs. This means that the loop is step 4 is executed O(log n) times. Step 2 is executed O(n^2) times. So we can conclude: So we have a total runtime complexity of O(r*n^2 + w*(ln n + n)) where r and w are the factors for environmental read and environmental write operations respectively for a reasonable amount of socks. The cost of the logical and arithmetical operations are omitted, because we suppose that it takes a constant amount of logical and arithmetical operations to decide whether 2 socks belong to the same pair. This may not be feasible in every scenario.  I came out with another solution which would not promise fewer operations, neither less time consumption, but it should be tried to see if it can be a good-enough heuristic to provide less time consumption in huge series of sock pairing. Preconditions: There is no guarantee that there are the same socks. If they are of the same color it doesn't mean they have the same size or pattern. Socks are randomly shuffled. There can be odd number of socks (some are missing, we don't know how many). Prepare to remember a variable "index" and set it to 0. The result will have one or two piles: 1. "matched" and 2. "missing" Heuristic: Also, there could be added check for damaged socks also, as if the removal of those. It could be inserted between 2 and 3, and between 13 and 14. I'm looking forward to hear about any experiences or corrections. When I sort socks, I do an approximate radix sort, dropping socks near other socks of the same colour/pattern type. Except in the case when I can see an exact match at/near the location I'm about to drop the sock I extract the pair at that point. Almost all the other algorithms (including the top scoring answer by usr) sort, then remove pairs. I find that, as a human, it is better to minimize the number of socks being considered at one time.  I do this by: This takes advantage of the human ability to fuzzy-match in O(1) time, which is somewhat equivalent to the establishment of a hash-map on a computing device. By pulling the distinctive socks first, you leave space to "zoom" in on the features which are less distinctive, to begin with. After eliminating the fluro coloured, the socks with stripes, and the three pairs of long socks, you might end up with mostly white socks roughly sorted by how worn they are. At some point, the differences between socks are small enough that other people won't notice the difference, and any further matching effort is not needed. Whenever you pick up a sock, put it in one place.  Then the next sock you pick up, if it doesn't match the first sock, set it beside the first one.  If it does, there's a pair.  This way it doesn't really matter how many combinations there are, and there are only two possibilities for each sock you pick up -- either it has a match that's already in your array of socks, or it doesn't, which means you add it to a place in the array. This also means that you will almost certainly never have all your socks in the array, because socks will get removed as they're matched. Consider a hash-table of  size  'N'. If we assume normal distribution, then the estimated number of 'insertions'  to have atleast one sock mapped to one bucket is NlogN  (ie, all buckets are full)  I had derived this as a part of another puzzle,but I would be happy to be proven wrong.  Here's my blog article on the same  Let 'N' correspond to an approximate upper-bound on the number of  number of unique colors/pattern of socks that you have. Once you have a collision(a.k.a  : a match) simply remove that pair of socks.   Repeat the same experiment with the next batch of NlogN socks.  The beauty of it is that you could be making NlogN parallel comparisons(collision-resolution) because of the way the human mind works.  :-) Socks, whether real ones or some analogous data structure, would be supplied in pairs.  The simplest answer is prior to allowing the pair to be separated, a single data structure for the pair should have been initialized that contained a pointer to the left and right sock, thus enabling socks to be referred to directly or via their pair. A sock may also be extended to contain a pointer to its partner. This solves any computational pairing problem by removing it with a layer of abstraction. Applying the same idea to the practical problem of pairing socks, the apparent answer is: don't allow your socks to ever be unpaired. Socks are provided as a pair, put in the drawer as a pair (perhaps by balling them together), worn as a pair. But the point where unpairing is possible is in the washer, so all that's required is a physical mechanism that allows the socks to stay together and be washed efficiently.  There are two physical possibilities: For a 'pair' object that keeps a pointer to each sock we could have a cloth bag that we use to keep the socks together. This seems like massive overhead. But for each sock to keep a reference to the other, there is a neat solution: a popper (or a 'snap button' if you're American), such as these: http://www.aliexpress.com/compare/compare-invisible-snap-buttons.html Then all you do is snap your socks together right after you take them off and put them in your washing basket, and again you've removed the problem of needing to pair your socks with a physical abstraction of the 'pair' concept. If the "move" operation is fairly expensive, and the "compare" operation is cheap, and you need to move the whole set anyway, into a buffer where search is much faster than in original storage... just integrate sorting into the obligatory move. I found integrating the process of sorting into hanging to dry makes it a breeze. I need to pick up each sock anyway, and hang it (move) and it costs me about nothing to hang it in a specific place on the strings. Now just not to force search of the whole buffer (the strings) I choose to place socks by color/shade. Darker left, brighter right, more colorful front etc. Now before I hang each sock, I look in its "right vicinity" if a matching one is there already - this limits "scan" to 2-3 other socks - and if it is, I hang the other one right next to it. Then I roll them into pairs while removing from the strings, when dry. Now this may not seem all that different from "forming piles by color" suggested by top answers but first, by not picking discrete piles but ranges, I have no problem classifying whether "purple" goes to "red" or "blue" pile; it just goes between. And then by integrating two operations (hang to dry and sort) the overhead of sorting while hanging is like 10% of what separate sorting would be. I've finished pairing my socks just right now, and I found that the best way to do it is the following: In the worst case it means that you will have n/2 different buckets, and you will have n-2 determinations about that which bucket contains the pair of the current sock. Obviously, this algorithm works well if you have just a few pairs; I did it with 12 pairs. It is not so scientific, but it works well:) My solution does not exactly correspond to your requirements, as it formally requires O(n) "extra" space. However, considering my conditions it is very efficient in my practical application. Thus I think it should be interesting. The special condition in my case is that I don't use drying machine, just hang my cloths on an ordinary cloth dryer. Hanging cloths requires O(n) operations (by the way, I always consider bin packing problem here) and the problem by its nature requires the linear "extra" space. When I take a new sock from the bucket I to try hang it next to its pair if the pair is already hung. If its a sock from a new pair I leave some space next to it.  It obviously requires some extra work to check if there is the matching sock already hanging somewhere and it would render solution O(n^2) with coefficient about 1/2 for a computer. But in this case the "human factor" is actually an advantage -- I usually can very quickly (almost O(1)) identify the matching sock if it was already hung (probably some imperceptible in-brain caching is involved) -- consider it a kind of limited "oracle" as in Oracle Machine ;-) We, the humans have these advantages over digital machines in some cases ;-) Thus connecting the problem of pairing socks with the problem of hanging cloths I get O(n) "extra space" for free, and have a solution that is about O(n) in time, requires just a little more work than simple hanging cloths and allows to immediately access complete pair of socks even in a very bad Monday morning... ;-) I hope I can contribute something new to this problem. I noticed that all of the answers neglect the fact that there are two points where you can perform preprocessing, without slowing down your overall laundry performance. Also, we don't need to assume a large number of socks, even for large families. Socks are taken out of the drawer and are worn, and then they are tossed in a place (maybe a bin) where they stay before being laundered. While I wouldn't call said bin a LIFO-Stack, I'd say it is safe to assume that  Since all washing machines I know about are limited in size (regardless of how many socks you have to wash), and the actual randomizing occurs in the washing machine, no matter how many socks we have, we always have small subsets which contain almost no singletons.  Our two preprocessing stages are "putting the socks on the clothesline" and "Taking the socks from the clothesline", which we have to do, in order to get socks which are not only clean but also dry. As with washing machines, clotheslines are finite, and I assume that we have the whole part of the line where we put our socks in sight. Here's the algorithm for put_socks_on_line(): Don't waste your time moving socks around or looking for the best match, this all should be done in O(n), which we would also need for just putting them on the line unsorted. The socks aren't paired yet, we only have several similarity clusters on the line. It's helpful that we have a limited set of socks here, as this helps us to create "good" clusters (for example, if there are only black socks in the set of socks, clustering by colours would not be the way to go) Here's the algorithm for take_socks_from_line(): I should point out that in order to improve the speed of the remaining steps, it is wise not to randomly pick the next sock, but to sequentially take sock after sock from each cluster. Both preprocessing steps don't take more time than just putting the socks on the line or in the basket, which we have to do no matter what, so this should greatly enhance the laundry performance. After this, it's easy to do the hash partitioning algorithm. Usually, about 75% of the socks are already paired, leaving me with a very small subset of socks, and this subset is already (somewhat) clustered (I don't introduce much entropy into my basket after the preprocessing steps). Another thing is that the remaining clusters tend to be small enough to be handled at once, so it is possible to take a whole cluster out of the basket. Here's the algorithm for sort_remaining_clusters(): After that, there are only a few socks left. This is where I introduce previously unpaired socks into the system and process the remaining socks without any special algorithm - the remaining socks are very few and can be processed visually very fast. For all remaining socks, I assume that their counterparts are still unwashed and put them away for the next iteration. If you register a growth of unpaired socks over time (a "sock leak"), you should check your bin - it might get randomized (do you have cats which sleep in there?) I know that these algorithms take a lot of assumptions: a bin which acts as some sort of LIFO stack, a limited, normal washing machine, and a limited, normal clothesline - but this still works with very large numbers of socks. About parallelism: As long as you toss both socks into the same bin, you can easily parallelize all of those steps. I have taken simple steps to reduce my effort into a process taking O(1) time. By reducing my inputs to one of two types of socks (white socks for recreation, black socks for work), I only need to determine which of two socks I have in hand. (Technically, since they are never washed together, I have reduced the process to O(0) time.) Some upfront effort is required to find desirable socks, and to purchase in sufficient quantity as to eliminate need for your existing socks. As I'd done this before my need for black socks, my effort was minimal, but mileage may vary. Such an upfront effort has been seen many times in very popular and effective code. Examples include #DEFINE'ing pi to several decimals (other examples exist, but that's the one that comes to mind right now). Create a hash table which will be used for unmatched socks, using the pattern as the hash. Iterate over the socks one by one. If the sock has a pattern match in the hash table, take the sock out of the table and make a pair. If the sock does not have a match, put it into the table. The problem of sorting your n pairs of socks is O(n). Before you throw them in the laundry basket, you thread the left one to the right one. On taking them out, you cut the thread and put each pair into your drawer - 2 operations on n pairs, so O(n). Now the next question is simply whether you do your own laundry and your wife does hers. That is a problem likely in an entirely different domain of problems. :)
__label__android __label__android-edittext __label__android-input-method __label__android-softkeyboard __label__soft-keyboard I have an EditText and a Button in my layout. After writing in the edit field and clicking on the Button, I want to hide the virtual keyboard when touching outside the keyboard. I assume that this is a simple piece of code, but where can I find an example of it? To help clarify this madness, I'd like to begin by apologizing on behalf of all Android users for Google's downright ridiculous treatment of the soft keyboard. The reason there are so many answers, each different, for the same simple question is that this API, like many others in Android, is horribly designed. I can think of no polite way to state it. I want to hide the keyboard.  I expect to provide Android with the following statement: Keyboard.hide(). The end. Thank you very much.  But Android has a problem.  You must use the InputMethodManager to hide the keyboard. OK, fine, this is Android's API to the keyboard. BUT! You are required to have a Context in order to get access to the IMM. Now we have a problem.  I may want to hide the keyboard from a static or utility class that has no use or need for any Context. or And FAR worse, the IMM requires that you specify what View (or even worse, what Window) you want to hide the keyboard FROM. This is what makes hiding the keyboard so challenging. Dear Google: When I'm looking up the recipe for a cake, there is no RecipeProvider on Earth that would refuse to provide me with the recipe unless I first answer WHO the cake will be eaten by AND where it will be eaten!! This sad story ends with the ugly truth: to hide the Android keyboard, you will be required to provide 2 forms of identification: a Context and either a View or a Window. I have created a static utility method that can do the job VERY solidly, provided you call it from an Activity. Be aware that this utility method ONLY works when called from an Activity! The above method calls getCurrentFocus of the target Activity to fetch the proper window token. But suppose you want to hide the keyboard from an EditText hosted in a DialogFragment? You can't use the method above for that: This won't work because you'll be passing a reference to the Fragment's host Activity, which will have no focused control while the Fragment is shown! Wow! So, for hiding the keyboard from fragments, I resort to the lower-level, more common, and uglier: Below is some additional information gleaned from more time wasted chasing this solution: About windowSoftInputMode There's yet another point of contention to be aware of. By default, Android will automatically assign initial focus to the first EditText or focusable control in your Activity.  It naturally follows that the InputMethod (typically the soft keyboard) will respond to the focus event by showing itself. The windowSoftInputMode attribute in AndroidManifest.xml, when set to stateAlwaysHidden, instructs the keyboard to ignore this automatically-assigned initial focus. Almost unbelievably, it appears to do nothing to prevent the keyboard from opening when you touch the control (unless focusable="false" and/or focusableInTouchMode="false" are assigned to the control). Apparently, the windowSoftInputMode setting applies only to automatic focus events, not to focus events triggered by touch events. Therefore, stateAlwaysHidden is VERY poorly named indeed. It should perhaps be called ignoreInitialFocus instead. Hope this helps. UPDATE: More ways to get a window token If there is no focused view (e.g. can happen if you just changed fragments), there are other views that will supply a useful window token. These are alternatives for the above code if (view == null)   view = new View(activity);  These don't refer explicitly to your activity. Inside a fragment class: Given a fragment fragment as a parameter: Starting from your content body: UPDATE 2: Clear focus to avoid showing keyboard again if you open the app from the background Add this line to the end of the method: view.clearFocus(); You can force Android to hide the virtual keyboard using the InputMethodManager, calling hideSoftInputFromWindow, passing in the token of the window containing your focused view. This will force the keyboard to be hidden in all situations. In some cases, you will want to pass in InputMethodManager.HIDE_IMPLICIT_ONLY as the second parameter to ensure you only hide the keyboard when the user didn't explicitly force it to appear (by holding down the menu). Note: If you want to do this in Kotlin, use: context?.getSystemService(Context.INPUT_METHOD_SERVICE) as InputMethodManager Kotlin Syntax Also useful for hiding the soft-keyboard is: This can be used to suppress the soft-keyboard until the user actually touches the editText View. I got one more solution to hide keyboard: Here pass HIDE_IMPLICIT_ONLY at the position of showFlag and 0 at the position of hiddenFlag. It will forcefully close soft Keyboard. Meier's solution works for me too. In my case, the top level of my App is a tab host and I want to hide the keyword when switching tabs - I get the window token from the tab host View. Please try this below code in  onCreate() Update: I don't know why this solution is not work any more ( I just tested on Android 23). Please use the solution of Saurabh Pareek instead. Here it is: Old answer:  If all the other answers here don't work for you as you would like them to, there's another way of manually controlling the keyboard. Create a function with that will manage some of the EditText's properties: Then, make sure that onFocus of the EditText you open/close the keyboard: Now, whenever you want to open the keyboard manually call:  And for closing call: Saurabh Pareek has the best answer so far. Might as well use the correct flags, though. Example of real use from so searching, here I found an answer that works for me In your OnClick listener call the onEditorAction of the EditText with IME_ACTION_DONE I feel this method is better, simpler and more aligned with Android's design pattern. In the simple example above (and usually in most of the common cases) you'll have an EditText that has/had focus and it also usually was the one to invoke the keyboard in the first place (it is definitely able to invoke it in many common scenarios). In that same way, it should be the one to release the keyboard, usually that can be done by an ImeAction. Just see how an EditText with android:imeOptions="actionDone" behaves, you want to achieve the same behavior by the same means. Check this related answer This should work: I'm using a custom keyboard to input an Hex number so I can't have the IMM keyboard show up... In v3.2.4_r1 setSoftInputShownOnFocus(boolean show) was added to control weather or not to display the keyboard when a TextView gets focus, but its still hidden so reflection must be used: For older versions, I got very good results (but far from perfect) with a OnGlobalLayoutListener, added with the aid of a ViewTreeObserver from my root view and then checking if the keyboard is shown like this: This last solution may show the keyboard for a split second and messes with the selection handles. When in the keyboard enters full screen, onGlobalLayout isn't called. To avoid that, use TextView#setImeOptions(int) or in the TextView XML declaration: Update: Just found what dialogs use to never show the keyboard and works in all versions:  I have spent more than two days working through all of the solutions posted in the thread and have found them lacking in one way or another.  My exact requirement is to have a button that will with 100% reliability show or hide the on screen keyboard.  When the keyboard is in its hidden state is should not re-appear, no matter what input fields the user clicks on.  When it is in its visible state the keyboard should not disappear no matter what buttons the user clicks.  This needs to work on Android 2.2+ all the way up to the latest devices. You can see a working implementation of this in my app clean RPN. After testing many of the suggested answers on a number of different phones (including froyo and gingerbread devices) it became apparent that android apps can reliably: For me, temporarily hiding the keyboard is not enough.  On some devices it will re-appear as soon as a new text field is focused.  As my app uses multiple text fields on one page, focusing a new text field will cause the hidden keyboard to pop back up again. Unfortunately item 2 and 3 on the list only work reliability when an activity is being started.  Once the activity has become visible you cannot permanently hide or show the keyboard.  The trick is to actually restart your activity when the user presses the keyboard toggle button.  In my app when the user presses on the toggle keyboard button, the following code runs: This causes the current activity to have its state saved into a Bundle, and then the activity is started, passing through an boolean which indicates if the keyboard should be shown or hidden. Inside the onCreate method the following code is run: If the soft keyboard should be shown, then the InputMethodManager is told to show the keyboard and the window is instructed to make the soft input always visible. If the soft keyboard should be hidden then the WindowManager.LayoutParams.FLAG_ALT_FOCUSABLE_IM is set. This approach works reliably on all devices I have tested on - from a 4 year old HTC phone running android 2.2 up to a nexus 7 running 4.2.2.  The only disadvantage with this approach is you need to be careful with handling the back button.  As my app essentially only has one screen (its a calculator) I can override onBackPressed() and return to the devices home screen. Alternatively to this all around solution, if you wanted to close the soft keyboard from anywhere without having a reference to the (EditText) field that was used to open the keyboard, but still wanted to do it if the field was focused, you could use this (from an Activity): Thanks to this SO answer, I derived the following which, in my case, works nicely when scrolling through the the fragments of a ViewPager... Above answers work for different scenario's but  If you want to hide the keyboard inside a view and struggling to get the right context try this: and to get the context fetch it from constructor:)  If you want to close the soft keyboard during a unit or functional test, you can do so by clicking the "back button" from your test: I put "back button" in quotes, since the above doesn't trigger the onBackPressed() for the Activity in question. It just closes the keyboard. Make sure to pause for a little while before moving on, since it takes a little while to close the back button, so subsequent clicks to Views, etc., won't be registered until after a short pause (1 second is long enough ime). Here's how you do it in Mono for Android (AKA MonoDroid) This worked for me for all the bizarre keyboard behavior Simple and Easy to use method, just call hideKeyboardFrom(YourActivity.this); to hide keyboard Add to your activity    android:windowSoftInputMode="stateHidden" in Manifest file. Example: For Open Keyboard : For Close/Hide Keyboard : Just use this optimized code in your activity: I have the case, where my EditText can be located also in an AlertDialog, so the keyboard should be closed on dismiss. The following code seems to be working anywhere: Works like magic touch every time after that call on onTouchListener: use this
__label__git __label__git-squash __label__squash __label__rebase How can I squash my last X commits together into one commit using Git? Use git rebase -i <after-this-commit> and replace "pick" on the second and subsequent commits with "squash" or "fixup", as described in the manual. In this example, <after-this-commit> is either the SHA1 hash or the relative location from the HEAD of the current branch from which commits are analyzed for the rebase command. For example, if the user wishes to view 5 commits from the current HEAD in the past the command is git rebase -i HEAD~5.  You can do this fairly easily without git rebase or git merge --squash. In this example, we'll squash the last 3 commits. If you want to write the new commit message from scratch, this suffices: If you want to start editing the new commit message with a concatenation of the existing commit messages (i.e. similar to what a pick/squash/squash/…/squash git rebase -i instruction list would start you with), then you need to extract those messages and pass them to git commit: Both of those methods squash the last three commits into a single new commit in the same way. The soft reset just re-points HEAD to the last commit that you do not want to squash. Neither the index nor the working tree are touched by the soft reset, leaving the index in the desired state for your new commit (i.e. it already has all the changes from the commits that you are about to “throw away”). You can use git merge --squash for this, which is slightly more elegant than git rebase -i.  Suppose you're on master and you want to squash the last 12 commits into one. WARNING: First make sure you commit your work—check that git status is clean (since git reset --hard will throw away staged and unstaged changes) Then: The documentation for git merge describes the --squash option in more detail. Update: the only real advantage of this method over the simpler git reset --soft HEAD~12 && git commit suggested by Chris Johnsen in his answer is that you get the commit message prepopulated with every commit message that you're squashing. I recommend avoiding git reset when possible -- especially for Git-novices.  Unless you really need to automate a process based on a number of commits, there is a less exotic way... The commit message will be prepopulated based on the squash. Thanks to this handy blog post I found that you can use this command to squash the last 3 commits: This is handy as it works even when you are on a local branch with no tracking information/remote repo. The command will open the interactive rebase editor which then allows you to reorder, squash, reword, etc as per normal. Using the interactive rebase editor: The interactive rebase editor shows the last three commits.  This constraint was determined by HEAD~3 when running the command git rebase -i HEAD~3. The most recent commit, HEAD, is displayed first on line 1.  The lines starting with a # are comments/documentation. The documentation displayed is pretty clear.  On any given line you can change the command from pick to a command of your choice. I prefer to use the command fixup as this "squashes" the commit's changes into the commit on the line above and discards the commit's message.   As the commit on line 1 is HEAD, in most cases you would leave this as pick.   You cannot use squash or fixup as there is no other commit to squash the commit into. You may also change the order of the commits. This allows you to squash or fixup commits that are not adjacent chronologically.  A practical everyday example I've recently committed a new feature.  Since then, I have committed two bug fixes.  But now I have discovered a bug (or maybe just a spelling error) in the new feature I committed.  How annoying!  I don't want a new commit polluting my commit history! The first thing I do is fix the mistake and make a new commit with the comment squash this into my new feature!. I then run git log or gitk and get the commit SHA of the new feature (in this case 1ff9460). Next, I bring up the interactive rebase editor with git rebase -i 1ff9460~.  The ~ after the commit SHA tells the editor to include that commit in the editor. Next, I move the commit containing the fix (fe7f1e0) to underneath the feature commit, and change pick to fixup. When closing the editor, the fix will get squashed into the feature commit and my commit history will look nice and clean! This works well when all the commits are local, but if you try to change any commits already pushed to the remote you can really cause problems for other devs that have checked out the same branch!  Based on Chris Johnsen's answer, Add a global "squash" alias from bash: (or Git Bash on Windows) ... or using Windows' Command Prompt:  Your ~/.gitconfig should now contain this alias:  Usage: ... Which automatically squashes together the last N commits, inclusive. Note: The resultant commit message is a combination of all the squashed commits, in order. If you are unhappy with that, you can always git commit --amend to modify it manually. (Or, edit the alias to match your tastes.) 2020 Simple solution without rebase : git reset --soft HEAD~2  git commit -m "new commit message" git push -f 2 means the last two commits will be squashed. You can replace it by any number To do this you can use following git command. n(=4 here) is the number of last commit. Then you got following options, Update like below pick one commit and squash the others into the most recent, For details click on the Link If you use TortoiseGit, you can the function Combine to one commit:  This function automatically executes all necessary single git steps. Unfortunatly only available for Windows. Based on this article I found this method easier for my usecase. My 'dev' branch was ahead of 'origin/dev' by 96 commits (so these commits were not pushed to the remote yet). I wanted to squash these commits into one before pushing the change. I prefere to reset the branch to the state of 'origin/dev' (this will leave all changes from the 96 commits unstaged) and then commit the changes at once: In the branch you would like to combine the commits on, run: example: This will open the text editor and you must switch the 'pick' in front of each commit with 'squash' if you would like these commits to be merged together. From documentation: For example, if you are looking to merge all the commits into one, the 'pick' is the first commit you made and all future ones (placed below the first) should be set to 'squash'. If using vim, use :x in insert mode to save and exit the editor. Then to continue the rebase: For more on this and other ways to rewrite your commit history see this helpful post 1) Identify the commit short hash Here even git log --oneline also can be used to get short hash. 2) If you want to squash (merge) last two commit 3) This opens up a nano editor for merging. And it looks like below 4) Rename the word pick to squash which is present before abcd1234. After rename it should be like below. 5) Now save and close the nano editor. Press ctrl + o and press Enter to save. And then press ctrl + x to exit the editor. 6) Then nano editor again opens for updating comments, if necessary update it. 7) Now its squashed successfully, you can verify it by checking logs. 8) Now push to repo. Note to add + sign before the branch name. This means forced push. Note : This is based on using git on ubuntu shell. If you are using different os (Windows or Mac) then above commands are same except editor. You might get different editor. Now this creates a new commit on top of HEAD with fixup1 <OLDCOMMIT_MSG>. Here ^ means the previous commit to OLDCOMMIT. This rebase command opens interactive window on a editor (vim or nano) on that we no need to do anything just save and exiting is sufficient. Because the option passed to this will automatically move the latest commit to next to old commit and change the operation to fixup (equivalent to squash). Then rebase continues and finishes. Here --amend merges the new changes to last commit cdababcd and generates new commit ID 1d4ab2e1 Anomies answer is good, but I felt insecure about this so I decided to add a couple of screenshots. See where you are with git log. Most important, find the commit hash of the first commit you don't want to squash. So only the :  Execute git rebase -i [your hash], in my case: In my case, I want to squash everything on the commit that was first in time. The ordering is from first to last, so exactly the other way as in git log. In my case, I want:  If you have picked only one commit and squashed the rest, you can adjust one commit message:  That's it. Once you save this (:wq), you're done. Have a look at it with git log. If for example, you want to squash the last 3 commits to a single commit in a branch (remote repository) in for example: https://bitbucket.org What I did is If you are on a remote branch(called feature-branch) cloned from a Golden Repository(golden_repo_name), then here's the technique to squash your commits into one: Checkout the golden repo Create a new branch from it(golden repo) as follows Squash merge with your local branch that you have already Commit your changes (this will be the only commit that goes in dev-branch) Push the branch to your local repository To squash the last 10 commits into 1 single commit: If you also want to update the remote branch with the squashed commit: I think the easiest way to do this is by making a new branch based on master and doing a merge --squash of the feature branch. Then you have all of the changes ready to commit. What can be really convenient: Find the commit hash you want to squash on top of, say d43e15. Now use This is super-duper kludgy, but in a kind of cool way, so I'll just toss it into the ring: Translation: provide a new "editor" for git which, if the filename to be edited is git-rebase-todo (the interactive rebase prompt) changes all but the first "pick" to "squash", and otherwise spawns vim - so that when you're prompted to edit the squashed commit message, you get vim. (And obviously I was squashing the last five commits on branch foo, but you could change that however you like.) I'd probably do what Mark Longair suggested, though. If you want to squish every commit into a single commit (e.g. when releasing a project publicly for the first time), try: Simple one-liner that always works, given that you are currently on the branch you want to squash, master is the branch it originated from, and the latest commit contains the commit message and author you wish to use: In this very abbreviated history of the https://github.com/fleetwood-mac/band-history repository you have opened a pull request to merge in the the Bill Clinton commit into the original (MASTER) Fleetwood Mac commit. You opened a pull request and on GitHub you see this: Four commits: Thinking that nobody would ever care to read the full repository history. (There actually is a repository, click the link above!) You decide to squash these commits. So you go and run git reset --soft HEAD~4 && git commit. Then you git push --force it onto GitHub to clean up your PR. And what happens? You just made single commit that get from Fritz to Bill Clinton. Because you forgot that yesterday you were working on the Buckingham Nicks version of this project. And git log doesn't match what you see on GitHub. If you don't care about the commit messages of the in-between commits, you can use  If you're working with GitLab, you can just click the Squash option in the Merge Request as shown below. The commit message will be the title of the Merge Request.  I find a more generic solution is not to specify 'N' commits, but rather the branch/commit-id you want to squash on top of.  This is less error-prone than counting the commits up to a specific commit—just specify the tag directly, or if you really want to count you can specify HEAD~N. In my workflow, I start a branch, and my first commit on that branch summarizes the goal (i.e. it's usually what I will push as the 'final' message for the feature to the public repository.)  So when I'm done, all I want to do is git squash master back to the first message and then I'm ready to push. I use the alias: This will dump the history being squashed before it does so—this gives you a chance to recover by grabbing an old commit ID off the console if you want to revert.  (Solaris users note it uses the GNU sed -i option, Mac and Linux users should be fine with this.) where the number of ^'s is X (in this case, squash the two last commits) In addition to other excellent answers, I'd like to add how git rebase -i always confuses me with the commit order - older to newer one or vice versa? So this is my workflow: Sources & additional reads: #1, #2. What about an answer for the question related to a workflow like this? I haven't seen a workflow like that on this page. (That may be my eyes.) If I understand rebase correctly, multiple merges would require multiple conflict resolutions. I do NOT want even to think about that! So, this seems to work for us. In question it could be ambiguous what is meant by "last".  for example git log --graph outputs the following (simplified): Then last commits by time are H0, merge, B0. To squash them you will have to rebase your merged branch on commit H1. The problem is that H0 contains H1 and H2 (and generally more commits before merge and after branching) while B0 don't. So you have to manage changes from H0, merge, H1, H2, B0 at least. It's possible to use rebase but in different manner then in others mentioned answers: rebase -i HEAD~2 This will show you choice options (as mentioned in other answers): Put squash instead of pick to H0: After save and exit rebase will apply commits in turn after H1. That means that it will ask you to resolve conflicts again (where HEAD will be H1 at first and then accumulating commits as they are applied). After rebase will finish you can choose message for squashed H0 and B0: P.S. If you just do some reset to BO:  (for example, using reset --mixed that is explained in more detail here https://stackoverflow.com/a/18690845/2405850): then you squash into B0 changes of H0, H1, H2 (losing completely commits for changes after branching and before merge. How can I squash my last X commits together into one commit using Git? The following content will be shown: For the commits that you want to squash, replace pick with fixup, so it becomes: If it's open in vim (default interface within terminal), then press Esc on your keyboard, type :wq and Enter to save the file. Verify: Check git log
__label__loops __label__list __label__python How do I access the index in a for loop like the following? I want to get this output: When I loop through it using a for loop, how do I access the loop index, from 1 to 5 in this case? Using an additional state variable, such as an index variable (which you would normally use in languages such as C or PHP), is considered non-pythonic. The better option is to use the built-in function enumerate(), available in both Python 2 and 3: Check out PEP 279 for more. Use enumerate to get the index with the element as you iterate: And note that Python's indexes start at zero, so you would get 0 to 4 with the above. If you want the count, 1 to 5, do this: What you are asking for is the Pythonic equivalent of the following, which is the algorithm most programmers of lower-level languages would use: Or in languages that do not have a for-each loop: or sometimes more commonly (but unidiomatically) found in Python: Python's enumerate function reduces the visual clutter by hiding the accounting for the indexes, and encapsulating the iterable into another iterable (an enumerate object) that yields a two-item tuple of the index and the item that the original iterable would provide. That looks like this: This code sample is fairly well the canonical example of the difference between code that is idiomatic of Python and code that is not. Idiomatic code is sophisticated (but not complicated) Python, written in the way that it was intended to be used. Idiomatic code is expected by the designers of the language, which means that usually this code is not just more readable, but also more efficient. Even if you don't need indexes as you go, but you need a count of the iterations (sometimes desirable) you can start with 1 and the final number will be your count. The count seems to be more what you intend to ask for (as opposed to index) when you said you wanted from 1 to 5. To break these examples down, say we have a list of items that we want to iterate over with an index: Now we pass this iterable to enumerate, creating an enumerate object: We can pull the first item out of this iterable that we would get in a loop with the next function: And we see we get a tuple of 0, the first index, and 'a', the first item: we can use what is referred to as "sequence unpacking" to extract the elements from this two-tuple: and when we inspect index, we find it refers to the first index, 0, and item refers to the first item, 'a'. So do this: It's pretty simple to start it from 1 other than 0:  As is the norm in Python there are several ways to do this. In all examples assume: lst = [1, 2, 3, 4, 5] This is also the safest option in my opinion because the chance of going into infinite recursion has been eliminated. Both the item and its index are held in variables and there is no need to write any further code to access the item. As explained before, there are other ways to do this that have not been explained here and they may even apply more in other situations. e.g using itertools.chain with for. It handles nested loops better than the other examples. The fastest way to access indexes of list within loop in Python 3.7 is to use the enumerate method for small, medium and huge lists. Please see different approaches which can be used to iterate over list and access index value and their performance metrics (which I suppose would be useful for you) in code samples below: See performance metrics for each method below: As the result, using enumerate method is the fastest method for iteration when the index needed. Adding some useful links below: What is the difference between range and xrange functions in Python 2.X? What is faster for loop using enumerate or for loop using xrange in Python? range(len(list)) or enumerate(list)? Old fashioned way: List comprehension: First of all, the indexes will be from 0 to 4. Programming languages start counting from 0; don't forget that or you will come across an index out of bounds exception. All you need in the for loop is a variable counting from 0 to 4 like so: Keep in mind that I wrote 0 to 5 because the loop stops one number before the max. :) To get the value of an index use for i in enumerate(items): print(i) Result: for i, val in enumerate(items): print(i, val) Result: for i, val in enumerate(items): print(i) Result: According to this discussion: http://bytes.com/topic/python/answers/464012-objects-list-index Loop counter iteration The current idiom for looping over the indices makes use of the built-in range function: Looping over both elements and indices can be achieved either by the old idiom or by using the new zip built-in function: or via http://www.python.org/dev/peps/pep-0212/ You can do it with this code: Use this code if you need to reset the index value at the end of the loop: Best solution for this problem is use enumerate in-build python function.  enumerate return tuple  first value is index  second value is element of array at that index If I were to iterate nums = [1, 2, 3, 4, 5] I would do Or get the length as l = len(nums) In your question, you write "how do I access the loop index, from 1 to 5 in this case?" However, the index for a list runs from zero.  So, then we need to know if what you actually want is the index and item for each item in a list, or whether you really want numbers starting from 1.  Fortunately, in Python, it is easy to do either or both. First, to clarify, the enumerate function iteratively returns the index and corresponding item for each item in a list. The output for the above is then, Notice that the index runs from 0. This kind of indexing is common among modern programming languages including Python and C. If you want your loop to span a part of the list, you can use the standard Python syntax for a part of the list. For example, to loop from the second item in a list up to but not including the last item, you could use Note that once again, the output index runs from 0, That brings us to the start=n switch for enumerate().  This simply offsets the index, you can equivalently simply add a number to the index inside the loop. for which the output is If there is no duplicate value in the list: You can also try this: The output is  You can use the index method EDIT Highlighted in the comment that this method doesn’t work if there are duplicates in ints, the method below should work for any values in ints: Or alternatively if you want to get both the index and the value in ints as a list of tuples. It uses the method of enumerate in the selected answer to this question, but with list comprehension, making it faster with less code. Simple answer using While Loop: Output:  To print tuple of (index, value) in list comprehension using a for loop: Output: This serves the purpose well enough:
__label__c# __label__enumeration __label__loops __label__.net __label__enums How can you enumerate an enum in C#? E.g. the following code does not compile: And it gives the following compile-time error: 'Suit' is a 'type' but is used like a 'variable' It fails on the Suit keyword, the second one. Note: The cast to (Suit[]) is not strictly necessary, but it does make the code 0.5 ns faster. It looks to me like you really want to print out the names of each enum, rather than the values. In which case Enum.GetNames() seems to be the right approach. By the way, incrementing the value is not a good way to enumerate the values of an enum. You should do this instead. I would use Enum.GetValues(typeof(Suit)) instead. I made some extensions for easy enum usage. Maybe someone can use it... The enum itself must be decorated with the FlagsAttribute: Some versions of the .NET framework do not support Enum.GetValues. Here's a good workaround from Ideas 2.0: Enum.GetValues in Compact Framework: As with any code that involves reflection, you should take steps to ensure it runs only once and results are cached. Use Cast<T>: There you go, IEnumerable<Suit>. I think this is more efficient than other suggestions because GetValues() is not called each time you have a loop. It is also more concise. And you get a compile-time error, not a runtime exception if Suit is not an enum. EnumLoop has this completely generic definition: You won't get Enum.GetValues() in Silverlight. Original Blog Post by Einar Ingebrigtsen: My solution works in .NET Compact Framework (3.5) and supports type checking at compile time: A call would look like this: I think you can use  I've heard vague rumours that this is   terifically slow. Anyone know? – Orion   Edwards Oct 15 '08 at 1:31 7    I think caching the array would speed it up considerably.  It looks like you're getting a new array (through reflection) every time.  Rather: That's at least a little faster, ja? Just by combining the top answers, I threw together a very simple extension: It is clean, simple, and, by @Jeppe-Stig-Nielsen's comment, fast. Three ways: I am not sure why GetEnumValues was introduced on type instances. It isn't very readable at all for me. Having a helper class like Enum<T> is what is most readable and memorable for me: Now you call: One can also use some sort of caching if performance matters, but I don't expect this to be an issue at all. There are two ways to iterate an Enum: The first will give you values in form on an array of **object**s, and the second will give you values in form of an array of **String**s. Use it in a foreach loop as below: I use ToString() then split and parse the spit array in flags. If enum values range strictly from 0 to n - 1, a generic alternative is: If enum values are contiguous and you can provide the first and last element of the enum, then: But that's not strictly enumerating, just looping. The second method is much faster than any other approach though... If you need speed and type checking at build and run time, this helper method is better than using LINQ to cast each element: And you can use it like below: Of course you can return IEnumerable<T>, but that buys you nothing here. Here is a working example of creating select options for a DDL: Add method public static IEnumerable<T> GetValues<T>() to your class, like: Call and pass your enum. Now you can iterate through it using foreach: (The current accepted answer has a cast that I don't think  is needed (although I may be wrong).) This question appears in Chapter 10 of "C# Step by Step 2013" The author uses a double for-loop to iterate through a pair of Enumerators (to create a full deck of cards): In this case, Suit and Value are both enumerations: and PlayingCard is a card object with a defined Suit and Value: I know it is a bit messy, but if you are fan of one-liners, here is one: A simple and generic way to convert an enum to something you can interact: And then: What if you know the type will be an enum, but you don't know what the exact type is at compile time? The method getListOfEnum uses reflection to take any enum type and returns an IEnumerable of all enum values. Usage: .NET 5 has introduced a a generic version for the GetValues method: Usage in a foreach loop: which is now by far the most convenient solution. enum types are called "enumeration types" not because they are containers that "enumerate" values (which they aren't), but because they are defined by enumerating the possible values for a variable of that type.  (Actually, that's a bit more complicated than that - enum types are considered to have an "underlying" integer type, which means each enum value corresponds to an integer value (this is typically implicit, but can be manually specified). C# was designed in a way so that you could stuff any integer of that type into the enum variable, even if it isn't a "named" value.) The System.Enum.GetNames method can be used to retrieve an array of strings which are the names of the enum values, as the name suggests. EDIT: Should have suggested the System.Enum.GetValues method instead. Oops. For getting a list of int from an enum, use the following. It works! Also you can bind to the public static members of the enum directly by using reflection: If you have: This: Will output: LINQ Generic Way: Usage:
__label__shell __label__posix __label__unix __label__bash What command can be used to check if a directory exists or not, within a Bash shell script? To check if a directory exists in a shell script, you can use the following: Or to check if a directory doesn't exist: However, as Jon Ericson points out, subsequent commands may not work as intended if you do not take into account that a symbolic link to a directory will also pass this check. E.g. running this: Will produce the error message: So symbolic links may have to be treated differently, if subsequent commands expect directories: Take particular note of the double-quotes used to wrap the variables. The reason for this is explained by 8jean in another answer. If the variables contain spaces or other unusual characters it will probably cause the script to fail. Remember to always wrap variables in double quotes when referencing them in a Bash script. Kids these days grow up with the idea that they can have spaces and lots of other funny characters in their directory names. (Spaces! Back in my days, we didn't have no fancy spaces! ;)) One day, one of those kids will run your script with $DIRECTORY set to "My M0viez" and your script will blow up. You don't want that. So use this. Note the -d test can produce some surprising results: File under: "When is a directory not a directory?"  The answer: "When it's a symlink to a directory."  A slightly more thorough test: You can find more information in the Bash manual on Bash conditional expressions and the [ builtin command and the [[ compound commmand. I find the double-bracket version of test makes writing logic tests more natural: Shorter form: To check if a directory exists you can use a simple if structure like this: You can also do it in the negative: Note: Be careful. Leave empty spaces on either side of both opening and closing braces. With the same syntax you can use: A simple script to test if a directory or file is present or not: A simple script to check whether the directory is present or not: The above scripts will check if the directory is present or not $?  if the last command is a success it returns "0", else a non-zero value. Suppose tempdir is already present. Then mkdir tempdir will give an error like below: mkdir: cannot create directory ‘tempdir’: File exists You can use test -d (see man test). -d file       True if file exists and is a directory. For example: Note: The test command is same as conditional expression [ (see: man [), so it's portable across shell scripts. [ - This is a synonym for the test builtin, but the last argument must, be a literal ], to match the opening [. For possible options or further help, check: Or for something completely useless: Here's a very pragmatic idiom: I typically wrap it in a function: Or: The nice thing about this approach is that I do not have to think of a good error message. cd will give me a standard one line message to standard error already. It will also give more information than I will be able to provide. By performing the cd inside a subshell ( ... ), the command does not affect the current directory of the caller.  If the directory exists, this subshell and the function are just a no-op. Next is the argument that we pass to cd: ${1:?pathname expected}.  This is a more elaborate form of parameter substitution which is explained in more detail below. Tl;dr: If the string passed into this function is empty, we again exit from the subshell ( ... ) and return from the function with the given error message. Quoting from the ksh93 man page: If  parameter  is set and is non-null then substitute its value;   otherwise, print word and exit from the shell (if not interactive).   If word is omitted then a standard message is printed. and If the colon : is omitted from  the  above  expressions,  then  the   shell only checks whether parameter is set or not. The phrasing here is peculiar to the shell documentation, as word may refer to any reasonable string, including whitespace. In this particular case, I know that the standard error message 1: parameter not set is not sufficient, so I zoom in on the type of value that we expect here - the pathname of a directory. A philosophical note: The shell is not an object oriented language, so the message says pathname, not directory. At this level, I'd rather keep it simple - the arguments to a function are just strings. The above code checks if the directory exists and if it is writable. Try online Check existence of the folder within sub-directories: Check existence of one or several folders based on a pattern within the current directory: Both combinations. In the following example, it checks the existence of the folder in the current directory: Type this code on the Bash prompt: Actually, you should use several tools to get a bulletproof approach: There isn't any need to worry about spaces and special characters as long as you use "${}". Note that [[]] is not as portable as [], but since most people work with modern versions of Bash (since after all, most people don't even work with command line :-p), the benefit is greater than the trouble. Have you considered just doing whatever you want to do in the if rather than looking before you leap? I.e., if you want to check for the existence of a directory before you enter it, try just doing this: If the path you give to pushd exists, you'll enter it and it'll exit with 0, which means the then portion of the statement will execute. If it doesn't exist, nothing will happen (other than some output saying the directory doesn't exist, which is probably a helpful side-effect anyways for debugging). It seems better than this, which requires repeating yourself: The same thing works with cd, mv, rm, etc... if you try them on files that don't exist, they'll exit with an error and print a message saying it doesn't exist, and your then block will be skipped. If you try them on files that do exist, the command will execute and exit with a status of 0, allowing your then block to execute. N.B: Quoting variables is a good practice. Explanation: To check more than one directory use this code: Check if the directory exists, else make one:  This answer wrapped up as a shell script Using the -e check will check for files and this includes directories. This is not completely true... If you want to go to that directory, you also need to have the execute rights on the directory. Maybe you need to have write rights as well. Therefore: As per Jonathan's comment: If you want to create the directory and it does not exist yet, then the simplest technique is to use mkdir -p which creates the directory — and any missing directories up the path — and does not fail if the directory already exists, so you can do it all at once with: The ls command in conjunction with -l (long listing) option returns attributes information about files and directories. In particular the first character of ls -l output it is usually a d or a - (dash). In case of a d the one listed is a directory for sure. The following command in just one line will tell you if the given ISDIR variable contains a path to a directory or not: Practical usage: Use the file program. Considering all directories are also files in Linux, issuing the following command would suffice: file $directory_name Checking a nonexistent file: file blah Output: cannot open 'blah' (No such file or directory) Checking an existing directory: file bluh Output: bluh: directory  There are great solutions out there, but ultimately every script will fail if you're not in the right directory. So code like this: will execute successfully only if at the moment of execution you're in a directory that has a subdirectory that you happen to check for. I understand the initial question like this: to verify if a directory exists irrespective of the user's position in the file system. So using the command 'find' might do the trick: This solution is good because it allows the use of wildcards, a useful feature when searching for files/directories. The only problem is that, if the searched directory doesn't exist, the 'find' command will print nothing to standard output (not an elegant solution for my taste) and will have nonetheless a zero exit. Maybe someone could improve on this. The below find can be used, (1) (2) (3) If an issue is found with one of the approaches provided above: With the ls command; the cases when a directory does not exists - an error message is shown -ksh: not: not found [No such file or directory]
__label__collections __label__java __label__hashtable __label__hashmap What are the differences between a HashMap and a Hashtable in Java? Which is more efficient for non-threaded applications? There are several differences between HashMap and Hashtable in Java: Hashtable is synchronized, whereas HashMap is not. This makes HashMap better for non-threaded applications, as unsynchronized Objects typically perform better than synchronized ones. Hashtable does not allow null keys or values.  HashMap allows one null key and any number of null values. One of HashMap's subclasses is LinkedHashMap, so in the event that you'd want predictable iteration order (which is insertion order by default), you could easily swap out the HashMap for a LinkedHashMap.  This wouldn't be as easy if you were using Hashtable. Since synchronization is not an issue for you, I'd recommend HashMap. If synchronization becomes an issue, you may also look at ConcurrentHashMap. Note, that a lot of the answers state that Hashtable is synchronised.  In practice this buys you very little.  The synchronization is on the accessor / mutator methods will stop two threads adding or removing from the map concurrently, but in the real world you will often need additional synchronisation. A very common idiom is to "check then put" — i.e. look for an entry in the Map, and add it if it does not already exist. This is not in any way an atomic operation whether you use Hashtable or HashMap. An equivalently synchronised HashMap can be obtained by: But to correctly implement this logic you need additional synchronisation of the form: Even iterating over a Hashtable's entries (or a HashMap obtained by Collections.synchronizedMap) is not thread safe unless you also guard the Map from being modified through additional synchronization. Implementations of the ConcurrentMap interface (for example ConcurrentHashMap) solve some of this by including thread safe check-then-act semantics such as: Hashtable is considered legacy code. There's nothing about Hashtable that can't be done using HashMap or derivations of HashMap, so for new code, I don't see any justification for going back to Hashtable. This question is often asked in interview to check whether candidate understands correct usage of collection classes and is aware of alternative solutions available. Note on Some Important Terms: HashMap can be synchronized by Map m = Collections.synchronizeMap(hashMap); Map provides Collection views instead of direct support for iteration  via Enumeration objects. Collection views greatly enhance the  expressiveness of the interface, as discussed later in this section.  Map allows you to iterate over keys, values, or key-value pairs;  Hashtable does not provide the third option. Map provides a safe way  to remove entries in the midst of iteration; Hashtable did not.  Finally, Map fixes a minor deficiency in the Hashtable interface.  Hashtable has a method called contains, which returns true if the  Hashtable contains a given value. Given its name, you'd expect this  method to return true if the Hashtable contained a given key, because  the key is the primary access mechanism for a Hashtable. The Map  interface eliminates this source of confusion by renaming the method  containsValue. Also, this improves the interface's consistency —  containsValue parallels containsKey. The Map Interface HashMap: An implementation of the Map interface that uses hash codes to index an array. Hashtable: Hi, 1998 called. They want their collections API back. Seriously though, you're better off staying away from Hashtable altogether. For single-threaded apps, you don't need the extra overhead of synchronisation. For highly concurrent apps, the paranoid synchronisation might lead to starvation, deadlocks, or unnecessary garbage collection pauses. Like Tim Howland pointed out, you might use ConcurrentHashMap instead. Keep in mind that HashTable was legacy class before Java Collections Framework (JCF) was introduced and was later retrofitted to implement the Map interface. So was Vector and Stack.  Therefore, always stay away from them in new code since there always better alternative in the JCF as others had pointed out. Here is the Java collection cheat sheet that you will find useful. Notice the gray block contains the legacy class HashTable,Vector and Stack.  There is many good answer already posted. I'm adding few new points and summarizing it. HashMap and Hashtable both are used to store data in key and value form. Both are using hashing technique to store unique keys. But there are many differences between HashMap and Hashtable classes that are given below. HashMap  Hashtable Further reading What is difference between HashMap and Hashtable in Java?  In addition to what izb said, HashMap allows null values, whereas the Hashtable does not. Also note that Hashtable extends the Dictionary class, which as the Javadocs state, is obsolete and has been replaced by the Map interface. Take a look at this chart. It provides comparisons between different data structures along with HashMap and Hashtable. The comparison is precise, clear and easy to understand. Java Collection Matrix Hashtable is similar to the HashMap and has a similar interface. It is recommended that you use HashMap, unless you require support for legacy applications or you need synchronisation, as the Hashtables methods are synchronised. So in your case as you are not multi-threading, HashMaps are your best bet. Another key difference between hashtable and hashmap is that Iterator in the HashMap is  fail-fast  while the enumerator for the Hashtable is not and throw ConcurrentModificationException if any other Thread modifies the map structurally  by adding or removing any element except Iterator's own remove()  method. But this is not a guaranteed behavior and will be done by JVM on best effort." My source: http://javarevisited.blogspot.com/2010/10/difference-between-hashmap-and.html Beside all the other important aspects already mentioned here, Collections API (e.g. Map interface) is being modified all the time to conform to the "latest and greatest" additions to Java spec. For example, compare Java 5 Map iterating: versus the old Hashtable approach: In Java 1.8 we are also promised to be able to construct and access HashMaps like in good old scripting languages: Update: No, they won't land in 1.8... :( Are Project Coin's collection enhancements going to be in JDK8? Hashtable is synchronized, whereas HashMap isn't. That makes Hashtable slower than Hashmap. For single thread applications, use HashMap since they are otherwise the same in terms of functionality. HashTable is synchronized, if you are using it in a single thread you can use HashMap, which is an unsynchronized version. Unsynchronized objects are often a little more performant. By the way if multiple threads access a HashMap concurrently, and at least one of the threads modifies the map structurally, it must be synchronized externally.  Youn can wrap a unsynchronized map in a synchronized one using : HashTable can only contain non-null object as a key or as a value. HashMap can contain one null key and null values. The iterators returned by Map are fail-fast, if the map is structurally modified at any time after the iterator is created, in any way except through the iterator's own remove method, the iterator will throw a ConcurrentModificationException. Thus, in the face of concurrent modification, the iterator fails quickly and cleanly, rather than risking arbitrary, non-deterministic behavior at an undetermined time in the future. Whereas the Enumerations returned by Hashtable's keys and elements methods are not fail-fast.  HashTable and HashMap are member of the Java Collections Framework (since Java 2 platform v1.2, HashTable was retrofitted to implement the Map interface). HashTable is considered legacy code, the documentation advise to use ConcurrentHashMap in place of Hashtable if a thread-safe highly-concurrent implementation is desired. HashMap doesn't guarantee the order in which elements are returned. For HashTable I guess it's the same but I'm not entirely sure, I don't find ressource that clearly state that. HashMap and Hashtable have significant algorithmic differences as well. No one has mentioned this before so that's why I am bringing it up. HashMap will construct a hash table with power of two size, increase it dynamically such that you have at most about eight elements (collisions) in any bucket and will stir the elements very well for general element types. However, the Hashtable implementation provides better and finer control over the hashing if you know what you are doing, namely you can fix the table size using e.g. the closest prime number to your values domain size and this will result in better performance than HashMap i.e. less collisions for some cases. Separate from the obvious differences discussed extensively in this question, I see the Hashtable as a "manual drive" car where you have better control over the hashing and the HashMap as the "automatic drive" counterpart that will generally perform well. Based on the info here, I'd recommend going with HashMap.  I think the biggest advantage is that Java will prevent you from modifying it while you are iterating over it, unless you do it through the iterator. A Collection — sometimes called a container — is simply an object that groups multiple elements into a single unit. Collections are used to store, retrieve, manipulate, and communicate aggregate data. A collections framework W is a unified architecture for representing and manipulating collections. The HashMap JDK1.2 and Hashtable JDK1.0, both are used to represent a group of objects that are represented in <Key, Value> pair. Each <Key, Value> pair is called Entry object. The collection of Entries is referred by the object of HashMap and Hashtable. Keys in a collection must be unique or distinctive. [as they are used to retrieve a mapped value a particular key. values in a collection can be duplicated.] « Superclass, Legacy and Collection Framework member Hashtable is a legacy class introduced in JDK1.0, which is a subclass of Dictionary class. From JDK1.2 Hashtable is re-engineered to implement the Map interface to make a member of collection framework. HashMap is a member of Java Collection Framework right from the beginning of its introduction in JDK1.2. HashMap is the subclass of the AbstractMap class. « Initial capacity and Load factor The capacity is the number of buckets in the hash table, and the initial capacity is simply the capacity at the time the hash table is created. Note that the hash table is open: in the case of a "hash collision", a single bucket stores multiple entries, which must be searched sequentially. The load factor is a measure of how full the hash table is allowed to get before its capacity is automatically increased. HashMap constructs an empty hash table with the default initial capacity (16) and the default load factor (0.75). Where as Hashtable constructs empty hashtable with a default initial capacity (11) and load factor/fill ratio (0.75).  « Structural modification in case of hash collision HashMap, Hashtable in case of hash collisions they store the map entries in linked lists. From Java8 for HashMap if hash bucket grows beyond a certain threshold, that bucket will switch from linked list of entries to a balanced tree. which improve worst-case performance from O(n) to O(log n). While converting the list to binary tree, hashcode is used as a branching variable. If there are two different hashcodes in the same bucket, one is considered bigger and goes to the right of the tree and other one to the left. But when both the hashcodes are equal, HashMap assumes that the keys are comparable, and compares the key to determine the direction so that some order can be maintained. It is a good practice to make the keys of HashMap comparable. On adding entries if bucket size reaches TREEIFY_THRESHOLD = 8 convert linked list of entries to a balanced tree, on removing entries less than TREEIFY_THRESHOLD  and at most UNTREEIFY_THRESHOLD = 6 will reconvert  balanced tree to linked list of entries. Java 8 SRC, stackpost « Collection-view iteration, Fail-Fast and Fail-Safe Iterator is a fail-fast in nature. i.e it throws ConcurrentModificationException if a collection is modified while iterating other than it’s own remove() method. Where as Enumeration is fail-safe in nature. It doesn’t throw any exceptions if a collection is modified while iterating. According to Java API Docs, Iterator is always preferred over the Enumeration. NOTE: The functionality of Enumeration interface is duplicated by the Iterator interface. In addition, Iterator adds an optional remove operation, and has shorter method names. New implementations should consider using Iterator in preference to Enumeration. In Java 5 introduced ConcurrentMap Interface: ConcurrentHashMap - a highly concurrent, high-performance ConcurrentMap implementation backed by a hash table. This implementation never blocks when performing retrievals and allows the client to select the concurrency level for updates. It is intended as a drop-in replacement for Hashtable: in addition to implementing ConcurrentMap, it supports all of the "legacy" methods peculiar to Hashtable. Each HashMapEntrys value is volatile thereby ensuring fine grain consistency for contended modifications and subsequent reads; each read reflects the most recently completed update Iterators and Enumerations are Fail Safe - reflecting the state at some point since the creation of iterator/enumeration; this allows for simultaneous reads and modifications at the cost of reduced consistency. They do not throw ConcurrentModificationException. However, iterators are designed to be used by only one thread at a time. Like Hashtable but unlike HashMap, this class does not allow null to be used as a key or value. « Null Keys And Null Values HashMap allows maximum one null key and any number of null values. Where as Hashtable doesn’t allow even a single null key and null value, if the key or value null is then it throws NullPointerException. Example « Synchronized, Thread Safe Hashtable is internally synchronized. Therefore, it is very much safe to use Hashtable in multi threaded applications. Where as HashMap is not internally synchronized. Therefore, it is not safe to use HashMap in multi threaded applications without external synchronization. You can externally synchronize HashMap using Collections.synchronizedMap() method. « Performance As Hashtable is internally synchronized, this makes Hashtable slightly slower than the HashMap. @See For threaded apps, you can often get away with ConcurrentHashMap- depends on your performance requirements. 1.Hashmap and HashTable both store key and value.  2.Hashmap can store one key as null. Hashtable can't store null. 3.HashMap is not synchronized but Hashtable is synchronized.  4.HashMap can be synchronized with Collection.SyncronizedMap(map) Apart from the differences already mentioned, it should be noted that since Java 8, HashMap dynamically replaces the Nodes (linked list) used in each bucket with TreeNodes (red-black tree), so that even if high hash collisions exist, the worst case when searching is O(log(n)) for HashMap Vs O(n) in Hashtable. *The aforementioned improvement has not been applied to Hashtable yet, but only to HashMap, LinkedHashMap, and ConcurrentHashMap. FYI, currently, There are 5 basic differentiations with HashTable and HashMaps.  My small contribution : First and most significant different between Hashtable and HashMap is that, HashMap is not thread-safe  while Hashtable is a thread-safe collection. Second important difference between Hashtable and HashMap is performance, since HashMap is not synchronized it perform better than Hashtable. Third difference on Hashtable vs HashMap is that Hashtable is obsolete class and you should be using ConcurrentHashMap in place of Hashtable in Java. HashMap: It is a class available inside java.util package and it is used to store the element in key and value format. Hashtable: It is a legacy class which is being recognized inside collection framework. HashTable is a legacy class in the jdk that shouldn't be used anymore. Replace usages of it with ConcurrentHashMap. If you don't require thread safety, use HashMap which isn't threadsafe but faster and uses less memory.  HashMap and HashTable  1) Hashtable and Hashmap implement the java.util.Map interface 2) Both Hashmap and Hashtable is the hash based collection. and working on hashing. so these are similarity of HashMap and HashTable.     1) First difference is HashMap is not thread safe While HashTable is ThreadSafe 2) HashMap is performance wise better because it is not thread safe. while Hashtable performance wise is not better because it is thread safe. so multiple thread can not access Hashtable at the same time. Hashtable: Hashtable is a data structure that retains values of key-value pair. It doesn’t allow null for both the keys and the values. You will get a NullPointerException if you add null value. It is synchronized. So it comes with its cost. Only one thread can access HashTable at a particular time. Example : HashMap: HashMap is like Hashtable but it also accepts key value pair. It allows null for both the keys and the values. Its performance better is better than HashTable, because it is unsynchronized. Example: HashMap and Hashtable both are used to store data in key and value form. Both are using hashing technique to store unique keys. ut there are many differences between HashMap and Hashtable classes that are given below.  HashMap is emulated and therefore usable in GWT client code whereas Hashtable is not. Old and classic topic, just want to add this helpful blog that explains this: http://blog.manishchhabra.com/2012/08/the-5-main-differences-betwen-hashmap-and-hashtable/ Blog by Manish Chhabra The 5 main differences betwen HashMap and Hashtable HashMap and Hashtable both implement java.util.Map interface but there   are some differences that Java developers must understand to write   more efficient code. As of the Java 2 platform v1.2, Hashtable class   was retrofitted to implement the Map interface, making it a member of   the Java Collections Framework. One of the major differences between HashMap and Hashtable is that HashMap is non-synchronized whereas Hashtable is synchronized, which   means Hashtable is thread-safe and can be shared between multiple   threads but HashMap cannot be shared between multiple threads without   proper synchronization. Java 5 introduced ConcurrentHashMap which is   an alternative of Hashtable and provides better scalability than   Hashtable in Java.Synchronized means only one thread can modify a hash   table at one point of time. Basically, it means that any thread before   performing an update on a hashtable will have to acquire a lock on the   object while others will wait for lock to be released. The HashMap class is roughly equivalent to Hashtable, except that it permits nulls. (HashMap allows null values as key and value whereas   Hashtable doesn’t allow nulls). The third significant difference between HashMap vs Hashtable is that Iterator in the HashMap is a fail-fast iterator while the   enumerator for the Hashtable is not and throw   ConcurrentModificationException if any other Thread modifies the map   structurally by adding or removing any element except Iterator’s own   remove() method. But this is not a guaranteed behavior and will be   done by JVM on best effort. This is also an important difference   between Enumeration and Iterator in Java. One more notable difference between Hashtable and HashMap is that because of thread-safety and synchronization Hashtable is much slower   than HashMap if used in Single threaded environment. So if you don’t   need synchronization and HashMap is only used by one thread, it out   perform Hashtable in Java. HashMap does not guarantee that the order of the map will remain constant over time. Note that HashMap can be synchronized by In Summary there are significant differences between Hashtable and   HashMap in Java e.g. thread-safety and speed and based upon that only   use Hashtable if you absolutely need thread-safety, if you are running   Java 5 consider using ConcurrentHashMap in Java.
__label__select __label__sql-server __label__sql __label__tsql In SQL Server, it is possible to insert rows into a table with an INSERT.. SELECT statement: Is it also possible to update a table with SELECT? I have a temporary table containing the values and would like to update another table using those values. Perhaps something like this:  In SQL Server 2008 (or better), use MERGE Alternatively:  I'd modify Robin's excellent answer to the following: Without a WHERE clause, you'll affect even rows that don't need to be affected, which could (possibly) cause index recalculation or fire triggers that really shouldn't have been fired. One way Another possibility not mentioned yet is to just chuck the SELECT statement itself into a CTE and then update the CTE. This has the benefit that it is easy to run the SELECT statement on its own first to sanity check the results, but it does requires you to alias the columns as above if they are named the same in source and target tables. This also has the same limitation as the proprietary UPDATE ... FROM syntax shown in four of the other answers. If the source table is on the many side of a one-to-many join then it is undeterministic which of the possible matching joined records will be used in the Update (an issue that MERGE avoids by raising an error if there is an attempt to update the same row more than once). For the record (and others searching like I was), you can do it in MySQL like this: Using alias: The simple way to do it is: This may be a niche reason to perform an update (for example, mainly used in a procedure), or may be obvious to others, but it should also be stated that you can perform an update-select statement without using join (in case the tables you're updating between have no common field). Here is another useful syntax: It checks if it is null or not by using "WHERE EXIST". I add this only so you can see a quick way to write it so that you can check what will be updated before doing the update. If you use MySQL instead of SQL Server, the syntax is: UPDATE from SELECT with INNER JOIN in SQL Database Since there are too many replies of this post, which are most heavily up-voted, I thought I would provide my suggestion here too. Although the question is very interesting, I have seen in many forum sites and made a solution using INNER JOIN with screenshots. At first, I have created a table named with schoolold and inserted few records with respect to their column names and execute it. Then I executed SELECT command to view inserted records.  Then I created a new table named with schoolnew and similarly executed above actions on it.  Then, to view inserted records in it, I execute SELECT command.  Now, Here I want to make some changes in third and fourth row, to complete this action, I execute UPDATE command with INNER JOIN.  To view the changes I execute the SELECT command.  You can see how Third and Fourth records of table schoolold easily replaced with table schoolnew by using INNER JOIN with UPDATE statement.  And if you wanted to join the table with itself (which won't happen too often): Updating through CTE is more readable than the other answers here: The following example uses a derived table, a SELECT statement after the FROM clause, to return the old and new values for further updates: If you are using SQL Server you can update one table from another without specifying a join and simply link the two from the where clause. This makes a much simpler SQL query: Consolidating all the different approaches here. Sample table structure is below and will update from Product_BAK to Product table. In this Merge statement, we can do inset if not finding a matching record in the target, but exist in the source and please find syntax: The other way is to use a derived table: Sample data To make sure you are updating what you want, select first There is even a shorter method and it might be surprising for you: Sample data set: Code:     Use: EITHER: OR: If the ID column name is the same in both tables then just put the table name before the table to be updated and use an alias for the selected table, i.e.: In the accepted answer, after the: I would add: What I usually do is putting everything in a roll backed transaction and using the "OUTPUT": in this way I see everything that is about to happen. When I am happy with what I see, I change the ROLLBACK into COMMIT. I usually need to document what I did, so I use the "results to Text" option when I run the roll-backed query and I save both the script and the result of the OUTPUT. (Of course this is not practical if I changed too many rows)  The below solution works for a MySQL database: The other way to update from a select statement: Option 1: Using Inner Join: Option 2: Co related Sub query The syntax for the UPDATE statement when updating one table with data from another table in SQL Server Important to point out, as others have, MySQL or MariaDB use a different syntax. Also it supports a very convenient USING syntax (in contrast to T/SQL). Also INNER JOIN is synonymous with JOIN. Therefor the query in the original question would be best implemented in MySQL thusly: I've not seen the a solution to the asked question in the other answers, hence my two cents. (tested on PHP 7.4.0 MariaDB 10.4.10)
__label__flatten __label__list __label__multidimensional-array __label__python I wonder whether there is a shortcut to make a simple list out of list of lists in Python. I can do that in a for loop, but maybe there is some cool "one-liner"? I tried it with reduce(), but I get an error. Code Error message Given a list of lists t, which means: is faster than the shortcuts posted so far. (t is the list to flatten.) Here is the corresponding function: As evidence, you can use the timeit module in the standard library: Explanation: the shortcuts based on + (including the implied use in sum) are, of necessity, O(T**2) when there are T sublists -- as the intermediate result list keeps getting longer, at each step a new intermediate result list object gets allocated, and all the items in the previous intermediate result must be copied over (as well as a few new ones added at the end). So, for simplicity and without actual loss of generality, say you have T sublists of k items each: the first k items are copied back and forth T-1 times, the second k items T-2 times, and so on; total number of copies is k times the sum of x for x from 1 to T excluded, i.e., k * (T**2)/2. The list comprehension just generates one list, once, and copies each item over (from its original place of residence to the result list) also exactly once. You can use itertools.chain(): Or you can use itertools.chain.from_iterable() which doesn't require unpacking the list with the * operator: Note from the author: This is inefficient. But fun, because monoids are awesome. It's not appropriate for production Python code. This just sums the elements of iterable passed in the first argument, treating second argument as the initial value of the sum (if not given, 0 is used instead and this case will give you an error). Because you are summing nested lists, you actually get [1,3]+[2,4] as a result of sum([[1,3],[2,4]],[]), which is equal to [1,3,2,4]. Note that only works on lists of lists. For lists of lists of lists, you'll need another solution. I tested most suggested solutions with perfplot (a pet project of mine, essentially a wrapper around timeit), and found to be the fastest solution, both when many small lists and few long lists are concatenated. (operator.iadd is equally fast.)   Code to reproduce the plot: The extend() method in your example modifies x instead of returning a useful value (which reduce() expects). A faster way to do the reduce version would be ...Pandas: ...Itertools: ...Matplotlib ...Unipath: ...Setuptools: Here is a general approach that applies to numbers, strings, nested lists and mixed containers. Code Notes:  Demo Reference If you want to flatten a data-structure where you don't know how deep it's nested you could use iteration_utilities.deepflatten1 It's a generator so you need to cast the result to a list or explicitly iterate over it. To flatten only one level and if each of the items is itself iterable you can also use iteration_utilities.flatten which itself is just a thin wrapper around itertools.chain.from_iterable: Just to add some timings (based on Nico Schlömer answer that didn't include the function presented in this answer):  It's a log-log plot to accommodate for the huge range of values spanned. For qualitative reasoning: Lower is better. The results show that if the iterable contains only a few inner iterables then sum will be fastest, however for long iterables only the itertools.chain.from_iterable, iteration_utilities.deepflatten or the nested comprehension have reasonable performance with itertools.chain.from_iterable being the fastest (as already noticed by Nico Schlömer). 1 Disclaimer: I'm the author of that library I take my statement back. sum is not the winner. Although it is faster when the list is small. But the performance degrades significantly with larger lists.  The sum version is still running for more than a minute and it hasn't done processing yet! For medium lists: Using small lists and timeit: number=1000000 There seems to be a confusion with operator.add! When you add two lists together, the correct term for that is concat, not add. operator.concat is what you need to use. If you're thinking functional, it is as easy as this:: You see reduce respects the sequence type, so when you supply a tuple, you get back a tuple. Let's try with a list:: Aha, you get back a list. How about performance:: from_iterable is pretty fast! But it's no comparison to reduce with concat. Why do you use extend? This should work fine. Consider installing the more_itertools package. It ships with an implementation for flatten (source, from the itertools recipes): Note: as mentioned in the docs, flatten requires a list of lists.  See below on flattening more irregular inputs. As of version 2.4, you can flatten more complicated, nested iterables with more_itertools.collapse (source, contributed by  abarnet). The reason your function didn't work is because the extend extends an array in-place and doesn't return it. You can still return x from lambda, using something like this: Note: extend is more efficient than + on lists.  Recursive version A bad feature of Anil's function above is that it requires the user to always manually specify the second argument to be an empty list []. This should instead be a default. Due to the way Python objects work, these should be set inside the function, not in the arguments. Here's a working function: Testing: The accepted answer did not work for me when dealing with text-based lists of variable lengths. Here is an alternate approach that did work for me. matplotlib.cbook.flatten() will work for nested lists even if they nest more deeply than the example. Result: This is 18x faster than underscore._.flatten: Following seem simplest to me: One can also use NumPy's flat: Edit 11/02/2016: Only works when sublists have identical dimensions. I personally find it hard to remember all the modules that needed to be imported. Thus I tend to use a simple method, even though I don't know how its performance is compared to other answers. If you just want to flatten nested lists, then the following will do the job: However, if you want to flatten a list of iterables (list and/or tuples), it can also do the job with a slight modification: Note: Below applies to Python 3.3+ because it uses yield_from.  six is also a third-party package, though it is stable.  Alternately, you could use sys.version. In the case of obj = [[1, 2,], [3, 4], [5, 6]], all of the solutions here are good, including list comprehension and itertools.chain.from_iterable. However, consider this slightly more complex case: There are several problems here: You can remedy this as follows: Here, you check that the sub-element (1) is iterable with Iterable, an ABC from itertools, but also want to ensure that (2) the element is not "string-like." You can use numpy : flat_list = list(np.concatenate(list_of_list)) The advantage of this solution over most others here is that if you have a list like: while most other solutions throw an error this solution handles them. If you are willing to give up a tiny amount of speed for a cleaner look, then you could use numpy.concatenate().tolist() or numpy.concatenate().ravel().tolist(): You can find out more here in the docs numpy.concatenate and numpy.ravel Fastest solution I have found (for large list anyway): Done! You can of course turn it back into a list by executing list(l) Simple code for underscore.py package fan It solves all flatten problems (none list item or complex nesting) You can install underscore.py with pip  This may not be the most efficient way but I thought to put a one-liner (actually a two-liner). Both versions will work on arbitrary hierarchy nested lists, and exploits language features (Python3.5) and recursion. The output is This works in a depth first manner. The recursion goes down until it finds a non-list element, then extends the local variable flist and then rolls back it to the parent. Whenever flist is returned, it is extended to the parent's flist in the list comprehension. Therefore, at the root, a flat list is returned. The above one creates several local lists and returns them which are used to extend the parent's list. I think the way around for this may be creating a gloabl flist, like below. The output is again Although I am not sure at this time about the efficiency. This Code also works fine as it just extend the list all the way. Although it is much similar but only have one for loop. So It have less complexity than adding 2 for loops.
__label__docker __label__containers __label__virtualization __label__virtual-machine I keep rereading the Docker documentation to try to understand the difference between Docker and a full VM. How does it manage to provide a full filesystem, isolated networking environment, etc. without being as heavy? Why is deploying software to a Docker image (if that's the right term) easier than simply deploying to a consistent production environment? Docker originally used LinuX Containers (LXC), but later switched to runC (formerly known as libcontainer), which runs in the same operating system as its host. This allows it to share a lot of the host operating system resources. Also, it uses a layered filesystem (AuFS) and manages networking. AuFS is a layered file system, so you can have a read only part and a write part which are merged together. One could have the common parts of the operating system as read only (and shared amongst all of your containers) and then give each container its own mount for writing. So, let's say you have a 1 GB container image; if you wanted to use a full VM, you would need to have 1 GB x number of VMs you want. With Docker and AuFS you can share the bulk of the 1 GB between all the containers and if you have 1000 containers you still might only have a little over 1 GB of space for the containers OS (assuming they are all running the same OS image). A full virtualized system gets its own set of resources allocated to it, and does minimal sharing. You get more isolation, but it is much heavier (requires more resources). With Docker you get less isolation, but the containers are lightweight (require fewer resources). So you could easily run thousands of containers on a host, and it won't even blink. Try doing that with Xen, and unless you have a really big host, I don't think it is possible. A full virtualized system usually takes minutes to start, whereas Docker/LXC/runC containers take seconds, and often even less than a second. There are pros and cons for each type of virtualized system. If you want full isolation with guaranteed resources, a full VM is the way to go. If you just want to isolate processes from each other and want to run a ton of them on a reasonably sized host, then Docker/LXC/runC seems to be the way to go. For more information, check out this set of blog posts which do a good job of explaining how LXC works. Why is deploying software to a docker image (if that's the right term) easier than simply deploying to a consistent production environment? Deploying a consistent production environment is easier said than done. Even if you use tools like Chef and Puppet, there are always OS updates and other things that change between hosts and environments. Docker gives you the ability to snapshot the OS into a shared image, and makes it easy to deploy on other Docker hosts. Locally, dev, qa, prod, etc.: all the same image. Sure you can do this with other tools, but not nearly as easily or fast. This is great for testing; let's say you have thousands of tests that need to connect to a database, and each test needs a pristine copy of the database and will make changes to the data. The classic approach to this is to reset the database after every test either with custom code or with tools like Flyway - this can be very time-consuming and means that tests must be run serially. However, with Docker you could create an image of your database and run up one instance per test, and then run all the tests in parallel since you know they will all be running against the same snapshot of the database. Since the tests are running in parallel and in Docker containers they could run all on the same box at the same time and should finish much faster. Try doing that with a full VM. From comments... Interesting! I suppose I'm still confused by the notion of "snapshot[ting] the OS". How does one do that without, well, making an image of the OS? Well, let's see if I can explain. You start with a base image, and then make your changes, and commit those changes using docker, and it creates an image. This image contains only the differences from the base. When you want to run your image, you also need the base, and it layers your image on top of the base using a layered file system: as mentioned above, Docker uses AuFS. AuFS merges the different layers together and you get what you want; you just need to run it. You can keep adding more and more images (layers) and it will continue to only save the diffs. Since Docker typically builds on top of ready-made images from a registry, you rarely have to "snapshot" the whole OS yourself. Good answers. Just to get an image representation of container vs VM, have a look at the one below.   Source It might be helpful to understand how virtualization and containers work at a low level. That will clear up lot of things. Note: I'm simplifying a bit in the description below. See references for more information. How does virtualization work at a low level? In this case the VM manager takes over the CPU ring 0 (or the "root mode" in newer CPUs) and intercepts all privileged calls made by the guest OS to create the illusion that the guest OS has its own hardware. Fun fact: Before 1998 it was thought to be impossible to achieve this on the x86 architecture because there was no way to do this kind of interception. The folks at VMware were the first who had an idea to rewrite the executable bytes in memory for privileged calls of the guest OS to achieve this. The net effect is that virtualization allows you to run two completely different OSes on the same hardware. Each guest OS goes through all the processes of bootstrapping, loading kernel, etc. You can have very tight security. For example, a guest OS can't get full access to the host OS or other guests and mess things up. How do containers work at a low level? Around 2006, people including some of the employees at Google implemented a new kernel level feature called namespaces (however the idea long before existed in FreeBSD). One function of the OS is to allow sharing of global resources like network and disks among processes. What if these global resources were wrapped in namespaces so that they are visible only to those processes that run in the same namespace? Say, you can get a chunk of disk and put that in namespace X and then processes running in namespace Y can't see or access it. Similarly, processes in namespace X can't access anything in memory that is allocated to namespace Y. Of course, processes in X can't see or talk to processes in namespace Y. This provides a kind of virtualization and isolation for global resources. This is how Docker works: Each container runs in its own namespace but uses exactly the same kernel as all other containers. The isolation happens because the kernel knows the namespace that was assigned to the process and during API calls it makes sure that the process can only access resources in its own namespace. The limitations of containers vs VMs should be obvious now: You can't run completely different OSes in containers like in VMs. However you can run different distros of Linux because they do share the same kernel. The isolation level is not as strong as in a VM. In fact, there was a way for a "guest" container to take over the host in early implementations. Also you can see that when you load a new container, an entire new copy of the OS doesn't start like it does in a VM. All containers share the same kernel. This is why containers are light weight. Also unlike a VM, you don't have to pre-allocate a significant chunk of memory to containers because we are not running a new copy of the OS. This enables running thousands of containers on one OS while sandboxing them, which might not be possible if we were running separate copies of the OS in their own VMs. I like Ken Cochrane's answer. But I want to add additional point of view, not covered in detail here. In my opinion Docker differs also in whole process. In contrast to VMs, Docker is not (only) about optimal resource sharing of hardware, moreover it provides a "system" for packaging application (preferable, but not a must, as a set of microservices). To me it fits in the gap between developer-oriented tools like rpm, Debian packages, Maven, npm + Git on one side and ops tools like Puppet, VMware, Xen, you name it... Why is deploying software to a docker image (if that's the right term) easier than simply deploying to a consistent production environment? Your question assumes some consistent production environment. But how to keep it consistent? Consider some amount (>10) of servers and applications, stages in the pipeline. To keep this in sync you'll start to use something like Puppet, Chef or your own provisioning scripts, unpublished rules and/or lot of documentation... In theory servers can run indefinitely, and be kept completely consistent and up to date. Practice fails to manage a server's configuration completely, so there is considerable scope for configuration drift, and unexpected changes to running servers. So there is a known pattern to avoid this, the so called immutable server. But the immutable server pattern was not loved. Mostly because of the limitations of VMs that were used before Docker. Dealing with several gigabytes big images, moving those big images around, just to change some fields in the application, was very very laborious. Understandable... With a Docker ecosystem, you will never need to move around gigabytes on "small changes" (thanks aufs and Registry) and you don't need to worry about losing performance by packaging applications into a Docker container at runtime. You don't need to worry about versions of that image. And finally you will even often be able to reproduce complex production environments even on your Linux laptop (don't call me if doesn't work in your case ;)) And of course you can start Docker containers in VMs (it's a good idea). Reduce your server provisioning on the VM level. All the above could be managed by Docker. P.S. Meanwhile Docker uses its own implementation "libcontainer" instead of LXC. But LXC is still usable. Docker isn't a virtualization methodology. It relies on other tools that actually implement container-based virtualization or operating system level virtualization. For that, Docker was initially using LXC driver, then moved to libcontainer which is now renamed as runc. Docker primarily focuses on automating the deployment of applications inside application containers. Application containers are designed to package and run a single service, whereas system containers are designed to run multiple processes, like virtual machines. So, Docker is considered as a container management or application deployment tool on containerized systems. In order to know how it is different from other virtualizations, let's go through virtualization and its types. Then, it would be easier to understand what's the difference there. Virtualization In its conceived form, it was considered a method of logically dividing mainframes to allow multiple applications to run simultaneously. However, the scenario drastically changed when companies and open source communities were able to provide a method of handling the privileged instructions in one way or another and allow for multiple operating systems to be run simultaneously on a single x86 based system. Hypervisor The hypervisor handles creating the virtual environment on which the guest virtual machines operate. It supervises the guest systems and makes sure that resources are allocated to the guests as necessary. The hypervisor sits in between the physical machine and virtual machines and provides virtualization services to the virtual machines. To realize it, it intercepts the guest operating system operations on the virtual machines and emulates the operation on the host machine's operating system.  The rapid development of virtualization technologies, primarily in cloud, has driven the use of virtualization further by allowing multiple virtual servers to be created on a single physical server with the help of hypervisors, such as Xen, VMware Player, KVM, etc., and incorporation of hardware support in commodity processors, such as Intel VT and AMD-V. Types of Virtualization The virtualization method can be categorized based on how it mimics hardware to a guest operating system and emulates a guest operating environment. Primarily, there are three types of virtualization: Emulation Emulation, also known as full virtualization runs the virtual machine OS kernel entirely in software. The hypervisor used in this type is known as Type 2 hypervisor. It is installed on the top of the host operating system which is responsible for translating guest OS kernel code to software instructions. The translation is done entirely in software and requires no hardware involvement. Emulation makes it possible to run any non-modified operating system that supports the environment being emulated.  The downside of this type of virtualization is an additional system resource overhead that leads to a decrease in performance compared to other types of virtualizations.  Examples in this category include VMware Player, VirtualBox, QEMU, Bochs, Parallels, etc. Paravirtualization Paravirtualization, also known as Type 1 hypervisor, runs directly on the hardware, or “bare-metal”, and provides virtualization services directly to the virtual machines running on it. It helps the operating system, the virtualized hardware, and the real hardware to collaborate to achieve optimal performance. These hypervisors typically have a rather small footprint and do not, themselves, require extensive resources. Examples in this category include Xen, KVM, etc.  Container-based Virtualization Container-based virtualization, also known as operating system-level virtualization, enables multiple isolated executions within a single operating system kernel. It has the best possible performance and density and features dynamic resource management. The isolated virtual execution environment provided by this type of virtualization is called a container and can be viewed as a traced group of processes.   The concept of a container is made possible by the namespaces feature added to Linux kernel version 2.6.24. The container adds its ID to every process and adding new access control checks to every system call. It is accessed by the clone() system call that allows creating separate instances of previously-global namespaces.  Namespaces can be used in many different ways, but the most common approach is to create an isolated container that has no visibility or access to objects outside the container. Processes running inside the container appear to be running on a normal Linux system although they are sharing the underlying kernel with processes located in other namespaces, same for other kinds of objects. For instance, when using namespaces, the root user inside the container is not treated as root outside the container, adding additional security.  The Linux Control Groups (cgroups) subsystem, the next major component to enable container-based virtualization, is used to group processes and manage their aggregate resource consumption. It is commonly used to limit the memory and CPU consumption of containers.  Since a containerized Linux system has only one kernel and the kernel has full visibility into the containers, there is only one level of resource allocation and scheduling. Several management tools are available for Linux containers, including LXC, LXD, systemd-nspawn, lmctfy, Warden, Linux-VServer, OpenVZ, Docker, etc.  Containers vs Virtual Machines Unlike a virtual machine, a container does not need to boot the operating system kernel, so containers can be created in less than a second. This feature makes container-based virtualization unique and desirable than other virtualization approaches. Since container-based virtualization adds little or no overhead to the host machine, container-based virtualization has near-native performance For container-based virtualization, no additional software is required, unlike other virtualizations. All containers on a host machine share the scheduler of the host machine saving need of extra resources. Container states (Docker or LXC images) are small in size compared to virtual machine images, so container images are easy to distribute. Resource management in containers is achieved through cgroups. Cgroups does not allow containers to consume more resources than allocated to them. However, as of now, all resources of host machine are visible in virtual machines, but can't be used. This can be realized by running top or htop on containers and host machine at the same time. The output across all environments will look similar. Update: How does Docker run containers in non-Linux systems? If containers are possible because of the features available in the Linux kernel, then the obvious question is how do non-Linux systems run containers. Both Docker for Mac and Windows use Linux VMs to run the containers. Docker Toolbox used to run containers in Virtual Box VMs. But, the latest Docker uses Hyper-V in Windows and Hypervisor.framework in Mac. Now, let me describe how Docker for Mac runs containers in detail.  Docker for Mac uses https://github.com/moby/hyperkit to emulate the hypervisor capabilities and Hyperkit uses hypervisor.framework in its core. Hypervisor.framework is Mac's native hypervisor solution. Hyperkit also uses VPNKit and DataKit to namespace network and filesystem respectively.  The Linux VM that Docker runs in Mac is read-only. However, you can bash into it by running: screen ~/Library/Containers/com.docker.docker/Data/vms/0/tty. Now, we can even check the Kernel version of this VM: # uname -a Linux linuxkit-025000000001 4.9.93-linuxkit-aufs #1 SMP Wed Jun 6 16:86_64 Linux. All containers run inside this VM. There are some limitations to hypervisor.framework. Because of that Docker doesn't expose docker0 network interface in Mac. So, you can't access containers from the host. As of now, docker0 is only available inside the VM. Hyper-v is the native hypervisor in Windows. They are also trying to leverage Windows 10's capabilities to run Linux systems natively. Most of the answers here talk about virtual machines. I'm going to give you a one-liner response to this question that has helped me the most over the last couple years of using Docker. It's this: Docker is just a fancy way to run a process, not a virtual machine. Now, let me explain a bit more about what that means. Virtual machines are their own beast. I feel like explaining what Docker is will help you understand this more than explaining what a virtual machine is. Especially because there are many fine answers here telling you exactly what someone means when they say "virtual machine". So... A Docker container is just a process (and its children) that is compartmentalized using cgroups inside the host system's kernel from the rest of the processes. You can actually see your Docker container processes by running ps aux on the host. For example, starting apache2 "in a container" is just starting apache2 as a special process on the host. It's just been compartmentalized from other processes on the machine. It is important to note that your containers do not exist outside of your containerized process' lifetime. When your process dies, your container dies. That's because Docker replaces pid 1 inside your container with your application (pid 1 is normally the init system). This last point about pid 1 is very important. As far as the filesystem used by each of those container processes, Docker uses UnionFS-backed images, which is what you're downloading when you do a docker pull ubuntu. Each "image" is just a series of layers and related metadata. The concept of layering is very important here. Each layer is just a change from the layer underneath it. For example, when you delete a file in your Dockerfile while building a Docker container, you're actually just creating a layer on top of the last layer which says "this file has been deleted". Incidentally, this is why you can delete a big file from your filesystem, but the image still takes up the same amount of disk space. The file is still there, in the layers underneath the current one. Layers themselves are just tarballs of files. You can test this out with docker save --output /tmp/ubuntu.tar ubuntu and then cd /tmp && tar xvf ubuntu.tar. Then you can take a look around. All those directories that look like long hashes are actually the individual layers. Each one contains files (layer.tar) and metadata (json) with information about that particular layer. Those layers just describe changes to the filesystem which are saved as a layer "on top of" its original state. When reading the "current" data, the filesystem reads data as though it were looking only at the top-most layers of changes. That's why the file appears to be deleted, even though it still exists in "previous" layers, because the filesystem is only looking at the top-most layers. This allows completely different containers to share their filesystem layers, even though some significant changes may have happened to the filesystem on the top-most layers in each container. This can save you a ton of disk space, when your containers share their base image layers. However, when you mount directories and files from the host system into your container by way of volumes, those volumes "bypass" the UnionFS, so changes are not stored in layers. Networking in Docker is achieved by using an ethernet bridge (called docker0 on the host), and virtual interfaces for every container on the host. It creates a virtual subnet in docker0 for your containers to communicate "between" one another. There are many options for networking here, including creating custom subnets for your containers, and the ability to "share" your host's networking stack for your container to access directly. Docker is moving very fast. Its documentation is some of the best documentation I've ever seen. It is generally well-written, concise, and accurate. I recommend you check the documentation available for more information, and trust the documentation over anything else you read online, including Stack Overflow. If you have specific questions, I highly recommend joining #docker on Freenode IRC and asking there (you can even use Freenode's webchat for that!). Through this post we are going to draw some lines of differences between VMs and LXCs. Let's first define them. VM: A virtual machine emulates a physical computing environment, but requests for CPU, memory, hard disk, network and other hardware resources are managed by a virtualization layer which translates these requests to the underlying physical hardware. In this context the VM is called as the Guest while the environment it runs on is called the host. LXCs: Linux Containers (LXC) are operating system-level capabilities that make it possible to run multiple isolated Linux containers, on one control host (the LXC host). Linux Containers serve as a lightweight alternative to VMs as they don’t require the hypervisors viz. Virtualbox, KVM, Xen, etc. Now unless you were drugged by Alan (Zach Galifianakis- from the Hangover series) and have been in Vegas for the last year, you will be pretty aware about the tremendous spurt of interest for Linux containers technology, and if I will be specific one container project which has created a buzz around the world in last few months is – Docker leading to some echoing opinions that cloud computing environments should abandon virtual machines (VMs) and replace them with containers due to their lower overhead and potentially better performance. But the big question is, is it feasible?, will it be sensible? a. LXCs are scoped to an instance of Linux. It might be different flavors of Linux (e.g. a Ubuntu container on a CentOS host but it’s still Linux.) Similarly, Windows-based containers are scoped to an instance of Windows now if we look at VMs they have a pretty broader scope and using the hypervisors you are not limited to operating systems Linux or Windows. b. LXCs have low overheads and have better performance as compared to VMs. Tools viz. Docker which are built on the shoulders of LXC technology have provided developers with a platform to run their applications and at the same time have empowered operations people with a tool that will allow them to deploy the same container on production servers or data centers. It tries to make the experience between a developer running an application, booting and testing an application and an operations person deploying that application seamless, because this is where all the friction lies in and purpose of DevOps is to break down those silos. So the best approach is the cloud infrastructure providers should advocate an appropriate use of the VMs and LXC, as they are each suited to handle specific workloads and scenarios. Abandoning VMs is not practical as of now. So both VMs and LXCs have their own individual existence and importance. Docker encapsulates an application with all its dependencies. A virtualizer encapsulates an OS that can run any applications it can normally run on a bare metal machine. They both are very different. Docker is lightweight and uses LXC/libcontainer (which relies on kernel namespacing and cgroups) and does not have machine/hardware emulation such as hypervisor, KVM. Xen which are heavy. Docker and LXC is meant more for sandboxing, containerization, and resource isolation. It uses the host OS's (currently only Linux kernel) clone API which provides namespacing for IPC, NS (mount), network, PID, UTS, etc. What about memory, I/O, CPU, etc.? That is controlled using cgroups where you can create groups with certain resource (CPU, memory, etc.) specification/restriction and put your processes in there. On top of LXC, Docker provides a storage backend (http://www.projectatomic.io/docs/filesystems/) e.g., union mount filesystem where you can add layers and share layers between different mount namespaces. This is a powerful feature where the base images are typically readonly and only when the container modifies something in the layer will it write something to read-write partition (a.k.a. copy on write). It also provides many other wrappers such as registry and versioning of images. With normal LXC you need to come with some rootfs or share the rootfs and when shared, and the changes are reflected on other containers. Due to lot of these added features, Docker is more popular than LXC. LXC is popular in embedded environments for implementing security around processes exposed to external entities such as network and UI. Docker is popular in cloud multi-tenancy environment where consistent production environment is expected. A normal VM (for example, VirtualBox and VMware) uses a hypervisor, and related technologies either have dedicated firmware that becomes the first layer for the first OS (host OS, or guest OS 0) or a software that runs on the host OS to provide hardware emulation such as CPU, USB/accessories, memory, network, etc., to the guest OSes. VMs are still (as of 2015) popular in high security multi-tenant environment. Docker/LXC can almost be run on any cheap hardware (less than 1 GB of memory is also OK as long as you have newer kernel) vs. normal VMs need at least 2 GB of memory, etc., to do anything meaningful with it. But Docker support on the host OS is not available in OS such as Windows (as of Nov 2014) where as may types of VMs can be run on windows, Linux, and Macs. Here is a pic from docker/rightscale :  This is probably the first impression for many docker learners.  First, docker images are usually smaller than VM images, makes it easy to build, copy, share. Second, Docker containers can start in several milliseconds, while VM starts in seconds. This is another key feature of Docker. Images have layers, and different images can share layers, make it even more space-saving and faster to build. If all containers use Ubuntu as their base images, not every image has its own file system, but share the same underline ubuntu files, and only differs in their own application data. Think of containers as processes! All containers running on a host is indeed a bunch of processes with different file systems. They share the same OS kernel, only encapsulates system library and dependencies.  This is good for most cases(no extra OS kernel maintains) but can be a problem if strict isolations are necessary between containers. All these seem like improvements, not revolution. Well, quantitative accumulation leads to qualitative transformation. Think about application deployment. If we want to deploy a new software(service) or upgrade one, it is better to change the config files and processes instead of creating a new VM. Because Creating a VM with updated service, testing it(share between Dev & QA), deploying to production takes hours, even days. If anything goes wrong, you got to start again, wasting even more time. So, use configuration management tool(puppet, saltstack, chef etc.) to install new software, download new files is preferred. When it comes to docker, it's impossible to use a newly created docker container to replace the old one. Maintainance is much easier!Building a new image, share it with QA, testing it, deploying it only takes minutes(if everything is automated), hours in the worst case. This is called immutable infrastructure: do not maintain(upgrade) software, create a new one instead. It transforms how services are delivered. We want applications, but have to maintain VMs(which is a pain and has little to do with our applications). Docker makes you focus on applications and smooths everything. Docker, basically containers, supports OS virtualization i.e. your application feels that it has a complete instance of an OS whereas VM supports hardware virtualization. You feel like it is a physical machine in which you can boot any OS. In Docker, the containers running share the host OS kernel, whereas in VMs they have their own OS files. The environment (the OS) in which you develop an application would be same when you deploy it to various serving environments, such as "testing" or "production". For example, if you develop a web server that runs on port 4000, when you deploy it to your "testing" environment, that port is already used by some other program, so it stops working. In containers there are layers; all the changes you have made to the OS would be saved in one or more layers and those layers would be part of image, so wherever the image goes the dependencies would be present as well. In the example shown below, the host machine has three VMs. In order to provide the applications in the VMs complete isolation, they each have their own copies of OS files, libraries and application code, along with a full in-memory instance of an OS.  Whereas the figure below shows the same scenario with containers. Here, containers simply share the host operating system, including the kernel and libraries, so they don’t need to boot an OS, load libraries or pay a private memory cost for those files. The only incremental space they take is any memory and disk space necessary for the application to run in the container. While the application’s environment feels like a dedicated OS, the application deploys just like it would onto a dedicated host. The containerized application starts in seconds and many more instances of the application can fit onto the machine than in the VM case.  Source: https://azure.microsoft.com/en-us/blog/containers-docker-windows-and-trends/ There are three different setups that providing a stack to run an application on (This will help us to recognize what a container is and what makes it so much powerful than other solutions): 1) Traditional server stack consist of a physical server that runs an operating system and your application. Advantages:  Utilization of raw resources Isolation Disadvantages:  2) The VM stack consist of a physical server which runs an operating system and a hypervisor that manages your virtual machine, shared resources, and networking interface. Each Vm runs a Guest Operating System, an application or set of applications. Advantages:  Disadvantages:  3) The Container Setup, the key difference with other stack is container-based virtualization uses the kernel of the host OS to rum multiple isolated guest instances. These guest instances are called as containers. The host can be either a physical server or VM. Advantages: Disadvantages:  By comparing the container setup with its predecessors, we can conclude that containerization is the fastest, most resource effective, and most secure setup we know to date. Containers are isolated instances that run your application. Docker spin up the container in a way, layers get run time memory with default storage drivers(Overlay drivers) those run within seconds and copy-on-write layer created on top of it once we commit into the container, that powers the execution of containers. In case of VM's that will take around a minute to load everything into the virtualize environment. These lightweight instances can be replaced, rebuild, and moved around easily. This allows us to mirror the production and development environment and is tremendous help in CI/CD processes. The advantages containers can provide are so compelling that they're definitely here to stay. In relation to:- "Why is deploying software to a docker image easier than simply   deploying to a consistent production environment ?" Most software is deployed to many environments, typically a minimum of three of the following: There are also the following factors to consider: As you can see the extrapolated total number of servers for an organisation is rarely in single figures, is very often in triple figures and can easily be significantly higher still. This all means that creating consistent environments in the first place is hard enough just because of sheer volume (even in a green field scenario), but keeping them consistent is all but impossible given the high number of servers, addition of new servers (dynamically or manually), automatic updates from o/s vendors, anti-virus vendors, browser vendors and the like, manual software installs or configuration changes performed by developers or server technicians, etc. Let me repeat that - it's virtually (no pun intended) impossible to keep environments consistent (okay, for the purist, it can be done, but it involves a huge amount of time, effort and discipline, which is precisely why VMs and containers (e.g. Docker) were devised in the first place). So think of your question more like this "Given the extreme difficulty of keeping all environments consistent, is it easier to deploying software to a docker image, even when taking the learning curve into account ?". I think you'll find the answer will invariably be "yes" - but there's only one way to find out, post this new question on Stack Overflow. There are many answers which explain more detailed on the differences, but here is my very brief explanation. One important difference is that VMs use a separate kernel to run the OS. That's the reason it is heavy and takes time to boot, consuming more system resources. In Docker, the containers share the kernel with the host; hence it is lightweight and can start and stop quickly. In Virtualization, the resources are allocated in the beginning of set up and hence the resources are not fully utilized when the virtual machine is idle during many of the times.  In Docker, the containers are not allocated with fixed amount of hardware resources and is free to use the resources depending on the requirements and hence it is highly scalable. Docker uses UNION File system .. Docker uses a copy-on-write technology to reduce the memory space consumed by containers. Read more here With a virtual machine, we have a server, we have a host operating system on that server, and then we have a hypervisor. And then running on top of that hypervisor, we have any number of guest operating systems with an application and its dependent binaries, and libraries on that server. It brings a whole guest operating system with it. It's quite heavyweight. Also there's a limit to how much you can actually put on each physical machine.  Docker containers on the other hand, are slightly different. We have the server. We have the host operating system. But instead a hypervisor, we have the Docker engine, in this case. In this case, we're not bringing a whole guest operating system with us. We're bringing a very thin layer of the operating system, and the container can talk down into the host OS in order to get to the kernel functionality there. And that allows us to have a very lightweight container. All it has in there is the application code and any binaries and libraries that it requires. And those binaries and libraries can actually be shared across different containers if you want them to be as well. And what this enables us to do, is a number of things. They have much faster startup time. You can't stand up a single VM in a few seconds like that. And equally, taking them down as quickly.. so we can scale up and down very quickly and we'll look at that later on.  Every container thinks that it’s running on its own copy of the operating system. It’s got its own file system, own registry, etc. which is a kind of a lie. It’s actually being virtualized.   Source:  Kubernetes in Action.   I have used Docker in production environments and staging very much. When you get used to it you will find it very powerful for building a multi container and isolated environments. Docker has been developed based on LXC (Linux Container) and works perfectly in many Linux distributions, especially Ubuntu. Docker containers are isolated environments. You can see it when you issue the top command in a Docker container that has been created from a Docker image. Besides that, they are very light-weight and flexible thanks to the dockerFile configuration. For example, you can create a Docker image and configure a DockerFile and tell that for example when it is running then wget 'this', apt-get 'that', run 'some shell script', setting environment variables and so on. In micro-services projects and architecture Docker is a very viable asset. You can achieve scalability, resiliency and elasticity with Docker, Docker swarm, Kubernetes and Docker Compose. Another important issue regarding Docker is Docker Hub and its community. For example, I implemented an ecosystem for monitoring kafka using Prometheus, Grafana, Prometheus-JMX-Exporter, and Docker. For doing that, I downloaded configured Docker containers for zookeeper, kafka, Prometheus, Grafana and jmx-collector then mounted my own configuration for some of them using YAML files, or for others, I changed some files and configuration in the Docker container and I build a whole system for monitoring kafka using multi-container Dockers on a single machine with isolation and scalability and resiliency that this architecture can be easily moved into multiple servers. Besides the Docker Hub site there is another site called quay.io that you can use to have your own Docker images dashboard there and pull/push to/from it. You can even import Docker images from Docker Hub to quay then running them from quay on your own machine. Note: Learning Docker in the first place seems complex and hard, but when you get used to it then you can not work without it. I remember the first days of working with Docker when I issued the wrong commands or removing my containers and all of data and configurations mistakenly. This is how Docker introduces itself: Docker is the company driving the container movement and the only container platform provider to address every application across the hybrid cloud. Today’s businesses are under pressure to digitally transform but are constrained by existing applications and infrastructure while rationalizing an increasingly diverse portfolio of clouds, datacenters and application architectures. Docker enables true independence between applications and infrastructure and developers and IT ops to unlock their potential and creates a model for better collaboration and innovation. So Docker is container based, meaning you have images and containers which can be run on your current machine. It's not including the operating system like VMs, but like a pack of different working packs like Java, Tomcat, etc. If you understand containers, you get what Docker is and how it's different from VMs... A container image is a lightweight, stand-alone, executable package of a piece of software that includes everything needed to run it: code, runtime, system tools, system libraries, settings. Available for both Linux and Windows based apps, containerized software will always run the same, regardless of the environment. Containers isolate software from its surroundings, for example differences between development and staging environments and help reduce conflicts between teams running different software on the same infrastructure.  So as you see in the image below, each container has a separate pack and running on a single machine share that machine's operating system... They are secure and easy to ship... There are a lot of nice technical answers here that clearly discuss the differences between VMs and containers as well as the origins of Docker. For me the fundamental difference between VMs and Docker is how you manage the promotion of your application. With VMs you promote your application and its dependencies from one VM to the next DEV to UAT to PRD. With Docker the idea is that you bundle up your application inside its own container along with the libraries it needs and then promote the whole container as a single unit. So at the most fundamental level with VMs you promote the application and its dependencies as discrete components whereas with Docker you promote everything in one hit. And yes there are issues with containers including managing them although tools like Kubernetes or Docker Swarm greatly simplify the task. In my opinion it depends, it can be seen from the needs of your application, why decide to deploy to Docker because Docker breaks the application into small parts according to its function, this becomes effective because when one application / function is an error it has no effect on other applications , in contrast to using full vm, it will be slower and more complex in configuration, but in some ways safer than docker The docker documentation (and self-explanation) makes a distinction between "virtual machines" vs. "containers". They have the tendency to interpret and use things in a little bit uncommon ways. They can do that because it is up to them, what do they write in their documentation, and because the terminology for virtualization is not yet really exact. Fact is what the Docker documentation understands on "containers", is paravirtualization (sometimes "OS-Level virtualization") in the reality, contrarily the hardware virtualization, which is docker not. Docker is a low quality paravirtualisation solution. The container vs. VM distinction is invented by the docker development, to explain the serious disadvantages of their product. The reason, why it became so popular, is that they "gave the fire to the ordinary people", i.e. it made possible the simple usage of typically server ( = Linux) environments / software products on Win10 workstations. This is also a reason for us to tolerate their little "nuance". But it does not mean that we should also believe it. The situation is made yet more cloudy by the fact that docker on Windows hosts used an embedded Linux in HyperV, and its containers have run in that. Thus, docker on Windows uses a combined hardware and paravirtualization solution. In short, Docker containers are low-quality (para)virtual machines with a huge advantage and a lot of disadvantages.
__label__methods __label__oop __label__python __label__python-decorators What is the difference between a function decorated with @staticmethod and one decorated with @classmethod? Maybe a bit of example code will help: Notice the difference in the call signatures of foo, class_foo and static_foo: Below is the usual way an object instance calls a method. The object instance, a, is implicitly passed as the first argument. With classmethods, the class of the object instance is implicitly passed as the first argument instead of self. You can also call class_foo using the class. In fact, if you define something to be a classmethod, it is probably because you intend to call it from the class rather than from a class instance. A.foo(1) would have raised a TypeError, but A.class_foo(1) works just fine: One use people have found for class methods is to create inheritable alternative constructors. With staticmethods, neither self (the object instance) nor  cls (the class) is implicitly passed as the first argument. They behave like plain functions except that you can call them from an instance or the class: Staticmethods are used to group functions which have some logical connection with a class to the class. foo is just a function, but when you call a.foo you don't just get the function, you get a "partially applied" version of the function with the object instance a bound as the first argument to the function. foo expects 2 arguments, while a.foo only expects 1 argument. a is bound to foo. That is what is meant by the term "bound" below: With a.class_foo, a is not bound to class_foo, rather the class A is bound to class_foo. Here, with a staticmethod, even though it is a method, a.static_foo just returns a good 'ole function with no arguments bound. static_foo expects 1 argument, and a.static_foo expects 1 argument too. And of course the same thing happens when you call static_foo with the class A instead. A staticmethod is a method that knows nothing about the class or instance it was called on. It just gets the arguments that were passed, no implicit first argument. It is basically useless in Python -- you can just use a module function instead of a staticmethod. A classmethod, on the other hand, is a method that gets passed the class it was called on, or the class of the instance it was called on, as first argument. This is useful when you want the method to be a factory for the class: since it gets the actual class it was called on as first argument, you can always instantiate the right class, even when subclasses are involved. Observe for instance how dict.fromkeys(), a classmethod, returns an instance of the subclass when called on a subclass: Basically @classmethod makes a method whose first argument is the class it's called from (rather than the class instance), @staticmethod does not have any implicit arguments. Official python docs: @classmethod A class method receives the class as   implicit first argument, just like an   instance method receives the instance.   To declare a class method, use this   idiom: The @classmethod form is a function   decorator – see the description of   function definitions in Function   definitions for details. It can be called either on the class   (such as C.f()) or on an instance   (such as C().f()). The instance is   ignored except for its class. If a   class method is called for a derived   class, the derived class object is   passed as the implied first argument. Class methods are different than C++   or Java static methods. If you want   those, see staticmethod() in this   section. @staticmethod A static method does not receive an   implicit first argument. To declare a   static method, use this idiom: The @staticmethod form is a function   decorator – see the description of   function definitions in Function   definitions for details. It can be called either on the class   (such as C.f()) or on an instance   (such as C().f()). The instance is   ignored except for its class. Static methods in Python are similar   to those found in Java or C++. For a   more advanced concept, see   classmethod() in this section. Here is a short article on this question @staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It’s definition is immutable via inheritance. @classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance. That’s because the first argument for @classmethod function must always be cls (class). To decide whether to use @staticmethod or @classmethod you have to look inside your method. If your method accesses other variables/methods in your class then use @classmethod. On the other hand, if your method does not touches any other parts of the class then use @staticmethod. You may have seen Python code like this pseudocode, which demonstrates the signatures of the various method types and provides a docstring to explain each: First I'll explain a_normal_instance_method. This is precisely called an "instance method". When an instance method is used, it is used as a partial function (as opposed to a total function, defined for all values when viewed in source code) that is, when used, the first of the arguments is predefined as the instance of the object, with all of its given attributes. It has the instance of the object bound to it, and it must be called from an instance of the object. Typically, it will access various attributes of the instance. For example, this is an instance of a string: if we use the instance method, join on this string, to join another iterable, it quite obviously is a function of the instance, in addition to being a function of the iterable list, ['a', 'b', 'c']: Instance methods can be bound via a dotted lookup for use later. For example, this binds the str.join method to the ':' instance: And later we can use this as a function that already has the first argument bound to it. In this way, it works like a partial function on the instance: The static method does not take the instance as an argument. It is very similar to a module level function. However, a module level function must live in the module and be specially imported to other places where it is used. If it is attached to the object, however, it will follow the object conveniently through importing and inheritance as well. An example of a static method is str.maketrans, moved from the string module in Python 3.  It makes a translation table suitable for consumption by str.translate. It does seem rather silly when used from an instance of a string, as demonstrated below, but importing the function from the string module is rather clumsy, and it's nice to be able to call it from the class, as in str.maketrans In python 2, you have to import this function from the increasingly less useful string module: A class method is a similar to an instance method in that it takes an implicit first argument, but instead of taking the instance, it takes the class. Frequently these are used as alternative constructors for better semantic usage and it will support inheritance. The most canonical example of a builtin classmethod is dict.fromkeys. It is used as an alternative constructor of dict, (well suited for when you know what your keys are and want a default value for them.) When we subclass dict, we can use the same constructor, which creates an instance of the subclass. See the pandas source code for other similar examples of alternative constructors, and see also the official Python documentation on classmethod and staticmethod. I started learning programming language with C++ and then Java and then Python and so this question bothered me a lot as well, until I understood the simple usage of each.  Class Method: Python unlike Java and C++ doesn't have constructor overloading.  And so to achieve this you could use classmethod. Following example will explain this  Let's consider we have a Person class which takes two arguments first_name and last_name and creates the instance of Person.  Now, if the requirement comes where you need to create a class using a single name only, just a first_name, you can't do something like this in Python.  This will give you an error when you will try to create an object (instance). However, you could achieve the same thing using @classmethod as mentioned below  Static Method: This is rather simple, it's not bound to instance or class and you can simply call that using class name.  So let's say in above example you need a validation that first_name should not exceed 20 characters, you can simply do this.  and you could simply call using class name I think a better question is "When would you use @classmethod vs @staticmethod?" @classmethod allows you easy access to private members that are associated to the class definition. this is a great way to do singletons, or factory classes that control the number of instances of the created objects exist. @staticmethod provides marginal performance gains, but I have yet to see a productive use of a static method within a class that couldn't be achieved as a standalone function outside the class. Static Methods: Benefits of Static Methods: More convenient to import versus module-level functions since each method does not have to be specially imported Class Methods: These are created with classmethod in-built function. @decorators were added in python 2.4 If you're using python < 2.4 you can use the classmethod() and staticmethod() function. For example, if you want to create a factory method (A function returning an instance of a different implementation of a class depending on what argument it gets) you can do something like: Also observe that this is a good example for using a classmethod and a static method, The static method clearly belongs to the class, since it uses the class Cluster internally. The classmethod only needs information about the class, and no instance of the object. Another benefit of making the _is_cluster_for method a classmethod is so a subclass can decide to change it's implementation, maybe because it is pretty generic and can handle more than one type of cluster, so just checking the name of the class would not be enough. Only the first argument differs: In more detail... When an object's method is called, it is automatically given an extra argument self as its first argument. That is, method must be called with 2 arguments. self is automatically passed, and it is the object itself. When the method is decorated the automatically provided argument is not self, but the class of self. When the method is decorated the method is not given any automatic argument at all. It is only given the parameters that it is called with. Let me tell the similarity between a method decorated with @classmethod vs @staticmethod first. Similarity: Both of them can be called on the Class itself, rather than just the instance of the class. So, both of them in a sense are Class's methods.  Difference: A classmethod will receive the class itself as the first argument, while a staticmethod does not. So a static method is, in a sense, not bound to the Class itself and is just hanging in there just because it may have a related functionality.  @staticmethod just disables the default function as method descriptor.  classmethod wraps your function in a container callable that passes a reference to the owning class as first argument: As a matter of fact, classmethod has a runtime overhead but makes it possible to access the owning class.  Alternatively I recommend using a metaclass and putting the class methods on that metaclass: The definitive guide on how to use static, class or abstract methods in Python is one good link for this topic, and summary it as following. @staticmethod function is nothing more than a function defined inside a class. It is callable without instantiating the class first. It’s definition is immutable via inheritance. @classmethod function also callable without instantiating the class, but its definition follows Sub class, not Parent class, via inheritance, can be overridden by subclass. That’s because the first argument for @classmethod function must always be cls (class). Another consideration with respect to staticmethod vs classmethod comes up with inheritance.  Say you have the following class: And you then want to override bar() in a child class: This works, but note that now the bar() implementation in the child class (Foo2) can no longer take advantage of anything specific to that class.  For example, say Foo2 had a method called magic() that you want to use in the Foo2 implementation of bar(): The workaround here would be to call Foo2.magic() in bar(), but then you're repeating yourself (if the name of Foo2 changes, you'll have to remember to update that bar() method). To me, this is a slight violation of the open/closed principle, since a decision made in Foo is impacting your ability to refactor common code in a derived class (ie it's less open to extension).  If bar() were a classmethod we'd be fine: Gives: In Foo2 MAGIC I will try to explain the basic difference using an example. 1 - we can directly call static and classmethods without initializing 2- Static method cannot call self method but can call other static and classmethod 3- Static method belong to class and will not use object at all. 4- Class method are not bound to an object but to a class. @classmethod : can be used to create a shared global access to all the instances created of that class..... like updating a record by multiple users.... I particulary found it use ful when creating singletons as well..:) @static method:  has nothing to do with the class or instance being associated with ...but for readability can use static method A class method receives the class as implicit first argument, just like an instance method receives the instance. It is a method which is bound to the class and not the object of the class.It has access to the state of the class as it takes a class parameter that points to the class and not the object instance. It can modify a class state that would apply across all the instances of the class. For example it can modify a class variable that will be applicable to all the instances.  On the other hand, a static method does not receive an implicit first argument, compared to class methods or instance methods. And can’t access or modify class state. It only belongs to the class because from design point of view that is the correct way. But in terms of functionality is not bound, at runtime, to the class. as a guideline, use static methods as utilities, use class methods for example as factory . Or maybe to define a singleton. And use instance methods to model the state and behavior of instances. Hope I was clear !  You might want to consider the difference between: and This has changed between python2 and python3: python2: python3: So using  @staticmethod for methods only called directly from the class has become optional in python3. If you want to call them from both class and instance, you still need to use the @staticmethod decorator. The other cases have been well covered by unutbus answer. My contribution demonstrates the difference amongst @classmethod, @staticmethod, and instance methods, including how an instance can indirectly call a @staticmethod. But instead of indirectly calling a @staticmethod from an instance, making it private may be more "pythonic." Getting something from a private method isn't demonstrated here but it's basically the same concept. Analyze @staticmethod literally providing different insights. A normal method of a class is an implicit dynamic method which takes the instance as first argument. In contrast, a staticmethod does not take the instance as first argument, so is called 'static'. A staticmethod is indeed such a normal function the same as those outside a class definition. It is luckily grouped into the class just in order to stand closer where it is applied, or you might scroll around to find it. I think giving a purely Python version of staticmethod and classmethod would help to understand the difference between them at language level. Both of them are non-data descriptors (It would be easier to understand them if you are familiar with descriptors first). Class methods, as the name suggests, are used to make changes to classes and not the objects. To make changes to classes, they will modify the class attributes(not object attributes), since that is how you update classes. This is the reason that class methods take the class(conventionally denoted by 'cls') as the first argument. Static methods on the other hand, are used to perform functionalities that are not bound to the class i.e. they will not read or write class variables. Hence, static methods do not take classes as arguments. They are used so that classes can perform functionalities that are not directly related to the purpose of the class. Instance Method: + Can modify object instance state + Can modify class state Class Method: - Can't modify object instance state + Can modify class state Static Method: - Can't modify object instance state - Can't modify class state output: The instance method we actually had access to the object instance , right so this was an instance off a my class object whereas with the class method we have access to the class itself. But not to any of the objects,  because the class method doesn't really care about an object existing. However you can both call a class method and static method on an object instance. This is going to work it doesn't really make a difference, so again when you call static method here it's going to work and it's going to know which method you want to call. The Static methods are used to do some utility tasks, and class methods are used for factory methods. The factory methods can return class objects for different use cases. And finally, a short example for better understanding: One pretty important practical difference occurs when subclassing. If you don't mind, I'll hijack @unutbu's example: In class_foo, the method knows which class it is called on: In static_foo, there is no way to determine whether it is called on A or B: Note that this doesn't mean you can't use other methods in a staticmethod, you just have to reference the class directly, which means subclasses' staticmethods will still reference the parent class: staticmethod has no access to attibutes of the object, of the class, or of parent classes in the inheritance hierarchy. It can be called at the class directly (without creating an object). classmethod has no access to attributes of the object. It however can access attributes of the class and of parent classes in the inheritance hierarchy. It can be called at the class directly (without creating an object). If called at the object then it is the same as normal method which doesn't access self.<attribute(s)> and accesses self.__class__.<attribute(s)> only. Think we have a class with b=2, we will create an object and re-set this to b=4 in it. Staticmethod cannot access nothing from previous. Classmethod can access .b==2 only, via cls.b. Normal method can access both: .b==4 via self.b and .b==2 via self.__class__.b. We could follow the KISS style (keep it simple, stupid): Don't use staticmethods and classmethods, don't use classes without instantiating them, access only the object's attributes self.attribute(s). There are languages where the OOP is implemented that way and I think it is not bad idea. :) Python comes with several built-in decorators. The big three are: @classmethod decorator can be called with with an instance of a class or directly by the class itself as its first argument. @staticmethod is a way of putting a function into a class (because it logically belongs there), while indicating that it does not require access to the class. Let's consider the following class: Let's see how it works: tldr; A staticmethod is essentially a function bound to a class (and consequently its instances) A classmethod is essentially an inheritable staticmethod. For details, see the excellent answers by others. First let's start with an example code that we'll use to understand both concepts: Class method A class method accepts the class itself as an implicit argument and -optionally- any other arguments specified in the definition. It’s important to understand that a class method, does not have access to object instances (like instance methods do). Therefore, class methods cannot be used to alter the state of an instantiated object but instead, they are capable of changing the class state which is shared amongst all the instances of that class. Class methods are typically useful when we need to access the class itself — for example, when we want to create a factory method, that is a method that creates instances of the class. In other words, class methods can serve as alternative constructors. In our example code, an instance of Employee can be constructed by providing three arguments; first_name , last_name and salary. Now let’s assume that there’s a chance that the name of an Employee can be provided in a single field in which the first and last names are separated by a whitespace. In this case, we could possibly use our class method called employee_from_full_name that accepts three arguments in total. The first one, is the class itself, which is an implicit argument which means that it won’t be provided when calling the method — Python will automatically do this for us: Note that it is also possible to call employee_from_full_name from object instances although in this context it doesn’t make a lot of sense: Another reason why we might want to create a class method, is when we need to change the state of the class. In our example, the class variable NO_OF_EMPLOYEES keeps track of the number of employees currently working for the company. This method is called every time a new instance of Employee is created and it updates the count accordingly: Static methods On the other hand, in static methods neither the instance (i.e. self) nor the class itself (i.e. cls) is passed as an implicit argument. This means that such methods, are not capable of accessing the class itself or its instances. Now one could argue that static methods are not useful in the context of classes as they can also be placed in helper modules instead of adding them as members of the class. In object oriented programming, it is important to structure your classes into logical chunks and thus, static methods are quite useful when we need to add a method under a class simply because it logically belongs to the class. In our example, the static method named get_employee_legal_obligations_txt simply returns a string that contains the legal obligations of every single employee of a company. This function, does not interact with the class itself nor with any instance. It could have been placed into a different helper module however, it is only relevant to this class and therefore we have to place it under the Employee class. A static method can be access directly from the class itself or from an instance of the class: References
__label__npm __label__package.json __label__semantic-versioning __label__node.js After I upgraded to the latest stable node and npm, I tried npm install moment --save. It saves the entry in the package.json with the caret ^ prefix. Previously, it was a tilde ~ prefix. See the NPM docs and semver docs: ~version “Approximately equivalent to version”, will update you to all future patch versions, without incrementing the minor version. ~1.2.3 will use releases from 1.2.3 to <1.3.0. ^version “Compatible with version”, will update you to all future minor/patch versions, without incrementing the major version. ^2.3.4 will use releases from 2.3.4 to <3.0.0. See Comments below for exceptions, in particular for pre-one versions, such as ^0.2.3 I would like to add the official npmjs documentation as well which describes all methods for version specificity including the ones referred to in the question -  https://docs.npmjs.com/files/package.json https://docs.npmjs.com/misc/semver#x-ranges-12x-1x-12- The above list is not exhaustive. Other version specifiers include GitHub urls and GitHub user repo's, local paths and packages with specific npm tags npm allows installing newer version of a package than the one specified. Using tilde (~) gives you bug fix releases and caret (^) gives you backwards-compatible new functionality as well.  The problem is old versions usually don't receive bug fixes that much, so npm uses caret (^) as the default for --save.  According to: "Semver explained - why there's a caret (^) in my package.json?". Note that the rules apply to versions above 1.0.0 and not every project follows semantic versioning. For versions 0.x.x the caret allows only patch updates, i.e., it behaves the same as the tilde. See "Caret Ranges" Here's a visual explanation of the concepts:  Source: "Semantic Versioning Cheatsheet". Set starting major-level and allow updates upward Freeze major-level Freeze minor-level Freeze patch-level Disallow updates Notice: Missing major, minor, patch or specifying beta without number, is the same as any for the missing level. Notice: When you install a package which has 0 as major level, the update will only install new beta/pr level version! That's because npm sets ^ as default in package.json and when installed version is like 0.1.3, it freezes all major/minor/patch levels. ~ fixes major and minor numbers. It is used when you're ready to accept bug-fixes in your dependency, but don't want any potentially incompatible changes. ^ fixes the major number only. It is used when you're closely watching your dependencies and are ready to quickly change your code if minor release will be incompatible. In addition to that, ^ is not supported by old npm versions, and should be used with caution. So, ^ is a good default, but it's not perfect. I suggest to carefully pick and configure the semver operator that is most useful to you. ~ : Reasonably close to ^: Compatible with ^ is 1.[any].[any] (latest minor version) ~ is 1.2.[any] (latest patch) A great read is this blog post on how semver applies to npm and what they're doing to make it match the semver standard http://blog.npmjs.org/post/98131109725/npm-2-0-0 ~ Tilde: ^ Caret: Hat matching may be considered "broken" because it wont update ^0.1.2 to 0.2.0. When the software is emerging use 0.x.y versions and hat matching will only match the last varying digit (y). This is done on purpose. The reason is that while the software is evolving the API changes rapidly: one day you have these methods and the other day you have those methods and the old ones are gone. If you don't want to break the code for people who already are using your library you go and increment the major version: e.g. 1.0.0 -> 2.0.0 -> 3.0.0. So, by the time your software is finally 100% done and full-featured it will be like version 11.0.0 and that doesn't look very meaningful, and actually looks confusing. If you were, on the other hand, using 0.1.x -> 0.2.x -> 0.3.x versions then by the time the software is finally 100% done and full-featured it is released as version 1.0.0 and it means "This release is a long-term service one, you can proceed and use this version of the library in your production code, and the author won't change everything tomorrow, or next month, and he won't abandon the package". The rule is: use 0.x.y versioning when your software hasn't yet matured and release it with incrementing the middle digit when your public API changes (therefore people having ^0.1.0 won't get 0.2.0 update and it won't break their code). Then, when the software matures, release it under 1.0.0 and increment the leftmost digit each time your public API changes (therefore people having ^1.0.0 won't get 2.0.0 update and it won't break their code). Tilde ~ matches minor version, if you have installed a package that has 1.4.2 and after your installation, versions 1.4.3 and 1.4.4 are also available if in your package.json it is used as ~1.4.2 then npm install in your project after upgrade will install 1.4.4 in your project. But there is 1.5.0 available for that package then it will not be installed by ~. It is called minor version. Caret ^ matches major version, if 1.4.2 package is installed in your project and after your installation 1.5.0 is released then ^ will install major version. It will not allow to install 2.1.0 if you have ^1.4.2. Fixed version if you don't want to change version of package on each installation then used fixed version with out any special character e.g "1.4.2" Latest Version * If you want to install latest version then only use * in front of package name. One liner explanation The standard versioning system is major.minor.build (e.g. 2.4.1) npm checks and fixes the version of a particular package based on these characters ~ : major version is fixed, minor version is fixed, matches any build number e.g. : ~2.4.1 means it will check for 2.4.x where x is anything ^ : major version is fixed, matches any minor version, matches any build number e.g. : ^2.4.1 means it will check for 2.x.x where x is anything You probably have seen the tilde (~) and caret (^) in the package.json. What is the difference between them? When you do npm install moment --save, It saves the entry in the package.json with the caret (^) prefix. In the simplest terms, the tilde (~) matches the most recent minor version (the middle number). ~1.2.3 will match all 1.2.x versions but will miss 1.3.0. The caret (^), on the other hand, is more relaxed. It will update you to the most recent major version (the first number). ^1.2.3 will match any 1.x.x release including 1.3.0, but will hold off on 2.0.0. Reference: https://medium.com/@Hardy2151/caret-and-tilde-in-package-json-57f1cbbe347b semver is separate in to 3 major sections which is broken by dots. These different major, minor and patch are using to identify different releases. tide (~) and caret (^) are using to identify which minor and patch version to be used in package versioning.  Tilde (~) major version is fixed, minor version is fixed, matches any build   number ~4.13.3 means it will check for 4.13.x where x is anything and 4.14.0     Caret (^)  major version is fixed, matches any minor version, matches any build   number ^3.0.0 means it will check for 3.x.x where x is anything The version number is in syntax which designates each section with different meaning. syntax is broken into three sections separated by a dot. major.minor.patch 1.0.2 Major, minor and patch represent the different releases of a package. npm uses the tilde (~) and caret (^) to designate which patch and minor versions to use respectively. So if you see ~1.0.2 it means to install version 1.0.2 or the latest patch version such as 1.0.4. If you see ^1.0.2 it means to install version 1.0.2 or the latest minor or patch version such as 1.1.0. carat ^ include everything greater than a particular version in the same major range. tilde ~ include everything greater than a particular version in the same minor range. For example, to specify acceptable version ranges up to 1.0.4, use the following syntax: For more information on semantic versioning syntax, see the npm semver calculator.  More from npm documentation About semantic versioning Not an answer, per se, but an observation that seems to have been overlooked. The description for carat ranges: see: https://github.com/npm/node-semver#caret-ranges-123-025-004 Allows changes that do not modify the left-most non-zero digit in the   [major, minor, patch] tuple. Means that ^10.2.3 matches 10.2.3 <= v < 20.0.0 I don't think that's what they meant. Pulling in versions 11.x.x through 19.x.x will break your code. I think they meant left most non-zero number field.  There is nothing in SemVer that requires number-fields to be single-digit. Related to this question you can review Composer documentation on versions, but here in short: So, with Tilde you will get automatic updates of patches but minor and major versions will not be updated. However, if you use Caret you will get patches and minor versions, but you will not get major (breaking changes) versions. Tilde Version is considered "safer" approach, but if you are using reliable dependencies (well-maintained libraries) you should not have any problems with Caret Version (because minor changes should not be breaking changes. You should probably review this stackoverflow post about differences between composer install and composer update. ~ specfices to minor version releases ^ specifies to major version releases For example if package version is 4.5.2 ,on Update ~4.5.2 will install latest 4.5.x version (MINOR VERSION) ^4.5.2 will install latest 4.x.x version (MAJOR VERSION)
__label__git __label__git-tag How do you delete a Git tag that has already been pushed? You can push an 'empty' reference to the remote tag name: Or, more expressively, use the --delete option (or -d if your git version is older than 1.8.0): Note that git has tag namespace and branch namespace so you may use the same name for a branch and for a tag. If you want to make sure that you cannot accidentally remove the branch instead of the tag, you can specify full ref which will never delete a branch: If you also need to delete the local tag, use: Pushing a branch, tag, or other ref to a remote repository involves specifying "which repo, what source, what destination?" A real world example where you push your master branch to the origin's master branch is: Which because of default paths, can be shortened to: Tags work the same way: Which can also be shortened to: By omitting the source ref (the part before the colon), you push 'nothing' to the destination, deleting the ref on the remote end. A more straightforward way is  IMO prefixing colon syntax is a little bit odd in this situation If you have a remote tag v0.1.0 to delete, and your remote is origin, then simply: If you also need to delete the tag locally: See Adam Franco's answer for an explanation of Git's unusual : syntax for deletion. Delete all local tags and get the list of remote tags: Remove all remote tags Clean up local tags To remove the tag from the remote repository: You may also want to delete the tag locally: The first line deletes your_tag_name from local repo and second line deletes your_tag_name from remote repo. For those who use GitHub, one more step is needed: discarding draft.  From your terminal, do this: Now go to Github.com and refresh, they disappear.  Just notice that, if you have a remote branch named as a remote tag, these commands are ambiguous: So you must use this command to delete the tag: and this one to delete the branch: If not, you would get an error like this: After reading through these answers while needing to delete over 11,000 tags, I learned these methods relying or xargs take far too long, unless you have hours to burn. Struggling, I found two much faster ways. For both, start with git tag or git ls-remote --tags to make a list of tags you want to delete on the remote. In the examples below you can omit or replace sorting_proccessing_etc with any greping, sorting, tailing or heading you want (e.g. grep -P "my_regex" | sort | head -n -200 etc) : How does this work? The normal, line-separated list of tags is converted to a single line of space-separated tags, each prepended with : so . . . Using git push with this format tag pushes nothing into each remote ref, erasing it (the normal format for pushing this way is local_ref_path:remote_ref_path). After both of these methods, you'll probably want to delete your local tags too. This is much faster so we can go back to using xargs and git tag -d, which is sufficient. OR similar to the remote delete:  If you use SourceTree - a great Git GUI - then you can easily do this without the command line by doing the following: YOUR_TAG_NAME will now be removed from your local repository and all remotes - be it GitHub, BitBucket, or wherever else you listed as a remote for that repository. Also, if you deleted a tag locally but not on the remote origins, and you want to delete it everywhere, then just create a new tag that has the same name and is attached at the same commit as the origins. Then, repeat the steps above to delete everywhere. If you have created a tag called release01 in a Git repository you would remove it from your repository by doing the following: To remove one from a Mercurial repository: Please reference https://confluence.atlassian.com/pages/viewpage.action?pageId=282175551 I wanted to remove all tags except for those that match a pattern so that I could delete all but the last couple of months of production tags, here's what I used to great success: Delete All Remote Tags & Exclude Expression From Delete Delete All Local Tags & Exclude Expression From Delete If you're using PowerShell, and you want to delete a bunch of them: Of course, you can also filter them before deleting: As @CubanX suggested, I've split this answer from my original: parallel has many operating modes, but generally parallelizes any command you give it while allowing you to set limits on the number of processes. You can alter the --jobs 2 parameter to allow faster operation, but I had problems with Github's rate limits, which are currently 5000/hr, but also seems to have an undocumented short-term limit as well. After this, you'll probably want to delete your local tags too. This is much faster so we can go back to using xargs and git tag -d, which is sufficient. The other answers point out how to accomplish this, but you should keep in mind the consequences since this is a remote repository. The git tag man page, in the On Retagging section, has a good explanation of how to courteously inform the remote repo's other users of the change. They even give a handy announcement template for communicating how others should get your changes. Simple script to remove given tag from both local and origin locations. With a check if tag really exists. How to use: Seems like a lot of work for something xargs already does. Looking back through this thread, I'm guessing the slowness with xargs that you experienced is because the original answer used xargs -n 1 when it didn't really need to. This is equivalent to your method one except that xargs automatically deals with the maximum command line length: xargs can run processes in parallel too. Method 2 with xargs: The above uses a maximum of 5 processes to handle a maximum of 100 arguments in each process. You can experiment with the arguments to find what works best for your needs. If you have a tag created starting with the # character, e.g. #ST002, you might find that u are unable to delete using normal patterns. i.e. Will not delete the tag, but wrapping it in a String Literal like so That will get it deleted. Hoping it will help someone who made the mistake of using # to write tag names. Here is a local testcase to test it locally without messing with a remote: Just wanted to share an alias I created which does the same thing: Add the following to your ~/.gitconfig The usage looks like: To delete a tag on your remote repository, you can use The way to interpret the above is to read it as the null value, the value before the colon is being pushed to remote tag name. For tortoise git users, at a scale of hundreds tags, you can delete multiple tags at once using UI, but the UI is well hidden under context menu. From explorer windows right click -> Browse references -> Right click on ref/refmotes/name -> choose 'Delete remote tags'  See https://tortoisegit.org/docs/tortoisegit/tgit-dug-browse-ref.html
__label__type-conversion __label__arrays __label__java __label__arraylist I have an array that is initialized like: I would like to convert this array into an object of the ArrayList class.  Given: The simplest answer is to do: This will work fine.  But some caveats: (old thread, but just 2 cents as none mention Guava or other libs and some other details) It's worth pointing out the Guava way, which greatly simplifies these shenanigans: Use the ImmutableList class and its of() and copyOf() factory methods (elements can't be null): Use the Lists class and its newArrayList() factory methods: Please also note the similar methods for other data structures in other classes, for instance in Sets. The main attraction could be to reduce the clutter due to generics for type-safety, as the use of the Guava factory methods allow the types to be inferred most of the time. However, this argument holds less water since Java 7 arrived with the new diamond operator. But it's not the only reason (and Java 7 isn't everywhere yet): the shorthand syntax is also very handy, and the methods initializers, as seen above, allow to write more expressive code. You do in one Guava call what takes 2 with the current Java Collections. Use the JDK's Arrays class and its asList() factory method, wrapped with a Collections.unmodifiableList(): Note that the returned type for asList() is a List using a concrete ArrayList implementation, but it is NOT java.util.ArrayList. It's an inner type, which emulates an ArrayList but actually directly references the passed array and makes it "write through" (modifications are reflected in the array). It forbids modifications through some of the List API's methods by way of simply extending an AbstractList (so, adding or removing elements is unsupported), however it allows calls to set() to override elements. Thus this list isn't truly immutable and a call to asList() should be wrapped with Collections.unmodifiableList(). See the next step if you need a mutable list. Same as above, but wrapped with an actual java.util.ArrayList: Since this question is pretty old, it surprises me that nobody suggested the simplest form yet: As of Java 5, Arrays.asList() takes a varargs parameter and you don't have to construct the array explicitly. Make sure that myArray is the same type as T. You'll get a compiler error if you try to create a List<Integer> from an array of int, for example. Another way (although essentially equivalent to the new ArrayList(Arrays.asList(array)) solution performance-wise: In Java 9, you can use List.of static factory method in order to create a List literal. Something like the following: This would return an immutable list containing three elements. If you want a mutable list, pass that list to the ArrayList constructor: JEP 269 provides some convenience factory methods for Java Collections API. These immutable static factory methods are built into the List, Set, and Map interfaces in Java 9 and later. You probably just need a List, not an ArrayList.  In that case you can just do: Another update, almost ending year 2014, you can do it with Java 8 too: A few characters would be saved, if this could be just a List  If you use : you may create and fill two lists ! Filling twice a big list is exactly what you don't want to do because it will create another Object[] array each time the capacity needs to be extended. Fortunately the JDK implementation is fast and Arrays.asList(a[]) is very well done. It create a kind of ArrayList named Arrays.ArrayList where the Object[] data points directly to the array. The dangerous side is that if you change the initial array, you change the List ! Are you sure you want that ? Maybe yes, maybe not. If not, the most understandable way is to do this : Or as said @glglgl, you can create another independant ArrayList with : I love to use Collections, Arrays, or Guava. But if it don't fit, or you don't feel it, just write another inelegant line instead. In Java 9 you can use: According with the question the answer using java 1.7 is: However it's better always use the interface:  You can convert using different methods List<Element> list = Arrays.asList(array); List<Element> list = new ArrayList(); Collections.addAll(list, array); Arraylist list = new Arraylist(); list.addAll(Arrays.asList(array)); For more detail you can refer to http://javarevisited.blogspot.in/2011/06/converting-array-to-arraylist-in-java.html Since Java 8 there is an easier way to transform: as all said this will do so  and the common newest way to create array is observableArrays ObservableList: A list that allows listeners to track changes when they occur. for Java SE you can try  that is according to Oracle Docs observableArrayList()   Creates a new empty observable list that is backed by an arraylist.   observableArrayList(E... items)   Creates a new observable array list with items added to it. also in Java 9 it's a little bit easy: You also can do it with stream in Java 8. If we see the definition of Arrays.asList() method you will get something like this:  So, you might initialize arraylist like this:  Note : each new Element(int args) will be treated as Individual Object and can be passed as a var-args. There might be another answer for this question too. If you see declaration for java.util.Collections.addAll() method you will get something like this: So, this code is also useful to do so Another simple way is to add all elements from the array to a new ArrayList using a for-each loop. If the array is of a primitive type, the given answers won't work. But since Java 8 you can use: You can do it in java 8 as follows Even though there are many perfectly written answers to this question, I will add my inputs.  Say you have Element[] array = { new Element(1), new Element(2), new Element(3) }; New ArrayList can be created in the following ways And they very well support all operations of ArrayList But the following operations returns just a List view of an ArrayList and not actual ArrayList. Therefore, they will give error when trying to make some ArrayList operations More on List representation of array link. Simplest way to do so is by adding following code. Tried and Tested. Another Java8 solution (I may have missed the answer among the large set. If so, my apologies). This creates an ArrayList (as opposed to a List) i.e. one can delete elements Output is...  Hello  world We can easily convert an array to ArrayList. We use Collection interface's addAll() method for the purpose of copying content from one list to another.  Use the following code to convert an element array into an ArrayList.  Given Object Array: Convert Array to List: Convert Array to ArrayList Convert Array to LinkedList Print List: OUTPUT 1 2 3 Already everyone has provided enough good answer for your problem.  Now from the all suggestions, you need to decided which will fit your requirement. There are two types of collection which you need to know. One is unmodified collection and other one collection which will allow you to modify the object later. So, Here I will give short example for two use cases. Immutable collection creation :: When you don't want to modify the collection object after creation List<Element> elementList = Arrays.asList(array) Mutable collection creation :: When you may want to modify the created collection object after creation. List<Element> elementList = new ArrayList<Element>(Arrays.asList(array)); Java 8’s Arrays class provides a stream() method which has overloaded versions accepting both primitive arrays and Object arrays. Below code seems nice way of doing this.
__label__android __label__java __label__usermanager I was looking at the new APIs introduced in Android 4.2. While looking at the UserManager class I came across the following method: Used to determine whether the user making this call is subject to teleportations. Returns whether the user making this call is a goat. How and when should this be used? From Android R, this method always returns false. Google says that this is done "to protect goat privacy": From their source, the method used to return false until it was changed in API 21. It looks like the method has no real use for us as developers. Someone has previously stated that it might be an Easter egg. In API 21 the implementation was changed to check if there is an installed app with the package com.coffeestainstudios.goatsimulator Here is the source and the change. I don't know if this was "the" official use case, but the following produces a warning in Java (that can further produce compile errors if mixed with return statements, leading to unreachable code): However this is legal: So I often find myself writing a silly utility method for the quickest way to dummy out a code block, then in completing debugging find all calls to it, so provided the implementation doesn't change this can be used for that. JLS points out if (false) does not trigger "unreachable code" for the specific reason that this would break support for debug flags, i.e., basically this use case (h/t @auselen). (static final boolean DEBUG = false; for instance). I replaced while for if, producing a more obscure use case. I believe you can trip up your IDE, like Eclipse, with this behavior, but this edit is 4 years into the future, and I don't have an Eclipse environment to play with. This appears to be an inside joke at Google. It's also featured in the Google Chrome task manager. It has no purpose, other than some engineers finding it amusing. Which is a purpose by itself, if you will. There is even a huge Chromium bug report about too many teleported goats.   The following Chromium source code snippet is stolen from the HN comments. Complementing the @djechlin answer (good answer by the way!), this function call could be also used as dummy code to hold a breakpoint in an IDE when you want to stop in some specific iteration or a particular recursive call, for example:  isUserAGoat() could be used instead of a dummy variable declaration that will be shown in the IDE as a warning and, in Eclipse particular case, will clog the breakpoint mark, making it difficult to enable/disable it. If the method is used as a convention, all the invocations could be later filtered by some script (during commit phase maybe?).  Google guys are heavy Eclipse users (they provide several of their projects as Eclipse plugins: Android SDK, GAE, etc), so the @djechlin answer and this complementary answer make a lot of sense (at least for me). There's a funny named method/constant/whatever in each version of Android.  The only practical use I ever saw was in the Last Call for Google I/O Contest where they asked what it was for a particular version, to see if contestants read the API diff report for each release. The contest had programming problems too, but generally some trivia that could be graded automatically first to get the number of submissions down to reasonable amounts that would be easier to check. In the discipline of speech recognition, users are divided into goats and sheeps.  For instance, here on page 89: Sheeps are people for whom speech recognition works exceptionally well, and goats are people for whom it works exceptionally poorly. Only the voice recognizer knows what separates them. People can't predict whose voice will be recognized easily and whose won't. The best policy is to design the interface so it can handle all kinds of voices in all kinds of environments Maybe, it is planned to mark Android users as goats in the future to be able to configure the speech recognition engine for goats' needs. ;-) As of API 21 (the first Android 5.0/Lollipop SDK), this detects whether the Goat Simulator app is installed: This should make it clear that djechlin's suggestion of using it as a warning-free if (false) is a potentially disastrous strategy. What previously returned false for every device now returns a seemingly random value: if this was buried deep enough in your code it could take a long time to figure out where your new bugs are coming from. Bottom line: if you don't control the implementation of a method and decide to use it for purposes other than stated in the API documentation, you're heading for trouble. Google has a serious liking for goats and goat based Easter eggs. There has even been previous Stack Overflow posts about it.  As has been mentioned in previous posts, it also exists within the Chrome task manager (it first appeared in the wild in 2009): And then in Windows, Linux and Mac versions of Chrome early 2010). The number of "Goats Teleported" is in fact random: Other Google references to goats include: The earliest correlation of goats and Google belongs in the original "Mowing with goats" blog post, as far as I can tell. We can safely assume that it's merely an Easter egg and has no real-world use, except for returning false. There is a similar call, isUserAMonkey(), that returns true if the MonkeyRunner tool is being used. The SDK explanation is just as curious as this one.  Returns true if the user interface is currently being messed with by a monkey. Here is the source.  I expect that this was added in anticipation of a new SDK tool named something with a goat and will actually be functional to test for the presence of that tool.   Also see a similar question, Strange function in ActivityManager: isUserAMonkey. What does this mean, what is its use?. Funny Easter Egg.  In Ubuntu version of Chrome, in Task Manager (shift+esc), with right-click you can add a sci-fi column that in italian version is "Capre Teletrasportate" (Teleported Goats). A funny theory about it is here. It's not an inside joke Apparently it's just an application checker for Goat Simulator - by Coffee Stain Studios If you have Goat Simulator installed, you're a goat. If you don't have it installed, you're not a goat. I imagine it was more of a personal experiment by one of the developers, most likely to find people with a common interest.
